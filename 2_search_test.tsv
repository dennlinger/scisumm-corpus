Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.	0	23
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	This approach is compared to another reordering scheme presented in (Berger et al., 1996).	0	15
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	However there is no global pruning.	0	166
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed.	0	49
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).	0	122
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	A modified language model probability pÃ(eje0; e00) is defined as follows: pÃ(eje0; e00) =  1:0 if Ã = 0 p(eje0; e00) if Ã = 1 : We associate a distribution p(Ã) with the two cases Ã = 0 and Ã = 1 and set p(Ã = 1) = 0:7.	0	61
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	The resulting algorithm has a complexity of O(n!).	0	42
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	This algorithm can be applied to statistical machine translation.	0	48
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	Here, the pruning threshold t0 = 10:0 is used.	0	173
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.	0	46
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.	0	20
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0	43
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment.	0	32
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0	2
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.	0	51
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	This algorithm can be applied to statistical machine translation.	0	48
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	The goal of machine translation is the translation of a text given in some source language into a target language.	0	5
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.	0	184
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.	0	46
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.	0	11
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	In this paper, we have presented a new, eÃcient DP-based search procedure for statistical machine translation.	0	192
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	A search restriction especially useful for the translation direction from German to English is presented.	0	3
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0	170
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	This approach is compared to another reordering scheme presented in (Berger et al., 1996).	0	15
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	A position is presented by the word at that position.	0	105
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed.	0	53
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).	0	54
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.	0	33
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	0	1
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	In Section 4, we present the performance measures used and give translation results on the Verbmobil task.	0	16
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.	1	94
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	This approach leads to a search procedure with complexity O(E3 J4).	1	139
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.	0	82
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	In German, the verbgroup usually consists of a left and a right verbal brace, whereas in English the words of the verbgroup usually form a sequence of consecutive words.	0	91
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The resulting algorithm has a complexity of O(n!).	0	42
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The complexity of the quasimonotone search is O(E3 J (R2+LR)).	0	119
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	While processing the source sentence monotonically, the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position.	0	100
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	Our approach uses word-to-word dependencies between source and target words.	0	8
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.	0	130
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment.	0	32
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	1	39
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0	43
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	The resulting algorithm has a complexity of O(n!).	0	42
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.	0	82
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	The complexity of the quasimonotone search is O(E3 J (R2+LR)).	0	119
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	This approach leads to a search procedure with complexity O(E3 J4).	0	139
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.	0	22
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).	0	9
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.	0	14
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0	2
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.	1	6
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	The alignment model uses two kinds of parameters: alignment probabilities p(aj jajô1; I; J), where the probability of alignment aj for position j depends on the previous alignment position ajô1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).	0	21
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	The final score is obtained from: max e;e0 j2fJôL;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.	0	118
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÃcient to consider only the best 50 words.	0	169
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).	0	112
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Ã;e00 j02Cnfjg fp(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once.	0	52
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	A modified language model probability pÃ(eje0; e00) is defined as follows: pÃ(eje0; e00) =  1:0 if Ã = 0 p(eje0; e00) if Ã = 1 : We associate a distribution p(Ã) with the two cases Ã = 0 and Ã = 1 and set p(Ã = 1) = 0:7.	0	61
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model.	0	7
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	The sentence length probability p(JjI) is omitted without any loss in performance.	0	30
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	(1), Pr(eI 1) is the language model, which is a trigram language model in this case.	0	19
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.	0	4
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	Table 4 shows translation results for the three approaches.	0	171
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0	170
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0	39
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).	0	122
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In this section, we brie y review our translation approach.	0	17
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.	0	14
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	The approach has been successfully tested on the 8 000-word Verbmobil task.	0	194
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In Section 2, we brie y review our approach to statistical machine translation.	0	13
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	Our approach uses word-to-word dependencies between source and target words.	0	8
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	In this paper, we have presented a new, eÃcient DP-based search procedure for statistical machine translation.	0	192
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0	2
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	Word Re-ordering and DP-based Search in Statistical Machine Translation	0	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	0	1
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.	0	14
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	A search restriction especially useful for the translation direction from German to English is presented.	0	3
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	In Section 4, we present the performance measures used and give translation results on the Verbmobil task.	0	16
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0	170
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	Additionally, it works about 3 times as fast as the IBM style search.	0	178
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	This work has been supported as part of the Verbmobil project (contract number 01 IV 601 A) by the German Federal Ministry of Education, Science, Research and Technology and as part of the Eutrans project (ESPRIT project number 30268) by the European Community.	0	198
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.	1	20
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).	0	9
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.	0	11
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	Our approach uses word-to-word dependencies between source and target words.	0	8
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.	0	51
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Ã;e00 j02Cnfjg fp(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once.	0	52
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).	0	25
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.	0	124
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment.	0	32
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.	0	195
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.	1	179
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.	0	22
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).	0	25
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	For the inverted alignment probability p(bijbiô1; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.	0	31
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.	0	20
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijeiô1 iô2) max bI 1 I Yi=1 [p(bijbiô1; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijeiô1 iô2) p(bijbiô1; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijeiô1 iô2) is the trigram language model probability.	0	27
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	A modified language model probability pÃ(eje0; e00) is defined as follows: pÃ(eje0; e00) =  1:0 if Ã = 0 p(eje0; e00) if Ã = 1 : We associate a distribution p(Ã) with the two cases Ã = 0 and Ã = 1 and set p(Ã = 1) = 0:7.	0	61
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	For Ã = 1, a new target language word is generated using the trigram language model p(eje0; e00).	0	59
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).	0	112
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	However there is no global pruning.	0	166
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	We apply a beam search concept as in speech recognition.	1	165
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0	39
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	0	1
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	The advantage is that we can recombine search hypotheses by dynamic programming.	0	50
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.	0	184
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0	2
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0	43
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	Translation errors are reported in terms of multireference word error rate (mWER) and subjective sentence error rate (SSER).	0	174
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	Table 5: Effect of the beam threshold on the number of search errors (147 sentences).	0	186
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited.	0	168
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÃcient to consider only the best 50 words.	1	169
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.	0	195
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) !	0	106
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series, we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0, and this number is reported as the number of search errors.	0	183
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	For Ã = 1, a new target language word is generated using the trigram language model p(eje0; e00).	0	59
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.	0	179
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.	0	167
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.	0	47
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).	0	122
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).	0	112
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Ã;e00 j02Cnfjg fp(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once.	0	52
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The final score is obtained from: max e;e0 j2fJôL;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.	0	118
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.	0	47
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Ã;e00 np(jjj0; J) p(Ã) pÃ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).	0	116
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	= p(fj je) max Ã;e00 j02Cnfjg np(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j).	0	80
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).	0	54
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.	0	11
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.	0	138
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.	0	6
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	f1; ; Jg denotes a coverage set including all positions from the starting position 1 to position J and j 2 fJ ôL; ; Jg.	0	117
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Ã = 0 or Ã = 1 new target words.	1	58
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.	0	6
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	The inverted alignment probability p(bijbiô1; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration.	0	28
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.	0	35
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment.	0	36
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	The alignment model uses two kinds of parameters: alignment probabilities p(aj jajô1; I; J), where the probability of alignment aj for position j depends on the previous alignment position ajô1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).	0	21
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	3) A tight coupling with the speech recognizer output.	0	197
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	For Ã = 1, a new target language word is generated using the trigram language model p(eje0; e00).	0	59
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	2) An improved language model, which takes into account syntactic structure, e.g. to ensure that a proper English verbgroup is generated.	0	196
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.	0	23
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	In this case, we have no finite-state restrictions for the search space.	0	137
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.	0	195
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	Otherwise for the predecessor search hypothesis, we would have chosen a position that would not have been among the first n uncovered positions.	0	132
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Ã = 0 or Ã = 1 new target words.	0	58
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet.	0	128
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.	0	46
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	The German finite verbs 'bin' (second example) and 'konnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions).	0	190
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.	0	85
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	While processing the source sentence monotonically, the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position.	0	100
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	The sequence of states needed to carry out the word reordering example in Fig.	0	101
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).	0	122
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0	39
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0	170
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.	0	35
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.	0	125
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.	0	195
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	This approach is compared to another reordering scheme presented in (Berger et al., 1996).	0	15
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	0	1
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	Our approach uses word-to-word dependencies between source and target words.	0	8
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	In this section, we brie y review our translation approach.	0	17
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	We apply a beam search concept as in speech recognition.	1	165
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0	2
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.	0	184
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	In this case, we have no finite-state restrictions for the search space.	0	137
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	The resulting algorithm has a complexity of O(n!).	0	42
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	This algorithm can be applied to statistical machine translation.	0	48
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	The resulting algorithm is depicted in Table 1.	0	81
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.	0	46
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.	0	51
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.	0	82
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	1	1
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0	43
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Ã;e00 np(jjj0; J) p(Ã) pÃ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).	0	116
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0	39
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	The advantage is that we can recombine search hypotheses by dynamic programming.	0	50
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup.	0	38
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.	0	51
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijeiô1 iô2) max bI 1 I Yi=1 [p(bijbiô1; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijeiô1 iô2) p(bijbiô1; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijeiô1 iô2) is the trigram language model probability.	0	27
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.	0	138
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	A modified language model probability pÃ(eje0; e00) is defined as follows: pÃ(eje0; e00) =  1:0 if Ã = 0 p(eje0; e00) if Ã = 1 : We associate a distribution p(Ã) with the two cases Ã = 0 and Ã = 1 and set p(Ã = 1) = 0:7.	0	61
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Our method is not able to find 43 (329 + 205) Ã 4499 = 362words in all 12 pe these translations.	0	143
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0	8
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	So we estimate that recall (for M = 10) is approximately 115 / 362 = 31.8% . pruning.	0	146
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Accuracy of our system in each period (M = 10) In Table 1, period 1 is Jul 01 â Jul 15, period 2 is Jul 16 â Jul 31, â¦, period 12 is Dec 16 â Dec 31.	0	121
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.	0	42
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0	12
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.	0	2
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.	0	55
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.	0	170
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.	0	16
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.	1	135
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	It is phonetic-based.	0	77
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	We call Section 4), which was used to rank the English candidate words based on transliteration.	0	106
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	0	17
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	Mining New Word Translations from Comparable Corpora	0	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0	173
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004).	0	95
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0	49
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	So we estimate that English translations are present in the English part of the corpus for Table 2.	0	132
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	1	11
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Comparable corpora refer to texts that are not direct translation but are about the same topic.	1	10
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0	12
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0	3
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0	8
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	But parallel corpora are scarce resources, especially for uncommon lan guage pairs.	0	9
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency them English translation candidate words.	0	84
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Mining New Word Translations from Comparable Corpora	0	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0	173
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.	0	157
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.	0	135
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.	0	18
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.	0	16
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.	0	170
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	We use a variant of Within IR, there is a new approach to document retrieval called the language modeling approach (Ponte & Croft, 98).	0	35
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.	0	30
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.	0	42
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.	0	14
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1	17
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0	173
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.	0	55
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.	0	14
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.	0	158
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P(Q | D) , (Knight and Graehl, 1998).	0	36
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.	0	170
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information.	0	162
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	Since the context of the correct translation e is similar to e , is considered as a document in IR.	0	28
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1	17
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0	12
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	0	11
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.	0	23
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	But parallel corpora are scarce resources, especially for uncommon lan guage pairs.	0	9
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0	8
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	Mining New Word Translations from Comparable Corpora	0	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.	0	66
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0	173
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1	17
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0	3
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0	173
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	Chinese Giga- word corpus consists of news from two agencies: = ââ P(l a a i | pi ) Xinhua News Agency and Central News Agency.	0	81
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	0	11
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates.	0	85
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service.	0	83
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	Mining New Word Translations from Comparable Corpora	0	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	Then we determined the new Chinese words in each half-month period p. By new Chinese words, we refer to those words that appeared in this period p but not from Jan to Jun 1995 or any other periods that preceded p. Among all these new words, we selected those occurring at least 5 times.	0	98
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1	17
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0	8
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0	12
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Mining New Word Translations from Comparable Corpora	0	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.	0	2
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0	49
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).	0	13
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.	0	42
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Comparable corpora refer to texts that are not direct translation but are about the same topic.	0	10
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm.	1	90
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0	12
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Mining New Word Translations from Comparable Corpora	0	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).	0	13
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.	0	23
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	The work that is most similar to ours is the recent research of (Huang et al., 2004).	0	168
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0	173
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0	49
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	0	17
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.	1	30
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	Chinese Giga- word corpus consists of news from two agencies: = ââ P(l a a i | pi ) Xinhua News Agency and Central News Agency.	0	81
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency them English translation candidate words.	0	84
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service.	0	83
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general.	0	41
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected.	0	92
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new, i.e., not occurring from Jan to Jun 1995.	0	80
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	The corpus of the period Jul to Dec 1995 was used to come up with new Chinese words c for translation into English.	0	79
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0	3
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	#c is the total number of new Chinese source words in the period.	0	122
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1	17
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.	0	42
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	If an English word e is the translation of a Chinese word c , they will have similar contexts.	0	29
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0	8
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	On the other hand, using our method of combining both sources of information and setting M = â, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except å·´ä½äº,å©å,æ®å©æ³) have their correct English translations at rank one position.	0	159
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	The work that is most similar to ours is the recent research of (Huang et al., 2004).	0	168
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.	0	170
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling.	0	166
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.	0	18
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	We use a variant of Within IR, there is a new approach to document retrieval called the language modeling approach (Ponte & Croft, 98).	0	35
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).	1	107
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	â t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.	0	47
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	rank these words e1 , e2 ,..., ek according to the P (Q | D ) = â P (t | D ) c t average of their rank positions in the two lists.	0	46
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination.	0	172
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.	0	158
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	Rankâ is the transliteration rank.	0	152
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	rankâ is the context rank, âTrans.	0	151
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.	0	55
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	We call Section 4), which was used to rank the English candidate words based on transliteration.	0	106
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	ranked list of candidate words, associating with each candidate a score estimated by the particular method.	0	40
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1	17
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Mining New Word Translations from Comparable Corpora	0	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0	4
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0	173
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0	12
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0	49
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).	0	13
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	The work that is most similar to ours is the recent research of (Huang et al., 2004).	0	168
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0	3
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	New words such as person names, organization names, technical terms, etc. appear frequently.	0	6
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.	1	7
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.	0	2
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	That is, Chinese is the source language and English is the target language.	0	21
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0	49
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0	3
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Mining New Word Translations from Comparable Corpora	0	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	0	11
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.	0	42
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0	12
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0	8
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases.	0	141
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	They attempted to improve named entity translation by combining phonetic and semantic information.	0	169
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	0	11
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	New words such as person names, organization names, technical terms, etc. appear frequently.	0	6
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	New words such as names, technical terms, etc appear frequently.	0	1
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	AlOnaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters.	0	75
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.	0	23
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P(Q | D) , (Knight and Graehl, 1998).	0	36
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	Each of the two individual methods provides a P(Q | D) is the one that best matches the query.	0	39
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	This pronunciation is converted to source language pronunciation and then to source language word that our method estimates stead of P(c | e) and P(e) .	0	72
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	1	204
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	In IE, creating the patterns which express the requested scenario, e.g. âmanagement successionâ or âcorporate merger and acquisitionâ is regarded as the hardest task.	0	200
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	0	22
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.	0	2
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Hereafter, each pair of NE categories will be called a domain; e.g. the âCompany â Companyâ domain, which we will call CC- domain (Step 2).	0	32
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet.	0	10
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0	18
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	D o m ai n Li n k ac cu ra cy W N c o v e r a g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0	152
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	If the expression is longer or complicated (like âA buys Bâ and âAâs purchase of Bâ), it is called âparaphraseâ, i.e. a set of phrases which express the same thing or event.	0	11
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	0	40
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	For example, in Information Retrieval (IR), we have to match a userâs query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâs question even if the formulation of the answer in the document is different from the question.	1	13
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	If two phrases can be used to express the same relationship within an information extraction application (âscenarioâ), these two phrases are paraphrases.	0	119
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0	18
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set.	0	201
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.	0	12
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	If the expression is longer or complicated (like âA buys Bâ and âAâs purchase of Bâ), it is called âparaphraseâ, i.e. a set of phrases which express the same thing or event.	0	11
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.	0	202
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event.	0	8
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.	0	203
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.	0	67
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	1	22
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.	0	203
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Automatic paraphrase discovery is an important but challenging task.	0	1
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	0	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.	0	12
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link âIBMâ and âLotusâ) (Step 4).	0	34
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].	0	167
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	This result suggests the benefit of using the automatic discovery method.	0	151
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.	0	2
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.	0	24
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	1	40
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	There have been other kinds of efforts to discover paraphrase automatically from corpora.	0	164
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Automatic paraphrase discovery is an important but challenging task.	0	1
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.	0	24
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	0	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set.	0	128
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	For example, we can easily imagine that the number of paraphrases for âA buys Bâ is enormous and it is not possible to create a comprehensive inventory by hand.	0	16
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.	0	203
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	The availability of comparable corpora is limited, which is a significant limitation on the approach.	0	166
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.	0	206
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	All the sentences have been analyzed by our chunker and NE tag- ger.	0	83
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	0	23
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	0	205
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.	0	3
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently.	0	14
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0	18
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	There have been other kinds of efforts to discover paraphrase automatically from corpora.	0	164
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.	0	197
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs.	0	170
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Our clue is the NE instance pairs.	0	66
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].	1	167
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.	1	110
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30.	0	159
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.	0	202
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs.	0	170
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	The similar explanation applies to the link to the âstakeâ set.	0	148
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	As can be seen in the example, the first two phrases have a different order of NE names from the last two, so we can determine that the last two phrases represent a reversed relation.	0	77
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	We have checked if there are similar verbs in other major domains, but this was the only one.	0	187
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.	0	3
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	0	23
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	For each pair we also record the context, i.e. the phrase between the two NEs (Step1).	1	29
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	0	204
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	0	22
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.	0	2
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.	0	24
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.	0	206
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.	0	38
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.	0	3
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	First, from a large corpus, we extract all the NE instance pairs.	0	27
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	0	23
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0	195
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, we donât know how many such paraphrase sets are necessary to cover even some everyday things or events.	0	17
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	As we shall see, most of the linked sets are paraphrases.	0	35
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	shows some keywords with their scores.	0	55
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	As was explained in the results section, âstrengthâ or âaddâ are not desirable keywords in the CC-domain.	0	181
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.	0	111
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0	18
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Overview of the method 2.2 Step by Step Algorithm.	0	52
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	In this section, we will explain the algorithm step by step with examples.	0	53
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	0	40
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	There have been other kinds of efforts to discover paraphrase automatically from corpora.	0	164
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Extract NE pair instances with contexts From the four years of newspaper corpus, we extracted 1.9 million pairs of NE instances.	0	89
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.	0	12
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	This result suggests the benefit of using the automatic discovery method.	0	151
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].	0	167
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.	0	24
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.	0	206
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.	0	202
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	For example, in Information Retrieval (IR), we have to match a userâs query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâs question even if the formulation of the answer in the document is different from the question.	0	13
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	0	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	0	40
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0	195
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set.	0	201
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.	0	111
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	The availability of comparable corpora is limited, which is a significant limitation on the approach.	0	166
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Out of those 15 links, 4 are errors, namely âbuy - payâ, âacquire - payâ, âpurchase - stakeâ âacquisition - stakeâ.	0	146
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited.	0	168
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	In IE, creating the patterns which express the requested scenario, e.g. âmanagement successionâ or âcorporate merger and acquisitionâ is regarded as the hardest task.	0	200
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Overview of the method 2.2 Step by Step Algorithm.	0	52
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	In this section, we will explain the algorithm step by step with examples.	0	53
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	For example, in Information Retrieval (IR), we have to match a userâs query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâs question even if the formulation of the answer in the document is different from the question.	0	13
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	0	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One possibility is to use n-grams based on mutual information.	0	178
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set.	0	201
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.	0	203
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.	0	202
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0	18
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	The data is sorted based on the frequency of the context (âa unit ofâ appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. âNBCâ and âGeneral Electric Co.â appeared 10 times with the context âa unit ofâ).	0	44
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	These results are promising and there are several avenues for improving on these results.	0	209
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	For example, the phrase â's New York-based trust unit,â is not a paraphrase of the other phrases in the âunitâ set.	0	126
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.	1	197
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.	0	2
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	0	22
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	0	204
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Limitations There are several limitations in the methods.	0	193
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	As can be seen in Figure 3, the phrases in the âagreeâ set include completely different relationships, which are not paraphrases.	0	136
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	So, we set a threshold that at least two examples are required to build a link.	0	72
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently.	0	14
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	We are not claiming that this method is almighty.	0	196
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Applications The discovered paraphrases have multiple applications.	0	198
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	There have been other kinds of efforts to discover paraphrase automatically from corpora.	1	164
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.	0	20
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0	195
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet.	0	10
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	0	40
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.	0	202
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.	0	203
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Overview of the method 2.2 Step by Step Algorithm.	0	52
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	In this section, we will explain the algorithm step by step with examples.	0	53
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	The availability of comparable corpora is limited, which is a significant limitation on the approach.	0	166
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	0	40
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	For example, in the CC-domain, 96 keywords are found which have TF/ITF scores above a threshold; some of them are shown in Figure 3.	0	96
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs.	0	170
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	A total of 13,976 phrases were grouped.	0	6
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	shows some keywords with their scores.	0	55
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger.	0	39
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	However, it is desirable if we can separate them.	0	175
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0	195
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Evaluation of links A link between two sets is considered correct if the majority of phrases in both sets have the same meaning, i.e. if the link indicates paraphrase.	0	144
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.	0	111
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Cluster phrases based on Links We now have a set of phrases which share a keyword.	1	62
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	The data is sorted based on the frequency of the context (âa unit ofâ appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. âNBCâ and âGeneral Electric Co.â appeared 10 times with the context âa unit ofâ).	0	44
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	As can be seen in the example, the first two phrases have a different order of NE names from the last two, so we can determine that the last two phrases represent a reversed relation.	0	77
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords.	0	182
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	In this subsection, we will report the results of the experiment, in terms of the number of words, phrases or clusters.	0	86
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the âcontextâ).	0	42
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.	0	110
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	As the two NE categories are the same, we canât differentiate phrases with different orders of par ticipants â whether the buying company or the to-be-bought company comes first.	0	75
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	They cluster NE instance pairs based on the words in the contexts using a bag- of-words method.	0	158
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	âAgreeâ is a subject control verb, which dominates another verb whose subject is the same as that of âagreeâ; the latter verb is generally the one of interest for extraction.	0	186
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0	18
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, âIBM plans to acquire Lotusâ.	0	28
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, we donât know how many such paraphrase sets are necessary to cover even some everyday things or events.	0	17
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	As we shall see, most of the linked sets are paraphrases.	0	35
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	However, there are phrases which express the same meanings even though they do not share the same keyword.	0	63
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently.	0	14
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	In this section, we will explain the algorithm step by step with examples.	0	53
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Overview of the method 2.2 Step by Step Algorithm.	0	52
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	0	40
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	For example, the phrase â's New York-based trust unit,â is not a paraphrase of the other phrases in the âunitâ set.	0	126
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	1	23
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	1	22
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.	0	3
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	0	205
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.	0	2
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	0	204
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	If two phrases can be used to express the same relationship within an information extraction application (âscenarioâ), these two phrases are paraphrases.	0	119
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.	0	197
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.	0	15
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.	0	67
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.	1	91
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	1	89
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	 In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.	0	-3
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0	93
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.	0	329
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	The most accurate characterization of Chinese writing is that it is morphosyllabic (DeFrancis 1984): each hanzi represents one morpheme lexically and semantically, and one syllable phonologiÂ­ cally.	0	76
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.	0	92
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0	22
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Nonstochastic lexical-knowledge-based approaches have been much more numerÂ­ ous.	0	104
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	1	297
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	Full Chinese personal names are in one respect simple: they are always of the form family+given.	0	228
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0	70
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0	300
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	Mandarin exhibits several such processes, including A-not-A question formation, ilÂ­ lustrated in (3a), and adverbial reduplication, illustrated in (3b): 3.	0	445
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	com Â§Cambridge, UK Email: nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.	0	16
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	There are clearly eight orthographic words in the example given, but if one were doing syntactic analysis one would probably want to consider I'm to consist of two syntactic words, namely I and am.	0	6
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.	0	335
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0	33
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	Our system fails in (a) because of$ shenl, a rare family name; the system identifies it as a family name, whereas it should be analyzed as part of the given name.	0	389
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	1	112
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1	108
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	 The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.	0	-5
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	Others depend upon various lexical heurisÂ­ tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.	0	115
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0	305
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	An initial step of any textÂ­ analysis task is the tokenization of the input into words.	0	2
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.	0	-1
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.	0	65
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	shortest match at each point.	0	308
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.	0	55
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	So, 1: f, xue2shengl+men0 (student+PL) 'students' occurs and we estimate its cost at 11.43; similarly we estimate the cost of f, jiang4+men0 (general+PL) 'generals' (as in 'J' f, xiao3jiang4+men0 'little generals'), at 15.02.	0	202
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases.	0	345
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	com Â§Cambridge, UK Email: nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.	0	16
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	na me =>1 ha nzi fa mi ly 1 ha nzi gi ve n 4.	0	233
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	For example, in Northern dialects (such as Beijing), a full tone (1, 2, 3, or 4) is changed to a neutral tone (0) in the final syllable of many words: Jll donglgual 'winter melon' is often pronounced donglguaO.	0	44
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	:zhong1 : 0.0 tjl :huo2 :0.0 (R:spub:/ic of Ch:ina) + .,_,...I : jlong4 :0.0 (mUifaty genG181) 0 Â£: _NC: 40.0 Figure 3 Partial Chinese Lexicon (NC = noun; NP = proper noun).c=- - I â¢=- :il: .;ss:;zhangt â¢ '-:.	0	193
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	The high 1 tone of J1l would not normally neutralize in this fashion if it were functioning as a word on its own.	0	45
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on.	0	242
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	The final estimating equation is then: (3) Since the total of all these class estimates was about 10% off from the Turing estimate n1/N for the probability of all unseen hanzi, we renormalized the estimates so that they would sum to n 1jN.	0	277
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	na me =>2 ha nzi fa mi ly 1 ha nzi gi ve n 6.1 ha nzi fa mi ly => ha nz ii 7.2 ha nzi fa mi ly => ha nzi i ha nz ij 8.1 ha nzi gi ve n => ha nz ii 9.2 ha nzi giv en => ha nzi i ha nz ij The difficulty is that given names can consist, in principle, of any hanzi or pair of hanzi, so the possible given names are limited only by the total number of hanzi, though some hanzi are certainly far more likely than others.	0	235
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.	1	51
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0	33
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0	137
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0	134
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0	418
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Turning now to (1), we have the similar problem that splitting.into.ma3 'horse' andlu4 'way' is more costly than retaining this as one word .ma3lu4 'road.'	0	440
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).	0	415
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.	0	3
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).	0	459
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1	108
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0	305
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0	112
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:Â¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1.	0	181
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0	135
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0	284
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	An anti-greedy algorithm, AG: instead of the longest match, take the.	0	307
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).	0	328
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0	134
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	Word frequencies are estimated by a re-estimation procedure that involves applyÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.	0	168
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1	325
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	First, the model assumes independence between the first and second hanzi of a double given name.	0	246
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0	300
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0	133
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0	33
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0	93
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name.	0	264
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.	0	221
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	The first point we need to address is what type of linguistic object a hanzi repreÂ­ sents.	0	74
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	This is a rather important source of errors in name identifiÂ­ cation, and it is not really possible to objectively evaluate a name recognition system without considering the main lexicon with which it is used.	0	374
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	1	33
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0	20
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	There are thus some very good reasons why segmentation into words is an important task.	0	53
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.	0	3
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	 For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.	0	-2
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces.	0	4
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	 In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.	0	-3
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	In the pinyin transliterations a dash(-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (+) is used, where relevant, to indicate morpheme boundaries of interest.	0	28
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.	0	457
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.	0	49
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1	325
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion.	0	410
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	(See also Wu and Fung [1994].)	0	303
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0	70
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).	0	415
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.	0	136
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	However, this result is consistent with the results of exÂ­ periments discussed in Wu and Fung (1994).	0	348
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0	133
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0	300
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0	54
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	1	399
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	0	281
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0	284
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0	170
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	constitute names, since we have only their segmentation, not the actual classification of the segmented words.	0	383
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.	0	286
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	0	283
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	4.5 Transliterations of Foreign Words.	0	280
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).	0	116
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	In Table 5 we present results from small test corÂ­ pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong.	0	395
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	1	89
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.	0	91
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.	0	121
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0	455
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ­ senting differences in the capabilities of the model per se.	0	370
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.	0	329
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	0	108
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	The second weakness is purely conceptual, and probably does not affect the perÂ­ formance of the model.	0	249
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0	54
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1	108
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0	112
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0	305
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0	284
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0	124
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	Several papers report the use of part-of-speech information to rank segmentations (Lin, Chiang, and Su 1993; Peng and Chang 1993; Chang and Chen 1993); typically, the probability of a segmentation is multiplied by the probability of the tagging(s) for that segmentation to yield an estimate of the total probability for the analysis.	0	122
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	shortest match at each point.	0	308
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	An anti-greedy algorithm, AG: instead of the longest match, take the.	0	307
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	The method just described segments dictionary words, but as noted in Section 1, there are several classes of words that should be handled that are not found in a standard dictionary.	0	187
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0	135
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.	0	92
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	However, it is almost universally the case that no clear definition of what constitutes a "correct" segmentation is given, so these performance measures are hard to evaluate.	0	129
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable.	0	433
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.	0	121
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.	0	412
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	each word in the lexicon whether or not each string is actually an instance of the word in question.	0	191
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).	0	328
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	The method just described segments dictionary words, but as noted in Section 1, there are several classes of words that should be handled that are not found in a standard dictionary.	0	187
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper-name identification; the third measures the performance on morphological analysis.	0	293
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.	0	298
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	1	297
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0	133
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.	0	130
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0	70
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0	33
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper-name identification; the third measures the performance on morphological analysis.	0	293
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable.	0	433
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ­ senting differences in the capabilities of the model per se.	0	370
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Fortunately, we were able to obtain a copy of the full set of sentences from Chang et al. on which Wang, Li, and Chang tested their system, along with the output of their system.18 In what follows we will discuss all cases from this set where our performance on names differs from that of Wang, Li, and Chang.	0	368
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	 We evaluate the system's performance by comparing its segmentation 'Tudgments" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.	0	-6
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	1	398
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	 In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.	0	-4
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0	461
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0	67
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Previous Work.	0	87
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0	89
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	0	138
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.	0	71
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0	153
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).	0	141
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1	325
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.	0	136
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	Yet, some hanzi are far more probable in women's names than they are in men's names, and there is a similar list of male-oriented hanzi: mixing hanzi from these two lists is generally less likely than would be predicted by the independence model.	0	247
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.	0	353
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	Under this scheme, n human judges are asked independently to segment a text.	0	350
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ­ senting differences in the capabilities of the model per se.	0	370
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	 We evaluate the system's performance by comparing its segmentation 'Tudgments" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.	0	-6
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0	70
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words.	0	346
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	As can be seen, GR and this "pared-down" statistical method perform quite similarly, though the statistical method is still slightly better.16 AG clearly performs much less like humans than these methods, whereas the full statistical algorithm, including morphological derivatives and names, performs most closely to humans among the automatic methods.	0	330
Similarly, Sproat et al.(1996) also uses multiple human judges.	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1	325
Similarly, Sproat et al.(1996) also uses multiple human judges.	Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.	0	412
Similarly, Sproat et al.(1996) also uses multiple human judges.	Under this scheme, n human judges are asked independently to segment a text.	0	350
Similarly, Sproat et al.(1996) also uses multiple human judges.	Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.	0	130
Similarly, Sproat et al.(1996) also uses multiple human judges.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0	70
Similarly, Sproat et al.(1996) also uses multiple human judges.	Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words.	0	346
Similarly, Sproat et al.(1996) also uses multiple human judges.	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	0	297
Similarly, Sproat et al.(1996) also uses multiple human judges.	For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.	0	353
Similarly, Sproat et al.(1996) also uses multiple human judges.	Methods that allow multiple segmentations must provide criteria for choosing the best segmentation.	0	113
Similarly, Sproat et al.(1996) also uses multiple human judges.	As can be seen, GR and this "pared-down" statistical method perform quite similarly, though the statistical method is still slightly better.16 AG clearly performs much less like humans than these methods, whereas the full statistical algorithm, including morphological derivatives and names, performs most closely to humans among the automatic methods.	0	330
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	1	399
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.	0	458
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on.	0	242
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	We can model this probability straightforwardly enough with a probabilistic version of the grammar just given, which would assign probabilities to the individual rules.	0	237
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name.	0	264
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	The model described here thus demonstrates great potential for use in widespread applications.	0	466
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	4.4 Chinese Personal Names.	0	227
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	21 In Chinese, numerals and demonstratives cannot modify nouns directly, and must be accompanied by.	0	462
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	Full Chinese personal names are in one respect simple: they are always of the form family+given.	0	228
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	Finally, quite a few hanzi are homographs, meaning that they may be pronounced in several different ways, and in extreme cases apparently represent different morphemes: The prenominal modifiÂ­ cation marker eg deO is presumably a different morpheme from the second morpheme of Â§eg mu4di4, even though they are written the same way.4 The second point, which will be relevant in the discussion of personal names in Section 4.4, relates to the internal structure of hanzi.	0	80
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1	325
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0	300
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	constitute names, since we have only their segmentation, not the actual classification of the segmented words.	0	383
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0	93
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.	0	136
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:Â¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1.	0	181
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0	21
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.	0	18
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.	0	352
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	Among these are words derived by various productive processes, including: 1.	0	56
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.	0	412
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion.	0	410
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.	0	408
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	computing the precision of the other's judgments relative to this standard.	0	314
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	computing the recall of the other's judgments relative to this standard.	0	318
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	Clearly, for judges h and h taking h as standard and computing the precision and recall for Jz yields the same results as taking h as the standard, and computing for h, 14 All evaluation materials, with the exception of those used for evaluating personal names were drawn.	0	319
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	For each pair of judges consider one judge as the standard,.	0	313
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	For each pair of judges, consider one judge as the standard,.	0	317
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong.	0	161
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.	0	128
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	1	21
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	1	33
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0	20
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.	0	49
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0	54
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas.	0	75
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.	0	120
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.	0	3
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	(b) 89 :1 t& tal de cai2neng2 hen3 he DE talent very 'He has great talent' f.b ga ol hig h While the current algorithm correctly handles the (b) sentences, it fails to handle the (a) sentences, since it does not have enough information to know not to group the sequences.ma3lu4 and?]cai2neng2 respectively.	0	427
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	1	138
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	 In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.	0	-4
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0	398
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0	153
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0	67
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.	0	434
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).	0	141
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.	0	71
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0	461
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	Then each arc of D maps either from an element of H to an element of p, or from E-i.e., the empty string-to an element of P. More specifically, each word is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element 5 of Hxp, which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word.	0	140
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1	108
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0	67
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.	0	456
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	The only way to handle such phenomena within the framework described here is simply to expand out the reduplicated forms beforehand, and incorporate the expanded forms into the lexical transducer.	0	454
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0	399
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0	461
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.	0	420
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	Thus, we feel fairly confident that for the examples we have considered from Gan's study a solution can be incorporated, or at least approximated, within a finite-state framework.	0	443
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	A totally nonÂ­ stochastic rule-based system such as Wang, Li, and Chang's will generally succeed in such cases, but of course runs the risk of overgeneration wherever the single-hanzi word is really intended.	0	393
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	1	138
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0	67
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0	398
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.	0	457
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	 In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.	0	-4
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.	0	434
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0	399
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0	153
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.	0	456
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.	0	420
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	1	138
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	 In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.	0	-4
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0	398
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0	114
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0	153
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0	67
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).	0	141
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0	461
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0	455
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1.	0	230
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0	284
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	na me =>2 ha nzi fa mi ly 1 ha nzi gi ve n 6.1 ha nzi fa mi ly => ha nz ii 7.2 ha nzi fa mi ly => ha nzi i ha nz ij 8.1 ha nzi gi ve n => ha nz ii 9.2 ha nzi giv en => ha nzi i ha nz ij The difficulty is that given names can consist, in principle, of any hanzi or pair of hanzi, so the possible given names are limited only by the total number of hanzi, though some hanzi are certainly far more likely than others.	0	235
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).	0	141
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on.	0	242
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	The first point we need to address is what type of linguistic object a hanzi repreÂ­ sents.	0	74
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	Then each arc of D maps either from an element of H to an element of p, or from E-i.e., the empty string-to an element of P. More specifically, each word is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element 5 of Hxp, which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word.	0	140
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	It is formally straightforward to extend the grammar to include these names, though it does increase the likelihood of overgeneration and we are unaware of any working systems that incorporate this type of name.	0	262
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	To evaluate proper-name identification, we randomly seÂ­ lected 186 sentences containing 12,000 hanzi from our test corpus and segmented the text automatically, tagging personal names; note that for names, there is always a sinÂ­ gle unambiguous answer, unlike the more general question of which segmentation is correct.	0	355
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name.	0	264
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	1	281
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identiÂ­ fication of such names is tricky.	0	282
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	4.5 Transliterations of Foreign Words.	0	280
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0	284
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	0	283
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	pronunciation depends upon word affiliation: tfJ is pronounced deO when it is a prenominal modification marker, but di4 in the word Â§tfJ mu4di4 'goal'; fl; is normally ganl 'dry,' but qian2 in a person's given name.	0	39
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.	0	158
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0	399
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.	0	286
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	In the case of, the most common usage is as an adverb with the pronunciation jiangl, so that variant is assigned the estimated cost of 5.98, and a high cost is assigned to nominal usage with the pronunciation jiang4.	0	177
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	1	283
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	1	284
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	4.5 Transliterations of Foreign Words.	0	280
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	0	281
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0	399
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.	0	286
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong.	0	161
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.	0	103
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN).	0	285
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0	24
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	1	88
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.	0	-1
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0	20
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0	21
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.	0	65
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	 The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.	0	-5
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0	33
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.	0	457
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0	54
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	1	284
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	0	281
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0	133
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.	0	103
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong.	0	161
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	It is. based on the traditional character set rather than the simplified character set used in Singapore and Mainland China.	0	162
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0	399
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0	24
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0	135
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0	33
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.	0	158
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper.	0	73
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0	21
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	0	283
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0	24
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong.	0	161
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	As the reviewer also points out, this is a problem that is shared by, e.g., probabilistic context-free parsers, which tend to pick trees with fewer nodes.	0	338
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	We thank United Informatics for providing us with our corpus of Chinese text, and BDC for the 'Behavior ChineseEnglish Electronic Dictionary.'	0	468
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	It is. based on the traditional character set rather than the simplified character set used in Singapore and Mainland China.	0	162
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	com Â§Cambridge, UK Email: nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.	0	16
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	1	54
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0	418
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0	135
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	We thank United Informatics for providing us with our corpus of Chinese text, and BDC for the 'Behavior ChineseEnglish Electronic Dictionary.'	0	468
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0	284
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0	134
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0	114
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0	93
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0	170
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0	33
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1	108
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	This method, one instance of which we term the "greedy algorithm" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginÂ­ ning) of the sentence is reached.	1	109
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0	89
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0	112
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)	0	402
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	A totally nonÂ­ stochastic rule-based system such as Wang, Li, and Chang's will generally succeed in such cases, but of course runs the risk of overgeneration wherever the single-hanzi word is really intended.	0	393
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0	305
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of: â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.	0	240
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.	0	335
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	1	88
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)	0	402
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval.	0	52
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0	461
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0	22
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0	21
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.	0	71
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper.	0	73
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion.	0	410
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	 In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.	0	-4
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0	398
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.	0	457
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0	153
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	0	138
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	With regard to purely morphological phenomena, certain processes are not hanÂ­ dled elegantly within the current framework Any process involving reduplication, for instance, does not lend itself to modeling by finite-state techniques, since there is no way that finite-state networks can directly implement the copying operations required.	0	444
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).	0	141
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0	461
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0	67
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.	0	190
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	For example, as Gan (1994) has noted, one can construct examples where the segmenÂ­ tation is locally ambiguous but can be determined on the basis of sentential or even discourse context.	0	421
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process.	0	118
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	So, 1: f, xue2shengl+men0 (student+PL) 'students' occurs and we estimate its cost at 11.43; similarly we estimate the cost of f, jiang4+men0 (general+PL) 'generals' (as in 'J' f, xiao3jiang4+men0 'little generals'), at 15.02.	0	202
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	The first point we need to address is what type of linguistic object a hanzi repreÂ­ sents.	0	74
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	TIS systems in general need to do more than simply compute the.	0	47
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	However, it is almost universally the case that no clear definition of what constitutes a "correct" segmentation is given, so these performance measures are hard to evaluate.	0	129
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	An initial step of any textÂ­ analysis task is the tokenization of the input into words.	0	2
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.	0	-1
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.	0	121
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	In (1) the sequencema3lu4 cannot be resolved locally, but depends instead upon broader context; similarly in (2), the sequence :::tcai2neng2 cannot be resolved locally: 1.	0	424
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0	24
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.	0	419
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0	135
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0	20
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular seÂ­ quence of hanzi might be a name, but that it is likely to be a name with some probabilÂ­ ity.	0	123
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0	54
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0	398
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0	170
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.	0	128
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).	1	415
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0	134
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0	135
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.	0	125
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	Note that hanzi that are not grouped into dictionary words (and are not identified as singleÂ­ hanzi words), or into one of the other categories of words discussed in this paper, are left unattached and tagged as unknown words.	0	159
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular seÂ­ quence of hanzi might be a name, but that it is likely to be a name with some probabilÂ­ ity.	0	123
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.	0	51
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.	0	120
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0	54
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	1	399
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0	170
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	In the pinyin transliterations a dash(-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (+) is used, where relevant, to indicate morpheme boundaries of interest.	0	28
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	0	281
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0	284
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	4.5 Transliterations of Foreign Words.	0	280
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	Note that hanzi that are not grouped into dictionary words (and are not identified as singleÂ­ hanzi words), or into one of the other categories of words discussed in this paper, are left unattached and tagged as unknown words.	0	159
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.	0	416
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable.	0	433
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.	0	286
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.	1	128
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.	0	296
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	First of all, most previous articles report perforÂ­ mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.	0	405
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.	0	361
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.	0	363
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.	0	142
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	The performance was 80.99% recall and 61.83% precision.	0	356
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	This method, one instance of which we term the "greedy algorithm" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginÂ­ ning) of the sentence is reached.	0	109
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.	0	132
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.	0	125
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0	20
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0	24
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	Both of these analyses are shown in Figure 4; fortunately, the correct analysis is also the one with the lowest cost, so it is this analysis that is chosen.	0	185
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval.	0	52
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.	0	65
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	In Section 6 we disÂ­ cuss other issues relating to how higher-order language models could be incorporated into the model.	0	154
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	This is an issue that we have not addressed at the current stage of our research.	0	340
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	The first issue relates to the completeness of the base lexicon.	0	371
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0	133
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0	124
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.	0	125
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	pronunciations of individual words; they also need to compute intonational phrase boundaries in long utterances and assign relative prominence to words in those utterances.	0	48
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Others depend upon various lexical heurisÂ­ tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.	0	115
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.	0	353
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular seÂ­ quence of hanzi might be a name, but that it is likely to be a name with some probabilÂ­ ity.	0	123
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).	0	116
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Finally, the statistical method fails to correctly group hanzi in cases where the individual hanzi comprising the name are listed in the dictionary as being relatively high-frequency single-hanzi words.	0	390
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	We of course also fail to identify, by the methods just described, given names used without their associated family name.	0	263
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Word frequencies are estimated by a re-estimation procedure that involves applyÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.	0	168
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	1	20
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.	0	367
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Papers that use this method or minor variants thereof include Liang (1986), Li et al.	0	110
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits.	0	357
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0	124
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	However, we have reason to doubt Chang et al.'s performance claims.	0	358
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	There are two weaknesses in Chang et al.'s model, which we improve upon.	0	245
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.	0	244
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	 In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.	0	-3
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Note that it is in precision that our overÂ­ all performance would appear to be poorer than the reported performance of Chang et al., yet based on their published examples, our system appears to be doing better precisionwise.	0	364
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1	108
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0	112
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0	305
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	Note also that the costs currently used in the system are actually string costs, rather than word costs.	0	174
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	On the other hand, in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English.	0	32
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	each word in the lexicon whether or not each string is actually an instance of the word in question.	0	191
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0	172
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.	0	190
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	shortest match at each point.	0	308
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	Then each arc of D maps either from an element of H to an element of p, or from E-i.e., the empty string-to an element of P. More specifically, each word is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element 5 of Hxp, which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word.	0	140
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Word frequencies are estimated by a re-estimation procedure that involves applyÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.	1	168
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0	114
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).	0	119
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.	0	19
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	With regard to purely morphological phenomena, certain processes are not hanÂ­ dled elegantly within the current framework Any process involving reduplication, for instance, does not lend itself to modeling by finite-state techniques, since there is no way that finite-state networks can directly implement the copying operations required.	0	444
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0	455
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0	172
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.	0	125
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.	0	420
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1	325
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0	300
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.	0	332
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	It can also be seen clearly in this plot that two of the Taiwan speakers cluster very closely together, and the third TaiÂ­ wan speaker is also close in the most significant dimension (the x axis).	0	331
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	In the denomi 11 We have two such lists, one containing about 17,000 full names, and another containing frequencies of.	0	257
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	Methods that allow multiple segmentations must provide criteria for choosing the best segmentation.	0	113
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical.	0	384
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	It may seem surprising to some readers that the interhuman agreement scores reported here are so low.	0	347
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ­ senting differences in the capabilities of the model per se.	0	370
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:Â¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1.	0	181
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	Again, famous place names will most likely be found in the dictionary, but less well-known names, such as 1PMÂ± R; bu4lang3-shi4wei2-ke4 'Brunswick' (as in the New Jersey town name 'New Brunswick') will not generally be found.	0	64
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0	137
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.	0	190
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0	54
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0	153
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	It may seem surprising to some readers that the interhuman agreement scores reported here are so low.	0	347
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0	21
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas.	0	75
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	 We evaluate the system's performance by comparing its segmentation 'Tudgments" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.	0	-6
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.	0	353
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	 In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.	0	-4
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.	0	65
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.	0	71
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0	455
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0	398
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0	153
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0	137
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0	67
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	For that application, at a minimum, one would want to know the phonological word boundaries.	0	35
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.	1	65
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.	0	19
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0	21
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0	137
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper.	0	73
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0	20
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	 In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.	0	-3
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.	0	51
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	 The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.	0	-5
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	The second concerns the methods used (if any) to exÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.	0	107
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0	114
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	It is. based on the traditional character set rather than the simplified character set used in Singapore and Mainland China.	0	162
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	There are two weaknesses in Chang et al.'s model, which we improve upon.	0	245
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).	0	328
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.	0	158
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0	398
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	 The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.	0	-5
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0	418
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0	170
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0	170
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	For example, given a sequence F1G1G2, where F1 is a legal single-hanzi family name, and Plural Nouns X g 0 g "' X X 0 T!i c"'.	0	238
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	In this example there are four "input characters," A, B, C and D, and these map respectively to four "pronunciations" a, b, c and d. Furthermore, there are four "words" represented in the dictionary.	0	145
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	As indicated in Figure 1(c), apart from this correct analysis, there is also the analysis taking B ri4 as a word (e.g., a common abbreviation for Japan), along with X:Â¥ wen2zhangl 'essay/ and f!!.	0	183
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	com Â§Cambridge, UK Email: nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.	0	16
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	"c' 0 + 0 "0 ' â¢ + a n t i g r e e d y x g r e e d y < > c u r r e n t m e t h o d o d i e t . o n l y â¢ Taiwan 0 Â·;; 0 c CD E i5 0"' 9 9 â¢ Mainland â¢ â¢ â¢ â¢ -0.30.20.1 0.0 0.1 0.2 Dimension 1 (62%) Figure 7 Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions.	0	342
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	That is, given a choice between segmenting a sequence abc into abc and ab, c, the former will always be picked so long as its cost does not exceed the summed costs of ab and c: while; it is possible for abc to be so costly as to preclude the larger grouping, this will certainly not usually be the case.	0	336
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	(a) I f f fi * fi :1 }'l ij 1Â§: {1M m m s h e n 3 m e 0 shi2 ho u4 wo 3 cai2 ne ng 2 ke4 fu 2 zh e4 ge 4 ku n4 w h a t ti m e I just be abl e ov er co m e thi s C L dif fic 'When will I be able to overcome this difficulty?'	0	426
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	:zhong1 : 0.0 tjl :huo2 :0.0 (R:spub:/ic of Ch:ina) + .,_,...I : jlong4 :0.0 (mUifaty genG181) 0 Â£: _NC: 40.0 Figure 3 Partial Chinese Lexicon (NC = noun; NP = proper noun).c=- - I â¢=- :il: .;ss:;zhangt â¢ '-:.	0	193
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	com t 600 Mountain Avenue, 2c278, Murray Hill, NJ 07974, USA.	0	13
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0	89
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Nonstochastic lexical-knowledge-based approaches have been much more numerÂ­ ous.	0	104
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0	114
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	The second concerns the methods used (if any) to exÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.	0	107
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0	135
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.	0	120
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.	0	329
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	0	108
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).	0	119
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1	325
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0	300
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	constitute names, since we have only their segmentation, not the actual classification of the segmented words.	0	383
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0	93
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.	0	136
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:Â¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1.	0	181
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0	21
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.	0	18
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.	0	352
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	Among these are words derived by various productive processes, including: 1.	0	56
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	1	93
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.	1	103
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	As a partial solution, for pairs of hanzi that co-occur sufficiently often in our namelists, we use the estimated bigram cost, rather than the independence-based cost.	0	248
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0	117
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0	89
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.	0	329
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	A related point is that mutual information is helpful in augmenting existing electronic dictionaries, (cf.	0	94
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Thus, we feel fairly confident that for the examples we have considered from Gan's study a solution can be incorporated, or at least approximated, within a finite-state framework.	0	443
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.	0	91
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Gan's solution depends upon a fairly sophisticated language model that attempts to find valid syntactic, semantic, and lexical relations between objects of various linguistic types (hanzi, words, phrases).	0	428
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.	0	130
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0	300
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	 For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.	0	-2
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0	70
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	0	297
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	0	283
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.	0	18
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.	0	51
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	 In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.	0	-3
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0	137
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1	325
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0	21
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0	22
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0	54
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.	0	132
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.	0	412
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	Various segmentation approaches were then compared with human performance: 1.	0	304
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	 We evaluate the system's performance by comparing its segmentation 'Tudgments" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.	0	-6
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	Others depend upon various lexical heurisÂ­ tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.	0	115
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0	70
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.	0	121
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).	0	119
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0	172
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Yet, some hanzi are far more probable in women's names than they are in men's names, and there is a similar list of male-oriented hanzi: mixing hanzi from these two lists is generally less likely than would be predicted by the independence model.	0	247
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Then each arc of D maps either from an element of H to an element of p, or from E-i.e., the empty string-to an element of P. More specifically, each word is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element 5 of Hxp, which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word.	0	140
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0	89
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0	153
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Word frequencies are estimated by a re-estimation procedure that involves applyÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.	0	168
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Finally, the statistical method fails to correctly group hanzi in cases where the individual hanzi comprising the name are listed in the dictionary as being relatively high-frequency single-hanzi words.	0	390
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	1	398
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0	67
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0	455
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.	0	190
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	With regard to purely morphological phenomena, certain processes are not hanÂ­ dled elegantly within the current framework Any process involving reduplication, for instance, does not lend itself to modeling by finite-state techniques, since there is no way that finite-state networks can directly implement the copying operations required.	0	444
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	logical rules, and personal names; the transitive closure of the resulting machine is then computed.	0	291
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	Our system does not currently make use of titles, but it would be straightforward to do so within the finite-state framework that we propose.	0	377
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	However, some caveats are in order in comparing this method (or any method) with other approaches to segÂ­ mentation reported in the literature.	0	404
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	This is a rather important source of errors in name identifiÂ­ cation, and it is not really possible to objectively evaluate a name recognition system without considering the main lexicon with which it is used.	0	374
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0	20
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0	134
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	One class comprises words derived by productive morphologiÂ­ cal processes, such as plural noun formation using the suffix ir, menD.	0	188
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	 The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.	0	-5
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0	399
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	Others depend upon various lexical heurisÂ­ tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.	0	115
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.	0	65
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).	0	415
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.	0	286
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	Among these are words derived by various productive processes, including: 1.	0	56
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0	137
We used a simple greedy algorithm described in [Sproat et al., 1996].	This method, one instance of which we term the "greedy algorithm" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginÂ­ ning) of the sentence is reached.	1	109
We used a simple greedy algorithm described in [Sproat et al., 1996].	An anti-greedy algorithm, AG: instead of the longest match, take the.	0	307
We used a simple greedy algorithm described in [Sproat et al., 1996].	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0	305
We used a simple greedy algorithm described in [Sproat et al., 1996].	We of course also fail to identify, by the methods just described, given names used without their associated family name.	0	263
We used a simple greedy algorithm described in [Sproat et al., 1996].	The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.	0	456
We used a simple greedy algorithm described in [Sproat et al., 1996].	Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1.	0	230
We used a simple greedy algorithm described in [Sproat et al., 1996].	Finally, asÂ­ suming a simple bigram backoff model, we can derive the probability estimate for the particular unseen word iÂ¥1J1l.	0	208
We used a simple greedy algorithm described in [Sproat et al., 1996].	Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process.	0	118
We used a simple greedy algorithm described in [Sproat et al., 1996].	In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).	0	328
We used a simple greedy algorithm described in [Sproat et al., 1996].	The model described here thus demonstrates great potential for use in widespread applications.	0	466
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.	1	190
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0	172
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0	170
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates.	0	173
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	Word frequencies are estimated by a re-estimation procedure that involves applyÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.	0	168
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.	0	408
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	We of course also fail to identify, by the methods just described, given names used without their associated family name.	0	263
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges.	0	171
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	We have argued that the proposed method performs well.	0	403
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0	124
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1	108
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0	305
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0	23
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0	112
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	An anti-greedy algorithm, AG: instead of the longest match, take the.	0	307
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	Wu and Fung introduce an evaluation method they call nk-blind.	0	349
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	(a) I f f fi * fi :1 }'l ij 1Â§: {1M m m s h e n 3 m e 0 shi2 ho u4 wo 3 cai2 ne ng 2 ke4 fu 2 zh e4 ge 4 ku n4 w h a t ti m e I just be abl e ov er co m e thi s C L dif fic 'When will I be able to overcome this difficulty?'	0	426
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	com Â§Cambridge, UK Email: nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.	0	16
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	The method just described segments dictionary words, but as noted in Section 1, there are several classes of words that should be handled that are not found in a standard dictionary.	0	187
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0	172
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.	1	142
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	The performance was 80.99% recall and 61.83% precision.	1	356
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.	0	128
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.	0	296
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	First of all, most previous articles report perforÂ­ mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.	0	405
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.	0	361
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.	0	363
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.	0	132
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.	0	125
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units.	0	302
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.	0	416
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular seÂ­ quence of hanzi might be a name, but that it is likely to be a name with some probabilÂ­ ity.	0	123
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0	418
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	An initial step of any textÂ­ analysis task is the tokenization of the input into words.	0	2
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0	137
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.	0	18
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	This method, one instance of which we term the "greedy algorithm" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginÂ­ ning) of the sentence is reached.	0	109
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.	0	419
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	We have argued that the proposed method performs well.	0	403
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0	124
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	1	104
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	1	25
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0	44
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0	45
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0	152
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0	101
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	2.1 Subword-based IOB tagging using CRFs.	0	30
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	using the FMM, and then labeled with âIOBâ tags by the CRFs.	0	100
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.	0	43
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.	1	3
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	This approach is effective for performing desired segmentation based on usersâ requirements to R-oov and R-iv.	0	154
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov).	0	13
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	While OOV recognition is very important in word segmentation, a higher IV rate is also desired.	0	15
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).	0	75
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	However, the R-iv rates were getting worse in return for higher R-oov rates.	0	105
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	By this approach we can change R-oovs and R-ivs and find an optimal tradeoff.	0	17
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach.	0	42
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold.	0	70
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0	25
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.	1	58
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	In this section we introduce a confidence measure approach to combine the two results.	0	57
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.	0	111
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.	0	108
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0	21
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	In Section 3.2 we will present the experimental segmentation results of the confidence measure approach.	0	71
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs.	0	148
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.	0	150
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	71 2 0.9 67 0.9 76 Table 3: Effects of combination using the confidence measure.	0	120
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	We will use this advantage in the confidence measure approach.	0	29
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.	0	150
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0	25
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.	0	61
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.	0	58
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach.	0	42
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0	101
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.	0	108
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.	0	54
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.	0	111
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.	1	67
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.	0	2
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold.	0	70
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results.	0	56
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0	101
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	0	8
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Its calculation is defined as: C M(tiob |w) = Î±C Miob (tiob |w) + (1 â Î±)Î´(tw , tiob )ng (2) where tiob is the word wâs IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.	0	60
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Each subword is given a prior IOB tag, tw . C Miob (t|w), a ï£« M ï£«ï£¶ï£¶ confidence probability derived in the process of IOB tag exp ï£¬)' ï£¬)' Î»k fk (tiâ1 , ti , W ) + )' Âµk gk (ti , W )ï£·ï£· /Z, ï£¬ï£­ i=1 ï£¬ï£­ k k ï£·ï£¸ ï£·ï£¸ (1) ging, is defined as Z = )' T =t0 t1 Â·Â·Â·tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 Â·Â·Â·tM ,ti =t P(T |W, wi ) T =t 0 t1 Â·Â·Â· tM P ( T | W ) where we call fk (tiâ1 , ti , W ) bigram feature functions because the features trigger the previous observation tiâ1 where the numerator is a sum of all the observation sequences with word wi labeled as t. Î´(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.	0	62
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging.	0	130
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.	0	37
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov).	0	13
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0	44
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.	0	3
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	While OOV recognition is very important in word segmentation, a higher IV rate is also desired.	0	15
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).	0	134
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	This approach is effective for performing desired segmentation based on usersâ requirements to R-oov and R-iv.	0	154
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.	0	2
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results.	0	56
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0	21
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.	0	150
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.	0	129
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.	0	4
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.	0	34
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.	0	137
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	0	104
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.	0	74
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0	44
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0	45
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	For AS corpus, âAdam Smithâ are two words in the training but become a one- word in the test, âAdamSmithâ.	0	143
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.	0	150
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	1	25
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0	44
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	0	104
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0	152
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0	101
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Taking the same example mentioned above, â (whole) (Beijing city)â is labeled as â (whole)/O (Beijing)/B (city)/Iâ in the subword-based tagging, where â (Beijing)/Bâ is labeled as one unit.	0	10
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	2.1 Subword-based IOB tagging using CRFs.	0	30
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0	45
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Each subword is given a prior IOB tag, tw . C Miob (t|w), a ï£« M ï£«ï£¶ï£¶ confidence probability derived in the process of IOB tag exp ï£¬)' ï£¬)' Î»k fk (tiâ1 , ti , W ) + )' Âµk gk (ti , W )ï£·ï£· /Z, ï£¬ï£­ i=1 ï£¬ï£­ k k ï£·ï£¸ ï£·ï£¸ (1) ging, is defined as Z = )' T =t0 t1 Â·Â·Â·tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 Â·Â·Â·tM ,ti =t P(T |W, wi ) T =t 0 t1 Â·Â·Â· tM P ( T | W ) where we call fk (tiâ1 , ti , W ) bigram feature functions because the features trigger the previous observation tiâ1 where the numerator is a sum of all the observation sequences with word wi labeled as t. Î´(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.	0	62
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.	0	2
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0	45
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	We found that so far all the existing implementations were using character-based IOB tagging.	0	7
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0	151
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	0	9
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	Its calculation is defined as: C M(tiob |w) = Î±C Miob (tiob |w) + (1 â Î±)Î´(tw , tiob )ng (2) where tiob is the word wâs IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.	0	60
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	Qc 2006 Association for Computational Linguistics input åã£á¯¹Ô£à³¼à£«Òá +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation å ã£ á¯¹ Ô£ à³¼ à£«Òá +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging å/% ã£/, á¯¹/, Ô£/2 à³¼/2 à£«Ò/% á/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation å/% ã£/, á¯¹/, Ô£/2 à³¼/2 à£«Ò/% á/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output åã£á¯¹ Ô£ à³¼ à£«Òá +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.	0	33
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	For a character-based IOB tagger, there is only one possibility of re-segmentation.	0	38
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).	0	75
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.	0	129
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.	0	4
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193â196, New York, June 2006.	0	32
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.	0	34
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	0	104
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.	0	137
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.	0	74
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0	44
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0	45
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	For AS corpus, âAdam Smithâ are two words in the training but become a one- word in the test, âAdamSmithâ.	0	143
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	1	25
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	0	104
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	Qc 2006 Association for Computational Linguistics input åã£á¯¹Ô£à³¼à£«Òá +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation å ã£ á¯¹ Ô£ à³¼ à£«Òá +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging å/% ã£/, á¯¹/, Ô£/2 à³¼/2 à£«Ò/% á/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation å/% ã£/, á¯¹/, Ô£/2 à³¼/2 à£«Ò/% á/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output åã£á¯¹ Ô£ à³¼ à£«Òá +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.	0	33
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.	0	1
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0	44
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.	0	4
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.	0	137
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.	0	37
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.	0	111
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	2.1 Subword-based IOB tagging using CRFs.	0	30
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0	101
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0	25
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0	152
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).	0	134
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	0	9
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	0	104
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0	44
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0	45
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.	1	72
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	1	8
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.	0	1
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0	151
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).	0	5
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0	21
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based.	0	135
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	It proves the proposed word-based IOB tagging was very effective.	0	131
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	If the subset consists of Chinese characters only, it is a character-based IOB tagger.	0	35
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	The segmentation results of the dictionary-based were re-segmented Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied.	0	89
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.	1	72
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.	0	4
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.	0	129
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.	0	37
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.	0	14
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	0	78
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0	21
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0	25
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	We achieved the highest F-scores in CITYU, PKU and MSR corpora.	0	138
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.	0	43
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	1	78
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Tri- gram LMs were generated using the SRI LM toolkit for disambiguation.	1	79
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	For the bigram features, we only used the previous and the current observations, tâ1 t0 . As to feature selection, we simply used absolute counts for each feature in the training data.	0	50
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	For AS corpus, âAdam Smithâ are two words in the training but become a one- word in the test, âAdamSmithâ.	0	143
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.	0	43
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.	0	72
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment.	0	81
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.	0	74
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.	0	37
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0	45
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	1	8
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0	101
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	0	9
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Taking the same example mentioned above, â (whole) (Beijing city)â is labeled as â (whole)/O (Beijing)/B (city)/Iâ in the subword-based tagging, where â (Beijing)/Bâ is labeled as one unit.	0	10
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	We regard the words in the subset as the subwords for the IOB tagging.	0	36
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0	151
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.	0	34
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.	0	61
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.	0	88
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.	1	2
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0	151
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Since the dictionary-based approach is a well-known method, we skip its technical descriptions.	0	27
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0	152
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based.	0	135
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	We think our proposed subword- based tagging played an important role for the good results.	0	139
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0	25
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0	21
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	0	78
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193â196, New York, June 2006.	0	32
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0	25
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.	0	1
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0	101
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.	0	137
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	2.1 Subword-based IOB tagging using CRFs.	0	30
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Qc 2006 Association for Computational Linguistics input åã£á¯¹Ô£à³¼à£«Òá +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation å ã£ á¯¹ Ô£ à³¼ à£«Òá +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging å/% ã£/, á¯¹/, Ô£/2 à³¼/2 à£«Ò/% á/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation å/% ã£/, á¯¹/, Ô£/2 à³¼/2 à£«Ò/% á/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output åã£á¯¹ Ô£ à³¼ à£«Òá +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.	0	33
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	1	9
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	1	8
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.	0	1
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0	151
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0	25
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).	0	5
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0	21
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0	19
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.	0	61
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.	0	74
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0	44
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.	0	14
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	There were no F-score changes for AS and PKU corpora, but the recall rates were improved.	0	103
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus.	0	102
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	Each subword is given a prior IOB tag, tw . C Miob (t|w), a ï£« M ï£«ï£¶ï£¶ confidence probability derived in the process of IOB tag exp ï£¬)' ï£¬)' Î»k fk (tiâ1 , ti , W ) + )' Âµk gk (ti , W )ï£·ï£· /Z, ï£¬ï£­ i=1 ï£¬ï£­ k k ï£·ï£¸ ï£·ï£¸ (1) ging, is defined as Z = )' T =t0 t1 Â·Â·Â·tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 Â·Â·Â·tM ,ti =t P(T |W, wi ) T =t 0 t1 Â·Â·Â· tM P ( T | W ) where we call fk (tiâ1 , ti , W ) bigram feature functions because the features trigger the previous observation tiâ1 where the numerator is a sum of all the observation sequences with word wi labeled as t. Î´(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.	0	62
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.	0	111
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0	25
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	0	8
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	Since the dictionary-based approach is a well-known method, we skip its technical descriptions.	0	27
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	1	2
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Another alternative is to only consider unambiguous synonyms with a single supersense in WORDNET.	0	168
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Supersense Tagging of Unknown Nouns using Semantic Similarity	0	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	This same technique as is used in our approach to supersense tagging.	0	64
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.	0	21
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy.	0	44
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.	0	47
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Another approach is to use the scores returned by the similarity system.	0	162
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself.	0	45
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	After the raw text has been POS tagged and chunked, the grammatical relation extraction algorithm is run over the chunks.	0	103
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.	1	147
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.	0	1
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).	0	217
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.	0	19
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORD- NET and the construction of new wordnets using the concept structure from English.	0	48
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	For lexical FreeNet, Beefer- man (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information.	0	49
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD- NET 1.7.1 test set.	0	75
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	0	13
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources.	0	18
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).	0	73
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	1	13
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.	0	1
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources.	0	18
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.	0	92
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics.	0	12
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.	0	19
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	For lexical FreeNet, Beefer- man (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information.	0	49
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Bur- gun and Bodenreider (2001) compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap.	0	11
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).	0	73
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.	0	231
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.	1	147
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.	0	1
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection.	0	154
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.	0	149
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	This is determined by looking at the frequency and number of attributes for the unknown word.	0	186
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	Any progressive verb which appears after a determiner or quantifier is considered a noun.	0	143
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.	0	41
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.	0	9
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.	0	31
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense.	0	72
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics.	0	12
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.	0	57
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.	0	41
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.	0	31
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.	0	9
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moenâs configuration for our super- sense tagging experiments.	0	106
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.	0	42
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.	0	198
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	Lex-files form a set of coarse-grained sense distinctions within WORDNET.	0	30
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	0	13
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.	1	231
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.	0	165
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.	0	149
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible.	0	94
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.	0	1
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.	0	47
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	0	65
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	0	2
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Supersense Tagging of Unknown Nouns using Semantic Similarity	0	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	For lexical FreeNet, Beefer- man (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information.	0	49
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise.	0	85
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	We have developed a 2 billion word corpus, shallow- parsed with a statistical NLP pipeline, which is by far the Table 2: Example nouns and their supersenses largest NLP processed corpus described in published re search.	0	77
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis.	0	223
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	3 M 4 9 1 M Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997).	0	84
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in S Y S T E M W N 1.6 W N 1.7 .1 Cia ra mit a an d Joh nso n bas eli ne 2 1 % 2 8 % Cia ra mit a an d Joh nso n per cep tro n 5 3 % 5 3 % Si mil arit y bas ed res ult s 6 8 % 6 3 % Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses.	0	169
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	Supersense Tagging of Unknown Nouns using Semantic Similarity	0	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.	0	231
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORD- NET and the construction of new wordnets using the concept structure from English.	0	48
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	We would expect the location supersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier.	0	201
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.	0	15
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	1	65
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Hearst and SchuÂ¨ tze (1993) flatten WORDNET into 726 categories using an algorithm which attempts to minimise the variance in category size.	0	58
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	This technique is similar to Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0	148
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0	225
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Lex-files form a set of coarse-grained sense distinctions within WORDNET.	0	30
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	0	13
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.	0	41
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself.	0	45
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.	0	31
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30).	0	33
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	1	65
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).	0	6
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003).	0	229
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1.	0	24
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.	0	234
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.	0	41
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.	0	69
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30).	0	33
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Some examples from the test sets are given in Table 2 with their supersenses.	0	76
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.	0	31
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.	1	87
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	The other measure they found to be successful was the entropy of the conditional distribution of surrounding words given the noun.	0	53
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?	0	167
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.	0	19
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy.	0	62
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary.	0	10
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word.	0	61
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).	0	217
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Here, the t-test compares the joint and product probability distributions of the headword and context: 6.3 Grammatical Relation Extraction.	0	102
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	SchuÂ¨ tzeâs (1992) WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem).	0	60
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.	0	47
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Supersense Tagging of Unknown Nouns using Semantic Similarity	0	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy.	0	44
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.	0	68
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	0	2
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	This same technique as is used in our approach to supersense tagging.	0	64
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.	0	87
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0	225
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.	0	69
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	The only similar performing tool is the Trigrams ânâ Tags tagger (Brants, 2000) which uses a much simpler statistical model.	0	109
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	1	65
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1.	0	24
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).	0	6
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstetteâs table-based results, i.e. the SEXTANT always prefers noun attachment.	0	111
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.	0	165
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word.	0	61
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.	0	198
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	0	13
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics.	0	12
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms.	0	63
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.	0	57
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Supersense Tagging of Unknown Nouns using Semantic Similarity	0	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0	225
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	These categories are used to label paragraphs with topics, effectively repeating Yarowskyâs (1992) experiments using the their categories rather than Rogetâs thesaurus.	0	59
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	0	13
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g. location and person) for labelling predominantly unseen terms.	0	43
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	SEXTANT extracts relation tuples (w, r, wt ) for each noun, where w is the headword, r is the relation type and wt is the other word.	0	93
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Not surprisingly, the similarity system works better than the guessing rules if it has any information at all.	0	187
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	0	2
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.	0	68
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0	38
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0	195
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0	42
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	However, when a preposition exists in the mention, its head word is set as the last word before the preposition.	0	75
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	1	51
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	It also shows that feature-based methods dramatically outperform kernel methods.	0	162
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0	215
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0	197
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0	44
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0	6
Most of the features used in our system are based on the work in (Zhou et al., 2005).	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0	108
Most of the features used in our system are based on the work in (Zhou et al., 2005).	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0	6
Most of the features used in our system are based on the work in (Zhou et al., 2005).	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	0	20
Most of the features used in our system are based on the work in (Zhou et al., 2005).	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0	44
Most of the features used in our system are based on the work in (Zhou et al., 2005).	The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.	0	212
Most of the features used in our system are based on the work in (Zhou et al., 2005).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
Most of the features used in our system are based on the work in (Zhou et al., 2005).	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
Most of the features used in our system are based on the work in (Zhou et al., 2005).	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0	18
Most of the features used in our system are based on the work in (Zhou et al., 2005).	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
Most of the features used in our system are based on the work in (Zhou et al., 2005).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	Section 3 and Section 4 describe our approach and various features employed respectively.	0	28
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0	60
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.	0	133
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	The rest of this paper is organized as follows.	0	26
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	This category of features concerns about the information inherent only in the full parse tree.	0	97
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	â¢ The usefulness of mention level features is quite limited.	0	134
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.	0	110
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.	0	147
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0	195
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1	17
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	0	40
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0	31
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0	93
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format.	0	8
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0	18
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0	42
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0	195
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0	41
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0	4
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	1	51
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.	0	32
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0	44
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	It also shows that feature-based methods dramatically outperform kernel methods.	0	162
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0	215
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	0	33
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.	0	164
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Covolution kernels for natural language.	0	204
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	1	34
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.	0	132
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0	93
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0	195
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head.	0	94
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser (Collins 1999) is employed for full parsing.	0	88
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	â¢ ET1DW1: combination of the entity type and the dependent word for M1 â¢ H1DW1: combination of the head word and the dependent word for M1 â¢ ET2DW2: combination of the entity type and the dependent word for M2 â¢ H2DW2: combination of the head word and the dependent word for M2 â¢ ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP â¢ ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP â¢ ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree.	0	96
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0	4
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0	31
 dependency kernel Zhou et al.(2005)	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	1	33
 dependency kernel Zhou et al.(2005)	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
 dependency kernel Zhou et al.(2005)	427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.	0	24
 dependency kernel Zhou et al.(2005)	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	0	51
 dependency kernel Zhou et al.(2005)	Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.	0	32
 dependency kernel Zhou et al.(2005)	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
 dependency kernel Zhou et al.(2005)	Dependency tree th parse trees.	0	210
 dependency kernel Zhou et al.(2005)	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
 dependency kernel Zhou et al.(2005)	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0	44
 dependency kernel Zhou et al.(2005)	Covolution kernels for natural language.	0	204
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	1	60
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1	17
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0	36
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0	126
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype âFounderâ under the type âROLEâ.	1	121
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes âBased-Inâ, âLocatedâ and âResidenceâ under the type âATâ, which are difficult even for human experts to differentiate.	1	122
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.	0	149
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set.	0	117
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first.	0	190
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998).	0	84
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0	6
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.	0	43
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.	0	160
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0	214
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0	31
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation.	0	69
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0	18
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0	41
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0	93
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes.	0	49
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head.	0	94
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Head-driven statistical models for natural language parsing.	0	200
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1	17
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output.	0	50
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC).	0	9
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	0	33
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0	38
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.	0	217
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: â¢ WM1: bag-of-words in M1 â¢ HM1: head word of M1 3 In ACE, each mention has a head annotation and an.	0	67
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.	0	30
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	1	20
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0	197
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	In this paper, we separate the features of base phrase chunking from those of full parsing.	0	86
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0	52
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important.	0	63
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0	93
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	We use the official ACE corpus from LDC.	0	112
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0	52
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.	0	46
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	For example, we want to determine whether a person is at a location, based on the evidence in the context.	0	15
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1.	0	124
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	Therefore, we must extend SVMs to multi-class (e.g. K) such as the ACE RDC task.	0	48
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1	17
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0	52
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	We use the official ACE corpus from LDC.	0	112
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	In this paper, we separate the features of base phrase chunking from those of full parsing.	0	86
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.	0	46
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0	45
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0	53
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser (Collins 1999) is employed for full parsing.	0	88
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1	17
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities.	0	11
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	It also shows that feature-based methods dramatically outperform kernel methods.	0	162
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	1	193
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0	41
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	0	20
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0	18
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0	130
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0	143
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0	42
Zhou et al.(2005) explore various features in relation extraction using SVM.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1	17
Zhou et al.(2005) explore various features in relation extraction using SVM.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Zhou et al.(2005) explore various features in relation extraction using SVM.	Exploring Various Knowledge in Relation Extraction	0	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
Zhou et al.(2005) explore various features in relation extraction using SVM.	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0	36
Zhou et al.(2005) explore various features in relation extraction using SVM.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
Zhou et al.(2005) explore various features in relation extraction using SVM.	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0	108
Zhou et al.(2005) explore various features in relation extraction using SVM.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
Zhou et al.(2005) explore various features in relation extraction using SVM.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
Zhou et al.(2005) explore various features in relation extraction using SVM.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0	4
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	However, The remaining words that do not have above four classes are manually classified.	0	120
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.	0	24
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.	0	46
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	Mentions have three levels: names, nomial expressions or pronouns.	0	13
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	â¢ Chunking features are very useful.	0	138
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0	195
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	This category of features concerns about the information inherent only in the full parse tree.	0	97
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0	143
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.	0	140
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	In this paper, we separate the features of base phrase chunking from those of full parsing.	0	86
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.	0	132
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0	4
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	â¢ Chunking features are very useful.	0	138
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0	131
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0	60
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	â¢ Chunking features are very useful.	0	138
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	â¢ The usefulness of mention level features is quite limited.	0	134
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6.	0	29
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.	1	110
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0	23
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.	0	147
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	explicit relations occur in text with explicit evidence suggesting the relationships.	0	22
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.	0	14
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation.	0	69
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	0	128
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0	214
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations.	0	113
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0	215
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	1	34
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0	93
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	In this paper, we separate the features of base phrase chunking from those of full parsing.	0	86
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser (Collins 1999) is employed for full parsing.	0	88
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0	18
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0	4
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.	0	133
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	Exploring Various Knowledge in Relation Extraction	0	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0	108
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	â¢ Chunking features are very useful.	0	138
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6.	0	29
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: â¢ WM1: bag-of-words in M1 â¢ HM1: head word of M1 3 In ACE, each mention has a head annotation and an.	1	67
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0	44
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.	0	32
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0	31
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.	0	164
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1.	0	189
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	1	93
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0	143
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0	195
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	0	33
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	â¢ ET1DW1: combination of the entity type and the dependent word for M1 â¢ H1DW1: combination of the head word and the dependent word for M1 â¢ ET2DW2: combination of the entity type and the dependent word for M2 â¢ H2DW2: combination of the head word and the dependent word for M2 â¢ ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP â¢ ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP â¢ ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree.	0	96
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.	0	32
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	1	20
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0	130
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0	4
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0	108
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	1	51
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.	0	32
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0	215
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0	44
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	It also shows that feature-based methods dramatically outperform kernel methods.	0	162
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	0	33
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Covolution kernels for natural language.	0	204
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.	0	164
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0	60
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.	0	149
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0	130
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0	215
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0	42
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.	0	160
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes.	0	49
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	Similar to word features, three categories of phrase heads are considered: 1) the phrase heads in between are also classified into three bins: the first phrase head in between, the last phrase head in between and other phrase heads in between; 2) the phrase heads before M1 are classified into two bins: the first phrase head before and the second phrase head before; 3) the phrase heads after M2 are classified into two bins: the first phrase head after and the second phrase head after.	0	90
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0	38
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0	126
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Table 4 separately measures the performance of different relation types and major subtypes.	0	148
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.	0	24
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Extracting semantic relationships between entities is challenging.	0	1
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0	214
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Exploring Various Knowledge in Relation Extraction	0	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0	45
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0	36
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.	0	30
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0	214
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0	52
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0	31
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0	23
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	1	130
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0	6
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0	36
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.	0	160
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	It achieves 52.8 F- measure on the 24 ACE relation subtypes.	0	35
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.	0	43
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	0	40
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.	1	110
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.	1	59
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.	0	123
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0	126
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	It also includes flags indicating whether the two mentions are in the same NP/PP/VP.	0	95
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	explicit relations occur in text with explicit evidence suggesting the relationships.	0	22
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0	31
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	Head-driven statistical models for natural language parsing.	0	200
In the future, we would like to use more effective feature sets Zhou et al.(2005)	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
In the future, we would like to use more effective feature sets Zhou et al.(2005)	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
In the future, we would like to use more effective feature sets Zhou et al.(2005)	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0	42
In the future, we would like to use more effective feature sets Zhou et al.(2005)	For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important.	0	63
In the future, we would like to use more effective feature sets Zhou et al.(2005)	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
In the future, we would like to use more effective feature sets Zhou et al.(2005)	427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.	0	24
In the future, we would like to use more effective feature sets Zhou et al.(2005)	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0	53
In the future, we would like to use more effective feature sets Zhou et al.(2005)	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0	195
In the future, we would like to use more effective feature sets Zhou et al.(2005)	We use the official ACE corpus from LDC.	0	112
In the future, we would like to use more effective feature sets Zhou et al.(2005)	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	1	40
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0	60
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0	130
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	It also shows that feature-based methods dramatically outperform kernel methods.	0	162
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0	36
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	Section 3 and Section 4 describe our approach and various features employed respectively.	0	28
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.	1	155
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	â¢ Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype âROLE.Citizen-Ofâ from âROLE.	0	145
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	Two features are defined to include this information: â¢ ET1Country: the entity type of M1 when M2 is a country name â¢ CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal.	0	102
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	This trigger word list is first gathered from WordNet by checking whether a word has the semantic class âperson|â¦|relativeâ.	0	103
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0	42
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes.	0	104
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	1	18
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0	42
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0	41
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0	60
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	0	128
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0	93
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0	18
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0	3
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees.	0	85
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	0	40
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0	34
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser (Collins 1999) is employed for full parsing.	0	88
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1	17
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	It also shows that feature-based methods dramatically outperform kernel methods.	0	162
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0	215
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0	108
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0	45
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0	52
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0	197
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.	0	30
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: â¢ WM1: bag-of-words in M1 â¢ HM1: head word of M1 3 In ACE, each mention has a head annotation and an.	0	67
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0	108
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	1	40
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0	60
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0	45
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0	130
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	It also shows that feature-based methods dramatically outperform kernel methods.	0	162
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0	36
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0	108
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	In this paper, we separate the features of base phrase chunking from those of full parsing.	0	86
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0	197
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Zhou et al.(2005) tested their system on the ACE 2003 data;.	427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.	0	24
Zhou et al.(2005) tested their system on the ACE 2003 data;.	It is not surprising that the performance on the relation type âNEARâ is low because it occurs rarely in both the training and testing data.	0	150
Zhou et al.(2005) tested their system on the ACE 2003 data;.	Table 4 also indicates the low performance on the relation type âATâ although it frequently occurs in both the training and testing data.	0	156
Zhou et al.(2005) tested their system on the ACE 2003 data;.	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
Zhou et al.(2005) tested their system on the ACE 2003 data;.	3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4: Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs: fea ture bas ed 8 4.	0	184
Zhou et al.(2005) tested their system on the ACE 2003 data;.	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	0	20
Zhou et al.(2005) tested their system on the ACE 2003 data;.	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0	44
Zhou et al.(2005) tested their system on the ACE 2003 data;.	Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set.	0	129
Zhou et al.(2005) tested their system on the ACE 2003 data;.	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0	6
Zhou et al.(2005) tested their system on the ACE 2003 data;.	The testing set is held out only for final evaluation.	0	115
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	However, this paper only uses the binary-class version.	0	56
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.	0	194
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	However, The remaining words that do not have above four classes are manually classified.	0	120
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0	23
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0	214
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0	18
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	This may be due to the fact that most of relations in the ACE corpus are quite local.	0	141
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.	0	218
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	1	128
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0	214
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype âFounderâ under the type âROLEâ.	0	121
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion.	0	191
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.	0	194
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.	0	43
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0	6
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0	18
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: â¢ WM1: bag-of-words in M1 â¢ HM1: head word of M1 3 In ACE, each mention has a head annotation and an.	0	67
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	This may be due to the fact that most of relations in the ACE corpus are quite local.	0	141
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1	17
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0	130
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0	60
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	0	20
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.	0	212
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0	6
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0	108
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.	0	100
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0	215
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0	131
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.	0	132
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.	0	100
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.	0	132
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.	0	155
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0	131
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4: Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs: fea ture bas ed 8 4.	0	184
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	0	128
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head.	0	94
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0	2
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0	45
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format.	0	8
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0	52
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.	0	30
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.	0	46
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	However, The remaining words that do not have above four classes are manually classified.	0	120
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.	0	7
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0	38
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0	213
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0	17
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	1	4
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0	45
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0	144
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0	196
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Mentions have three levels: names, nomial expressions or pronouns.	0	13
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0	214
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.	0	7
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	1	193
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0	197
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0	4
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0	93
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	This category of features concerns about the information inherent only in the full parse tree.	0	97
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0	195
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0	143
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	0	51
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0	31
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees.	0	85
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	The rest of this paper is organized as follows.	0	26
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	It also shows that feature-based methods dramatically outperform kernel methods.	0	162
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	For details about SVMLight, please see http://svmlight.joachims.org/	0	57
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.	0	24
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.	0	218
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	For example, we want to determine whether a person is at a location, based on the evidence in the context.	0	15
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6.	0	29
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0	25
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important.	0	63
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	In this paper, we separate the features of base phrase chunking from those of full parsing.	0	86
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	We use the official ACE corpus from LDC.	0	112
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0	93
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0	53
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.	0	24
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	Section 3 and Section 4 describe our approach and various features employed respectively.	0	28
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0	19
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	5
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0	211
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	1	40
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0	216
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0	163
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Mentions have three levels: names, nomial expressions or pronouns.	0	13
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0	38
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.	0	7
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0	193
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0	37
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0	192
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Last, effective ways need to be explored to incorporate information embedded in the full Collins M.	0	198
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.).	1	12
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	Therefore, we must extend SVMs to multi-class (e.g. K) such as the ACE RDC task.	0	48
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	For example, we want to determine whether a person is at a location, based on the evidence in the context.	0	15
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.	0	14
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference.	0	10
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	For example, the head word of the name mention âUniversity of Michiganâ is âUniversityâ.	0	76
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0	39
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	Extracting semantic relationships between entities is challenging.	0	1
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes âBased-Inâ, âLocatedâ and âResidenceâ under the type âATâ, which are difficult even for human experts to differentiate.	0	122
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0	38
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0	100
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996).	0	18
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as "separate by V-ing," where "V" is the basic meaning of that verb (Levin, 1993).	0	67
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	In pursuing this goal, we are currently implementÂ­ ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998).	0	174
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	One of the most controversial areas has to do with polysemy.	0	11
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	TAGs have also been applied to PorÂ­ tuguese in previous work, resulting in a small Portuguese grammar (Kipper, 1994).	0	175
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0	110
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	Verbs in the slide/roll/run intersecÂ­ tion are also allowed to appear in the dative alternation (Carla slid the book to Dale, Carla slid Dale the book), in which the sense of change of location is extended to change of possession.When used intransitively with a path prepo sitional phrase, some of the manner of motion verbs can take on a sense of pseudo-motional existence, in which the subject does not actuÂ­ ally move, but has a shape that could describe a path for the verb (e.g., The stream twists through the valley).	0	127
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	The difficulty of achieving adequate handÂ­ crafted semantic representations has limited the field of natural language processing to applicaÂ­ tions that can be contained within well-defined subdomains.	0	7
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	Simultaneously, the verb was removed from the membership lists of those existing classes.	0	63
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap beÂ­ tween Levin and WordNet.	0	41
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.	0	109
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class.	0	62
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0	110
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Dorr, 1997).	0	39
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0	100
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	The adÂ­ junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.	0	70
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compoÂ­ nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.	0	73
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.	0	22
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	However, apart can also be used with other classes of verbs, including many verbs of motion.	0	114
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	For each such S = {ct, ...	0	58
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	In pursuing this goal, we are currently implementÂ­ ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998).	0	174
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.	0	44
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	Second, since the translation mapÂ­ pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve â¢ c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.	0	166
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	Whereas each WordNet synset is hierarchicalized accordÂ­ ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.	0	157
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.	0	146
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	WordNet is an onÂ­ line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each repÂ­ resenting a lexicalized concept.	0	21
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	1	4
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0	15
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0	29
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993).	0	26
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.	0	44
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect unÂ­ derlying semantic components that constrain alÂ­ lowable arguments.	0	30
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	It would be straightÂ­ forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and inÂ­ clude corresponding syntactic information.	0	117
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes.	0	74
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Investigating regular sense extensions based on intersective Levin classes	0	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed?	0	45
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	1	4
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0	156
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0	110
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0	5
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.	0	158
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	0	47
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0	6
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes.	0	74
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semanÂ­ tic components.	0	40
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	Investigating regular sense extensions based on intersective Levin classes	0	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	If only one or two verbs were shared between two classes, we assumed this might be due to hoÂ­ mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship inÂ­ volving coherent sets of verbs.	0	49
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0	6
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.	0	109
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0	15
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compoÂ­ nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.	0	73
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English.	0	161
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0	29
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect unÂ­ derlying semantic components that constrain alÂ­ lowable arguments.	0	30
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	However, the field has yet to develop a clear consensus on guidelines for a computaÂ­ tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and SanÂ­ filippo, 1993), (Lowe et al., 1997), (Dorr, 1997).	0	9
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	"Meander Verbs" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.	0	129
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	1	47
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	2.1 Ambiguities in Levin classes.	1	38
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	WordNet is an onÂ­ line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each repÂ­ resenting a lexicalized concept.	0	21
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.	0	22
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0	5
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	In pursuing this goal, we are currently implementÂ­ ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998).	0	174
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	However, apart can also be used with other classes of verbs, including many verbs of motion.	0	114
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases.	0	1
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases.	0	14
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	Whereas each WordNet synset is hierarchicalized accordÂ­ ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.	0	157
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semanÂ­ tic components.	1	40
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	The distribution of syntactic frames in which a verb can appear determines its class memberÂ­ ship.	0	27
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.	0	42
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0	29
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	The fundamental assumption is that the syntactic frames are a direct reflection of the unÂ­ derlying semantics.	0	28
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect unÂ­ derlying semantic components that constrain alÂ­ lowable arguments.	0	30
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993).	0	26
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	In pursuing this goal, we are currently implementÂ­ ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998).	0	174
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes.	0	74
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	We will be using reÂ­ sources such as dictionaries and online corpora to investigate potential additional members of our classes.	0	165
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Whereas each WordNet synset is hierarchicalized accordÂ­ ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.	0	157
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Even though the Levin verb classes are defined by their syntactic behavior, many reflect semanÂ­ tic distinctions made by WordNet, a classificaÂ­ tion hierarchy defined in terms of purely seÂ­ mantic word relations (synonyms, hypernyms, etc.).	0	97
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0	110
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0	100
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.	0	109
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semanÂ­ tic components.	0	40
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	In addition, only one verb (pull) has a WordNet sense corÂ­ responding to the change of state - separation semantic component associated with the split class.	0	112
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	On the other hand, the scribble verbs do form an intersective class with the perforÂ­ mance verbs, since paint and write are also in both classes, in addition to draw.	0	52
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0	29
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	The adÂ­ junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.	1	70
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	Where break is concerned, the only thing speciÂ­ fied is the resulting change of state where the object becomes separated into pieces.	0	36
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	In addition, only one verb (pull) has a WordNet sense corÂ­ responding to the change of state - separation semantic component associated with the split class.	0	112
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be generÂ­ ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb.	0	115
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	The only escape from this limÂ­ itation will be through the use of automated or semi-automated methods of lexical acquisiÂ­ tion.	0	8
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	Whereas each WordNet synset is hierarchicalized accordÂ­ ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.	0	157
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	Words and synsets are interrelated by means of lexical and semantic-conceptual links, respecÂ­ tively.	0	23
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the midÂ­ dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily.	0	31
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	When examining in detail the intersecÂ­ tive classes just described, which emphasize not only the individual classes, but also their relaÂ­ tion to other classes, we see a rich semantic latÂ­ tice much like WordNet.	0	98
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	The difficulty of achieving adequate handÂ­ crafted semantic representations has limited the field of natural language processing to applicaÂ­ tions that can be contained within well-defined subdomains.	0	7
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English.	0	161
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	We expect these cross-linguistic features to be useful for capturing translation generalizations between languages as discussed in the literaÂ­ ture (Palmer and Rosenzweig, 1996), (CopesÂ­ take and Sanfilippo, 1993), (Dorr, 1997).	0	173
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	"Meander Verbs" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.	0	129
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	In this experiment, we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class.	0	172
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and plaÂ­ nar (glide) in the closely related roll/run class.	0	155
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0	6
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.	0	109
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	However, we have found interesting parÂ­ allels in how Portuguese and English treat regÂ­ ular sense extensions.	0	19
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	However, there are some interesting differences in which sense extensions are allowed.	0	132
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	1	47
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	0	4
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Levin verb classes are based on an underlying latÂ­ tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.	0	108
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Investigating regular sense extensions based on intersective Levin classes	0	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0	5
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.	0	146
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.	0	158
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0	156
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0	15
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993).	0	26
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Levin verb classes are based on an underlying latÂ­ tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.	0	108
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	The only escape from this limÂ­ itation will be through the use of automated or semi-automated methods of lexical acquisiÂ­ tion.	0	8
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	It is possible for these acÂ­ tions to be performed without the end result being achieved, but where the cutting manner can still be recognized, i.e., John cut at the loaf.	0	35
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	In addition, only one verb (pull) has a WordNet sense corÂ­ responding to the change of state - separation semantic component associated with the split class.	0	112
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	This might be approxiÂ­ mated by automatically extracting the syntacÂ­ tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.	0	160
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	WordNet is an onÂ­ line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each repÂ­ resenting a lexicalized concept.	0	21
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Words and synsets are interrelated by means of lexical and semantic-conceptual links, respecÂ­ tively.	0	23
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Even though the Levin verb classes are defined by their syntactic behavior, many reflect semanÂ­ tic distinctions made by WordNet, a classificaÂ­ tion hierarchy defined in terms of purely seÂ­ mantic word relations (synonyms, hypernyms, etc.).	0	97
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	The most specific hypernym common to all the verbs in this intersective class is move, displace, which is also a hypernym for other carry verbs not in the intersection.	0	111
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simiÂ­ lar properties with the English verbs, including the causative/inchoative.	0	153
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the midÂ­ dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily.	0	31
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	However, apart can also be used with other classes of verbs, including many verbs of motion.	0	114
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	For exÂ­ ample, Eu puxei o ramo e o galho separandoos As in English, derivar and planar are not exterÂ­ nally controllable actions and thus don't take the causativejinchoative alternation common to other verbs in the roll class.	0	139
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0	100
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	First, since our experiÂ­ ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete.	0	164
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compoÂ­ nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.	0	73
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0	6
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English.	0	161
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0	5
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.	0	44
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed?	0	45
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semanÂ­ tic components.	0	40
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.	0	146
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative.	0	93
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	0	4
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	0	47
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0	156
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	1	4
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0	15
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	These fringe split verbs appear in several other interÂ­ sective classes that highlight the force aspect of their meaning.	0	71
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes.	0	74
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	0	47
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semanÂ­ tic components.	0	40
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0	29
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect unÂ­ derlying semantic components that constrain alÂ­ lowable arguments.	0	30
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0	156
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	The distribution of syntactic frames in which a verb can appear determines its class memberÂ­ ship.	0	27
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	If only one or two verbs were shared between two classes, we assumed this might be due to hoÂ­ mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship inÂ­ volving coherent sets of verbs.	0	49
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English.	0	161
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of "separating" belong to this class because of the component of force in their meaning.	0	68
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	0	4
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	Words and synsets are interrelated by means of lexical and semantic-conceptual links, respecÂ­ tively.	0	23
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0	6
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	We expect these cross-linguistic features to be useful for capturing translation generalizations between languages as discussed in the literaÂ­ ture (Palmer and Rosenzweig, 1996), (CopesÂ­ take and Sanfilippo, 1993), (Dorr, 1997).	0	173
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	"Meander Verbs" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.	0	129
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative.	0	93
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	In this experiment, we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class.	0	172
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0	15
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	0	4
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0	110
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.	0	146
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	These fringe split verbs appear in several other interÂ­ sective classes that highlight the force aspect of their meaning.	0	71
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Levin verb classes are based on an underlying latÂ­ tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.	0	108
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0	156
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semanÂ­ tic components.	0	40
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0	5
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, ..â¢ ,c} such that S C S' and v E ci n ... n dm (subset criterion).	0	60
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	1. Enumerate all sets S = {c1, ...	0	54
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	Second, since the translation mapÂ­ pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve â¢ c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.	0	166
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	The algorithm we used is given in Figure 1.	0	53
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows.	0	61
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	3.1 Using intersective Levin classes to.	0	64
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	1.	0	75
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	2.1 Ambiguities in Levin classes.	0	38
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	4.1 Similar sense extensions.	0	133
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	Table 1: Portuguese carry verbs with their alÂ­ ternations (I pulled the twig and the branch apart ) , and Ele empurrou contra a parede (He pushed against the walQ.	0	142
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	The distribution of syntactic frames in which a verb can appear determines its class memberÂ­ ship.	0	27
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	In pursuing this goal, we are currently implementÂ­ ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998).	0	174
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as "separate by V-ing," where "V" is the basic meaning of that verb (Levin, 1993).	0	67
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993).	0	26
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0	29
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	SimÂ­ ilarly, draw and yank can be viewed as carry verbs alÂ­ though they are not listed as such.	0	80
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	It would be straightÂ­ forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and inÂ­ clude corresponding syntactic information.	0	117
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	What constitutes a clear sepaÂ­ ration into senses for any one verb, and how can these senses be computationally characterized and distinguished?	0	12
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes.	0	74
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.	0	42
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	Two current approaches to English verb classiÂ­ fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).	1	20
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0	3
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0	5
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0	156
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	In this experiment, we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class.	0	172
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	When examining in detail the intersecÂ­ tive classes just described, which emphasize not only the individual classes, but also their relaÂ­ tion to other classes, we see a rich semantic latÂ­ tice much like WordNet.	0	98
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	0	47
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0	110
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	Whereas each WordNet synset is hierarchicalized accordÂ­ ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.	0	157
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.	0	109
