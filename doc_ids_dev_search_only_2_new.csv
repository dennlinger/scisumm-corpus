query	doc_id	label	similiarity
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	[21, 5, 3, 0, 217, 15, 39, 220, 17, 37]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.46471619606018066, 0.22344861924648285, 0.34108367562294006, 0.5738793015480042, 0.18606698513031006, 0.2381070852279663, 0.19594237208366394, 0.23349636793136597, 0.1219618022441864, 0.3037256896495819]
'Nivre' is Nivre's swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser.org).	[119, 13, 17, 18, 99, 101, 97, 6, 40, 86]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.41142553091049194, 0.5190408229827881, 0.6051956415176392, 0.4063774049282074, 0.1882210224866867, 0.37455838918685913, 0.29415708780288696, 0.1787065863609314, 0.3775557577610016, 0.06702471524477005]
(Alshawi et al, 2000) represents each production in parallel dependency trees as a finite-state transducer.	[23, 0, 16, 73, 20, 69, 37, 123, 57, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7464934587478638, 0.4483489990234375, 0.18136928975582123, 0.32126346230506897, 0.45483025908470154, 0.3275604248046875, 0.22228704392910004, 0.22329992055892944, 0.37059542536735535, 0.20799611508846283]
(Breidt, 1995) has evaluated the usefulness of the Point-wise Mutual Information measure (as suggested by (Church and Hanks, 1989)) for the extraction of V-N collocations from German text corpora.	[25, 0, 28, 29, 41, 42, 135, 138, 83, 87]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31445324420928955, 0.7268049716949463, 0.10717534273862839, 0.1390039026737213, 0.21250058710575104, 0.16374501585960388, 0.1625748574733734, 0.11251981556415558, 0.21412628889083862, 0.08226751536130905]
(Cardie and Pierce, 1998) present an approach to chunking based on a mixture of finite state and context-free techniques.	[178, 3, 4, 42, 19, 150, 168, 6, 169, 66]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5746809840202332, 0.7176923751831055, 0.14463619887828827, 0.16613377630710602, 0.0705338791012764, 0.12328674644231796, 0.10433349013328552, 0.07314514368772507, 0.13093242049217224, 0.1360284984111786]
(Cardie and Pierce, 1998) store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data.	[3, 169, 26, 59, 60, 126, 25, 23, 119, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30194368958473206, 0.21155951917171478, 0.15711387991905212, 0.12782002985477448, 0.2449931651353836, 0.11117152124643326, 0.1815577894449234, 0.16477711498737335, 0.24783523380756378, 0.05181879550218582]
(Carletta 1996) is another method of comparing inter-annotator agreement.	[76, 74, 47, 78, 0, 58, 59, 45, 1, 4]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4541233777999878, 0.056443314999341965, 0.1287723034620285, 0.14207959175109863, 0.5184791088104248, 0.09950070083141327, 0.09607167541980743, 0.12862756848335266, 0.06583241373300552, 0.06583241373300552]
(Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications.	[0, 8, 63, 4, 1, 38, 6, 96, 18, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5284271836280823, 0.1839524656534195, 0.0944114699959755, 0.10216084867715836, 0.07374835759401321, 0.08602406084537506, 0.16181756556034088, 0.14837335050106049, 0.053212739527225494, 0.11077356338500977]
(Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round.	[85, 128, 91, 127, 60, 18, 1, 86, 27, 15]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8348677754402161, 0.517009973526001, 0.3113826811313629, 0.21683546900749207, 0.3270038962364197, 0.08219943195581436, 0.18146184086799622, 0.3913329243659973, 0.06298359483480453, 0.11375671625137329]
(DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases.	[8, 52, 2, 19, 11, 12, 65, 4, 3, 1]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7938258051872253, 0.5648128390312195, 0.1582781970500946, 0.12807178497314453, 0.09835149347782135, 0.10613105446100235, 0.17387312650680542, 0.059714365750551224, 0.10831063985824585, 0.0914430320262909]
(Fader et al (2011) found that this set covers 69% of their corpus).	[12, 125, 109, 127, 206, 37, 64, 83, 59, 107]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7411335706710815, 0.5272760987281799, 0.6896554827690125, 0.09793557971715927, 0.06164175271987915, 0.21185526251792908, 0.1386999487876892, 0.09271172434091568, 0.086771659553051, 0.09580574184656143]
(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense.	[7, 16, 93, 129, 89, 160, 94, 1, 171, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33179402351379395, 0.14025452733039856, 0.39352232217788696, 0.163284569978714, 0.28682324290275574, 0.157670259475708, 0.1188015267252922, 0.12232287228107452, 0.10421739518642426, 0.3019273579120636]
(Goldberg and Zhu, 2006) adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features.	[1, 84, 0, 22, 9, 157, 120, 148, 5, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6131930351257324, 0.45708775520324707, 0.3528063893318176, 0.36012351512908936, 0.1778162121772766, 0.20068717002868652, 0.3112950921058655, 0.3953663408756256, 0.2097158432006836, 0.18224117159843445]
(Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set.	[20, 185, 144, 19, 28, 60, 196, 4, 109, 83]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5966277718544006, 0.5960671305656433, 0.2743898034095764, 0.23580695688724518, 0.32572147250175476, 0.27800750732421875, 0.2228407859802246, 0.20681282877922058, 0.11203428357839584, 0.08659261465072632]
(Han and Baldwin, 2011) developed classifiers for detecting the ill-formed word sand generated corrections based on the morphophonemic similarity.	[3, 26, 208, 83, 196, 86, 85, 212, 126, 131]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8109555840492249, 0.31032291054725647, 0.3413875997066498, 0.2944158911705017, 0.22536566853523254, 0.19444280862808228, 0.16518642008304596, 0.41359302401542664, 0.165263831615448, 0.07291899621486664]
(Johnson et al, 2007) has presented a technique for pruning the phrase table in a phrase based SMT system using Fisher's exact test.	[61, 117, 24, 1, 80, 79, 4, 102, 9, 203]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6830728054046631, 0.6582714915275574, 0.09584208577871323, 0.26764827966690063, 0.33382052183151245, 0.34892594814300537, 0.08766117691993713, 0.21506176888942719, 0.09647393226623535, 0.11149394512176514]
(Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state.	[9, 2, 29, 27, 150, 72, 22, 40, 35, 14]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6430819630622864, 0.47076407074928284, 0.2319951355457306, 0.1754779815673828, 0.3177800476551056, 0.37575563788414, 0.15472956001758575, 0.20053277909755707, 0.3100983202457428, 0.2015378773212433]
(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework.	[47, 49, 25, 65, 51, 55, 98, 23, 32, 63]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25076165795326233, 0.5289599299430847, 0.11573958396911621, 0.24989227950572968, 0.28877362608909607, 0.15878824889659882, 0.13063959777355194, 0.09199031442403793, 0.06545841693878174, 0.19233918190002441]
(Lin, 1997) also tries to solve word ambiguity by adding syntactic dependency as context.	[0, 39, 44, 47, 23, 151, 40, 33, 136, 5]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.8029528260231018, 0.7627302408218384, 0.3209940791130066, 0.3917976915836334, 0.10883055627346039, 0.23455236852169037, 0.2565319240093231, 0.09349283576011658, 0.3928269147872925, 0.5887619256973267]
(Mann and Yarowsky, 2003) first extract biographical information, such as birthdates, birthplaces ,occupations, and so on.	[31, 63, 19, 155, 41, 24, 14, 154, 26, 3]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.5748765468597412, 0.4812535345554352, 0.48622795939445496, 0.48352012038230896, 0.07283938676118851, 0.511664628982544, 0.24781569838523865, 0.10772322118282318, 0.07966014742851257, 0.06229204684495926]
(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query.	[100, 10, 7, 13, 90, 107, 108, 4, 104, 88]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6092942357063293, 0.7093169093132019, 0.16438227891921997, 0.3907211422920227, 0.06259289383888245, 0.06020237132906914, 0.19004298746585846, 0.06628688424825668, 0.12926983833312988, 0.37981322407722473]
(Pado et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic roles.	[74, 20, 16, 3, 129, 39, 44, 80, 12, 100]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46249455213546753, 0.5169990658760071, 0.23079636693000793, 0.20826558768749237, 0.18355253338813782, 0.10820130258798599, 0.12845776975154877, 0.05258141830563545, 0.13898438215255737, 0.07452885061502457]
(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster.	[64, 0, 42, 20, 3, 25, 171, 173, 60, 1]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.8175738453865051, 0.7243980765342712, 0.7271448969841003, 0.6992624998092651, 0.4481988847255707, 0.2212264984846115, 0.2566758096218109, 0.2746591866016388, 0.22249779105186462, 0.11014585942029953]
(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB.	[51, 46, 43, 148, 62, 39, 65, 222, 34, 216]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5230215191841125, 0.3244350552558899, 0.3577604293823242, 0.12910296022891998, 0.052758850157260895, 0.18415555357933044, 0.2063124030828476, 0.1249038428068161, 0.07894773781299591, 0.20561650395393372]
(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data.	[1, 68, 175, 4, 7, 136, 43, 164, 166, 11]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7867410778999329, 0.8154518008232117, 0.39957383275032043, 0.7564628720283508, 0.2982521057128906, 0.13243958353996277, 0.30727577209472656, 0.1571521759033203, 0.2126276195049286, 0.06890716403722763]
(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data.	[79, 115, 96, 91, 103, 81, 109, 122, 45, 102]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.813758134841919, 0.8308295607566833, 0.7917028069496155, 0.6208106279373169, 0.667512059211731, 0.2605736553668976, 0.3450302481651306, 0.3803733289241791, 0.07685776799917221, 0.15873849391937256]
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	[23, 22, 268, 4, 254, 11, 261, 1, 211, 3]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4797686040401459, 0.7282605767250061, 0.37105950713157654, 0.334966242313385, 0.4243301451206207, 0.24003645777702332, 0.48184630274772644, 0.10909444838762283, 0.16524362564086914, 0.19306603074073792]
(Taskar et al, 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.	[23, 14, 89, 29, 16, 10, 28, 43, 90, 35]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8545280694961548, 0.8399882316589355, 0.7572198510169983, 0.2101966291666031, 0.30548521876335144, 0.14154955744743347, 0.30829402804374695, 0.05945005267858505, 0.15936508774757385, 0.08802502602338791]
(Tjong Kim Sang and Buchholz, 2000) Unlike a parse tree, a set of syntactic chunks has no hierarchical information on how sequences of words relate to each other.	[9, 2, 65, 8, 94, 57, 4, 107, 117, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3869306147098541, 0.0812620297074318, 0.4163318872451782, 0.24296827614307404, 0.06866385042667389, 0.13926656544208527, 0.1446644365787506, 0.15550802648067474, 0.12526950240135193, 0.126902773976326]
(Toutanova and Moore, 2002) improved the string to string edits model by modeling pronunciation similarities between words.	[11, 0, 3, 19, 60, 45, 47, 18, 169, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7122872471809387, 0.4825513958930969, 0.38078299164772034, 0.18516027927398682, 0.05765363201498985, 0.14597837626934052, 0.10871005803346634, 0.2666856348514557, 0.08475816994905472, 0.12830017507076263]
(Turney, 2002) worked on product reviews.	[54, 7, 38, 30, 118, 80, 42, 73, 36, 31]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5677497982978821, 0.4065089523792267, 0.1820729821920395, 0.26274237036705017, 0.29874080419540405, 0.23030643165111542, 0.18919357657432556, 0.29210972785949707, 0.3362017571926117, 0.33263346552848816]
(Vieira and Poesio, 2000) is run, which attempts to find an head-matching antecedent within a given window and taking premodification into account.	[123, 264, 121, 95, 447, 236, 225, 118, 330, 132]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8436382412910461, 0.25467830896377563, 0.6472824215888977, 0.3620564639568329, 0.2317742258310318, 0.4922521114349365, 0.3158336877822876, 0.08463479578495026, 0.23525623977184296, 0.08562770485877991]
(Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement.	[13, 92, 33, 53, 59, 12, 21, 44, 106, 87]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7944816946983337, 0.06509604305028915, 0.2697724997997284, 0.08356504887342453, 0.2643600404262543, 0.3877459764480591, 0.22561132907867432, 0.07221537828445435, 0.06235722452402115, 0.0653127133846283]
(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback.	[1, 31, 8, 22, 5, 43, 44, 68, 132, 73]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7539580464363098, 0.3835699260234833, 0.11930403113365173, 0.0639629065990448, 0.05818065628409386, 0.21501798927783966, 0.10859718918800354, 0.07934124022722244, 0.07916256785392761, 0.05610780417919159]
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	[16, 15, 154, 17, 3, 70, 0, 142, 52, 146]	[1, 1, 1, 0, 0, 0, 0, 1, 0, 0]	[0.8232477307319641, 0.5931691527366638, 0.6283933520317078, 0.27939361333847046, 0.4145149886608124, 0.242089182138443, 0.3399619162082672, 0.5266230702400208, 0.1379910409450531, 0.0881669819355011]
5 articles from FlyBase (the same data were used by Medlock and Briscoe (2007) for evaluating sentence-level hedge classifiers) and 4 articles from the open access BMC Bioinformatics website were downloaded and annotated for negations, uncertainty and their scopes.	[44, 89, 78, 40, 90, 27, 51, 38, 81, 42]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.369633287191391, 0.3129427433013916, 0.09419798105955124, 0.11624707281589508, 0.10556274652481079, 0.09558691084384918, 0.05621220916509628, 0.182698592543602, 0.05859808996319771, 0.06935753673315048]
5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011).	[153, 28, 73, 150, 100, 146, 93, 177, 5, 64]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8314344882965088, 0.24616944789886475, 0.48114240169525146, 0.4603498876094818, 0.26038211584091187, 0.4789307415485382, 0.09805964678525925, 0.14279714226722717, 0.4048084020614624, 0.17660947144031525]
6.2.3 Hybrid Approaches The methods of Gouws et al2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionary based approaches; this is largely caused by lexical variant detection errors.	[168, 172, 161, 196, 171, 198, 129, 45, 28, 25]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.789671003818512, 0.6607043743133545, 0.4325663149356842, 0.4448874890804291, 0.5254316926002502, 0.08808150142431259, 0.08508562296628952, 0.11601515859365463, 0.061877742409706116, 0.17903171479701996]
A Feature-based TAG (FTAG, (Vijay-Shanker and Joshi, 1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction.	[20, 48, 11, 59, 52, 1, 18, 67, 108, 159]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7868362069129944, 0.23591049015522003, 0.319341778755188, 0.21347679197788239, 0.3881961405277252, 0.16531231999397278, 0.19817349314689636, 0.13917715847492218, 0.26752975583076477, 0.15369603037834167]
A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010).	[5, 10, 418, 17, 51, 24, 116, 91, 26, 129]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31970566511154175, 0.31970566511154175, 0.16144396364688873, 0.2574387192726135, 0.31485626101493835, 0.4120493233203888, 0.31609439849853516, 0.19665439426898956, 0.3158436417579651, 0.2434070110321045]
A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007).	[142, 167, 151, 18, 17, 34, 125, 26, 9, 161]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7953874468803406, 0.21745187044143677, 0.718679666519165, 0.2108573317527771, 0.2608855068683624, 0.17440134286880493, 0.0812913179397583, 0.09133335947990417, 0.055029772222042084, 0.21372725069522858]
A first step towards a more comprehensive notion of entailment was taken with RTE-3 (Giampiccolo et al,2007), when paragraph-length texts were first included and constituted 17% of the texts in the test set.	[23, 44, 16, 42, 14, 43, 75, 121, 41, 30]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5808752179145813, 0.8310546875, 0.2227410078048706, 0.5283549427986145, 0.15403608977794647, 0.264658659696579, 0.09376256912946701, 0.11330858618021011, 0.15863604843616486, 0.05184178426861763]
A more recent instance is D-Tree Substitution Grammars (DSG) (Rambow et al, 1995), where the derivations are also interpreted as dependency relations.	[0, 164, 74, 17, 184, 22, 11, 12, 240, 92]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8317828178405762, 0.4349727928638458, 0.34946754574775696, 0.18454404175281525, 0.05404684320092201, 0.11573895812034607, 0.123838409781456, 0.05522461235523224, 0.10105199366807938, 0.18454404175281525]
A natural way to go about update summarization would be extracting temporal tags (dates, elapsed times, temporal expressions...) (Mani and Wilson, 2000) or to automatically construct the timeline from documents (Swan and Allan, 2000).	[10, 24, 4, 39, 58, 38, 125, 30, 13, 137]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6893355250358582, 0.3384573757648468, 0.32185375690460205, 0.2564646303653717, 0.30680006742477417, 0.10805562138557434, 0.06010829657316208, 0.1501646339893341, 0.06458167731761932, 0.3769229054450989]
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	[25, 18, 333, 262, 9, 53, 26, 215, 263, 212]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5142989158630371, 0.5029511451721191, 0.5250496864318848, 0.4367538094520569, 0.32791417837142944, 0.4872634708881378, 0.43129754066467285, 0.16527985036373138, 0.2836129367351532, 0.22157280147075653]
A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al, 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al, 2000).	[19, 47, 51, 117, 14, 35, 24, 18, 49, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26780083775520325, 0.18159978091716766, 0.36117681860923767, 0.25952860713005066, 0.2595674991607666, 0.4564041495323181, 0.49077895283699036, 0.06852783262729645, 0.0807306170463562, 0.2995244264602661]
A perceptron algorithm gives 97.11% (Collins, 2002).	[12, 15, 3, 0, 9, 10, 13, 8, 4, 1]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4036782383918762, 0.8410942554473877, 0.28495675325393677, 0.330800861120224, 0.3782465159893036, 0.18268385529518127, 0.08057662099599838, 0.061451785266399384, 0.0656786784529686, 0.10719995200634003]
A perceptron like algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004).	[48, 51, 137, 208, 104, 57, 14, 118, 111, 200]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7676784992218018, 0.3009299039840698, 0.43295952677726746, 0.43953776359558105, 0.20254381000995636, 0.11317455768585205, 0.09410863369703293, 0.11636607348918915, 0.16464006900787354, 0.1098252460360527]
A primary question is whether such lexicons improve performance over a translate-to-English strategy (Banea et al, 2008).	[6, 141, 119, 107, 149, 9, 22, 18, 47, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4733330011367798, 0.12230987101793289, 0.06458582729101181, 0.05350590497255325, 0.15385280549526215, 0.05443080887198448, 0.12563484907150269, 0.08452703058719635, 0.07165894657373428, 0.05958867818117142]
A serious problem when working with bridging reference sis the fact that subjects, when asked for judgments about bridging references in general, have a great deal of difficulty in agreeing on which expressions in the corpus are bridging references, and what their anchors are (Poesio and Vieira, 1998).	[357, 261, 102, 360, 289, 285, 301, 379, 288, 326]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38731127977371216, 0.33512789011001587, 0.3473913371562958, 0.25153598189353943, 0.369927316904068, 0.35270509123802185, 0.18352431058883667, 0.12113524228334427, 0.21440088748931885, 0.1515468806028366]
A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al, 2001).	[0, 115, 114, 4, 15, 10, 1, 51, 62, 59]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7211556434631348, 0.47673678398132324, 0.3215900659561157, 0.4558885395526886, 0.15591959655284882, 0.5158716440200806, 0.25973793864250183, 0.10587860643863678, 0.11492712050676346, 0.17305126786231995]
A spelling-based model that directly maps English letter sequences into Arabic letters was developed by Al-Onaizan and Knight (2002).	[71, 47, 46, 41, 79, 81, 86, 120, 39, 44]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8499635457992554, 0.8493292927742004, 0.19260656833648682, 0.6779935956001282, 0.22043298184871674, 0.36552155017852783, 0.1905805468559265, 0.27090540528297424, 0.24046240746974945, 0.16620561480522156]
A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation.	[24, 181, 75, 0, 58, 45, 39, 126, 55, 28]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7931777834892273, 0.7961955666542053, 0.6335246562957764, 0.4993939697742462, 0.2934236526489258, 0.22767727077007294, 0.2569439113140106, 0.2634130120277405, 0.14129266142845154, 0.08580159395933151]
A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall.	[0, 186, 1, 162, 21, 8, 170, 19, 106, 113]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.805173933506012, 0.25535082817077637, 0.2103932946920395, 0.19182941317558289, 0.20860156416893005, 0.12546196579933167, 0.22250887751579285, 0.21660061180591583, 0.29379794001579285, 0.16514065861701965]
A third, clean-up pass is then performed to partially disambiguate the identified WordNet glosses with Brill's part-of-speech tagger (Brill, 1995), which performs with up to 95% accuracy, and eliminates errors introduced into the list by part-of-speech ambiguity of some words acquired in pass 1 and from the seed list.	[36, 122, 16, 28, 164, 193, 0, 49, 51, 225]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8073096871376038, 0.18239019811153412, 0.19137896597385406, 0.1098347082734108, 0.18551240861415863, 0.34544840455055237, 0.2419051229953766, 0.10463616997003555, 0.226053386926651, 0.35639119148254395]
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	[36, 33, 84, 173, 53, 56, 187, 5, 30, 44]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32762718200683594, 0.2852958142757416, 0.2934349775314331, 0.2697628140449524, 0.223543182015419, 0.09444807469844818, 0.24949657917022705, 0.11334875971078873, 0.07943478226661682, 0.20165038108825684]
A variety of methods have been developed for semantic role labeling with reasonably good performance (F 1 measures in the low 80s on standard test collections for English; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al, 2007) for an overview of the state-of-the-art).	[0, 5, 15, 100, 93, 51, 14, 21, 52, 4]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8325579762458801, 0.36854690313339233, 0.11585026234388351, 0.41834279894828796, 0.1800791323184967, 0.10764873027801514, 0.07965908199548721, 0.062376074492931366, 0.09193520247936249, 0.06910227239131927]
A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines.	[20, 5, 135, 3, 10, 30, 1, 111, 140, 79]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4080909788608551, 0.29963287711143494, 0.055450644344091415, 0.17777393758296967, 0.10900525748729706, 0.154881551861763, 0.12146415561437607, 0.05536873638629913, 0.07534171640872955, 0.12596310675144196]
Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the error rate of either hypothesis.	[47, 56, 64, 67, 57, 62, 44, 139, 123, 102]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7030316591262817, 0.7566418051719666, 0.3994845449924469, 0.6847901344299316, 0.2786387801170349, 0.3015859127044678, 0.2188415229320526, 0.28697797656059265, 0.2340000569820404, 0.07281219959259033]
According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found.	[173, 169, 177, 175, 176, 36, 174, 47, 105, 179]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.8456089496612549, 0.7193140983581543, 0.5285317301750183, 0.4235488474369049, 0.6494659185409546, 0.24170519411563873, 0.25434544682502747, 0.22475774586200714, 0.10885386168956757, 0.22734913229942322]
Accordingly, the performance results given in (Verhagen et al, 2007) are reported using metrics of precision, recall and F-measure.	[75, 78, 58, 103, 74, 104, 119, 122, 109, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21087104082107544, 0.48319515585899353, 0.3501432240009308, 0.22517867386341095, 0.15219391882419586, 0.07542206346988678, 0.10741353780031204, 0.08521467447280884, 0.14679357409477234, 0.06912210583686829]
Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification.	[1, 0, 7, 9, 5, 35, 168, 41, 4, 135]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.79789137840271, 0.8053197860717773, 0.2941409945487976, 0.330448180437088, 0.3533632457256317, 0.34837043285369873, 0.15775066614151, 0.33149442076683044, 0.08661665767431259, 0.1712135225534439]
Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported.	[155, 64, 144, 119, 143, 77, 63, 34, 26, 140]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2844793498516083, 0.12749382853507996, 0.18155375123023987, 0.18265095353126526, 0.06748854368925095, 0.06923336535692215, 0.14394988119602203, 0.07621061056852341, 0.11620256304740906, 0.10945074260234833]
Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hy ponyms (Hearst, 1992) ,meronyms (Girju, 2003), synonyms (Lin et al, 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al, 2003).	[182, 119, 29, 36, 86, 178, 148, 18, 75, 33]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4479357898235321, 0.31586697697639465, 0.1354694962501526, 0.09398192167282104, 0.09617622196674347, 0.16411088407039642, 0.12526880204677582, 0.13484688103199005, 0.09091544896364212, 0.2661997079849243]
Additionally, it may be possible to refine the classications automatically using methods such as those described in (Wiebe et al, 1999).	[99, 75, 128, 35, 3, 23, 30, 17, 1, 126]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6559824347496033, 0.7925837635993958, 0.10760913044214249, 0.37999311089515686, 0.28103572130203247, 0.1655004620552063, 0.10855378210544586, 0.06197834387421608, 0.0794830173254013, 0.3273804783821106]
Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4.	[29, 87, 10, 7, 88, 108, 1, 48, 4, 106]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.6547872424125671, 0.7402544617652893, 0.6915595531463623, 0.34099844098091125, 0.6400183439254761, 0.30202555656433105, 0.4761982858181, 0.4737287163734436, 0.38145482540130615, 0.2608434855937958]
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program.	[81, 162, 147, 4, 16, 14, 2, 26, 139, 131]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.632115364074707, 0.16482818126678467, 0.28193333745002747, 0.1500128209590912, 0.19590936601161957, 0.2649106979370117, 0.11793962866067886, 0.2846967875957489, 0.2749594748020172, 0.2129496932029724]
Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011).	[49, 0, 23, 13, 7, 6, 15, 19, 3, 50]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5516088604927063, 0.6633913516998291, 0.15953846275806427, 0.2177964448928833, 0.36312025785446167, 0.3831334710121155, 0.4332934617996216, 0.156340092420578, 0.1163814440369606, 0.21894223988056183]
Additionally, we present Joshua's implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning.	[1, 92, 21, 16, 79, 66, 2, 74, 15, 61]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5758373737335205, 0.4568450152873993, 0.2829468846321106, 0.24738827347755432, 0.45088687539100647, 0.23911555111408234, 0.3176553547382355, 0.1737673580646515, 0.12771324813365936, 0.1444273293018341]
After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009).	[31, 109, 62, 17, 69, 79, 29, 40, 23, 34]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5591619610786438, 0.23517906665802002, 0.21416613459587097, 0.09355199337005615, 0.20398789644241333, 0.1151033267378807, 0.17360812425613403, 0.11815577000379562, 0.11868292838335037, 0.07933027297258377]
Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al, 2004) achieves an f score of 79.6%.	[33, 157, 34, 4, 179, 183, 121, 71, 184, 95]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5983189344406128, 0.8287670016288757, 0.34106582403182983, 0.2888734042644501, 0.27600419521331787, 0.08800821751356125, 0.22790877521038055, 0.2565307319164276, 0.1016843393445015, 0.24899229407310486]
All alignments that occurred in the first two sentences of each paragraph were marked as hard boundaries for the Gale and Church (1993) program as provided in their paper.	[277, 301, 44, 101, 102, 3, 17, 45, 119, 88]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7291513681411743, 0.4105503559112549, 0.4168626070022583, 0.6830576658248901, 0.4556148648262024, 0.4137630760669708, 0.4137630760669708, 0.12712182104587555, 0.12453319877386093, 0.37737587094306946]
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	[63, 56, 51, 64, 229, 96, 44, 65, 335, 254]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8353456854820251, 0.49852272868156433, 0.4672152101993561, 0.5100327730178833, 0.219954714179039, 0.3259718418121338, 0.06972304731607437, 0.2885218858718872, 0.13815172016620636, 0.10573945939540863]
All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction).	[28, 27, 9, 45, 74, 3, 87, 25, 50, 47]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4265936017036438, 0.5915570259094238, 0.13749249279499054, 0.06563176214694977, 0.25345680117607117, 0.1263851523399353, 0.06251001358032227, 0.09214863181114197, 0.30868858098983765, 0.0614127553999424]
All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sangand De Meulder, 2003).	[0, 9, 11, 64, 123, 134, 55, 22, 87, 60]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.722501277923584, 0.3373878598213196, 0.4163878858089447, 0.26368990540504456, 0.2771044671535492, 0.2066914439201355, 0.22684697806835175, 0.3107803761959076, 0.19672979414463043, 0.1727701872587204]
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	[224, 140, 18, 133, 186, 130, 223, 0, 131, 211]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2801167964935303, 0.24861681461334229, 0.1442040503025055, 0.17420852184295654, 0.3496151864528656, 0.2556958496570587, 0.3712352216243744, 0.42512357234954834, 0.19680602848529816, 0.36192211508750916]
All sentences are aligned using a tool based on the Gale and Church (1991) algorithm.	[49, 71, 137, 102, 2, 159, 46, 43, 41, 126]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6229400634765625, 0.16566789150238037, 0.11705153435468674, 0.41443532705307007, 0.24803473055362701, 0.24823132157325745, 0.14549583196640015, 0.17015552520751953, 0.1035895124077797, 0.05650540068745613]
Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels.	[4, 111, 5, 22, 14, 35, 20, 34, 11, 26]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6432792544364929, 0.3661770224571228, 0.11399131268262863, 0.2402646541595459, 0.224975124001503, 0.14405500888824463, 0.23090843856334686, 0.12780100107192993, 0.09406451880931854, 0.05410751700401306]
Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al, 2009).	[2, 0, 9, 40, 113, 24, 6, 125, 127, 44]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28526586294174194, 0.6965652108192444, 0.2040562927722931, 0.19518794119358063, 0.3338823914527893, 0.11490567773580551, 0.2011575996875763, 0.1208612471818924, 0.05709962919354439, 0.2495039403438568]
Also, as with any generative model, it should be easy to improve the parser's accuracy with discriminative reranking, such as discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without the introduction of any additional linguistic features.	[138, 167, 145, 16, 65, 141, 160, 143, 7, 165]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.569251298904419, 0.3313754200935364, 0.3055940866470337, 0.3591204285621643, 0.0727047473192215, 0.3435295522212982, 0.16978323459625244, 0.09649837017059326, 0.3160727918148041, 0.4013035297393799]
Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007).	[48, 22, 127, 9, 100, 131, 142, 85, 69, 53]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47277694940567017, 0.344568133354187, 0.1975928395986557, 0.09546374529600143, 0.32701700925827026, 0.200697660446167, 0.12187343835830688, 0.31977519392967224, 0.09741045534610748, 0.1951383650302887]
Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009).	[220, 225, 136, 62, 69, 49, 37, 213, 20, 58]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09833718836307526, 0.09695829451084137, 0.22016511857509613, 0.17681121826171875, 0.08335884660482407, 0.10224057734012604, 0.06589667499065399, 0.31572315096855164, 0.06347005069255829, 0.23485323786735535]
Also, we ignore the ambiguity between particles and adverbs, which is the principal reason for our evaluation being much higher than that reported by McCarthy et al (2003).	[41, 57, 59, 21, 47, 50, 12, 51, 77, 138]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7632777094841003, 0.5494251251220703, 0.14476367831230164, 0.18840396404266357, 0.06156132370233536, 0.6189455389976501, 0.07348979264497757, 0.21211597323417664, 0.29302191734313965, 0.2116573452949524]
Although Morante and Daelemans (2009) reported the performance of 95.8%-98.7% on negation signal finding, it lowers the performance of negation scope finding by about 7.29%-16.52% in PCS measure.	[9, 165, 160, 150, 1, 140, 166, 177, 173, 142]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5590986013412476, 0.31561145186424255, 0.3441644608974457, 0.28826624155044556, 0.37489789724349976, 0.28500255942344666, 0.175009623169899, 0.09188113361597061, 0.1912771314382553, 0.1665109395980835]
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004).	[85, 11, 1, 0, 125, 13, 12, 107, 141, 162]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4492862820625305, 0.2033778578042984, 0.4038650393486023, 0.48351550102233887, 0.1565014123916626, 0.09619605541229248, 0.2632690370082855, 0.09447892755270004, 0.0589601993560791, 0.082340307533741]
Although hybrid approaches, such as dependency grammars augmented with phrase-structure information (Alshawi et al., 2000), can do re-ordering easily.	[69, 67, 107, 34, 62, 136, 169, 176, 65, 151]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6402735710144043, 0.1513514667749405, 0.13634344935417175, 0.13211722671985626, 0.10870847851037979, 0.06221619248390198, 0.05881547927856445, 0.05135669186711311, 0.05172840133309364, 0.04874878376722336]
Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al, 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al's (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more complicated algorithms (variational EM and MBRdecoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data).	[9, 8, 20, 28, 146, 179, 163, 167, 155, 103]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5382024049758911, 0.2747116982936859, 0.24391917884349823, 0.24041278660297394, 0.35356515645980835, 0.2536025941371918, 0.23122969269752502, 0.16015024483203888, 0.16666436195373535, 0.24465279281139374]
Although log identifies collocations much better than competing approaches (Dunning 1993) in terms of its recall, it suffers from its relatively poor precision rates.	[129, 7, 8, 17, 38, 16, 46, 24, 22, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2684849202632904, 0.2572566270828247, 0.3208516836166382, 0.3208516836166382, 0.2520833909511566, 0.2572566270828247, 0.18395505845546722, 0.36507365107536316, 0.09164609014987946, 0.0813136100769043]
Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556).	[105, 93, 262, 8, 73, 58, 103, 223, 410, 271]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7155892252922058, 0.5565913915634155, 0.34707337617874146, 0.24703557789325714, 0.3371043801307678, 0.31882572174072266, 0.18422308564186096, 0.25531262159347534, 0.21197889745235443, 0.22028380632400513]
Although our train-test splits may differ slightly, the best B-Cubed F1 score reported in Ng (2005) is 69.3%, which is considerably lower than the 79.3% obtained with our method.	[21, 103, 109, 169, 166, 24, 170, 40, 135, 143]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06694433838129044, 0.08926074951887131, 0.23394909501075745, 0.06159878894686699, 0.07379674166440964, 0.06956259161233902, 0.06736244261264801, 0.05452108383178711, 0.06583289802074432, 0.23801898956298828]
Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete an notations using gradient ascent iterations (Sha and Pereira, 2003).	[54, 39, 43, 45, 106, 34, 125, 117, 22, 108]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3709327280521393, 0.11946374922990799, 0.15957362949848175, 0.20028728246688843, 0.3923347294330597, 0.061507679522037506, 0.08536127209663391, 0.05899021029472351, 0.05922810360789299, 0.39519715309143066]
Although, there are asymmetric measures such as the Monge-Elkan measure (1996) and the measure proposed by Corley and Mihalcea (Corley and Mihalcea, 2005), they are outnumbered by the symmetric measures.	[11, 60, 20, 84, 4, 0, 1, 14, 72, 12]	[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.35212478041648865, 0.5276718735694885, 0.32778534293174744, 0.2415711134672165, 0.10794903337955475, 0.4108496308326721, 0.1326378583908081, 0.09631123393774033, 0.5532751679420471, 0.2055320143699646]
Among the few research efforts in this direction, Wilson et al (2009) use a list of modal words.	[534, 480, 159, 156, 94, 359, 104, 494, 493, 76]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.335591584444046, 0.8006094694137573, 0.42994827032089233, 0.32178518176078796, 0.10669145733118057, 0.06258565932512283, 0.05982065200805664, 0.05855870619416237, 0.11901345103979111, 0.07770265638828278]
Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations.	[104, 33, 39, 6, 132, 84, 70, 8, 67, 37]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0821700245141983, 0.09344064444303513, 0.3375405967235565, 0.058301135897636414, 0.2068593055009842, 0.17010092735290527, 0.1747787594795227, 0.29389622807502747, 0.325825959444046, 0.10984773933887482]
Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT.	[6, 72, 9, 79, 29, 49, 76, 175, 23, 4]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2188023030757904, 0.08365336805582047, 0.12580662965774536, 0.19745510816574097, 0.0866445004940033, 0.13720636069774628, 0.08492305129766464, 0.14334656298160553, 0.07970727980136871, 0.09367769956588745]
An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006).	[132, 127, 20, 60, 47, 128, 125, 33, 39, 53]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.5621039867401123, 0.6095050573348999, 0.23328636586666107, 0.29279083013534546, 0.40116649866104126, 0.5939735770225525, 0.07939326018095016, 0.05073969066143036, 0.227044016122818, 0.10813049226999283]
An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length.	[163, 111, 114, 109, 160, 88, 4, 19, 122, 112]	[0, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.4541036784648895, 0.7544543743133545, 0.5242840051651001, 0.6462520956993103, 0.2999154329299927, 0.18424329161643982, 0.39918196201324463, 0.32535573840141296, 0.3867020606994629, 0.0922577977180481]
An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training.	[2, 11, 163, 34, 190, 114, 53, 188, 63, 45]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14117874205112457, 0.19397686421871185, 0.4896068274974823, 0.15842264890670776, 0.16543640196323395, 0.1644166260957718, 0.15705245733261108, 0.4350564479827881, 0.10367169976234436, 0.12341737747192383]
An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet.	[91, 192, 75, 90, 77, 201, 76, 130, 118, 26]	[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]	[0.7702592611312866, 0.8347708582878113, 0.7110379338264465, 0.7479503750801086, 0.5370000004768372, 0.6135532259941101, 0.34280258417129517, 0.4541444182395935, 0.15014095604419708, 0.06167211756110191]
An interesting result is that a per-node accuracy of 86.3 (i.e. only 3 points under the state of-the art on the same data set, (Johansson and Nugues, 2008b)) is achieved.	[167, 94, 13, 59, 146, 142, 82, 122, 145, 162]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7136183381080627, 0.5522693991661072, 0.1544930338859558, 0.17885705828666687, 0.13850261270999908, 0.07988154143095016, 0.5517842173576355, 0.13986001908779144, 0.3018065392971039, 0.27222365140914917]
Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005).	[43, 0, 18, 21, 38, 28, 1, 3, 29, 11]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.7293893694877625, 0.7665040493011475, 0.5102376937866211, 0.5835798382759094, 0.42163658142089844, 0.6473501324653625, 0.437875360250473, 0.3357546329498291, 0.32476815581321716, 0.2554872930049896]
And finally, we show that the parsing algorithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996).	[20, 98, 141, 136, 137, 17, 142, 6, 100, 13]	[0, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.48153990507125854, 0.8234822154045105, 0.5279403328895569, 0.5053758025169373, 0.7796869874000549, 0.19564899802207947, 0.18986929953098297, 0.08641017228364944, 0.1061183512210846, 0.0661563128232956]
Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner.	[144, 76, 110, 147, 85, 71, 2, 9, 26, 131]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21124184131622314, 0.16516877710819244, 0.0791596993803978, 0.1186228021979332, 0.06091687083244324, 0.05310279503464699, 0.15906234085559845, 0.15906234085559845, 0.10786156356334686, 0.0524781234562397]
Annotation was highly reliable with a kappa (Carletta, 1996) of 3.	[71, 1, 23, 19, 18, 0, 2, 32, 4, 81]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.51880943775177, 0.5873247385025024, 0.36905354261398315, 0.05871681869029999, 0.0887470543384552, 0.35330766439437866, 0.3522220551967621, 0.18220272660255432, 0.5873247385025024, 0.40346845984458923]
Another WSD approach incorporating context-dependent phrasal translation lexicons is given in (Carpuatand Wu, 2007) and has been evaluated on several translation tasks.	[49, 35, 101, 40, 41, 23, 72, 12, 53, 24]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15523403882980347, 0.11358775198459625, 0.16815997660160065, 0.17034395039081573, 0.1311732530593872, 0.1832398623228073, 0.20469389855861664, 0.12253421545028687, 0.13974453508853912, 0.11627364903688431]
Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004).	[83, 168, 104, 155, 166, 106, 93, 107, 111, 103]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3627501130104065, 0.24240277707576752, 0.17727068066596985, 0.062270279973745346, 0.2863169312477112, 0.13004162907600403, 0.17554673552513123, 0.4921926259994507, 0.24414484202861786, 0.26441490650177]
Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002).	[276, 144, 85, 91, 3, 7, 146, 156, 270, 155]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8393682837486267, 0.2438872754573822, 0.1288398951292038, 0.1344350278377533, 0.21961067616939545, 0.21961067616939545, 0.11970902234315872, 0.10678088665008545, 0.09527793526649475, 0.10993720591068268]
Another method which could be used for class labelling is given by the conceptual density algorithm ofAgirre and Rigau (1996), which those authors applied to word sense disambiguation.	[186, 183, 197, 142, 0, 40, 53, 64, 8, 61]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.46158817410469055, 0.0715704932808876, 0.18577280640602112, 0.06534865498542786, 0.6499463319778442, 0.32024964690208435, 0.3176717758178711, 0.2225525677204132, 0.18824954330921173, 0.3083992898464203]
Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences.	[1, 0, 8, 132, 24, 15, 12, 112, 107, 152]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8228250741958618, 0.8361484408378601, 0.6708296537399292, 0.21987731754779816, 0.18910089135169983, 0.33805736899375916, 0.07203581184148788, 0.05692584440112114, 0.058905553072690964, 0.13028301298618317]
Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways.	[37, 112, 18, 57, 10, 63, 127, 56, 113, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3711077570915222, 0.07422187179327011, 0.0876179039478302, 0.0799739733338356, 0.061880793422460556, 0.06106853112578392, 0.08934496343135834, 0.07318902760744095, 0.12136346846818924, 0.07347109168767929]
Applying such a model to information extraction, in AutoSlog Riloff (1993) builds a list of patterns for filling in semantic slots in a specific domain, as well as a method for automatic acquisition of case frames (Riloff and Schmelzenbach, 1998).	[84, 290, 129, 175, 161, 32, 60, 163, 130, 234]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]	[0.3499314785003662, 0.3499314785003662, 0.3259151875972748, 0.3978726267814636, 0.5677861571311951, 0.3771058917045593, 0.3893299996852875, 0.2850073277950287, 0.1749659776687622, 0.5605245232582092]
Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b): dependency features representing predicate-argument dependencies (with and without distance measures); rule instantiation features encoding the combining categories together with the result category (wit hand without a lexical head); lexical category features, consisting of word category pairs at the leaf nodes; and root category features, consisting of head word category pairs at the root nodes.	[44, 30, 46, 71, 75, 21, 87, 67, 161, 29]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.55877685546875, 0.7188414931297302, 0.31823810935020447, 0.36769166588783264, 0.32192549109458923, 0.14902883768081665, 0.11016175895929337, 0.3606969118118286, 0.1630391776561737, 0.2692881226539612]
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011).	[11, 245, 0, 2, 155, 285, 227, 22, 109, 183]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.17429274320602417, 0.05180491879582405, 0.592464029788971, 0.16537950932979584, 0.16791296005249023, 0.15189288556575775, 0.06354261934757233, 0.2914070785045624, 0.14431394636631012, 0.13488340377807617]
As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicate argument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b).	[34, 66, 32, 46, 70, 41, 35, 15, 24, 44]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2133687287569046, 0.32594621181488037, 0.16880068182945251, 0.13511265814304352, 0.13140377402305603, 0.06111492961645126, 0.08510284870862961, 0.08446824550628662, 0.23618310689926147, 0.09954303503036499]
As a Japanese dependency analyzer based on the cascaded chunking model, we use the publicly available version of CaboCha (Kudo and Matsumoto, 2002), which is trained with the manually parsed sentences of Kyoto text corpus (Kurohashi and Nagao, 1998), that are 38,400 sentences selected from the 1995 Mainichi newspaper text.	[85, 81, 145, 12, 82, 36, 1, 37, 33, 103]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8388713598251343, 0.8332922458648682, 0.4234870672225952, 0.17351874709129333, 0.12681549787521362, 0.18539908528327942, 0.22973735630512238, 0.1832730770111084, 0.07484947144985199, 0.17881764471530914]
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	[2, 125, 50, 360, 354, 298, 121, 247, 301, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7234579920768738, 0.2854403257369995, 0.15934422612190247, 0.1748119294643402, 0.16992069780826569, 0.13330206274986267, 0.3079371154308319, 0.23964142799377441, 0.14876937866210938, 0.26054781675338745]
As a somewhat radical alternative to taxonomical relationships, other ways of measuring semantic similarity based on distributional evidence have been put forward in the literature (see, among others, Brown et al 1991, Gale et al 1992, Pereira and Tishby 1992), which emphasise the role played by context in this game.	[14, 53, 42, 51, 36, 88, 2, 3, 59, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07779721170663834, 0.05963842198252678, 0.08672340959310532, 0.08907344937324524, 0.0914706289768219, 0.09412600845098495, 0.13432465493679047, 0.07031898200511932, 0.07187309861183167, 0.07222473621368408]
As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG).	[30, 16, 108, 151, 109, 57, 17, 54, 4, 153]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3938196301460266, 0.29583799839019775, 0.3787686824798584, 0.17814281582832336, 0.31419286131858826, 0.07244986295700073, 0.10361269116401672, 0.15555530786514282, 0.062818244099617, 0.3208474814891815]
As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005).	[105, 96, 103, 106, 86, 100, 88, 56, 133, 156]	[1, 0, 0, 0, 0, 1, 1, 0, 0, 0]	[0.8089885115623474, 0.2743605077266693, 0.42594701051712036, 0.2571167051792145, 0.2438790500164032, 0.6330140829086304, 0.5599125623703003, 0.18378464877605438, 0.06533162295818329, 0.20426850020885468]
As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER.	[0, 54, 32, 157, 3, 175, 45, 192, 129, 15]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5606796741485596, 0.4475126266479492, 0.32857730984687805, 0.4673958718776703, 0.07606517523527145, 0.09465864300727844, 0.17361457645893097, 0.06036942079663277, 0.38793909549713135, 0.17565736174583435]
As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.	[10, 138, 6, 53, 24, 41, 40, 43, 5, 19]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34828558564186096, 0.6116933822631836, 0.1352619081735611, 0.1674722582101822, 0.15932168066501617, 0.23019687831401825, 0.2076721489429474, 0.16181394457817078, 0.181604266166687, 0.11959852278232574]
As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively.	[21, 39, 38, 35, 24, 29, 146, 106, 4, 37]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.8333897590637207, 0.8363342881202698, 0.4074752926826477, 0.14977416396141052, 0.12839549779891968, 0.1195162683725357, 0.06801246851682663, 0.44265779852867126, 0.7637073993682861, 0.20191778242588043]
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	[11, 278, 15, 47, 168, 71, 225, 285, 293, 61]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3396010994911194, 0.2215709537267685, 0.19583848118782043, 0.24374832212924957, 0.16834360361099243, 0.22690334916114807, 0.23136006295681, 0.08320234715938568, 0.19997964799404144, 0.1988082081079483]
As has been observed before by Bertoldi and Federico (2009), it did not matter whether the synthetic data were used on their own or in addition to the original training data.	[64, 148, 91, 1, 26, 95, 77, 103, 90, 49]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.7628458142280579, 0.614459216594696, 0.3879035711288452, 0.21051833033561707, 0.15180538594722748, 0.468723326921463, 0.08860482275485992, 0.3447556495666504, 0.6633282899856567, 0.31665146350860596]
As in other uses of parallel corpora, good alignment is essential in order for the results to be meaningful (Och and Ney, 2000).	[76, 32, 97, 67, 15, 19, 98, 18, 34, 106]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32949477434158325, 0.30046796798706055, 0.336503803730011, 0.12035562843084335, 0.16513270139694214, 0.12714484333992004, 0.1574530303478241, 0.07124984264373779, 0.0732562392950058, 0.1352965086698532]
As in the case of Mellish et al (1998) we construct an acceptable ordering rather than the best possible one.	[35, 43, 39, 80, 1, 155, 76, 13, 40, 52]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7446357607841492, 0.23525124788284302, 0.519223153591156, 0.2786032557487488, 0.25882503390312195, 0.06448917835950851, 0.06898573786020279, 0.20842106640338898, 0.18412750959396362, 0.18464168906211853]
As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach.	[57, 108, 27, 116, 90, 24, 1, 0, 69, 95]	[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.4726187288761139, 0.6913632154464722, 0.2571638822555542, 0.5903523564338684, 0.2810819447040558, 0.3977246880531311, 0.12810422480106354, 0.5416430830955505, 0.1020531952381134, 0.28981122374534607]
As noted above, systems were allowed to make use of gender and number predictions for NPs using the table from Bergsma and Lin (Bergsma and Lin, 2006).	[121, 119, 36, 23, 115, 88, 45, 4, 138, 168]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7281745076179504, 0.785065233707428, 0.4716149866580963, 0.5292208790779114, 0.4320065379142761, 0.11094168573617935, 0.10060705989599228, 0.20799821615219116, 0.06985940784215927, 0.3110904097557068]
As noted in (Wiebe and Mihalcea, 2006), sentences containing objective senses may not be objective.	[50, 138, 166, 77, 98, 158, 47, 35, 41, 13]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.831822395324707, 0.4556867480278015, 0.4389868378639221, 0.2249535471200943, 0.33982712030410767, 0.2843937277793884, 0.35063469409942627, 0.33863943815231323, 0.1686011105775833, 0.27115190029144287]
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	[15, 34, 20, 116, 7, 8, 30, 179, 122, 5]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7691157460212708, 0.29028618335723877, 0.2423575073480606, 0.19186609983444214, 0.1100706234574318, 0.1724855750799179, 0.14842753112316132, 0.09911223500967026, 0.16389930248260498, 0.10773870348930359]
As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information.	[222, 601, 86, 218, 321, 529, 234, 585, 520, 531]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.851617693901062, 0.8031087517738342, 0.4010692834854126, 0.3656039237976074, 0.34916430711746216, 0.30750787258148193, 0.4083874523639679, 0.07943690568208694, 0.10295281559228897, 0.1295706182718277]
As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al, 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources.	[6, 18, 2, 7, 23, 4, 5, 38, 34, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8472685813903809, 0.08836233615875244, 0.13751740753650665, 0.19877053797245026, 0.1172477975487709, 0.23904283344745636, 0.12299801409244537, 0.11743724346160889, 0.16643749177455902, 0.1740986853837967]
As shown in the following parts of this paper, it works very well with the existing techniques, such as rule com posing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008).	[45, 74, 133, 51, 46, 50, 19, 65, 38, 30]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7372934818267822, 0.3043035864830017, 0.4763354957103729, 0.2201806902885437, 0.09464987367391586, 0.22872294485569, 0.06724809855222702, 0.25985297560691833, 0.06241003796458244, 0.48383426666259766]
As such, it emphasizes robustness at Web scale, without taking advantage of existing specification languages for representing events and temporal expressions occurring in text (Pustejovsky et al, 2003), and forgoing the potential benefits of more complex methods that extract temporal relations from relatively clean text collections (Mani et al, 2006).	[63, 4, 145, 27, 1, 12, 151, 133, 104, 139]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.28790226578712463, 0.12628377974033356, 0.216287299990654, 0.30388781428337097, 0.11346177011728287, 0.11209103465080261, 0.10766149312257767, 0.5275724530220032, 0.1532675325870514, 0.47554606199264526]
As to the research on computing word sense relatedness, Dagan et al (1993) did some pilot work and Lee (1997) and Resnik (1999) contributed to the research on semantic similarity.	[19, 161, 86, 25, 32, 72, 21, 43, 104, 101]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.773024320602417, 0.3040395975112915, 0.39805465936660767, 0.19332067668437958, 0.12560667097568512, 0.15648601949214935, 0.058046113699674606, 0.32022199034690857, 0.06630309671163559, 0.07738037407398224]
As we said at the out 211 set, we don't necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work.	[65, 39, 29, 107, 49, 40, 26, 120, 11, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14851266145706177, 0.06535275280475616, 0.11317833513021469, 0.244502991437912, 0.17024844884872437, 0.10102040320634842, 0.1291503757238388, 0.07350116223096848, 0.12641756236553192, 0.10335977375507355]
At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008).	[34, 81, 72, 31, 110, 30, 20, 101, 49, 48]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.8503720164299011, 0.667171835899353, 0.7739456295967102, 0.5906327962875366, 0.3318415582180023, 0.40902361273765564, 0.09295408427715302, 0.17941775918006897, 0.24917450547218323, 0.31444433331489563]
At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)).	[135, 145, 3, 15, 1, 125, 11, 0, 139, 14]	[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]	[0.2286144644021988, 0.32361018657684326, 0.2068289816379547, 0.5003659129142761, 0.20805764198303223, 0.2965332567691803, 0.10339169204235077, 0.5365748405456543, 0.19275203347206116, 0.35431990027427673]
At last, all the punctuations in Penn Chinese Treebank 5.1 (N.Xue et al,2002).	[3, 12, 4, 0, 1, 14, 9, 7, 19, 15]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8116830587387085, 0.36252298951148987, 0.09366199374198914, 0.29577770829200745, 0.12976761162281036, 0.08487443625926971, 0.06163356453180313, 0.17504963278770447, 0.15966996550559998, 0.17393721640110016]
At phase level, Turney (2002) presents a technique for inferring the orientation and intensity of a phrase according to its PMI-IR statistical association with a set of strongly-polarized seed words.	[58, 22, 23, 24, 49, 52, 3, 18, 5, 78]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47150808572769165, 0.7872324585914612, 0.3579345941543579, 0.13804535567760468, 0.2560884654521942, 0.147785022854805, 0.34159210324287415, 0.3041585087776184, 0.21351812779903412, 0.08607173711061478]
At the level of short passages or sentences, (Hatzivassiloglou et al, 1999) goes beyond N-gram, taking advantage of WordNet synonyms, as well as ordering and distance between shared words.	[49, 14, 173, 0, 34, 191, 7, 36, 193, 46]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5217198133468628, 0.664131760597229, 0.4203539192676544, 0.3343659043312073, 0.10807705670595169, 0.10807705670595169, 0.20290304720401764, 0.11109203845262527, 0.11109203845262527, 0.07578293979167938]
At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system.	[247, 189, 218, 140, 74, 252, 230, 100, 19, 192]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0863928347826004, 0.08539413660764694, 0.10293327271938324, 0.19184398651123047, 0.252162367105484, 0.09950024634599686, 0.19311675429344177, 0.08027686178684235, 0.08159337192773819, 0.28986939787864685]
At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al, 1988), Head driven Phrase Structure Grammar (HPSG) (Pollardand Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000), which represent deep syntactic structures that can not be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.	[29, 78, 24, 33, 36, 22, 169, 140, 50, 239]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.4973605275154114, 0.6830689311027527, 0.41463300585746765, 0.4474107027053833, 0.23736000061035156, 0.3966202437877655, 0.2775876522064209, 0.2649122178554535, 0.22423826158046722, 0.5884752869606018]
Automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations (Navigli, 2006).	[142, 45, 4, 10, 8, 134, 94, 141, 145, 36]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7037383317947388, 0.18306861817836761, 0.270044207572937, 0.25035494565963745, 0.3111940324306488, 0.3044576942920685, 0.1095617413520813, 0.3110806345939636, 0.1798725426197052, 0.07389100641012192]
Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation.	[17, 45, 18, 37, 41, 44, 11, 57, 93, 84]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.8385200500488281, 0.7540045380592346, 0.2840305268764496, 0.33806663751602173, 0.30107972025871277, 0.5152986645698547, 0.11966925859451294, 0.12329742312431335, 0.18763722479343414, 0.07886825501918793]
BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity.	[118, 121, 114, 120, 62, 8, 129, 113, 87, 50]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14483529329299927, 0.10105468332767487, 0.09581933915615082, 0.17480827867984772, 0.19921231269836426, 0.08731188625097275, 0.12223588675260544, 0.0718282014131546, 0.0901375263929367, 0.0945475697517395]
Bangalore and Joshi (1999) indicated that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enabled effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing.	[87, 73, 7, 17, 94, 303, 35, 246, 39, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7748226523399353, 0.24847669899463654, 0.387935608625412, 0.387935608625412, 0.19409123063087463, 0.2877441942691803, 0.30292725563049316, 0.24691078066825867, 0.20401230454444885, 0.2466290295124054]
Bangalore and Joshi (1999), Clark and Curran (2004) and Matsuzaki et al (2007) show that by using a supertagger before (CCG and HPSG) parsing, the space required for discriminative training is drastically reduced.	[246, 279, 278, 247, 276, 92, 70, 126, 300, 107]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43110179901123047, 0.3932839632034302, 0.19469298422336578, 0.42175009846687317, 0.10092062503099442, 0.25471338629722595, 0.315134733915329, 0.21907754242420197, 0.06860383599996567, 0.1494205743074417]
Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000).	[0, 18, 188, 15, 7, 191, 180, 45, 44, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2620493173599243, 0.06148524582386017, 0.3658246397972107, 0.13847360014915466, 0.13192963600158691, 0.1435791254043579, 0.20344196259975433, 0.04885021224617958, 0.07326070219278336, 0.0735396221280098]
Banko and Etzioni (2008) cite a precision score of 88% for their system.	[174, 123, 22, 21, 133, 118, 29, 131, 23, 171]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6308238506317139, 0.8078896999359131, 0.1710345596075058, 0.1099184975028038, 0.23528379201889038, 0.10692994296550751, 0.33878791332244873, 0.10535617917776108, 0.07518213242292404, 0.06311243772506714]
Banko et al (2000) propose a bag-of-words model for headline generation.	[81, 0, 32, 75, 60, 19, 90, 96, 94, 53]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29564785957336426, 0.6287968754768372, 0.13608554005622864, 0.210077166557312, 0.16211874783039093, 0.16927723586559296, 0.0703914687037468, 0.07652448117733002, 0.2275829166173935, 0.054025352001190186]
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators.	[96, 120, 31, 40, 29, 1, 151, 51, 0, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2668273448944092, 0.12598083913326263, 0.09966207295656204, 0.2506222426891327, 0.08959067612886429, 0.16258282959461212, 0.058289941400289536, 0.08218096196651459, 0.4605969786643982, 0.16055984795093536]
Barzilay and Lapata (2005) exploited the use of the distributional and referential information of discourse entities to improve summary coherence.	[13, 155, 28, 43, 47, 38, 11, 178, 34, 199]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43972936272621155, 0.3479578495025635, 0.20995883643627167, 0.27906590700149536, 0.24606090784072876, 0.0976613238453865, 0.15002289414405823, 0.14999160170555115, 0.10290359705686569, 0.14167727530002594]
Barzilay and Lapata (2005) showed that their entity based model is able to distinguish a source text from its permutation accurately.	[23, 2, 103, 19, 38, 186, 105, 191, 112, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5324133038520813, 0.18580256402492523, 0.08178837597370148, 0.11357332766056061, 0.1251225769519806, 0.09731067717075348, 0.08910087496042252, 0.10368822515010834, 0.15299731492996216, 0.49426183104515076]
Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles.	[521, 73, 519, 564, 474, 149, 52, 98, 619, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37637025117874146, 0.16504573822021484, 0.17397606372833252, 0.4743405878543854, 0.1899958699941635, 0.0758933499455452, 0.1000354140996933, 0.051937878131866455, 0.06809636950492859, 0.07627074420452118]
Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations.	[160, 625, 561, 597, 443, 439, 161, 309, 229, 254]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8439145088195801, 0.5403777956962585, 0.3487573564052582, 0.05942258983850479, 0.12053830921649933, 0.09996727854013443, 0.057268716394901276, 0.3171195685863495, 0.14672209322452545, 0.06494800746440887]
Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations.	[33, 67, 64, 24, 0, 2, 75, 34, 48, 43]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2854609787464142, 0.2735111713409424, 0.2622753381729126, 0.11717893928289413, 0.4018930196762085, 0.17277419567108154, 0.12925010919570923, 0.10291437804698944, 0.1009124293923378, 0.13547736406326294]
Barzilay and McKeown (2001) and Callison Burch et al (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source.	[2, 64, 210, 0, 17, 213, 25, 77, 16, 188]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8464394211769104, 0.7820791006088257, 0.47890931367874146, 0.7390353083610535, 0.33632010221481323, 0.09872627258300781, 0.33570948243141174, 0.23561911284923553, 0.15747575461864471, 0.3354586064815521]
Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non-overlapping summary sentence.	[115, 390, 181, 23, 317, 318, 312, 315, 33, 182]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1363171637058258, 0.1682591289281845, 0.15583647787570953, 0.08173870295286179, 0.23647451400756836, 0.2408299446105957, 0.12187355011701584, 0.055932365357875824, 0.06866065412759781, 0.17185209691524506]
Barzilay et al (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation.	[166, 14, 174, 15, 55, 124, 13, 19, 2, 63]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8339201807975769, 0.7853227853775024, 0.1591312289237976, 0.7803047895431519, 0.4704433083534241, 0.40581923723220825, 0.12703341245651245, 0.17052949965000153, 0.14392906427383423, 0.05959763377904892]
Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance.	[89, 144, 141, 146, 138, 88, 83, 140, 125, 12]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6823527216911316, 0.8291761875152588, 0.4094153642654419, 0.20622321963310242, 0.7640135884284973, 0.08765784651041031, 0.12247530370950699, 0.15750472247600555, 0.09530644863843918, 0.10453187674283981]
Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection.	[78, 108, 97, 92, 87, 81, 27, 24, 129, 317]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7788209915161133, 0.19568036496639252, 0.2306702584028244, 0.21380089223384857, 0.09706851840019226, 0.06659310311079025, 0.10187505185604095, 0.07433240860700607, 0.11657463014125824, 0.06788790225982666]
Basic modification events are defined similarly to the PHOSPHORYLATION event type targeted in the 09 and the 2011 GE and ID tasks (Kim et al, 2011b; Pyysalo et al, 2011b), with the full task extending previously defined arguments with two additional ones, Sidechain and Contextgene.	[42, 23, 22, 41, 26, 28, 17, 73, 40, 24]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.36118292808532715, 0.18124693632125854, 0.15098951756954193, 0.2995125949382782, 0.21717782318592072, 0.1775749921798706, 0.1256776601076126, 0.1066717877984047, 0.18586350977420807, 0.5295159220695496]
Bean and Riloff (1999) and Uryupina (2003) construct quite accurate classifiers to detect unique NPs.	[39, 38, 33, 93, 56, 66, 94, 118, 27, 95]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4067322611808777, 0.19310282170772552, 0.15374331176280975, 0.10089508444070816, 0.06567111611366272, 0.0960477665066719, 0.08536238968372345, 0.10425020754337311, 0.0822967067360878, 0.06755854189395905]
Bean and Riloff (1999) and Uryupina (2003) have already employed a definite probability measure in a similar way, although the way the ratio is computed is slightly different.	[93, 123, 13, 134, 85, 135, 132, 140, 75, 18]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8438361287117004, 0.6142921447753906, 0.30284813046455383, 0.2208392471075058, 0.3949998915195465, 0.34933581948280334, 0.08932781964540482, 0.376803994178772, 0.07405558228492737, 0.07777194678783417]
Because none of the algorithms proposed by Choi and Cardie (2008) is designed to handle the neutral polarity, we invent our own version as shown in Figure 2.	[81, 129, 9, 74, 26, 154, 90, 69, 140, 80]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25934162735939026, 0.38891005516052246, 0.11474218219518661, 0.076682448387146, 0.34240949153900146, 0.07724101841449738, 0.07021909207105637, 0.08752108365297318, 0.11661787331104279, 0.07258714735507965]
Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al, 1998).	[11, 30, 7, 41, 58, 14, 56, 15, 6, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.521759033203125, 0.20670002698898315, 0.15020804107189178, 0.11978382617235184, 0.1218988224864006, 0.09442221373319626, 0.060121089220047, 0.22363659739494324, 0.07209198921918869, 0.4594440162181854]
Because we demonstrate our new SPD measure on the same problem as McCarthy (2000), we provide more detail of her method here, for comparison.	[194, 208, 141, 218, 15, 143, 89, 12, 102, 95]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7660525441169739, 0.37645861506462097, 0.2943030893802643, 0.17820240557193756, 0.1351127177476883, 0.3318488895893097, 0.10708174109458923, 0.493704617023468, 0.08741573989391327, 0.0641648918390274]
Bergsma and Lin (2006) determine the likelihood of coreference along the syntactic path connecting a pronoun to a possible antecedent, by looking at the distribution of the path in text.	[15, 156, 38, 31, 34, 41, 2, 129, 128, 79]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6733450293540955, 0.5550001263618469, 0.4558262228965759, 0.48071756958961487, 0.3019454777240753, 0.4025823175907135, 0.4033582806587219, 0.2855689823627472, 0.24363107979297638, 0.35780683159828186]
Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011).	[108, 138, 61, 110, 71, 106, 96, 8, 46, 120]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30130279064178467, 0.2856736183166504, 0.20264111459255219, 0.32929667830467224, 0.20727486908435822, 0.40869101881980896, 0.25002187490463257, 0.1263025999069214, 0.07527314871549606, 0.07361865043640137]
Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank.	[0, 61, 10, 1, 60, 59, 7, 14, 69, 2]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8466004133224487, 0.7661479115486145, 0.5970202684402466, 0.34990444779396057, 0.39403828978538513, 0.1771269142627716, 0.22635236382484436, 0.10769490897655487, 0.06011179834604263, 0.3120231032371521]
Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective.	[14, 1, 29, 3, 17, 121, 116, 93, 131, 7]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.05537247285246849, 0.07871093600988388, 0.23837494850158691, 0.06518068164587021, 0.2835039794445038, 0.08269838988780975, 0.6307424902915955, 0.07478180527687073, 0.059227339923381805, 0.07479559630155563]
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	[17, 6, 217, 3, 5, 71, 0, 30, 15, 2]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.2698303759098053, 0.08906421810388565, 0.3063029646873474, 0.2611733078956604, 0.2448979765176773, 0.7111746668815613, 0.32232487201690674, 0.12003661692142487, 0.22315473854541779, 0.15045617520809174]
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study.	[110, 6, 118, 11, 83, 63, 38, 9, 103, 105]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.536088228225708, 0.10818672180175781, 0.13063150644302368, 0.3739181458950043, 0.05794670060276985, 0.11922101676464081, 0.12848980724811554, 0.05273456126451492, 0.0948994979262352, 0.06021396443247795]
Binarizing the grammars (Zhang et al, 2006) further increases the size of these sets, due to the introduction of virtual nonterminals.	[21, 2, 31, 119, 10, 48, 154, 45, 47, 32]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7721359133720398, 0.21009258925914764, 0.5837146043777466, 0.20938792824745178, 0.11327127367258072, 0.31609684228897095, 0.059431951493024826, 0.3044087886810303, 0.42449384927749634, 0.36818674206733704]
Biographic Data Past work on this task (e.g. Bagga and Baldwin, 1998) has primarily approached personal name disambiguation using document context profiles or vectors, which recognize and distinguish identical name instances based on partially indicative words in context such as computer or car in the Clark case.	[71, 0, 48, 3, 10, 16, 12, 22, 37, 36]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4000012278556824, 0.5619905591011047, 0.134635791182518, 0.10925838351249695, 0.06627637892961502, 0.11510288715362549, 0.10237569361925125, 0.052562061697244644, 0.07836273312568665, 0.0707329586148262]
Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally).	[125, 54, 57, 1, 99, 32, 17, 43, 53, 61]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6197164058685303, 0.1940850168466568, 0.243708997964859, 0.22671234607696533, 0.2104591280221939, 0.08707190304994583, 0.11618354171514511, 0.09437724947929382, 0.15631578862667084, 0.05131923034787178]
Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data.	[183, 37, 185, 173, 17, 14, 38, 104, 124, 143]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7362263202667236, 0.21928313374519348, 0.18707004189491272, 0.25569677352905273, 0.5679840445518494, 0.08009722828865051, 0.25264620780944824, 0.12270519137382507, 0.20315822958946228, 0.39548736810684204]
Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score.	[135, 130, 9, 139, 111, 14, 11, 10, 138, 106]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.8450226783752441, 0.8194897770881653, 0.16853271424770355, 0.3804345428943634, 0.2821604907512665, 0.635421872138977, 0.12163428962230682, 0.13260041177272797, 0.20655006170272827, 0.12678475677967072]
Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features.	[6, 249, 16, 172, 181, 96, 14, 180, 244, 134]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.709683358669281, 0.4382464587688446, 0.2854209244251251, 0.10533638298511505, 0.17698028683662415, 0.284638911485672, 0.2646763026714325, 0.36298537254333496, 0.26880431175231934, 0.3108459413051605]
Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing.	[0, 22, 15, 17, 2, 149, 19, 130, 162, 56]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6743873357772827, 0.6360694766044617, 0.36163759231567383, 0.17845764756202698, 0.38959208130836487, 0.22901290655136108, 0.27854129672050476, 0.2100197970867157, 0.3018128275871277, 0.13366465270519257]
Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision.	[86, 11, 16, 19, 1, 150, 37, 162, 47, 87]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5635934472084045, 0.13208918273448944, 0.32083097100257874, 0.08871867507696152, 0.11177511513233185, 0.32989048957824707, 0.21985569596290588, 0.1415463536977768, 0.08176794648170471, 0.29409271478652954]
"Both Brcnt (1993) and Manning (1993), who attempt to induce a lexicon of sub categorization features do so by completely discarding all preexisting knowledge; both systems are stand-ahmc, without a parsing engine to test or use the ""learned"" information."	[0, 17, 34, 95, 19, 36, 27, 32, 97, 81]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7047478556632996, 0.3731842637062073, 0.09020010381937027, 0.2626124620437622, 0.10186386853456497, 0.39082473516464233, 0.1770363599061966, 0.09198041260242462, 0.19666387140750885, 0.27694597840309143]
Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory.	[30, 104, 101, 29, 15, 2, 133, 31, 26, 103]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8240684270858765, 0.18741892278194427, 0.15520909428596497, 0.4332025945186615, 0.18275241553783417, 0.14433464407920837, 0.17062009871006012, 0.25449058413505554, 0.1781083643436432, 0.12498503923416138]
Both Moses and our system are evaluated with and without lexicalized reordering (Tillmann, 2004) .	[94, 15, 7, 8, 88, 102, 16, 118, 4, 6]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7603756189346313, 0.05977332219481468, 0.28548598289489746, 0.2806863486766815, 0.14996294677257538, 0.06135004758834839, 0.05245295912027359, 0.08333775401115417, 0.05460471659898758, 0.07461101561784744]
Both achieve BLEU score improvements for SMT: 25.2% to 26.8% for (Collins et al, 2005) and 28.52 to 30.86 for (Wang et al, 2007).	[26, 6, 11, 21, 24, 17, 4, 28, 19, 12]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.8493277430534363, 0.8428373336791992, 0.11290929466485977, 0.26823747158050537, 0.07289259135723114, 0.18097010254859924, 0.06468550115823746, 0.13024084270000458, 0.13363748788833618, 0.5993189215660095]
Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as an NP-hard search problem (Knight, 1999).	[137, 23, 1, 7, 2, 8, 81, 5, 11, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.4098990261554718, 0.34295377135276794, 0.2850789725780487, 0.2850789725780487, 0.22468902170658112, 0.22468902170658112, 0.21385833621025085, 0.4079683721065521, 0.4079683721065521, 0.5276928544044495]
Both the IOB representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular.	[86, 131, 46, 38, 28, 34, 91, 102, 138, 67]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32930701971054077, 0.07750853151082993, 0.06322138011455536, 0.09668544679880142, 0.08962787687778473, 0.085517518222332, 0.08212587982416153, 0.07090094685554504, 0.09056530892848969, 0.06707197427749634]
Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL.	[79, 67, 54, 27, 25, 77, 4, 93, 66, 126]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.7789212465286255, 0.5074302554130554, 0.4015022814273834, 0.4896891415119171, 0.18072882294654846, 0.25721409916877747, 0.17680618166923523, 0.12279971688985825, 0.6944759488105774, 0.24498216807842255]
Boyd-Graber et al (2007) are the first to integrate semantics into the topic model framework.	[0, 16, 40, 39, 222, 48, 49, 226, 38, 12]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7090604305267334, 0.1954333633184433, 0.2742621898651123, 0.07120435684919357, 0.050890713930130005, 0.05794978514313698, 0.10059536248445511, 0.051785994321107864, 0.181614488363266, 0.21763379871845245]
Brill (Brill, 1995) outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus.	[211, 214, 227, 101, 182, 64, 200, 196, 128, 193]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16008591651916504, 0.31736308336257935, 0.2994040548801422, 0.41972631216049194, 0.16517779231071472, 0.13791798055171967, 0.09743665903806686, 0.234099879860878, 0.33855369687080383, 0.13754981756210327]
Brill and Moore (2000) learn misspelled-word to correctly-spelled-word similarities for spelling correction.	[0, 13, 64, 14, 3, 118, 97, 34, 122, 33]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39986735582351685, 0.08703155815601349, 0.11850973218679428, 0.07734214514493942, 0.18882229924201965, 0.06829774379730225, 0.15333348512649536, 0.05451127141714096, 0.10482172667980194, 0.06873097270727158]
Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account.	[0, 23, 1, 103, 130, 136, 154, 135, 15, 26]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39510324597358704, 0.6068837642669678, 0.38549381494522095, 0.29924529790878296, 0.3712146580219269, 0.2387075424194336, 0.19647793471813202, 0.14794206619262695, 0.0783701241016388, 0.2080642580986023]
Buchholz et al (1999) achieve 71.2 F-score for grammatical relation assignment on automatically tagged and chunked text after training on about 40,000 Wall Street Journal sentences.	[53, 0, 11, 70, 26, 63, 157, 3, 60, 61]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7916430234909058, 0.6811739802360535, 0.41226986050605774, 0.277831107378006, 0.14193713665008545, 0.0520319789648056, 0.07953986525535583, 0.15130165219306946, 0.07205899059772491, 0.050573162734508514]
Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking.	[101, 4, 52, 146, 14, 75, 72, 9, 13, 26]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5268882513046265, 0.3928029239177704, 0.241426020860672, 0.20249679684638977, 0.08244602382183075, 0.2551509141921997, 0.11067399382591248, 0.17270533740520477, 0.12433817237615585, 0.195360466837883]
But it makes obvious that (Ratnaparkhi et al, 1994) were tackling a problem different from (Hindle and Rooth, 1993) given the fact that their baseline was at 59% guessing noun attachment (rather than 67% in the Hindle and Rooth experiments).	[100, 142, 112, 5, 22, 81, 9, 89, 13, 16]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2795262038707733, 0.8205695152282715, 0.1773708462715149, 0.35188159346580505, 0.3945675790309906, 0.0969659611582756, 0.12166670709848404, 0.09340309351682663, 0.08470755815505981, 0.09495413303375244]
By also including as features the posteriors of the model of Liang et al (2006), we achieve AER of 3.8, and 96.7/95.5 precision/recall.	[53, 46, 85, 117, 34, 38, 119, 6, 109, 145]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.800500214099884, 0.46730244159698486, 0.21098579466342926, 0.23018452525138855, 0.054109200835227966, 0.11412738263607025, 0.062367431819438934, 0.0716358870267868, 0.16286592185497284, 0.17194974422454834]
By exploiting a local ordering amongst derivations, we can be more conservative about combination and gain the advantages of a lazy successor function (Huang and Chiang, 2005).	[77, 134, 110, 82, 183, 174, 227, 158, 139, 173]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.675168514251709, 0.26548710465431213, 0.295448899269104, 0.18118448555469513, 0.23586276173591614, 0.10623332113027573, 0.09802456200122833, 0.20810867846012115, 0.06302548944950104, 0.09262280911207199]
By finding semantic differences between the selectional preferences, it can articulate the higher-order structure of conceptual metaphors ((Mason, 2004), p. 24), finding mappings like LIQUID -> MONEY.	[32, 29, 31, 290, 5, 33, 13, 91, 103, 94]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.8522564172744751, 0.7415322661399841, 0.3324037790298462, 0.30286654829978943, 0.3803904056549072, 0.4103988707065582, 0.3803904056549072, 0.44435635209083557, 0.6851279735565186, 0.37045085430145264]
By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb.	[45, 30, 138, 46, 44, 100, 0, 12, 15, 107]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6815609335899353, 0.11694227159023285, 0.09743519127368927, 0.21688885986804962, 0.47699761390686035, 0.3681924343109131, 0.4263433814048767, 0.0864216685295105, 0.10316132754087448, 0.060850001871585846]
By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011).	[24, 78, 69, 37, 23, 227, 82, 84, 72, 77]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5315983295440674, 0.6801511645317078, 0.34248706698417664, 0.38178980350494385, 0.22191694378852844, 0.24609434604644775, 0.12786488234996796, 0.07768245786428452, 0.06682570278644562, 0.11756300926208496]
By training the C&C tagger (Curran and Clark, 2003) on the gold-standard corpora an dour new Wikipedia-derived training data, we evaluate the usefulness of the latter and explore the nature of the training corpus as a variable in NER.	[52, 29, 0, 21, 62, 10, 8, 83, 67, 11]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.517731249332428, 0.3315693140029907, 0.3225802183151245, 0.1385432928800583, 0.12157601863145828, 0.08745743334293365, 0.1316211074590683, 0.1847599893808365, 0.115325927734375, 0.14465215802192688]
By using a Japanese grammar (JACY: Siegel and Bender (2002)) based on a monostratal theory of grammar (HPSG: Pollard and Sag (1994)) we could simultaneously annotate syntactic and semantic structure without overburdening the annotator.	[191, 16, 1, 22, 110, 31, 13, 27, 21, 91]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.8047778010368347, 0.8372776508331299, 0.20697087049484253, 0.3233981430530548, 0.11327651143074036, 0.5604532361030579, 0.13082686066627502, 0.14426106214523315, 0.07366742193698883, 0.10109782963991165]
C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations.	[22, 3, 12, 48, 35, 154, 25, 21, 66, 112]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.39724013209342957, 0.294720858335495, 0.24609988927841187, 0.5034723281860352, 0.2213250696659088, 0.23537246882915497, 0.30043256282806396, 0.348760724067688, 0.08791010081768036, 0.3599279224872589]
CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar.	[153, 157, 54, 154, 155, 55, 241, 127, 379, 330]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.3660714328289032, 0.34423771500587463, 0.37480786442756653, 0.1342085748910904, 0.18945127725601196, 0.1053665429353714, 0.12955690920352936, 0.15560583770275116, 0.5517973303794861, 0.20900435745716095]
CCGbank (Hockenmaier and Steedman, 2007) is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank.	[0, 385, 16, 67, 32, 76, 1, 6, 2, 7]	[1, 0, 1, 0, 0, 0, 0, 0, 1, 1]	[0.8502218723297119, 0.30306631326675415, 0.5334414839744568, 0.24626033008098602, 0.3144286870956421, 0.3483956456184387, 0.3482098877429962, 0.4436582624912262, 0.5806154012680054, 0.5806154012680054]
CRFs have been used successfully for Named Entity recognition (e.g., McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a).	[0, 19, 77, 4, 20, 30, 42, 18, 2, 82]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4131366014480591, 0.23276957869529724, 0.2249755561351776, 0.13617481291294098, 0.13140487670898438, 0.05152518302202225, 0.08202241361141205, 0.060584656894207, 0.054871611297130585, 0.09203461557626724]
Callison-Burch et al (2006) proposed a novel method which substitutes a paraphrase for an unknown source word or phrase in the input sentence, and then proceeds to use the translation of that paraphrase in the production of the target-language result.	[70, 12, 81, 4, 28, 31, 30, 36, 75, 60]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7802906036376953, 0.8055292963981628, 0.22501182556152344, 0.2061602622270584, 0.14668038487434387, 0.1025238111615181, 0.09740408509969711, 0.12236590683460236, 0.21330976486206055, 0.2976570725440979]
Cao and Li (2002) propose a new method to translate base noun phrases.	[13, 2, 222, 1, 0, 50, 8, 11, 32, 225]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7312986850738525, 0.8488345146179199, 0.37010475993156433, 0.24739469587802887, 0.4649307131767273, 0.25000107288360596, 0.19581179320812225, 0.29026925563812256, 0.05232970789074898, 0.16754280030727386]
Cao and Li (2002) restricted candidate bilingual compound term pairs by consulting a seed bilingual lexicon and requiring their constituent words to be translation of each other across languages.	[28, 50, 225, 35, 135, 58, 14, 46, 43, 37]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13545812666416168, 0.18233700096607208, 0.15858881175518036, 0.10800556093454361, 0.04979082569479942, 0.19247424602508545, 0.1564890444278717, 0.06182866171002388, 0.06375064700841904, 0.13791446387767792]
Cardie and Wagstaff (1999) combined the use of WordNet with proper name gazetteers in order to obtain information on the compatibility of coreferential NPs in their clustering algorithm.	[66, 63, 72, 136, 35, 77, 64, 142, 149, 114]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.828192412853241, 0.6743082404136658, 0.26003146171569824, 0.4302939772605896, 0.2463301569223404, 0.10418768227100372, 0.06355780363082886, 0.10394994169473648, 0.20164597034454346, 0.2614898085594177]
Cardie and Wagstaff (1999) describe an unsupervised clustering approach to noun phrase coreference resolution in which features are assigned to single noun phrases only.	[1, 4, 6, 178, 186, 40, 23, 16, 45, 180]	[1, 0, 0, 1, 0, 0, 0, 1, 0, 0]	[0.6366344094276428, 0.2902156114578247, 0.22490260004997253, 0.5632729530334473, 0.34401658177375793, 0.3079911172389984, 0.28760722279548645, 0.5895584225654602, 0.246296226978302, 0.24575422704219818]
Carreras (2007) employs his own extension of Eisner's algorithm for the case of projective trees and second-order models that include head grandparent relations.	[52, 3, 10, 2, 35, 12, 14, 11, 32, 9]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.6933743357658386, 0.7511612772941589, 0.2858826816082001, 0.472330242395401, 0.19406087696552277, 0.26862847805023193, 0.5109902620315552, 0.2111663818359375, 0.18303914368152618, 0.13011829555034637]
Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing co referring arguments, followed by temporal classification to induce partial order.	[20, 24, 30, 13, 1, 72, 74, 4, 28, 145]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7846782207489014, 0.4969203472137451, 0.18123102188110352, 0.23790855705738068, 0.17441090941429138, 0.209145650267601, 0.20804068446159363, 0.10200610756874084, 0.19475390017032623, 0.10503821074962616]
Chambers and Jurafsky (2009, 2008) propose an unsupervised method for learning narrative schemas, chains of events whose arguments are filled with participant semantic roles defined over words.	[1, 140, 2, 0, 67, 10, 64, 37, 57, 20]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 1]	[0.8301817774772644, 0.7420281171798706, 0.6258447766304016, 0.7838101983070374, 0.49073588848114014, 0.20201021432876587, 0.14651407301425934, 0.37956154346466064, 0.1438531130552292, 0.6229174137115479]
Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation.	[29, 2, 31, 64, 70, 32, 36, 3, 12, 0]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6805112361907959, 0.28317180275917053, 0.20885583758354187, 0.2018161118030548, 0.8064558506011963, 0.19012971222400665, 0.4484545588493347, 0.11885129660367966, 0.11889831721782684, 0.3736385405063629]
Charniak (Charniak, 2000) developed a state-of-the-art statistical CFG parser and then built an effective language model based on it (Charniak, 2001).	[1, 15, 137, 4, 70, 6, 93, 127, 124, 103]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4184670150279999, 0.3605840802192688, 0.2169083207845688, 0.15572671592235565, 0.17171452939510345, 0.13681845366954803, 0.09980231523513794, 0.0661308541893959, 0.18237654864788055, 0.07871287316083908]
Chelba and Acero (2004) first traina classifier on the source data.	[8, 30, 28, 84, 0, 4, 5, 18, 15, 51]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3684491515159607, 0.10733401775360107, 0.07852181792259216, 0.20990848541259766, 0.2985508441925049, 0.11366889625787735, 0.23074525594711304, 0.15859995782375336, 0.16056351363658905, 0.07816853374242783]
Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method.	[50, 19, 145, 176, 149, 4, 148, 130, 136, 135]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09969165176153183, 0.06076807528734207, 0.05238018557429314, 0.06025754287838936, 0.05322106555104256, 0.0875217542052269, 0.06861720234155655, 0.056569572538137436, 0.07313882559537888, 0.06611304730176926]
Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment.	[194, 0, 4, 74, 16, 144, 18, 3, 173, 6]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.8061919808387756, 0.8229116797447205, 0.5152878761291504, 0.7371129393577576, 0.5028532147407532, 0.43387773633003235, 0.33770036697387695, 0.4766657054424286, 0.2641087770462036, 0.0910552591085434]
Chiang (2010) extended SAMT-style labels to both source and target-side parses, also introducing a mechanism by which SCFG rules may apply at runtime even if their labels do not match.	[44, 24, 53, 77, 45, 38, 3, 50, 43, 85]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5977707505226135, 0.06858275085687637, 0.29678967595100403, 0.18520207703113556, 0.1898616999387741, 0.10961804538965225, 0.1823386549949646, 0.1112804189324379, 0.1999799609184265, 0.2105412632226944]
Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks.	[43, 50, 3, 69, 54, 36, 44, 66, 57, 89]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8430585861206055, 0.8079111576080322, 0.1542961746454239, 0.6148297786712646, 0.11888191103935242, 0.2395392805337906, 0.14829109609127045, 0.25651898980140686, 0.19466590881347656, 0.09190166741609573]
Choi et al (2005) and Choi et al (2006) explore conditional random fields, Wieg and and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence.	[40, 69, 1, 66, 59, 63, 25, 62, 65, 170]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.3689137101173401, 0.3603951632976532, 0.5550375580787659, 0.365119069814682, 0.08422454446554184, 0.2864326536655426, 0.3272893726825714, 0.15232571959495544, 0.053238946944475174, 0.2856692969799042]
Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009).	[63, 143, 64, 62, 65, 116, 114, 58, 48, 113]	[0, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.3115515410900116, 0.5458709597587585, 0.36320677399635315, 0.16419784724712372, 0.20128338038921356, 0.539459764957428, 0.20283570885658264, 0.0855141133069992, 0.42966321110725403, 0.11302588135004044]
Church and Hanks (1989) discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntacfic co-occurrence onstraints between verbs and prepositions (content word/function word).	[64, 101, 79, 63, 139, 28, 7, 0, 46, 13]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.6279872059822083, 0.17462854087352753, 0.3575678765773773, 0.2340666502714157, 0.3787671625614166, 0.09728831797838211, 0.13041670620441437, 0.7302741408348083, 0.23949895799160004, 0.08672358840703964]
Clark and Weir (2002) investigate the task of generalizing a single relation concept pair.	[244, 47, 41, 46, 253, 241, 77, 32, 174, 51]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14926837384700775, 0.1786152571439743, 0.1315247267484665, 0.15407925844192505, 0.38303571939468384, 0.23747481405735016, 0.05140112712979317, 0.4349413514137268, 0.17470073699951172, 0.07189100980758667]
Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available.	[55, 180, 57, 186, 133, 19, 10, 1, 16, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11619361490011215, 0.06294586509466171, 0.09357000887393951, 0.12748771905899048, 0.14173921942710876, 0.055258605629205704, 0.1515381932258606, 0.0882696658372879, 0.07319200038909912, 0.14404283463954926]
Clustering by committee has also been used to discover concepts from a text by grouping terms into conceptually related clusters (Lin and Pantel, 2002).	[3, 204, 0, 7, 210, 206, 21, 18, 4, 99]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7608985304832458, 0.7251250743865967, 0.6653115153312683, 0.09248991310596466, 0.09248991310596466, 0.3827145993709564, 0.05678384751081467, 0.0634513720870018, 0.15019923448562622, 0.34005722403526306]
Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts.	[148, 24, 157, 165, 8, 19, 173, 4, 159, 18]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1944047510623932, 0.7898200154304504, 0.17158938944339752, 0.13089986145496368, 0.16930077970027924, 0.1378449648618698, 0.20780257880687714, 0.10015454888343811, 0.0900472104549408, 0.17053915560245514]
Cohn and Lapata (2007) explores how to utilize multilingual parallel data (rather than pivot data) to improve translation performance.	[18, 175, 42, 40, 7, 49, 145, 56, 163, 136]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28794071078300476, 0.20705467462539673, 0.15046043694019318, 0.11652973294258118, 0.12027839571237564, 0.1229494959115982, 0.16592168807983398, 0.0681951716542244, 0.11071915179491043, 0.4580048620700836]
Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.	[1, 0, 158, 15, 2, 73, 135, 49, 131, 19]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8520137667655945, 0.7993369698524475, 0.5995882749557495, 0.493125319480896, 0.34680160880088806, 0.28904446959495544, 0.4572608768939972, 0.27913233637809753, 0.13164368271827698, 0.24783535301685333]
Collins (1997)'s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic.	[56, 58, 4, 23, 103, 88, 18, 20, 85, 5]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06019531190395355, 0.06906125694513321, 0.13016384840011597, 0.07453858107328415, 0.06358981877565384, 0.0626712217926979, 0.06754433363676071, 0.05861516296863556, 0.055416759103536606, 0.06188289076089859]
Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature.	[148, 163, 124, 159, 120, 134, 24, 117, 111, 11]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 1]	[0.5518512725830078, 0.6112120151519775, 0.30501866340637207, 0.09384341537952423, 0.5578216314315796, 0.3562016189098358, 0.08097401261329651, 0.07758071273565292, 0.32947438955307007, 0.690391480922699]
Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010).	[49, 70, 50, 219, 2, 39, 104, 68, 195, 59]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7459866404533386, 0.09484899044036865, 0.13328441977500916, 0.16257628798484802, 0.20782367885112762, 0.17952415347099304, 0.14956197142601013, 0.11470460146665573, 0.06457763910293579, 0.09623851627111435]
Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments.	[128, 105, 56, 75, 114, 78, 175, 123, 127, 109]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48213204741477966, 0.8398107886314392, 0.15540656447410583, 0.0591418482363224, 0.22426684200763702, 0.06377124041318893, 0.07807766646146774, 0.06775932013988495, 0.20836670696735382, 0.1557011902332306]
Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009).	[3, 125, 7, 122, 134, 0, 18, 52, 184, 79]	[1, 1, 1, 1, 0, 1, 1, 0, 0, 0]	[0.5662498474121094, 0.825050950050354, 0.5877412557601929, 0.6832187175750732, 0.14168985188007355, 0.5315375924110413, 0.5569478273391724, 0.43326982855796814, 0.3429088294506073, 0.19363993406295776]
Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002).	[92, 106, 84, 95, 109, 43, 62, 40, 131, 135]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.320037305355072, 0.6179173588752747, 0.3479059338569641, 0.2244819849729538, 0.40574949979782104, 0.40070414543151855, 0.27814990282058716, 0.057701047509908676, 0.18703193962574005, 0.1724018156528473]
Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorithm described in (Johnson, 1998).	[227, 177, 164, 232, 11, 10, 217, 86, 168, 32]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43507277965545654, 0.7618523240089417, 0.1686306595802307, 0.415216863155365, 0.23557205498218536, 0.2800174951553345, 0.13726620376110077, 0.1314154416322708, 0.3647097051143646, 0.10464244335889816]
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004).	[1, 0, 12, 85, 86, 84, 141, 125, 92, 19]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8477134108543396, 0.80270916223526, 0.5998907089233398, 0.37092486023902893, 0.32739612460136414, 0.46017926931381226, 0.25774818658828735, 0.21869896352291107, 0.14618049561977386, 0.09980776906013489]
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	[86, 92, 85, 11, 1, 104, 8, 15, 40, 68]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4878276288509369, 0.3642570972442627, 0.19853830337524414, 0.2494799941778183, 0.2868722677230835, 0.21040275692939758, 0.19940531253814697, 0.34445852041244507, 0.32619261741638184, 0.36507707834243774]
Consider, for instance, the model of morphology described in Goldsmith (2001).	[274, 275, 155, 569, 609, 342, 1, 5, 232, 586]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5278437733650208, 0.6048641800880432, 0.20912893116474152, 0.3732679784297943, 0.3222842216491699, 0.3244726359844208, 0.09191884845495224, 0.09191884845495224, 0.1250321865081787, 0.19906336069107056]
Context Representations SenseClusters supports two different representations of context, first order context vectors as used by (Pedersen and Bruce, 1997) and second order context vectors as suggested by (Schutze,1998).	[207, 219, 206, 225, 34, 55, 81, 37, 170, 80]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3099801540374756, 0.20797491073608398, 0.26021304726600647, 0.15187682211399078, 0.09794192016124725, 0.17139621078968048, 0.24982482194900513, 0.058036308735609055, 0.053623903542757034, 0.07830914855003357]
Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc.	[82, 116, 83, 112, 35, 69, 111, 84, 7, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39776962995529175, 0.1381644457578659, 0.061477284878492355, 0.22331437468528748, 0.07118944078683853, 0.11732961982488632, 0.25262725353240967, 0.09658558666706085, 0.057388775050640106, 0.0806674212217331]
Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule.	[80, 36, 82, 158, 101, 12, 13, 43, 110, 127]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5965589880943298, 0.6788069009780884, 0.2574366629123688, 0.2072938233613968, 0.0716824010014534, 0.4379788935184479, 0.1038772389292717, 0.3885222375392914, 0.272381454706192, 0.11375986784696579]
Conventionally there are two kinds of methods for role assignments, one is using only statistical information (Gildea and Jurafsky, 2002) and the other is combining with grammar rules (Gildea and Hockenmaier, 2003).	[61, 42, 1, 0, 57, 7, 58, 15, 69, 62]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.41629576683044434, 0.5126584768295288, 0.2548939287662506, 0.6168731451034546, 0.2827414572238922, 0.1678621470928192, 0.08894061297178268, 0.0693540945649147, 0.0564553402364254, 0.1455860286951065]
Copestake et al (2001) mention a third feature to be included in the hook as an externally visible variable, which they instantiate with the index of the controlled subject in equi constructions and which is also used to implement the semantics of predicative modification.	[19, 88, 99, 13, 111, 90, 102, 89, 94, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23860040307044983, 0.11390729248523712, 0.09705468267202377, 0.07634291052818298, 0.08754504472017288, 0.1701708287000656, 0.058288950473070145, 0.081418976187706, 0.06873603910207748, 0.052398502826690674]
Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)).	[4, 3, 15, 143, 135, 138, 8, 23, 14, 54]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.3016635775566101, 0.26506444811820984, 0.7073822021484375, 0.052720725536346436, 0.3579201400279999, 0.18807101249694824, 0.3640296459197998, 0.20649273693561554, 0.38625025749206543, 0.22680102288722992]
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	[57, 0, 3, 20, 80, 21, 11, 12, 9, 4]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23733389377593994, 0.5019000768661499, 0.3763326406478882, 0.4138853847980499, 0.37767380475997925, 0.07782251387834549, 0.09736434370279312, 0.1834888607263565, 0.2601247727870941, 0.10612677782773972]
Currently, FLO supports the LinGOrealiser (Carrollet al, 1999), but we are also looking at FLO modules for RealPro (Lavoie and Rambow, 1997) and FUF/SURGE (Elhadad et al, 1997).	[60, 8, 6, 9, 16, 48, 7, 3, 36, 24]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7717878818511963, 0.716439962387085, 0.4028654098510742, 0.10972899943590164, 0.14215902984142303, 0.08320431411266327, 0.10023974627256393, 0.11028800159692764, 0.31113317608833313, 0.11887174844741821]
DAR presupposes the discourse structure described by Grosz and Sidner (1986).	[646, 9, 21, 478, 68, 87, 10, 22, 69, 127]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5289027690887451, 0.3677154779434204, 0.3677154779434204, 0.23905347287654877, 0.13007809221744537, 0.07876190543174744, 0.2494886815547943, 0.2494886815547943, 0.11451394110918045, 0.05442015826702118]
DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b).	[58, 43, 8, 28, 81, 6, 154, 26, 84, 103]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32142576575279236, 0.3186376094818115, 0.10275095701217651, 0.06695497781038284, 0.08435866236686707, 0.07470277696847916, 0.08182768523693085, 0.13176405429840088, 0.1201043576002121, 0.1619635820388794]
Decrease in H (XjX n) for Chinese characters when n is increased software such as (Zhang et al, 2003) whose performance is also high.	[4, 25, 22, 81, 70, 11, 59, 0, 17, 63]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.44278761744499207, 0.061825480312108994, 0.11488892138004303, 0.08431615680456161, 0.4154501259326935, 0.20282462239265442, 0.1878712773323059, 0.31229206919670105, 0.19133606553077698, 0.09218145161867142]
Deductive logic (Pereira and Warren, 1983), extended with semi rings (Goodman, 1999), is an established formal ism used in parsing.	[39, 68, 7, 3, 17, 116, 1, 169, 120, 171]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.665741503238678, 0.1458284705877304, 0.1285269409418106, 0.19440370798110962, 0.14443904161453247, 0.4211806058883667, 0.10882221162319183, 0.16799688339233398, 0.1639496386051178, 0.08678115904331207]
Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem.	[152, 100, 16, 2, 24, 151, 22, 176, 133, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.276341050863266, 0.4395700991153717, 0.3652973473072052, 0.27589476108551025, 0.24728399515151978, 0.36409834027290344, 0.11942025274038315, 0.25092118978500366, 0.17441146075725555, 0.19454385340213776]
Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009).	[113, 114, 247, 37, 51, 46, 62, 225, 130, 164]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8425906896591187, 0.6535612940788269, 0.327481210231781, 0.1021023690700531, 0.08566346764564514, 0.15812411904335022, 0.056829869747161865, 0.06558749079704285, 0.35981664061546326, 0.06694351136684418]
Developing a better TM is a fundamental issue for those applica tions. Researchers at IBM first described such a statistical TM in (Brown et al, 1988).	[4, 108, 187, 0, 130, 35, 48, 8, 201, 202]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3029825687408447, 0.20332542061805725, 0.18877430260181427, 0.21898239850997925, 0.12069442123174667, 0.21128447353839874, 0.05845922604203224, 0.05266319215297699, 0.3346704840660095, 0.28439560532569885]
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi 196 that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word.	[96, 138, 99, 34, 87, 18, 98, 121, 2, 44]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6311869025230408, 0.6986581087112427, 0.11990652978420258, 0.13966628909111023, 0.11698723584413528, 0.20692004263401031, 0.11909127235412598, 0.20615215599536896, 0.06461464613676071, 0.07503806054592133]
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word.	[96, 138, 99, 34, 87, 18, 98, 121, 2, 44]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6092562675476074, 0.6483303308486938, 0.12423791736364365, 0.20439554750919342, 0.11585108935832977, 0.21541213989257812, 0.12532439827919006, 0.21242541074752808, 0.0670057013630867, 0.07810066640377045]
Discourse relations are essentially rhetorical structure theory (RST) relations [Mann and Thompson, 1987], and messages are represented using a deep-syntactic representation, which is loosely based on RealPro [Lavoie and Rambow, 1997].	[21, 11, 12, 19, 9, 23, 53, 13, 2, 18]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6301091909408569, 0.5669514536857605, 0.3744194805622101, 0.2296108901500702, 0.1546870768070221, 0.11251479387283325, 0.25554123520851135, 0.3414711654186249, 0.09343543648719788, 0.0978027731180191]
Discriminative classifiers, which directly model the posterior distribution of class label given features, i.e. SVM (Isozaki and Kazawa 2002) and Maximum Entropy model for NER (Chieu and Ng 2003), have been shown to outperform generative model based classifiers.	[52, 51, 27, 11, 15, 23, 75, 14, 4, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3973887860774994, 0.3811646103858948, 0.12907032668590546, 0.1234876960515976, 0.44270414113998413, 0.14484117925167084, 0.06291201710700989, 0.3845316767692566, 0.37038612365722656, 0.314120352268219]
Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al (2005) and Agirre et al (2009b).	[40, 18, 163, 25, 42, 33, 139, 39, 1, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21356570720672607, 0.2940112352371216, 0.2819795608520508, 0.10922831296920776, 0.11919606477022171, 0.07919134944677353, 0.4354110658168793, 0.08108031749725342, 0.12637026607990265, 0.07788865268230438]
Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing.	[7, 0, 1, 60, 18, 26, 44, 9, 2, 148]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18585452437400818, 0.6048272252082825, 0.1479238122701645, 0.1643662005662918, 0.13753876090049744, 0.17821761965751648, 0.10189962387084961, 0.10748536139726639, 0.11052178591489792, 0.13311873376369476]
Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000).	[71, 66, 131, 72, 108, 55, 2, 123, 14, 84]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8377670049667358, 0.12204943597316742, 0.3433823585510254, 0.288528174161911, 0.19142243266105652, 0.351734459400177, 0.3007454574108124, 0.20783117413520813, 0.07839649170637131, 0.20301297307014465]
Due to its significant growth, the WWW has become an attractive database for different systems applications as, machine translation (Resnik and Smith, 2003), question answering (Kwok et al, 2001), commonsense retrieval (Matuszek et al, 2005), and so forth.	[86, 83, 111, 291, 186, 298, 389, 2, 6, 361]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09088306874036789, 0.12077357620000839, 0.11921683698892593, 0.1340702623128891, 0.05752645432949066, 0.06310317665338516, 0.16783307492733002, 0.08423063904047012, 0.08423063904047012, 0.06964100897312164]
Due to space constraints, details and proof of correctness are available in Lopez (2007a).	[164, 208, 106, 86, 206, 266, 166, 75, 232, 209]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8501230478286743, 0.41859742999076843, 0.2434912919998169, 0.1260390281677246, 0.06821192800998688, 0.05966765061020851, 0.06491710245609283, 0.059218067675828934, 0.1030612513422966, 0.06703805178403854]
Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1, although (Pantel et al, 2004) demonstrated how to scale up their algorithms for the Web.	[215, 6, 18, 1, 16, 179, 88, 216, 9, 13]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6278189420700073, 0.3520333468914032, 0.2800457179546356, 0.14593420922756195, 0.07199527323246002, 0.07704037427902222, 0.11401676386594772, 0.3439958989620209, 0.3092346489429474, 0.06996043771505356]
Dyer (2009) applied this to German using a lattice encoding different segmentations of German words.	[109, 102, 4, 28, 34, 26, 85, 25, 11, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23956680297851562, 0.13652299344539642, 0.18296201527118683, 0.14567427337169647, 0.07806283980607986, 0.28627341985702515, 0.1852886825799942, 0.08464735746383667, 0.25414538383483887, 0.0898759737610817]
Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system.	[119, 3, 0, 13, 99, 1, 2, 111, 14, 116]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8392047882080078, 0.8221567869186401, 0.7293840050697327, 0.42663368582725525, 0.20828253030776978, 0.24849604070186615, 0.40565669536590576, 0.07888122648000717, 0.24855630099773407, 0.23457516729831696]
Dyer et al (2008) use it to encode different Chinese word segmentations or Arabic morphological analyses.	[148, 93, 10, 97, 149, 4, 125, 92, 88, 87]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7559450268745422, 0.6876683831214905, 0.23247841000556946, 0.33264976739883423, 0.13973645865917206, 0.14070339500904083, 0.10661019384860992, 0.4392406940460205, 0.07085246592760086, 0.6329591870307922]
Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010).	[80, 16, 75, 35, 17, 3, 244, 213, 186, 236]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8481659293174744, 0.7474126815795898, 0.23477856814861298, 0.2812102735042572, 0.24379751086235046, 0.05633065104484558, 0.06568180024623871, 0.2108108252286911, 0.4198666512966156, 0.3996901512145996]
Early studies of cross-lingual annotation projection were accomplished for lexically-based tasks; for example part-of-speech tagging (Yarowsky and Ngai, 2001).	[31, 30, 81, 3, 10, 78, 67, 15, 11, 32]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4621228873729706, 0.4375344514846802, 0.12586122751235962, 0.367929071187973, 0.07889736443758011, 0.1135874018073082, 0.17927271127700806, 0.14425569772720337, 0.18611173331737518, 0.07092529535293579]
Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the Verb Net verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.	[34, 0, 33, 1, 36, 13, 27, 15, 208, 14]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7054828405380249, 0.7934732437133789, 0.39273184537887573, 0.24732376635074615, 0.12962521612644196, 0.17627765238285065, 0.2946743369102478, 0.22594024240970612, 0.12219955027103424, 0.10365872830152512]
Eisner (1996, p.81) in fact suggested that the labeling system can be implemented in the grammar by templates, or in the processor by labeling the chart entries.	[71, 131, 45, 41, 129, 137, 43, 67, 3, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3872948884963989, 0.0783388763666153, 0.36106234788894653, 0.12005291134119034, 0.2594048082828522, 0.1388702094554901, 0.054623525589704514, 0.34368857741355896, 0.07179240882396698, 0.3970392942428589]
Eisner (2002) gives a general EM algorithm for parameter estimation in probabilistic finite-state transducers.	[0, 228, 53, 179, 4, 218, 40, 25, 133, 83]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.851651132106781, 0.6056263446807861, 0.3175778388977051, 0.1935872733592987, 0.15492472052574158, 0.5136758685112, 0.19262820482254028, 0.1137060821056366, 0.339328795671463, 0.09056904166936874]
Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005).	[47, 16, 184, 104, 49, 65, 72, 53, 250, 103]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10298177599906921, 0.5631034970283508, 0.37852686643600464, 0.10888645052909851, 0.08516741544008255, 0.2779506742954254, 0.19041089713573456, 0.31975921988487244, 0.07234138250350952, 0.12011075019836426]
Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus, we continue to integrate the statistical word-alignment techniques (Melamed, 1997) and dictionaries to find the translation candidates for each of the two collocates.	[10, 99, 118, 102, 34, 5, 91, 97, 40, 112]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3649871349334717, 0.6803015470504761, 0.39517274498939514, 0.06993554532527924, 0.18712246417999268, 0.4253402352333069, 0.09002392739057541, 0.11096259206533432, 0.17451220750808716, 0.12447261810302734]
Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation.	[104, 124, 47, 105, 155, 55, 52, 157, 58, 37]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41071969270706177, 0.40173494815826416, 0.13176479935646057, 0.40283751487731934, 0.4492727220058441, 0.09129177778959274, 0.07364148646593094, 0.04913967102766037, 0.07511553913354874, 0.16201063990592957]
Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011).	[147, 54, 110, 184, 10, 186, 190, 38, 35, 139]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10557656735181808, 0.07870794832706451, 0.22142931818962097, 0.14887705445289612, 0.06138420104980469, 0.10758981853723526, 0.15963654220104218, 0.05928976461291313, 0.17662306129932404, 0.05807281658053398]
Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al, 1996)) ,statistical alignment models (e.g. IBM models from statistical machine translation (Brown et al, 1993)), or string similarity measures (e.g. the longest common sub-sequence ratio (Melamed,1995)).	[175, 155, 98, 205, 192, 1, 130, 94, 16, 180]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.24712716042995453, 0.12429078668355942, 0.08982378989458084, 0.08643966168165207, 0.12702576816082, 0.1742439568042755, 0.5182742476463318, 0.06928424537181854, 0.05787210538983345, 0.06876407563686371]
Esuli and Sebastiani (2006) also address this problem testing three different variants of a semi-supervised method, and classify the input into positive, negative or neutral.	[6, 17, 146, 96, 59, 21, 22, 42, 41, 104]	[1, 1, 1, 0, 0, 1, 0, 1, 0, 0]	[0.8444299697875977, 0.8471024036407471, 0.7357118129730225, 0.306016206741333, 0.4470909833908081, 0.519225537776947, 0.14850082993507385, 0.7472777962684631, 0.052209027111530304, 0.07076619565486908]
Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English.	[173, 155, 148, 24, 128, 159, 157, 129, 167, 145]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4865383505821228, 0.4432050585746765, 0.10756326466798782, 0.1537913680076599, 0.18725547194480896, 0.12643393874168396, 0.07951243221759796, 0.1748376190662384, 0.12737269699573517, 0.13281820714473724]
Evaluation Metrics Miller et al (1996) report accuracy rates for recovering correct SQL annotations on the test set.	[116, 25, 16, 27, 108, 17, 117, 26, 22, 121]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4061521589756012, 0.15465407073497772, 0.13334524631500244, 0.27681952714920044, 0.10177973657846451, 0.07256250828504562, 0.08701921254396439, 0.054279476404190063, 0.2664967179298401, 0.07754962891340256]
Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator.	[80, 8, 1, 67, 94, 17, 104, 6, 16, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25759977102279663, 0.19521895051002502, 0.17800354957580566, 0.20793378353118896, 0.21689103543758392, 0.12699748575687408, 0.21599648892879486, 0.0997670590877533, 0.1000509262084961, 0.17346502840518951]
Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences.	[41, 48, 5, 43, 52, 58, 39, 34, 47, 36]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7931497693061829, 0.2009098380804062, 0.4668411910533905, 0.11973795294761658, 0.15502238273620605, 0.06925337761640549, 0.06125802919268608, 0.1471582055091858, 0.0891374945640564, 0.10771480947732925]
Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007).	[201, 3, 33, 35, 130, 1, 34, 218, 68, 213]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.307861328125, 0.4174076318740845, 0.3371768891811371, 0.20782220363616943, 0.09016650915145874, 0.15102820098400116, 0.26118937134742737, 0.22053860127925873, 0.14096102118492126, 0.5012344717979431]
Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006).	[21, 2, 0, 33, 187, 39, 46, 12, 181, 3]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6939298510551453, 0.6593937277793884, 0.5307432413101196, 0.288114070892334, 0.49670010805130005, 0.08197023719549179, 0.21203355491161346, 0.4346872568130493, 0.318092942237854, 0.05350131541490555]
Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003).	[26, 53, 133, 9, 50, 110, 16, 104, 12, 28]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5244253277778625, 0.24515609443187714, 0.07910540699958801, 0.23067906498908997, 0.39010512828826904, 0.11831703782081604, 0.15099968016147614, 0.06118179112672806, 0.12586897611618042, 0.07330290228128433]
Examples of this include McDonald and Pereira's (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson's (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. Descriptive dependency labels.	[22, 198, 23, 65, 77, 108, 15, 173, 230, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1529684215784073, 0.22553576529026031, 0.21081560850143433, 0.12308397144079208, 0.09211817383766174, 0.15178988873958588, 0.08933223783969879, 0.06460261344909668, 0.12389557808637619, 0.07426749169826508]
Examples of this work include a system by Liu et al (1990), and experiments by Hindle and Rooth (1993), and Resnik and Hearst (1993).	[128, 17, 19, 18, 25, 15, 234, 105, 232, 118]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4622051417827606, 0.14829091727733612, 0.1906859576702118, 0.3209027349948883, 0.1242200955748558, 0.2578144371509552, 0.3866273760795593, 0.06403907388448715, 0.1567802131175995, 0.08663360029459]
Exceptions are existant but few (2.5%): abstract pronouns (such as that in English) referring to non neuter or plural NPs, plural pronouns co-referring with singular collective NPs (Ge et al, 1998), antecedent and anaphor matching in natural gender rather than grammatical gender.	[151, 29, 136, 30, 10, 9, 196, 134, 78, 91]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7319188117980957, 0.46683070063591003, 0.3141975402832031, 0.17615444958209991, 0.5497627258300781, 0.14471057057380676, 0.24989263713359833, 0.28677165508270264, 0.26288673281669617, 0.18077245354652405]
Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996).	[7, 19, 22, 5, 0, 1, 33, 38, 148, 62]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8024072647094727, 0.44283512234687805, 0.3972342312335968, 0.27633562684059143, 0.4715932309627533, 0.3014037311077118, 0.4371092915534973, 0.18066845834255219, 0.15874317288398743, 0.10325103253126144]
Experimental design Like Rooth et al (1999) we evaluate selectional preference induction approaches in a pseudo disambiguation task.	[10, 55, 135, 1, 6, 3, 14, 19, 9, 26]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4757523536682129, 0.6957167983055115, 0.12432310730218887, 0.17553861439228058, 0.1607402116060257, 0.07929398864507675, 0.14422495663166046, 0.14882783591747284, 0.07119788974523544, 0.08388723433017731]
Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system.	[148, 108, 91, 15, 116, 149, 150, 141, 107, 142]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7205497026443481, 0.547631025314331, 0.27033108472824097, 0.3198562264442444, 0.4200359284877777, 0.40298622846603394, 0.5595628023147583, 0.1998063027858734, 0.06743922084569931, 0.22677449882030487]
Extending this notion, (Knight and Graehl, 1997) built five probability distributions:.	[48, 59, 47, 43, 40, 100, 67, 51, 94, 42]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.6636287569999695, 0.6302455067634583, 0.3230779767036438, 0.21258921921253204, 0.10847888886928558, 0.4366559386253357, 0.5781868696212769, 0.062481097877025604, 0.26200002431869507, 0.14926300942897797]
Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002).	[105, 113, 104, 164, 48, 109, 39, 0, 37, 187]	[1, 1, 0, 0, 1, 0, 0, 1, 1, 0]	[0.5568315386772156, 0.7049791216850281, 0.4399041533470154, 0.2214261144399643, 0.507724940776825, 0.153238445520401, 0.13573551177978516, 0.8260554671287537, 0.5401778221130371, 0.17439110577106476]
Fader et al (2011) utilizes a confidence function.	[139, 12, 142, 157, 116, 136, 47, 50, 45, 213]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28850919008255005, 0.7316117882728577, 0.2400248944759369, 0.29536086320877075, 0.15976198017597198, 0.16228187084197998, 0.05088254436850548, 0.16773158311843872, 0.05674625188112259, 0.052547309547662735]
Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003).	[1, 89, 35, 25, 98, 94, 71, 139, 157, 110]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3685340881347656, 0.8167375326156616, 0.21449880301952362, 0.11329490691423416, 0.20096798241138458, 0.12888024747371674, 0.13367879390716553, 0.28512224555015564, 0.2807052731513977, 0.1733725368976593]
Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003).	[54, 19, 25, 32, 1, 48, 118, 3, 17, 80]	[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.4204089343547821, 0.7700272798538208, 0.3383341431617737, 0.39023205637931824, 0.27669602632522583, 0.28915104269981384, 0.23218435049057007, 0.15421175956726074, 0.5314942002296448, 0.12764045596122742]
Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al, 2010).	[40, 19, 25, 43, 173, 60, 26, 3, 68, 31]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5269250869750977, 0.8220020532608032, 0.11969514191150665, 0.18491636216640472, 0.16237802803516388, 0.13100016117095947, 0.44752976298332214, 0.21288709342479706, 0.0896039754152298, 0.2694971561431885]
Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003).	[0, 24, 132, 4, 13, 17, 15, 44, 133, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41089048981666565, 0.13717620074748993, 0.12781943380832672, 0.1021561548113823, 0.26004528999328613, 0.1620163470506668, 0.12463023513555527, 0.059757377952337265, 0.05536101385951042, 0.0626073032617569]
Figure 1: The tree structure for an imperative sentence part-of-speech is the WP in Penn Treebank (Marcus et al, 1994) POS tag set) or is an adverb (WRB).	[92, 3, 5, 0, 7, 24, 150, 20, 35, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13359957933425903, 0.16803298890590668, 0.0894971564412117, 0.21065881848335266, 0.3356647789478302, 0.16834205389022827, 0.38909295201301575, 0.13770705461502075, 0.13770705461502075, 0.15048018097877502]
Figure 2: Change in the polarity of the sentences citing Church (1988) paper how the way a published work is perceived by the research community over time.	[28, 123, 29, 62, 3, 30, 45, 83, 44, 31]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.38187918066978455, 0.1020239070057869, 0.08241759240627289, 0.08714798837900162, 0.12681938707828522, 0.13241025805473328, 0.07808216661214828, 0.7001304030418396, 0.09417261183261871, 0.08683314174413681]
Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names.	[444, 42, 47, 451, 334, 331, 592, 530, 364, 309]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 1]	[0.8053084015846252, 0.5070926547050476, 0.06383296102285385, 0.685293972492218, 0.05643980950117111, 0.061287302523851395, 0.10683316737413406, 0.24022236466407776, 0.06498004496097565, 0.5327143669128418]
Filatova and Hovy (2001) infer time values based on the most recently assigned date or the date of the article.	[68, 102, 71, 97, 94, 100, 95, 108, 73, 54]	[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]	[0.8359238505363464, 0.7819758057594299, 0.7053609490394592, 0.78655606508255, 0.6289297342300415, 0.7336429357528687, 0.5171895623207092, 0.5552176237106323, 0.39856860041618347, 0.3862079679965973]
Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al, 2004).	[0, 148, 23, 152, 231, 2, 107, 234, 156, 17]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8389789462089539, 0.8075045943260193, 0.23166261613368988, 0.11543998122215271, 0.14291977882385254, 0.14269036054611206, 0.10016834735870361, 0.1591113805770874, 0.19338876008987427, 0.09347022324800491]
Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web.	[1, 121, 10, 139, 94, 49, 40, 52, 14, 60]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8300834894180298, 0.8005197644233704, 0.29479843378067017, 0.6623216271400452, 0.3144150972366333, 0.41075244545936584, 0.3927287757396698, 0.2724997103214264, 0.14605888724327087, 0.17222163081169128]
Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems.	[68, 29, 36, 27, 55, 19, 69, 1, 26, 10]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6905823349952698, 0.6670311093330383, 0.4641665816307068, 0.30006927251815796, 0.4312935471534729, 0.20009949803352356, 0.20196598768234253, 0.33913275599479675, 0.056394197046756744, 0.1293371617794037]
Finally, surface realization is performed by interfacing RealPro (Lavoie and Rambow, 1997) with a language model.	[2, 19, 4, 1, 5, 53, 10, 51, 15, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3064177930355072, 0.16686750948429108, 0.15938544273376465, 0.12237585335969925, 0.067160964012146, 0.2400616854429245, 0.0746908113360405, 0.26088884472846985, 0.05643855035305023, 0.27516838908195496]
Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008).	[0, 16, 44, 48, 40, 12, 24, 177, 117, 191]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8611114025115967, 0.4316488206386566, 0.1789959967136383, 0.07382501661777496, 0.21503190696239471, 0.18377716839313507, 0.35727459192276, 0.07672233879566193, 0.06573206186294556, 0.15067680180072784]
Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011).	[6, 30, 193, 87, 136, 137, 186, 29, 88, 197]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6873338222503662, 0.33177030086517334, 0.3216855525970459, 0.18992550671100616, 0.4180663228034973, 0.06150899827480316, 0.061927471309900284, 0.08159522712230682, 0.12498787045478821, 0.10290734469890594]
Finally, when evaluated on the datasets of the recent ACL 07 MT workshop (Callison-Burch et al, 2007).	[107, 6, 60, 9, 2, 11, 28, 10, 105, 90]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8486140966415405, 0.5348519682884216, 0.0584695003926754, 0.09651103615760803, 0.05953817069530487, 0.2589607536792755, 0.10633191466331482, 0.4460793137550354, 0.21645624935626984, 0.17790979146957397]
Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations.	[177, 158, 164, 167, 134, 41, 22, 87, 19, 16]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.3082135319709778, 0.24021999537944794, 0.25242191553115845, 0.3318262994289398, 0.1322462111711502, 0.0945245623588562, 0.19859591126441956, 0.5555918216705322, 0.20120452344417572, 0.12217504531145096]
Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser.	[20, 36, 15, 32, 145, 125, 33, 7, 38, 138]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4338661730289459, 0.06582829356193542, 0.1776401549577713, 0.21694017946720123, 0.08360831439495087, 0.15878736972808838, 0.07889467477798462, 0.06290477514266968, 0.06097535416483879, 0.16048353910446167]
First, a quasi logical form allows the under-specification of several types of information, such as anaphoric references, ellipsis and semantic relations (Alshawi and Crouch, 1992).	[2, 8, 62, 37, 128, 6, 27, 119, 137, 1]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.22412553429603577, 0.19646799564361572, 0.19991867244243622, 0.5399988889694214, 0.17447306215763092, 0.15564444661140442, 0.13142289221286774, 0.07315125316381454, 0.20088493824005127, 0.16073930263519287]
First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing: raise the probabilities in the sampling distribution to the 1T power, where T is a temperature parameter, decrease T to wards zero, and take a single sample.	[87, 52, 72, 79, 78, 84, 73, 77, 55, 97]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6182015538215637, 0.09818290919065475, 0.30885449051856995, 0.12251322716474533, 0.4965554475784302, 0.19664062559604645, 0.07563566416501999, 0.1684866100549698, 0.07786381244659424, 0.09708511084318161]
First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998).	[212, 149, 73, 36, 65, 67, 120, 114, 107, 99]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5089948177337646, 0.38459551334381104, 0.05424197390675545, 0.1788109838962555, 0.4339578449726105, 0.0869467630982399, 0.06542370468378067, 0.23040911555290222, 0.1287851631641388, 0.07341834157705307]
Florian et al (2003) employed the same technique in a combination of learners.	[12, 35, 0, 1, 4, 11, 5, 29, 58, 2]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07826686650514603, 0.0759032815694809, 0.31440991163253784, 0.14237932860851288, 0.13458023965358734, 0.08562982082366943, 0.08444439619779587, 0.20002995431423187, 0.06458140909671783, 0.05734623968601227]
Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models.	[135, 2, 76, 7, 51, 16, 17, 70, 123, 29]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4714450240135193, 0.3508271872997284, 0.20236587524414062, 0.05787644162774086, 0.46484285593032837, 0.19391103088855743, 0.41696879267692566, 0.17107030749320984, 0.08659664541482925, 0.08009176701307297]
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	[101, 102, 88, 165, 100, 164, 90, 167, 174, 118]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.4336199164390564, 0.32787904143333435, 0.13176660239696503, 0.30534112453460693, 0.18763533234596252, 0.3273507058620453, 0.520744800567627, 0.29073089361190796, 0.10422709584236145, 0.0680084228515625]
Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as.	[119, 91, 17, 23, 32, 33, 110, 137, 88, 24]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10823839157819748, 0.19682252407073975, 0.17031517624855042, 0.0892920196056366, 0.4005749821662903, 0.07444331794977188, 0.08009582757949829, 0.06898164004087448, 0.05956356227397919, 0.0952097624540329]
Following (Sudo et al, 2003) we are interested only in the lexemes which are near neighbors of the most frequent verbs.	[60, 49, 28, 99, 117, 21, 10, 6, 58, 51]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.629892110824585, 0.11417075991630554, 0.08584262430667877, 0.07504461705684662, 0.07054772973060608, 0.05425550043582916, 0.17916052043437958, 0.17672868072986603, 0.05842478200793266, 0.12526124715805054]
Following Clark and Curran (2007), we assume that each input word has been assigned a POS-tag (from the Penn Treebank tag set) and a set of CCG lexical categories.	[467, 487, 777, 486, 144, 565, 210, 483, 479, 459]	[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.4408457279205322, 0.5094048976898193, 0.43628787994384766, 0.5270540714263916, 0.10736280679702759, 0.46426117420196533, 0.27014100551605225, 0.5455601215362549, 0.2976830005645752, 0.2543689012527466]
Following Kudo et al (2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab2, to our POS tagging task.	[8, 24, 137, 30, 39, 145, 1, 46, 53, 75]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10180123150348663, 0.08391666412353516, 0.185825377702713, 0.33924373984336853, 0.31527575850486755, 0.31395676732063293, 0.18370552361011505, 0.18406935036182404, 0.1736125349998474, 0.1818423867225647]
Following Lee et al.(2010) we used only the training sections for each language.	[176, 164, 72, 111, 64, 51, 202, 169, 80, 112]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.072248674929142, 0.6546493172645569, 0.062075432389974594, 0.15222442150115967, 0.0736268162727356, 0.06029152497649193, 0.08790696412324905, 0.05968395993113518, 0.061751578003168106, 0.07062797248363495]
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	[183, 184, 200, 197, 222, 92, 158, 176, 218, 182]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 1]	[0.8459287285804749, 0.8211942911148071, 0.7198503613471985, 0.6324750781059265, 0.40850409865379333, 0.06528252363204956, 0.37778332829475403, 0.17284828424453735, 0.46543729305267334, 0.6025997400283813]
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	[117, 120, 222, 121, 228, 129, 118, 125, 54, 27]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.8162484169006348, 0.850501537322998, 0.8043864965438843, 0.8078215718269348, 0.7567208409309387, 0.06527465581893921, 0.16682551801204681, 0.29282480478286743, 0.19917772710323334, 0.06180437654256821]
Following Whittaker and Stenton (1988), we use utterance tags to determine whether an utterance shows initiative: forward functions show initiative while others do not.	[34, 126, 106, 26, 35, 13, 127, 128, 4, 133]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6532231569290161, 0.3077373802661896, 0.07691164314746857, 0.2993488609790802, 0.20704364776611328, 0.06474501639604568, 0.06696179509162903, 0.07463684678077698, 0.08708491176366806, 0.11565098166465759]
Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs.	[150, 124, 125, 40, 161, 62, 80, 46, 39, 128]	[1, 1, 0, 0, 1, 0, 0, 0, 1, 0]	[0.7951655983924866, 0.7130966186523438, 0.1593206375837326, 0.21552714705467224, 0.6632719039916992, 0.07503514736890793, 0.16123326122760773, 0.09915345162153244, 0.5475108623504639, 0.07453074306249619]
Following a suggestion in Gale and Church (1993), the alignment was aided by the use of additional anchors that were available for the language pair.	[192, 98, 12, 26, 159, 87, 2, 16, 115, 57]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20550362765789032, 0.12103233486413956, 0.10175158083438873, 0.10175158083438873, 0.1739920824766159, 0.12334856390953064, 0.194926917552948, 0.194926917552948, 0.2673548758029938, 0.1531834602355957]
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	[83, 259, 152, 27, 192, 245, 212, 330, 74, 176]	[1, 1, 0, 0, 1, 1, 0, 0, 0, 0]	[0.8378964066505432, 0.5340868830680847, 0.49451759457588196, 0.21209649741649628, 0.7482509016990662, 0.5114554762840271, 0.4132382869720459, 0.3166636824607849, 0.152986541390419, 0.13479608297348022]
For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al,1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al, 2002) respectively.	[3, 12, 9, 1, 7, 0, 4, 14, 10, 15]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7837650775909424, 0.15738244354724884, 0.1952652931213379, 0.0864444300532341, 0.2825222611427307, 0.23489555716514587, 0.11611882597208023, 0.06727996468544006, 0.16892030835151672, 0.17114609479904175]
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	[29, 40, 10, 1, 32, 31, 27, 66, 26, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3848339915275574, 0.15978316962718964, 0.2176014930009842, 0.21203719079494476, 0.2606882154941559, 0.21604646742343903, 0.33740538358688354, 0.3163069784641266, 0.2853676676750183, 0.29082855582237244]
For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate.	[58, 2, 12, 8, 1, 10, 57, 43, 49, 29]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4957972466945648, 0.12144941091537476, 0.20064638555049896, 0.3400040864944458, 0.2591460049152374, 0.07602124661207199, 0.17404860258102417, 0.08437835425138474, 0.08589839935302734, 0.3132992386817932]
For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003).	[105, 34, 113, 4, 88, 96, 110, 36, 153, 17]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7250185012817383, 0.18183961510658264, 0.08501345664262772, 0.1792251467704773, 0.12293879687786102, 0.19402474164962769, 0.09227742999792099, 0.08549322932958603, 0.11928889900445938, 0.15933863818645477]
For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)).	[90, 50, 111, 33, 95, 3, 128, 167, 113, 121]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8537360429763794, 0.7453183531761169, 0.22133970260620117, 0.1672198474407196, 0.12978458404541016, 0.12186703085899353, 0.1000002771615982, 0.05372127890586853, 0.1618301123380661, 0.12295552343130112]
For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003).	[5, 196, 84, 64, 59, 25, 204, 212, 33, 14]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.78167325258255, 0.5516487956047058, 0.07991736382246017, 0.16480694711208344, 0.3065277338027954, 0.16452251374721527, 0.1033821776509285, 0.06458775699138641, 0.09586842358112335, 0.1202397271990776]
For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002).	[1, 4, 37, 115, 3, 110, 61, 35, 0, 34]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36276403069496155, 0.19306223094463348, 0.16875781118869781, 0.2620490789413452, 0.11074479669332504, 0.2435338795185089, 0.13546082377433777, 0.2706613540649414, 0.49485695362091064, 0.09362399578094482]
For attribute selection, we use the graph-based algorithm of Krahmer et al (2003), one of the highest scoring attribute selection methods in the TUNA 2008 Challenge (Gatt et al (2008), table 11).	[252, 264, 48, 56, 265, 50, 241, 55, 2, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22820639610290527, 0.30211541056632996, 0.09234754741191864, 0.060881756246089935, 0.05879444628953934, 0.05846073105931282, 0.06205890327692032, 0.06054946407675743, 0.07378657162189484, 0.07378657162189484]
For both tree banks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996).	[65, 20, 5, 6, 80, 14, 82, 96, 2, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2929742932319641, 0.24716508388519287, 0.16643638908863068, 0.25075846910476685, 0.1659158319234848, 0.11837437003850937, 0.09963872283697128, 0.184760183095932, 0.11312944442033768, 0.0911686047911644]
For chunking Arabic, we use the AMIRA (ASVMT) toolkit (Diab et al, 2004).	[7, 105, 69, 109, 12, 71, 108, 79, 0, 98]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1281047910451889, 0.16035215556621552, 0.1038929745554924, 0.26866206526756287, 0.09164869040250778, 0.18496346473693848, 0.07983168214559555, 0.10893033444881439, 0.2779814302921295, 0.08033014833927155]
For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA.	[173, 42, 154, 65, 23, 153, 197, 147, 193, 215]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3525677025318146, 0.0664578527212143, 0.1783040314912796, 0.1375492513179779, 0.11244441568851471, 0.11997951567173004, 0.06034368276596069, 0.06391396373510361, 0.05372242629528046, 0.1254042088985443]
For dialogue initiative annotation, I am using Walker and Whittaker's utterance based allocation of control rules (Walker and Whittaker, 1990), which are widely used to identify dialogue initiative.	[27, 0, 54, 46, 8, 19, 138, 238, 40, 55]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7451345920562744, 0.504706859588623, 0.34170547127723694, 0.5188499093055725, 0.1696968674659729, 0.09618702530860901, 0.2570328712463379, 0.20235471427440643, 0.0702093318104744, 0.1006082221865654]
For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005).	[57, 37, 3, 36, 35, 38, 42, 49, 145, 53]	[1, 1, 1, 1, 0, 0, 1, 0, 0, 1]	[0.8467630743980408, 0.839706301689148, 0.8060925006866455, 0.5710718035697937, 0.31832706928253174, 0.33638447523117065, 0.7998093366622925, 0.30460456013679504, 0.20823700726032257, 0.7937467098236084]
For efficiency reasons, we use a coarse-to-fine pruning scheme like that of Caraballo and Charniak (1998).	[22, 154, 1, 5, 20, 203, 60, 29, 62, 2]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4232102632522583, 0.5957438945770264, 0.08549610525369644, 0.08549610525369644, 0.06469756364822388, 0.12208683788776398, 0.05871395394206047, 0.27773118019104004, 0.08910347521305084, 0.17365866899490356]
For evaluation the parser returns dependency structures, but we have also developed a module which builds first order semantic representations from the derivations, which can be used for inference (Bos et al, 2004).	[12, 2, 13, 16, 6, 1, 0, 3, 18, 5]	[1, 1, 0, 0, 0, 1, 1, 0, 0, 0]	[0.5415292978286743, 0.765553891658783, 0.2295912355184555, 0.13828691840171814, 0.26032570004463196, 0.5053603053092957, 0.6686655282974243, 0.13473699986934662, 0.46385878324508667, 0.1171293705701828]
For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007).	[27, 31, 36, 58, 14, 3, 48, 22, 24, 122]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10800816863775253, 0.1226625069975853, 0.09202849864959717, 0.08992134034633636, 0.09997450560331345, 0.08406807482242584, 0.0915021151304245, 0.10222302377223969, 0.0720374807715416, 0.1252070516347885]
For example Galley and Manning (2008) propose a shift-reduce style method to allow hieararchical non-local reorderings in a phrase-based decoder.	[28, 97, 88, 3, 115, 113, 23, 130, 12, 129]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5721790194511414, 0.7312219738960266, 0.23756253719329834, 0.12729404866695404, 0.24499960243701935, 0.2073245793581009, 0.05393117666244507, 0.09961720556020737, 0.10346344858407974, 0.1870138943195343]
For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set.	[37, 96, 38, 81, 67, 192, 139, 57, 142, 45]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7648852467536926, 0.23405860364437103, 0.23150797188282013, 0.17427316308021545, 0.09344341605901718, 0.14077970385551453, 0.1500697135925293, 0.10786785930395126, 0.060849424451589584, 0.18725888431072235]
For example transducer determinization (Mohri, 1997) uses a set of pairs of states and weights.	[97, 240, 379, 493, 569, 361, 213, 585, 254, 212]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8356125354766846, 0.8067746758460999, 0.34072309732437134, 0.2968653440475464, 0.3755371570587158, 0.13236665725708008, 0.3560732305049896, 0.3974986970424652, 0.38581401109695435, 0.4553590416908264]
For example, (Pang et al, 2002) collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g., number of stars) given by the reviewer.	[36, 37, 33, 8, 39, 47, 45, 43, 9, 101]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6358685493469238, 0.566489040851593, 0.2851162552833557, 0.19241443276405334, 0.4445198178291321, 0.5610460042953491, 0.2929057478904724, 0.2178209275007248, 0.24850858747959137, 0.31146663427352905]
For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators.	[132, 129, 90, 223, 121, 83, 201, 23, 128, 89]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.790277361869812, 0.41952764987945557, 0.394453227519989, 0.2519243359565735, 0.33545413613319397, 0.3158923387527466, 0.20719937980175018, 0.2219027727842331, 0.1702040284872055, 0.2605985105037689]
For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm.	[180, 42, 33, 101, 198, 195, 31, 105, 213, 229]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7524205446243286, 0.24406798183918, 0.20368732511997223, 0.31100404262542725, 0.16248519718647003, 0.09462979435920715, 0.05309053510427475, 0.11562570929527283, 0.0812288448214531, 0.17696398496627808]
For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question.	[88, 70, 16, 108, 44, 148, 20, 150, 201, 35]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8113227486610413, 0.6572768092155457, 0.4753052890300751, 0.13630178570747375, 0.08519595861434937, 0.22852714359760284, 0.2811351418495178, 0.09185731410980225, 0.1732335388660431, 0.17063958942890167]
For example, Li et al (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it is beneficial to integrate word segmentation and part-of-speech tagging into one model.	[0, 12, 1, 139, 117, 67, 7, 120, 104, 56]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4433387815952301, 0.3151933252811432, 0.2313593327999115, 0.16232182085514069, 0.23723964393138885, 0.09179340302944183, 0.06006516516208649, 0.23259207606315613, 0.09482839703559875, 0.09943105280399323]
For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees.	[58, 74, 134, 66, 55, 60, 33, 68, 67, 75]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5244207978248596, 0.19915178418159485, 0.18231205642223358, 0.2896733283996582, 0.09431392699480057, 0.4407784342765808, 0.05869905650615692, 0.22472098469734192, 0.2583779990673065, 0.21087318658828735]
For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system.	[102, 105, 88, 106, 104, 4, 99, 103, 18, 80]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8307235836982727, 0.20945823192596436, 0.7922618389129639, 0.32824185490608215, 0.34647005796432495, 0.19689851999282837, 0.1878822296857834, 0.13036422431468964, 0.09791477024555206, 0.13094595074653625]
For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software.	[38, 147, 40, 153, 2, 77, 101, 74, 120, 72]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7803090810775757, 0.811532199382782, 0.6999260187149048, 0.24242648482322693, 0.28703123331069946, 0.4496293365955353, 0.29942476749420166, 0.17904280126094818, 0.3537518382072449, 0.22899357974529266]
For example, Pedersen and Bruce (1997) cluster the occurrences of an ambiguous word by constructing a vector of terms occurring in the context of the target.	[206, 207, 125, 219, 120, 115, 28, 122, 126, 203]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3173166513442993, 0.1860547959804535, 0.3715563416481018, 0.39041629433631897, 0.4091930091381073, 0.2601637840270996, 0.21880032122135162, 0.13170954585075378, 0.10861153900623322, 0.20181097090244293]
For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words.	[171, 70, 11, 1, 175, 33, 37, 45, 0, 4]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6711167693138123, 0.6853975653648376, 0.10129968822002411, 0.09371811151504517, 0.0890544131398201, 0.2422860562801361, 0.06736408174037933, 0.26822197437286377, 0.43223804235458374, 0.1636149287223816]
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form.	[6, 39, 0, 209, 141, 162, 197, 8, 157, 196]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.7596301436424255, 0.21171368658542633, 0.4360774755477905, 0.21183966100215912, 0.24273981153964996, 0.39103972911834717, 0.09813214093446732, 0.13250011205673218, 0.5453557372093201, 0.0735415518283844]
For example, given golden negation signals on the Bioscope corpus, Morante and Daelemans (2009) only got the performance of 50.26% in PCS (percentage of correct scope) measure on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS measure on the clinical reports subcorpus (6.6 words per sentence on average).	[152, 167, 169, 156, 171, 165, 170, 15, 138, 155]	[0, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.3990573287010193, 0.8095141053199768, 0.5481666922569275, 0.7256461381912231, 0.31622809171676636, 0.5535743236541748, 0.40438759326934814, 0.18718841671943665, 0.14910076558589935, 0.3704390227794647]
For example, motivated by the fact that some coreference relations are harder to identify than the others (see Harabagiu et al (2001)), Ng and Cardie (2002a) present a method for mining easy positive instances, in an attempt to avoid the inclusion of hard training instances that may complicate the acquisition of an accurate coreference model.	[0, 50, 21, 72, 35, 24, 33, 60, 19, 22]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7457964420318604, 0.17541196942329407, 0.2372688502073288, 0.08659010380506516, 0.10587380826473236, 0.06553327292203903, 0.12432321906089783, 0.06311079114675522, 0.07727424055337906, 0.09521280974149704]
For example, the discriminative training techniques successfully applied in (Henderson, 2004) to the feed-forward neural network model can be directly applied to the mean field model proposed in this paper.	[167, 79, 8, 19, 4, 0, 164, 113, 57, 158]	[1, 1, 0, 1, 1, 1, 0, 0, 0, 0]	[0.834832489490509, 0.8408335447311401, 0.36570823192596436, 0.5889754891395569, 0.5626901984214783, 0.6799647808074951, 0.2940896153450012, 0.1427905559539795, 0.09691163152456284, 0.09521864354610443]
For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.	[91, 93, 92, 103, 132, 87, 4, 19, 52, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35365232825279236, 0.3185424208641052, 0.14070409536361694, 0.10500378906726837, 0.09329194575548172, 0.44082990288734436, 0.05869605392217636, 0.051621705293655396, 0.12371964752674103, 0.32044291496276855]
For example, the lexical cohesion profile (Kozima, 1993) should be perfectly usable with our fragmentation method.	[13, 27, 1, 23, 25, 69, 2, 20, 14, 15]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 1]	[0.43862324953079224, 0.775119423866272, 0.3655891418457031, 0.5048721432685852, 0.28563863039016724, 0.31643033027648926, 0.25530093908309937, 0.2674255967140198, 0.2728016674518585, 0.6157520413398743]
For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the state split grammars of Petrov et al (2006) are all too large to construct unpruned charts in memory.	[0, 7, 111, 24, 1, 5, 13, 40, 90, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5019188523292542, 0.12505005300045013, 0.05264449864625931, 0.1465623527765274, 0.08281444758176804, 0.06521505862474442, 0.35547390580177307, 0.14814819395542145, 0.055354371666908264, 0.054086022078990936]
For example, the pattern& quot; NP1 ,a|an NP2& quot;, ranked among the top IS-A pat terns by (Pantel et al, 2004), can represent both apposition (entailing) and a list of co-hyponyms (non-entailing).	[140, 20, 27, 47, 139, 149, 174, 98, 69, 25]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26740267872810364, 0.13584020733833313, 0.14216619729995728, 0.3175606429576874, 0.095908522605896, 0.08033398538827896, 0.1089228093624115, 0.11343216150999069, 0.1287716031074524, 0.2081885188817978]
For example, we anticipate that sentences with quotation marks will be problematic, as other researchers have observed that quoted text requires special handling for pronoun resolution (Kennedy and Boguraev, 1996).	[149, 158, 16, 41, 10, 164, 160, 161, 11, 144]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.5236382484436035, 0.22175723314285278, 0.16328926384449005, 0.14419418573379517, 0.18761126697063446, 0.08141811192035675, 0.7224681377410889, 0.058198850601911545, 0.09594392031431198, 0.1846240758895874]
For experiments with PropBank, we used the Ontonotes corpus (Hovy et al, 2006), version 4.0, and only made use of the Wall Street Journal documents; we used sections 221 for training, section 24 for development and section 23 for testing.	[21, 10, 26, 3, 33, 7, 17, 65, 37, 44]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7460916638374329, 0.5039084553718567, 0.5468331575393677, 0.16572074592113495, 0.14001697301864624, 0.1351345330476761, 0.2545045018196106, 0.2572035789489746, 0.0897279605269432, 0.06529053300619125]
For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%.	[92, 133, 39, 200, 110, 98, 23, 101, 38, 45]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8097870349884033, 0.8236774802207947, 0.06422815471887589, 0.09191393107175827, 0.06568346917629242, 0.13729098439216614, 0.07455844432115555, 0.14915405213832855, 0.08581532537937164, 0.18654057383537292]
For getting the syntax trees, the latest version of Collins' parser (Collins, 1997) was used.	[11, 4, 15, 90, 3, 24, 8, 123, 18, 122]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5659363269805908, 0.2756655216217041, 0.38263916969299316, 0.2889522314071655, 0.2816098928451538, 0.31460076570510864, 0.15318816900253296, 0.08495625853538513, 0.07435309886932373, 0.1078701987862587]
For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes.	[97, 106, 84, 5, 76, 71, 96, 32, 81, 67]	[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.31901657581329346, 0.7618764042854309, 0.3519521653652191, 0.5155169367790222, 0.3108232915401459, 0.22782742977142334, 0.1302444338798523, 0.6558422446250916, 0.30507028102874756, 0.13340511918067932]
For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to co occur in time across languages.	[15, 10, 35, 0, 1, 28, 26, 21, 25, 23]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48682746291160583, 0.7240679860115051, 0.06153050437569618, 0.4979875385761261, 0.10621752589941025, 0.055785343050956726, 0.062291596084833145, 0.08927661180496216, 0.07563506066799164, 0.06356657296419144]
For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier (2004) use Turney's (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration.	[16, 30, 87, 31, 56, 3, 132, 139, 85, 14]	[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.7752760052680969, 0.27052390575408936, 0.5929813981056213, 0.6252833604812622, 0.4909064769744873, 0.15179702639579773, 0.23999904096126556, 0.1845981925725937, 0.2372434139251709, 0.14485064148902893]
For instance, the classical Dale and Reiter algorithms compute purely conjunctive formulas; van Deemter (2002) extends this language by adding the other propositional connectives, whereas Dale and Haddock (1991) extends it by allowing existential quantification.	[53, 7, 57, 24, 210, 22, 32, 41, 183, 16]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8211871385574341, 0.24973063170909882, 0.3823178708553314, 0.1536560207605362, 0.16256913542747498, 0.24935109913349152, 0.23780453205108643, 0.06936393678188324, 0.13367237150669098, 0.20251083374023438]
For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008).	[165, 106, 166, 142, 44, 102, 145, 6, 116, 181]	[1, 0, 0, 0, 1, 1, 0, 0, 0, 0]	[0.5951898694038391, 0.3557460308074951, 0.3622978627681732, 0.3073837161064148, 0.6650670766830444, 0.503153383731842, 0.1439978927373886, 0.2081904113292694, 0.30016204714775085, 0.06267887353897095]
For our experiments we use the Wall Street Journal dataset created by Ratnaparkhi et al (1994).	[87, 77, 96, 54, 19, 82, 146, 8, 69, 78]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.8383103609085083, 0.39663413166999817, 0.32611703872680664, 0.13209934532642365, 0.08617641031742096, 0.19208526611328125, 0.5974792242050171, 0.07238058000802994, 0.3319893181324005, 0.235954150557518]
For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002).	[98, 15, 55, 109, 114, 112, 14, 4, 110, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5069676637649536, 0.17671522498130798, 0.15435729920864105, 0.2546602487564087, 0.2355951964855194, 0.23062027990818024, 0.07962141931056976, 0.09289336204528809, 0.19661003351211548, 0.16116894781589508]
For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000).	[0, 1, 152, 24, 38, 91, 4, 144, 61, 124]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7271173000335693, 0.4945583641529083, 0.1104983538389206, 0.20449425280094147, 0.09071928262710571, 0.07059124857187271, 0.26609885692596436, 0.12148216366767883, 0.09292034804821014, 0.1275634616613388]
For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al,2001), PropBank (Palmer et al, 2005), and NomBank (Meyers et al, 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al, 2002), and the NAIST Text Corpus (Iida et al, 2007) in Japanese.	[8, 3, 51, 6, 1, 171, 7, 2, 31, 15]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8034996390342712, 0.20482416450977325, 0.17790821194648743, 0.21655160188674927, 0.2087753713130951, 0.1721327304840088, 0.15187956392765045, 0.14535681903362274, 0.11287232488393784, 0.10150765627622604]
For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used.	[61, 66, 3, 54, 16, 104, 29, 6, 5, 7]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7716925144195557, 0.7154063582420349, 0.3611074388027191, 0.31017518043518066, 0.1517411470413208, 0.11219453066587448, 0.2322266399860382, 0.21992555260658264, 0.10214285552501678, 0.4783863425254822]
For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al, 2005), which is a bracketed corpus in (almost) PTB style.	[188, 185, 6, 53, 192, 150, 87, 175, 116, 104]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37670937180519104, 0.17131440341472626, 0.05964019522070885, 0.05155302211642265, 0.11294933408498764, 0.058953866362571716, 0.05635727569460869, 0.4018556475639343, 0.05280403047800064, 0.06818099319934845]
For semantically oriented tools such as SRL systems, it is important to also assess their results w.r.t. the task which they are meant support namely reasoning: Do the semantic representations built by SRL help in making the correct inferences? Can they be used, for instance, to determine whether a given sentence answers a given question? or whether the content of one sentence follow from that another? As explained in (Giampiccolo et al, 2007), entailment recognition is a first, major step towards answering these questions.	[67, 41, 9, 68, 69, 145, 8, 70, 122, 6]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.4330381155014038, 0.22053149342536926, 0.1487467885017395, 0.18494123220443726, 0.34171798825263977, 0.1041719987988472, 0.5035170912742615, 0.07550495117902756, 0.18601618707180023, 0.17188973724842072]
For sentences out of coverage, it employs the robustness techniques (fragment parsing, 'skimming') implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis.	[14, 17, 66, 12, 72, 96, 31, 106, 127, 134]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7617000937461853, 0.5697385668754578, 0.34626147150993347, 0.26608577370643616, 0.18191497027873993, 0.29704076051712036, 0.3050667345523834, 0.12686637043952942, 0.12051603198051453, 0.238398015499115]
For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)).	[32, 138, 16, 167, 4, 12, 68, 30, 6, 162]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8265035152435303, 0.7667568922042847, 0.6401657462120056, 0.18242433667182922, 0.12560683488845825, 0.1959654539823532, 0.14078259468078613, 0.1038598120212555, 0.11438990384340286, 0.06752223521471024]
For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig.	[56, 150, 69, 36, 178, 21, 196, 29, 16, 104]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2938721179962158, 0.0597035251557827, 0.09628651291131973, 0.08841397613286972, 0.12469732016324997, 0.06149232015013695, 0.1317528933286667, 0.07216483354568481, 0.06652095168828964, 0.057012856006622314]
For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy to calculate exact prefix probabilities using the algorithm of Stolcke (1995).	[223, 425, 0, 424, 1, 6, 193, 120, 431, 199]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6510098576545715, 0.7143605351448059, 0.6807717084884644, 0.22723297774791718, 0.21316871047019958, 0.21316871047019958, 0.2801397144794464, 0.35995975136756897, 0.10715687274932861, 0.32325997948646545]
For the algorithm discussed in Section 4.1, we derived our descriptive properties using the output of the dependency analysis generated by the Minipar (Lin, 1994) dependency parser.	[15, 196, 1, 17, 19, 73, 6, 201, 160, 21]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6942623853683472, 0.2437882423400879, 0.2942982614040375, 0.1512463390827179, 0.11477114260196686, 0.05679912492632866, 0.058586716651916504, 0.18035957217216492, 0.09065701067447662, 0.0970521941781044]
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality.	[22, 0, 16, 1, 171, 10, 38, 24, 64, 163]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6008914709091187, 0.47505366802215576, 0.16264213621616364, 0.16130122542381287, 0.11529933661222458, 0.09210729598999023, 0.07448068261146545, 0.05660850554704666, 0.057821452617645264, 0.07063841819763184]
For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given.	[110, 119, 3, 6, 37, 101, 65, 41, 92, 90]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49834585189819336, 0.24559414386749268, 0.1267622411251068, 0.11729373782873154, 0.30792537331581116, 0.06418731808662415, 0.29423338174819946, 0.08192520588636398, 0.09442729502916336, 0.05556931719183922]
For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)).	[15, 11, 28, 47, 132, 122, 37, 67, 69, 133]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2475767731666565, 0.11464798450469971, 0.14133530855178833, 0.16955973207950592, 0.21857930719852448, 0.15819421410560608, 0.11114764213562012, 0.12528979778289795, 0.1594446450471878, 0.154560849070549]
For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction.	[38, 2, 53, 19, 43, 164, 11, 59, 48, 37]	[1, 0, 1, 1, 1, 0, 0, 0, 0, 0]	[0.8184793591499329, 0.361423134803772, 0.5921977758407593, 0.7853657603263855, 0.5580131411552429, 0.38530537486076355, 0.17973390221595764, 0.20132532715797424, 0.22784198820590973, 0.14318634569644928]
For the part of speech tagging, the memory-based tagger MBT (Daelemans et al, 1996), trained on the Wall Street Journal corpus2, was used.	[0, 89, 1, 78, 3, 10, 12, 5, 4, 121]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.794933021068573, 0.1356840282678604, 0.6201170086860657, 0.25395452976226807, 0.2414415031671524, 0.38830727338790894, 0.18089193105697632, 0.22909653186798096, 0.13233008980751038, 0.07740771025419235]
For the predicate identification, we used the features suggested by Johansson and Nugues (2008).	[79, 88, 143, 66, 75, 39, 29, 84, 125, 12]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.8373410105705261, 0.8255159854888916, 0.3821915090084076, 0.6545769572257996, 0.5917955040931702, 0.2738194465637207, 0.14134345948696136, 0.3018350601196289, 0.13763679563999176, 0.33254751563072205]
For the topic level, they achieve similar results as (Galley et al, 2003), with the supervised approach outperforming LCSeg.	[126, 144, 91, 40, 43, 138, 140, 19, 22, 24]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6830925345420837, 0.29152703285217285, 0.23312459886074066, 0.23046255111694336, 0.11670000106096268, 0.18815051019191742, 0.117032490670681, 0.19356754422187805, 0.17892302572727203, 0.3684292435646057]
For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009).	[44, 45, 22, 52, 50, 147, 46, 148, 24, 137]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5841085910797119, 0.3649235963821411, 0.45149120688438416, 0.22854676842689514, 0.1376204937696457, 0.1861523538827896, 0.1547078788280487, 0.1778714656829834, 0.0629369392991066, 0.07413209229707718]
For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).	[1, 7, 6, 18, 85, 0, 4, 13, 5, 41]	[1, 1, 0, 0, 0, 1, 0, 1, 0, 0]	[0.6937893033027649, 0.8336721658706665, 0.3798346221446991, 0.2464538812637329, 0.12240885198116302, 0.5308173894882202, 0.3923971354961395, 0.6190054416656494, 0.20310541987419128, 0.14453402161598206]
"For this preliminary experiment, we used the ""line"" dataset of a word sense disambiguation task (Leacock et al, 1993)."	[26, 120, 27, 8, 32, 159, 17, 2, 25, 79]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4865604043006897, 0.42462897300720215, 0.11108655482530594, 0.13802361488342285, 0.6305502653121948, 0.2809401750564575, 0.1776297241449356, 0.1349271833896637, 0.13242462277412415, 0.06374545395374298]
For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb.	[10, 14, 34, 70, 103, 104, 28, 85, 61, 88]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.054886236786842346, 0.41415858268737793, 0.14630912244319916, 0.056934431195259094, 0.08555569499731064, 0.33853578567504883, 0.12397924065589905, 0.13242720067501068, 0.14104726910591125, 0.10349765419960022]
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	[11, 10, 15, 4, 34, 255, 129, 19, 264, 145]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7766193151473999, 0.23232822120189667, 0.28169184923171997, 0.2877884805202484, 0.4537655711174011, 0.19906969368457794, 0.1478661298751831, 0.2557796835899353, 0.07830578833818436, 0.06834083050489426]
Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3.	[0, 106, 102, 120, 14, 66, 103, 113, 110, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8349238038063049, 0.13411207497119904, 0.38314300775527954, 0.18544454872608185, 0.12641720473766327, 0.09554349631071091, 0.1684916615486145, 0.13883183896541595, 0.07636173069477081, 0.34540191292762756]
From now on, we shall refer to the Chunk tag of a word as its IOB value (IOB was named by Tjong Kim Sang and Jorn Veeenstra (Tjong Kim Sang and Veenstra, 1999) after Ratnaparkhi (Ratnaparkhi, 1998)).	[11, 28, 46, 117, 69, 104, 62, 6, 110, 88]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6085144281387329, 0.7600809335708618, 0.2837626338005066, 0.07249962538480759, 0.17162063717842102, 0.10505172610282898, 0.07851766049861908, 0.08337384462356567, 0.09787006676197052, 0.2094154804944992]
From the different diagnostics proposed in the literature some are quite consistent among various authors (R. Grishman et al 1994, C. Pollard and I. Sag 1987, C. Verspoor 1997).	[47, 105, 55, 104, 54, 32, 40, 82, 97, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11046929657459259, 0.18059344589710236, 0.2662036716938019, 0.10299281030893326, 0.2351645678281784, 0.0989191010594368, 0.14245103299617767, 0.09319174289703369, 0.09278365969657898, 0.062378596514463425]
Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002).	[31, 16, 233, 186, 44, 38, 60, 210, 209, 90]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5216948986053467, 0.29743239283561707, 0.1958034187555313, 0.372188001871109, 0.13249845802783966, 0.15184932947158813, 0.12173658609390259, 0.09253807365894318, 0.3009844720363617, 0.2853686511516571]
Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009).	[71, 220, 250, 0, 5, 211, 218, 4, 201, 197]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.691180944442749, 0.5521436929702759, 0.24138203263282776, 0.5127072930335999, 0.22021153569221497, 0.45699700713157654, 0.25912123918533325, 0.19122710824012756, 0.12589958310127258, 0.18478797376155853]
Furthermore, previous research shows the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks (Callison-Burch, 2009) and the effectiveness of using MTurk for a variety of natural language automation tasks (Snow, Jurafsy, & O'Connor, 2008).	[13, 138, 34, 11, 162, 42, 57, 100, 170, 95]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8425945043563843, 0.3741390109062195, 0.0818312019109726, 0.1183755174279213, 0.37506482005119324, 0.08112718909978867, 0.06660719960927963, 0.12166031450033188, 0.11728932708501816, 0.06571446359157562]
Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's t, which is a measure of how much an ordering differs from the OSO --- the underlying assumption is that most reasonable sentence orderings should be fairly similar to it.	[42, 20, 30, 3, 219, 64, 34, 148, 44, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17492729425430298, 0.24499674141407013, 0.23636268079280853, 0.06365121155977249, 0.08855688571929932, 0.05133542791008949, 0.05518815666437149, 0.0655408650636673, 0.1551729142665863, 0.09644857794046402]
Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010).	[11, 70, 38, 50, 25, 49, 22, 4, 42, 45]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26835113763809204, 0.2830256521701813, 0.14523254334926605, 0.14885516464710236, 0.16687960922718048, 0.10877266526222229, 0.15329031646251678, 0.05236222222447395, 0.07437320798635483, 0.058423422276973724]
Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information.	[50, 121, 73, 45, 111, 150, 8, 60, 81, 52]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5801502466201782, 0.3644365072250366, 0.1000448539853096, 0.22873467206954956, 0.06479769945144653, 0.08925491571426392, 0.1962912678718567, 0.08295487612485886, 0.06595879048109055, 0.06857462227344513]
Gale and Church (1991) noted that the byte length ratio of target sentence to source sentence is normally distributed.	[45, 95, 157, 171, 77, 55, 162, 154, 170, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39887309074401855, 0.2594088912010193, 0.46537500619888306, 0.09732618182897568, 0.21595928072929382, 0.08764377981424332, 0.18249288201332092, 0.296135276556015, 0.19050543010234833, 0.05685211718082428]
Galley et al (2006 )argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling.	[8, 197, 2, 7, 109, 108, 209, 214, 169, 208]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6626563668251038, 0.5601531863212585, 0.3175690472126007, 0.09217075258493423, 0.1704498529434204, 0.08632290363311768, 0.08950232714414597, 0.33374834060668945, 0.09520053118467331, 0.13968929648399353]
Gamon et al (2008) worked on a similar approach using only tagged trigram left and right contexts: a model of prepositions uses serves to identify preposition errors and the Web provides examples of correct form.	[46, 71, 164, 102, 35, 3, 0, 2, 165, 133]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.44724786281585693, 0.3235169053077698, 0.36636242270469666, 0.1368139684200287, 0.5840479135513306, 0.07679208368062973, 0.4662415087223053, 0.2271069884300232, 0.05268460139632225, 0.1303182989358902]
Ge and Mooney also presented a statistical method (Ge and Mooney, 2005) by merging syn tactic and semantic information.	[0, 37, 14, 17, 2, 55, 29, 35, 23, 5]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7422728538513184, 0.28355416655540466, 0.1604067087173462, 0.19769199192523956, 0.20616844296455383, 0.06746994704008102, 0.2594257593154907, 0.4427263140678406, 0.08943301439285278, 0.055656805634498596]
Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists.	[3, 93, 67, 63, 2, 41, 47, 10, 35, 64]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.5056867003440857, 0.40631303191185, 0.056253474205732346, 0.11259011179208755, 0.07155577093362808, 0.767993152141571, 0.4850771427154541, 0.37220343947410583, 0.1478883922100067, 0.2963660955429077]
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.	[7, 8, 117, 3, 111, 18, 106, 61, 51, 71]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5264458656311035, 0.14875610172748566, 0.14501503109931946, 0.12053489685058594, 0.21723482012748718, 0.23919782042503357, 0.24373455345630646, 0.1699339747428894, 0.30583131313323975, 0.4742319881916046]
Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy.	[12, 14, 51, 3, 108, 69, 10, 75, 2, 49]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2642195224761963, 0.36316755414009094, 0.3453134000301361, 0.11317615956068039, 0.16379979252815247, 0.27563104033470154, 0.08098027110099792, 0.16763493418693542, 0.07474255561828613, 0.165583536028862]
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	[1, 14, 2, 0, 130, 259, 11, 106, 81, 67]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8380769491195679, 0.7469478249549866, 0.6952129602432251, 0.2618798017501831, 0.23459382355213165, 0.26110824942588806, 0.11915455013513565, 0.20303809642791748, 0.4142336845397949, 0.17733246088027954]
Given a weight vector w, the scorew? f (x, y) ranks possible labelings of x, and we denote by Yk, w (x) the set of k top scoring labelings for x. We use the standard B, I, O encoding for named entities (Ramshaw and Marcus, 1995).	[81, 108, 82, 121, 139, 120, 10, 61, 62, 112]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16551567614078522, 0.08218471705913544, 0.14279845356941223, 0.08748870342969894, 0.10523755848407745, 0.07794535160064697, 0.0657714232802391, 0.07950356602668762, 0.10600224137306213, 0.08445165306329727]
Given an automatically parsed corpus, Bergsma and Lin (2006) extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.	[35, 13, 12, 15, 2, 84, 74, 38, 108, 78]	[1, 1, 1, 1, 0, 0, 1, 0, 0, 0]	[0.8193272948265076, 0.7071828842163086, 0.8038791418075562, 0.54226154088974, 0.4335503876209259, 0.36924052238464355, 0.5310716032981873, 0.42608270049095154, 0.19106163084506989, 0.11948494613170624]
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	[9, 1, 236, 12, 8, 230, 186, 22, 34, 61]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34183573722839355, 0.15655553340911865, 0.15530197322368622, 0.08556824922561646, 0.0788571834564209, 0.133794903755188, 0.0818173736333847, 0.35416343808174133, 0.05760063976049423, 0.061149124056100845]
Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and Ngai, 2001).	[9, 45, 43, 38, 79, 18, 59, 7, 10, 60]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15052227675914764, 0.06414849311113358, 0.05935550481081009, 0.05579614266753197, 0.16370296478271484, 0.09744739532470703, 0.05857154354453087, 0.11001021414995193, 0.05964277312159538, 0.05217171460390091]
Given the semantic objects defined in the previous section, we design a convolution kernel in a way similar to the parse-tree kernel proposed in (Collins and Duffy, 2002).	[10, 15, 2, 11, 19, 7, 9, 22, 0, 14]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8055598139762878, 0.7847813367843628, 0.08551386743783951, 0.17335934937000275, 0.19828253984451294, 0.10879601538181305, 0.07569803297519684, 0.11892281472682953, 0.20124897360801697, 0.10700052231550217]
Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions.	[2, 0, 188, 23, 36, 46, 25, 22, 41, 21]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8105855584144592, 0.8166516423225403, 0.7790929079055786, 0.25347161293029785, 0.2764401137828827, 0.24498747289180756, 0.2882520854473114, 0.2081713229417801, 0.2530496418476105, 0.23504731059074402]
Golding (1995) builds a classifier based on a rich set of context features.	[18, 330, 16, 9, 74, 264, 199, 333, 262, 53]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4137212634086609, 0.4405810534954071, 0.3033689260482788, 0.20324158668518066, 0.2105235755443573, 0.13086454570293427, 0.1194816529750824, 0.3815056085586548, 0.3415590226650238, 0.22805124521255493]
Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models.	[142, 184, 141, 23, 61, 20, 0, 167, 113, 81]	[1, 1, 1, 0, 0, 0, 1, 0, 0, 0]	[0.8526166677474976, 0.8560220003128052, 0.7162726521492004, 0.2249384969472885, 0.31510788202285767, 0.20836764574050903, 0.6856418251991272, 0.08820521086454391, 0.06849391758441925, 0.4993002116680145]
Goodman (1996) observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse.	[97, 67, 25, 6, 27, 37, 2, 1, 72, 17]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7961997985839844, 0.6152019500732422, 0.5174112915992737, 0.17479506134986877, 0.3511270582675934, 0.4760499894618988, 0.15657910704612732, 0.2001061886548996, 0.3373895585536957, 0.09632138162851334]
Graph based SSL learning has been successfully applied to opinion detection (Pang and Lee, 2004) but is not appropriate for dealing with large scale data sets.	[4, 28, 104, 19, 58, 2, 46, 9, 22, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0991039052605629, 0.05762943625450134, 0.05899370089173317, 0.16891765594482422, 0.06268130987882614, 0.10901924967765808, 0.058098942041397095, 0.06565068662166595, 0.1652534008026123, 0.05839145556092262]
Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features.	[111, 22, 2, 15, 5, 14, 66, 26, 159, 18]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6029909253120422, 0.5086925625801086, 0.21845178306102753, 0.1734859198331833, 0.22658926248550415, 0.13260653614997864, 0.09007421135902405, 0.2946830093860626, 0.2246243953704834, 0.07320144772529602]
Hall (2003) is a lattice-parser related to Charniak (2001).	[13, 18, 31, 1, 41, 16, 19, 4, 6, 40]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42404982447624207, 0.32706981897354126, 0.28216373920440674, 0.17096352577209473, 0.313299298286438, 0.0546368733048439, 0.2543765902519226, 0.1089332178235054, 0.10479546338319778, 0.07583688944578171]
Hatzivassiloglou and McKeown (1997) clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering.	[138, 3, 83, 93, 122, 86, 87, 64, 85, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2634669244289398, 0.3801295757293701, 0.1724015176296234, 0.3251091539859772, 0.19118936359882355, 0.21481111645698547, 0.10167275369167328, 0.08003763854503632, 0.06842268258333206, 0.24649646878242493]
Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model.	[2, 125, 80, 17, 4, 178, 18, 60, 76, 6]	[1, 1, 1, 0, 0, 1, 1, 0, 0, 0]	[0.7886637449264526, 0.7980467081069946, 0.772494912147522, 0.22007064521312714, 0.41690686345100403, 0.5590936541557312, 0.7318485975265503, 0.27382445335388184, 0.1590394377708435, 0.1584472954273224]
Hence, besides gathering examples from the widely usedSEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996).	[121, 188, 32, 126, 81, 151, 14, 127, 138, 94]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5036036968231201, 0.20405526459217072, 0.3116796016693115, 0.35755279660224915, 0.29293447732925415, 0.11434479057788849, 0.08913213014602661, 0.16468653082847595, 0.11544524133205414, 0.2420758455991745]
Here we used the Tchai algorithm (Komachi and Suzuki, 2008), a modified version of Espresso (Pantel and Pennacchiotti, 2006) to collect such candidates.	[52, 111, 104, 12, 148, 156, 149, 48, 35, 25]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46918168663978577, 0.5277210474014282, 0.4821767210960388, 0.15503761172294617, 0.06590577960014343, 0.06010020524263382, 0.20951201021671295, 0.1458963304758072, 0.10691869258880615, 0.4761013686656952]
Here, a standard technique of estimating bilingual term correspondences from comparable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed.	[26, 104, 107, 19, 0, 11, 18, 2, 17, 5]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.0728292465209961, 0.2604755759239197, 0.29147958755493164, 0.26355594396591187, 0.5244191884994507, 0.1848503202199936, 0.17121875286102295, 0.10194502025842667, 0.0593474879860878, 0.06008129566907883]
History-based parsing models rely on features of the derivation history to predict the next parser action (Black et al, 1992).	[40, 54, 0, 35, 34, 101, 151, 124, 56, 120]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5438270568847656, 0.7791642546653748, 0.44488683342933655, 0.364782452583313, 0.0980328917503357, 0.17991994321346283, 0.14056803286075592, 0.1171272024512291, 0.06424131989479065, 0.0655517578125]
However unlike (Zhang et al, 2006), their method only addresses simple grammars.	[87, 4, 29, 8, 62, 83, 37, 10, 1, 22]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3777891993522644, 0.5465543270111084, 0.165924534201622, 0.32093700766563416, 0.157805398106575, 0.10649245232343674, 0.351066529750824, 0.12426722049713135, 0.12595772743225098, 0.29900112748146057]
However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions.	[83, 94, 51, 81, 61, 85, 86, 62, 34, 97]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11520711332559586, 0.07081035524606705, 0.3208620846271515, 0.21079717576503754, 0.11959070712327957, 0.3825540840625763, 0.16114471852779388, 0.10218900442123413, 0.33153802156448364, 0.07030009478330612]
However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al, 1991) and separation of senses (Dyvik, 1998).	[44, 6, 25, 46, 110, 45, 37, 100, 58, 52]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33423858880996704, 0.6341586112976074, 0.1516595482826233, 0.1907300055027008, 0.3123641014099121, 0.17578917741775513, 0.269315242767334, 0.13523253798484802, 0.2578302025794983, 0.26626402139663696]
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes.	[72, 97, 38, 21, 0, 11, 6, 7, 27, 20]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5061329007148743, 0.08087320625782013, 0.11723784357309341, 0.21983546018600464, 0.45038607716560364, 0.30573782324790955, 0.19169177114963531, 0.0701378807425499, 0.3627043664455414, 0.3496672511100769]
However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees.	[67, 107, 99, 16, 106, 71, 113, 2, 94, 115]	[1, 1, 0, 0, 0, 1, 0, 1, 0, 0]	[0.7446917295455933, 0.5266243815422058, 0.4141724705696106, 0.452227920293808, 0.3790229260921478, 0.583626389503479, 0.4698830544948578, 0.6711036562919617, 0.3461049795150757, 0.36490219831466675]
However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers.	[3, 122, 7, 115, 5, 1, 51, 8, 18, 132]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6140394806861877, 0.5511996746063232, 0.5580609440803528, 0.24035745859146118, 0.12625351548194885, 0.45992010831832886, 0.3788292407989502, 0.2803908884525299, 0.11696172505617142, 0.158948615193367]
However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network's ability to semantically generalize (Mikolov et al, 2013) and learn nonlinear relationships.	[8, 22, 12, 9, 72, 26, 78, 3, 5, 10]	[0, 1, 0, 1, 0, 1, 0, 0, 0, 0]	[0.4645768702030182, 0.8313882946968079, 0.18384535610675812, 0.7443621754646301, 0.36012640595436096, 0.6920799612998962, 0.05801204591989517, 0.05385490506887436, 0.05656875669956207, 0.10784401744604111]
However, other such modules, e.g., those from NLTK (Loper and Bird, 2002), can be used for such assignments.	[110, 112, 61, 174, 202, 84, 134, 43, 206, 19]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8241921663284302, 0.5217434763908386, 0.09802411496639252, 0.505899965763092, 0.339572012424469, 0.06849313527345657, 0.32783448696136475, 0.07499024271965027, 0.29608404636383057, 0.13092170655727386]
However, our performance on tagging when trained on Training I and tested on just the XH part of the test set is 94.44%, which might be a more relevant comparison to Xue et al (2002).	[20, 7, 9, 8, 17, 13, 4, 2, 21, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2154882550239563, 0.2266772836446762, 0.09743703901767731, 0.11888234317302704, 0.11627697944641113, 0.06336659938097, 0.0703064352273941, 0.05977286398410797, 0.12453964352607727, 0.09460993111133575]
However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse.	[21, 17, 146, 174, 18, 171, 48, 4, 159, 66]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30190417170524597, 0.49218475818634033, 0.12784485518932343, 0.11577677726745605, 0.08247699588537216, 0.056147847324609756, 0.19193610548973083, 0.1155858114361763, 0.4340263605117798, 0.27618950605392456]
However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery.	[76, 8, 124, 47, 11, 127, 74, 158, 161, 57]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.8343220949172974, 0.7609319090843201, 0.5124187469482422, 0.23610679805278778, 0.21022352576255798, 0.608867347240448, 0.10007873177528381, 0.44155141711235046, 0.19920040667057037, 0.1089458018541336]
However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007).	[28, 0, 19, 156, 59, 41, 12, 2, 32, 9]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.552253007888794, 0.8144074082374573, 0.20858678221702576, 0.2184748649597168, 0.4219553768634796, 0.12088729441165924, 0.21157139539718628, 0.08890584856271744, 0.1443360596895218, 0.3170224130153656]
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008).	[13, 56, 1, 2, 6, 12, 9, 35, 16, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2509453296661377, 0.35609346628189087, 0.14023825526237488, 0.06434595584869385, 0.34007829427719116, 0.149526447057724, 0.062052201479673386, 0.12976902723312378, 0.05864090099930763, 0.060418084263801575]
However, those works targeted a small subset of Levin classes, and a limited number of monosemous verbs; for example, Merlo and Stevenson (2001) studied three classes and 59 verbs, and Joanis et al (2008) focused on 14 classes and 835 verbs.	[42, 18, 157, 454, 396, 554, 439, 81, 80, 25]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3443053364753723, 0.47289735078811646, 0.3732871413230896, 0.3147677481174469, 0.09488948434591293, 0.27068600058555603, 0.13538849353790283, 0.10090849548578262, 0.14919720590114594, 0.1152949258685112]
Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source.	[124, 76, 126, 66, 35, 115, 119, 118, 36, 120]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5611204504966736, 0.4185639023780823, 0.07427272200584412, 0.26390939950942993, 0.07423394173383713, 0.09644728899002075, 0.10578536987304688, 0.07751405984163284, 0.38759225606918335, 0.1196770966053009]
Hwa et al (2002) have noticed that applying elementary linguistic transformations considerably increases precision and recall when projecting syntactic relations, at least for the English/Chinese language pair.	[70, 103, 53, 59, 100, 51, 109, 52, 57, 104]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5796605944633484, 0.24980893731117249, 0.18696562945842743, 0.13785921037197113, 0.10068454593420029, 0.08630818873643875, 0.12416797876358032, 0.16644933819770813, 0.09778613597154617, 0.05651472881436348]
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	[25, 53, 263, 177, 258, 9, 52, 194, 4, 14]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4450541138648987, 0.39864489436149597, 0.3837496042251587, 0.3064829111099243, 0.5996225476264954, 0.2778356075286865, 0.1010398417711258, 0.4953886866569519, 0.19820503890514374, 0.18648770451545715]
ICTCLAS is developed by Chinese Academy of Science, the precision of which is 97.58% on tagging general words (Huaping Zhang et al, 2003).	[11, 73, 19, 18, 0, 41, 2, 8, 74, 37]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5409072041511536, 0.6746118664741516, 0.12130118906497955, 0.17182983458042145, 0.3607877492904663, 0.1809636801481247, 0.10747648775577545, 0.0604739785194397, 0.05775746330618858, 0.054931022226810455]
Ide and Veronis (1998) present a very concise survey of the history of ideas used in word sense disambiguation; for a recent survey of the state-of-the-art one can refer to (Navigli, 2009).	[0, 23, 398, 71, 33, 464, 455, 14, 102, 12]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7832865118980408, 0.5226600170135498, 0.12064603716135025, 0.12589140236377716, 0.09154997766017914, 0.08689860999584198, 0.3013084828853607, 0.10665823519229889, 0.07084028422832489, 0.13207237422466278]
If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well.	[2, 28, 205, 9, 49, 52, 207, 16, 213, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3850765526294708, 0.4468540549278259, 0.1899241954088211, 0.14908845722675323, 0.11825285851955414, 0.06968716531991959, 0.10508407652378082, 0.08274529874324799, 0.09261351823806763, 0.06591543555259705]
If it is desired to have a large number of latent topics as is common in LDA, this model could be combined with a standard topic model without sequential dependencies, as explored by Ritter et al (2010).	[59, 92, 81, 25, 180, 28, 91, 98, 173, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4445176422595978, 0.27587318420410156, 0.19854652881622314, 0.2808760702610016, 0.15370798110961914, 0.10938083380460739, 0.2530244290828705, 0.24396242201328278, 0.29866692423820496, 0.20255523920059204]
If the current modifier bunsetsu is a distinctive key bunsetsu (Kurohashi and Nagao, 1994, page 510), these features are triggered.	[73, 76, 234, 255, 24, 240, 233, 60, 222, 59]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7120471000671387, 0.37741997838020325, 0.3237607479095459, 0.1718389391899109, 0.4317266643047333, 0.16859431564807892, 0.1934720277786255, 0.6036828756332397, 0.13301292061805725, 0.19742132723331451]
If we extend these positional tags to include POS information ,segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004).	[105, 3, 16, 135, 12, 21, 112, 89, 125, 144]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8156937956809998, 0.6442205309867859, 0.3647094666957855, 0.09204145520925522, 0.3345775604248047, 0.23552896082401276, 0.21078450977802277, 0.2522038221359253, 0.48874226212501526, 0.49437370896339417]
Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example.	[127, 99, 34, 118, 60, 23, 61, 11, 38, 5]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32018133997917175, 0.392518550157547, 0.05843282490968704, 0.14299437403678894, 0.29763180017471313, 0.19918197393417358, 0.09432937204837799, 0.08403563499450684, 0.14768429100513458, 0.15424023568630219]
In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described.	[114, 197, 107, 214, 18, 193, 5, 12, 39, 111]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28141355514526367, 0.3882465064525604, 0.12810540199279785, 0.2660021483898163, 0.14097414910793304, 0.13877591490745544, 0.07297144830226898, 0.07297144830226898, 0.05240662768483162, 0.0578249990940094]
In (Dagan et al., 1993) and (Pereira et al, 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.	[136, 132, 27, 0, 106, 2, 39, 8, 135, 105]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.3478963077068329, 0.40595731139183044, 0.3693576157093048, 0.522896945476532, 0.15567541122436523, 0.1138877347111702, 0.44853857159614563, 0.10941222310066223, 0.07055020332336426, 0.0707058534026146]
In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced.	[25, 0, 21, 139, 29, 71, 2, 73, 24, 1]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4696439802646637, 0.729750394821167, 0.5290551781654358, 0.24790328741073608, 0.16187956929206848, 0.265471488237381, 0.2488882690668106, 0.17148457467556, 0.06830398738384247, 0.16166351735591888]
In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000).	[147, 0, 1, 67, 45, 9, 6, 22, 30, 18]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8494718670845032, 0.7937751412391663, 0.3868253827095032, 0.4698721170425415, 0.18078865110874176, 0.30061280727386475, 0.2938328683376312, 0.14435064792633057, 0.11315213888883591, 0.47254928946495056]
In (Goldberg et al, 2006), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well.	[55, 24, 3, 2, 5, 1, 20, 56, 33, 41]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4208984076976776, 0.36644917726516724, 0.2511478066444397, 0.09607565402984619, 0.10446452349424362, 0.18636095523834229, 0.10862521082162857, 0.15170009434223175, 0.06294628977775574, 0.146624356508255]
In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus.	[195, 2, 25, 180, 100, 45, 189, 61, 23, 192]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4248318374156952, 0.13313400745391846, 0.15162937343120575, 0.12503597140312195, 0.09520558267831802, 0.09281750023365021, 0.16728796064853668, 0.05751810595393181, 0.07556581497192383, 0.056624364107847214]
In (Och et al, 2004), the effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated.	[5, 0, 190, 1, 208, 207, 198, 116, 85, 10]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46449726819992065, 0.8197515606880188, 0.26725664734840393, 0.1398591846227646, 0.15642111003398895, 0.17381200194358826, 0.10934099555015564, 0.11092248558998108, 0.054313212633132935, 0.057959768921136856]
In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels.	[160, 69, 98, 27, 173, 164, 26, 93, 63, 112]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25867927074432373, 0.4386816918849945, 0.29548972845077515, 0.09217628836631775, 0.24245788156986237, 0.35311755537986755, 0.09418544918298721, 0.25154200196266174, 0.2480698525905609, 0.056250933557748795]
In CLIR or multilingual corpus alignment (Virga and Khudanpur, 2003), N-best results will be very helpful to increase chances of correct hits.	[73, 114, 30, 99, 9, 75, 78, 41, 5, 123]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36386480927467346, 0.12170517444610596, 0.08135077357292175, 0.06102457642555237, 0.07626954466104507, 0.1867644488811493, 0.059827279299497604, 0.14089924097061157, 0.1539442241191864, 0.1470518857240677]
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG.	[69, 78, 12, 11, 1, 116, 174, 70, 42, 171]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.852128267288208, 0.8239628076553345, 0.31248030066490173, 0.08857190608978271, 0.20777952671051025, 0.14510034024715424, 0.389926016330719, 0.11965666711330414, 0.1627984344959259, 0.14288057386875153]
In Fung and McKeown (1997), a translation model applied to a pair of unrelated languages (English/Japanese) with a random selection of test words, many of them multi-word terms, gives a precision around 30% when only the top candidate is proposed.	[4, 14, 141, 120, 115, 116, 101, 133, 151, 10]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7071255445480347, 0.4895733892917633, 0.2778332233428955, 0.6413453817367554, 0.40633538365364075, 0.24169032275676727, 0.12422646582126617, 0.10521459579467773, 0.06683015078306198, 0.12013237178325653]
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994).	[14, 3, 27, 118, 110, 113, 37, 22, 0, 51]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8452381491661072, 0.402599960565567, 0.14210939407348633, 0.144208624958992, 0.19692836701869965, 0.20933720469474792, 0.05254266411066055, 0.3202226758003235, 0.45304858684539795, 0.1189659908413887]
In Rule 14, we use FrameNet (Baker et al 1998) to determine whether med/situation should be assigned to an NP, NPi.	[33, 34, 32, 58, 31, 2, 67, 53, 19, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40071338415145874, 0.37176525592803955, 0.19674941897392273, 0.06837036460638046, 0.29008182883262634, 0.24724802374839783, 0.12787197530269623, 0.10536912828683853, 0.3926277458667755, 0.1215200424194336]
In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004).	[91, 19, 152, 154, 89, 16, 116, 144, 126, 85]	[0, 0, 0, 0, 1, 0, 1, 0, 0, 0]	[0.4502559006214142, 0.41994595527648926, 0.2898619771003723, 0.19432778656482697, 0.6046140193939209, 0.23320335149765015, 0.5223804116249084, 0.21805012226104736, 0.19302190840244293, 0.3237481415271759]
In STAN-ANNOTATION, we annotate thetreebank symbols with annotations from the Stanford parser (Klein and Manning, 2003).	[25, 185, 215, 11, 169, 159, 36, 15, 8, 40]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6664035320281982, 0.7800899147987366, 0.3621627390384674, 0.37799447774887085, 0.37799447774887085, 0.06725969165563583, 0.07058995217084885, 0.09918580949306488, 0.12583044171333313, 0.06537270545959473]
In Table 5, Hasegawa's Method1 means the test used the word feature as Hasegawa et al (2004) while Hasegawa's Method2 means the test used the same feature set as our method.	[178, 180, 111, 36, 93, 38, 192, 61, 159, 170]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.300984263420105, 0.27256453037261963, 0.09272373467683792, 0.19144214689731598, 0.05984770134091377, 0.09754809737205505, 0.1551724523305893, 0.1990896314382553, 0.08276490122079849, 0.3342008590698242]
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	[4, 148, 89, 77, 87, 96, 80, 13, 84, 114]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 1]	[0.7540391683578491, 0.5168771743774414, 0.36463552713394165, 0.5385522842407227, 0.5220656394958496, 0.32146692276000977, 0.2175079882144928, 0.22483330965042114, 0.15702997148036957, 0.7761061787605286]
In a certain sense, described in greater detail below, this precomputation of exact heuristics is equivalent to the k-best extraction algorithm of Huang and Chiang (2005).	[20, 33, 29, 8, 13, 234, 22, 159, 133, 162]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6413630247116089, 0.18512579798698425, 0.25015801191329956, 0.10860895365476608, 0.13901512324810028, 0.06253603845834732, 0.10220978409051895, 0.12462080270051956, 0.20105870068073273, 0.1550654023885727]
In a similar vein, our model is both similar and distinct in comparison to the soft clustering approaches by Pereira et al (1993) and Korhonenetal.	[56, 21, 25, 17, 5, 16, 15, 40, 14, 12]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7183094620704651, 0.08597724139690399, 0.12721849977970123, 0.12151727825403214, 0.2072189599275589, 0.06285101175308228, 0.3744172155857086, 0.07029882073402405, 0.06989119201898575, 0.2515852451324463]
In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction.	[42, 50, 51, 66, 40, 35, 91, 53, 57, 58]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6852424740791321, 0.057399261742830276, 0.05422600731253624, 0.06638823449611664, 0.08208338171243668, 0.05234062299132347, 0.08314627408981323, 0.07365886121988297, 0.05152655392885208, 0.05037864297628403]
In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000).	[101, 29, 28, 39, 32, 30, 37, 36, 1, 11]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6655449271202087, 0.5763651728630066, 0.6211665272712708, 0.46411678194999695, 0.24829432368278503, 0.2959839701652527, 0.22854387760162354, 0.4005768895149231, 0.11663858592510223, 0.1546376794576645]
In addition to conveying information about one's own mental state, pragmatic principles and rules, such as those we have presented, may be deployed to reason about the intentions and beliefs of others (Perrault and Allen, 1980).	[106, 124, 25, 226, 11, 119, 158, 331, 49, 349]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5455127954483032, 0.5052401423454285, 0.5952253937721252, 0.3594212532043457, 0.22189337015151978, 0.20332470536231995, 0.23150475323200226, 0.1385514885187149, 0.1302468329668045, 0.1471880078315735]
In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments).	[46, 20, 148, 156, 187, 206, 44, 189, 45, 169]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31910619139671326, 0.568306565284729, 0.13731735944747925, 0.1481500118970871, 0.07611381262540817, 0.12890663743019104, 0.053767237812280655, 0.13070093095302582, 0.12651576101779938, 0.06813689321279526]
In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction.	[85, 81, 0, 11, 164, 1, 82, 21, 147, 86]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5013754367828369, 0.42722538113594055, 0.42959776520729065, 0.3318037688732147, 0.32659009099006653, 0.3338773250579834, 0.12703797221183777, 0.21065104007720947, 0.06078876927495003, 0.056649100035429]
In addition, Dagan and Itai (1991) undertook additional pre-editing such as the removal of sentences for which the parser failed to produce a reasonable parse, cases where the antecedent was not an NP etc.; Kennedy and Boguraev (1996) manually removed 30 occurrences of pleonastic pronouns (which could not be recognised by their pleonastic recogniser) as well as 6 occurrences of it which referred to a VP or prepositional constituent.	[155, 39, 35, 152, 18, 51, 93, 92, 90, 158]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7653038501739502, 0.3753277361392975, 0.24296647310256958, 0.1267847865819931, 0.12020251899957657, 0.1275796890258789, 0.35524988174438477, 0.3334295451641083, 0.07105126976966858, 0.1267978847026825]
In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations.	[73, 16, 3, 107, 13, 67, 34, 25, 20, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4351394474506378, 0.2655774652957916, 0.2421189248561859, 0.18620148301124573, 0.062247294932603836, 0.06790172308683395, 0.2829177975654602, 0.06665549427270889, 0.05967819690704346, 0.11395098268985748]
In addition, each predicate has a time function to show at what stage of the event the predicate holds true, in a manner similar to the event decomposition of Moens and Steedman (1988).	[276, 91, 70, 20, 353, 274, 149, 33, 290, 22]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8008489608764648, 0.2805999517440796, 0.32300934195518494, 0.17475813627243042, 0.2920151948928833, 0.07869081944227219, 0.2999340295791626, 0.07464971393346786, 0.14571964740753174, 0.25974756479263306]
In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs.	[198, 155, 28, 25, 50, 127, 91, 62, 213, 45]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4741683006286621, 0.6302000284194946, 0.2180185616016388, 0.29440584778785706, 0.11803150922060013, 0.4701712727546692, 0.369320273399353, 0.16299979388713837, 0.15119542181491852, 0.2978951632976532]
In addition, we also analyze CCM's sensitivity to initialization, and compare our results to Seginer's algorithm (Seginer, 2007).	[23, 20, 97, 4, 22, 6, 131, 13, 41, 60]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5874084830284119, 0.1580016314983368, 0.2986449897289276, 0.13249653577804565, 0.09771151095628738, 0.06954474002122879, 0.06803891062736511, 0.08451828360557556, 0.07580319046974182, 0.06760266423225403]
In addition, we exploit syntactic constructions shown to be useful by other studies - lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997).	[31, 76, 26, 25, 27, 185, 19, 176, 56, 36]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7249403595924377, 0.6536784768104553, 0.27069664001464844, 0.09472551196813583, 0.09486442804336548, 0.16638846695423126, 0.2396540343761444, 0.14488637447357178, 0.14504030346870422, 0.07089460641145706]
In addition, we show in Table 7 the F-score results provided by Snow et al (2007) for their SVM-based system and for the mapping-based approach of Navigli (2006), denoted by ODE.	[21, 123, 44, 29, 2, 131, 65, 77, 7, 43]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5425418019294739, 0.7251131534576416, 0.4290982484817505, 0.4908097982406616, 0.1828620284795761, 0.1018638014793396, 0.05597151070833206, 0.11782107502222061, 0.06800384074449539, 0.5580031275749207]
In all experiments, the FrameNet 1.3 version and the dependency based system using the LTH parser (Johansson and Nugues, 2008a) have been employed.	[172, 32, 166, 164, 151, 200, 127, 70, 58, 85]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7853131294250488, 0.46568524837493896, 0.14702913165092468, 0.20527631044387817, 0.1689147651195526, 0.08959672600030899, 0.0691603571176529, 0.058163177222013474, 0.1073126345872879, 0.25077131390571594]
In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006).	[7, 0, 5, 156, 1, 27, 40, 170, 160, 17]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4305115044116974, 0.6324253082275391, 0.06735900044441223, 0.2935183644294739, 0.15261149406433105, 0.4615480899810791, 0.15002983808517456, 0.10972446203231812, 0.25139686465263367, 0.16411525011062622]
In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy.	[86, 28, 84, 37, 41, 38, 208, 224, 60, 30]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8373860120773315, 0.3311622738838196, 0.1450124979019165, 0.1250649094581604, 0.3087451756000519, 0.10506562888622284, 0.16676738858222961, 0.4030395746231079, 0.0910785123705864, 0.373789519071579]
In applications such as the syntax based machine translation model of (Yamada and Knight, 2001), a low quality tree might lead to errorenous translation of the sentence.	[148, 0, 1, 34, 61, 6, 8, 40, 38, 107]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.7076600193977356, 0.7707518935203552, 0.7133833169937134, 0.6343382000923157, 0.12389729171991348, 0.2633615732192993, 0.10092324763536453, 0.1225963905453682, 0.296656996011734, 0.16102536022663116]
In comparison, in (Yamada and Knight, 2002), which was a phrasal structure based statistical MT system for Chinese to English translation, the Bleu score reported for short sentences (less than 14 words) is 0.099 to 0.102.	[123, 0, 144, 194, 56, 5, 131, 7, 174, 20]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8114365339279175, 0.737791121006012, 0.21927964687347412, 0.21142780780792236, 0.3545815646648407, 0.21654288470745087, 0.1361624002456665, 0.1532985121011734, 0.27288949489593506, 0.0854053869843483]
In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007).	[1, 7, 13, 201, 133, 19, 42, 153, 18, 203]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7748400568962097, 0.7748400568962097, 0.570891261100769, 0.31500160694122314, 0.2312997728586197, 0.11651143431663513, 0.12830224633216858, 0.16652576625347137, 0.151484876871109, 0.15222296118736267]
In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence.	[83, 53, 77, 13, 103, 18, 67, 27, 40, 2]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4300750195980072, 0.4171507656574249, 0.4321383237838745, 0.16087496280670166, 0.3409709930419922, 0.17735624313354492, 0.27005016803741455, 0.05105480179190636, 0.08028010278940201, 0.2804723083972931]
In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al 2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available.	[12, 49, 56, 85, 188, 24, 213, 179, 50, 19]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5564598441123962, 0.48939743638038635, 0.2729490399360657, 0.09512440115213394, 0.12989769876003265, 0.13544438779354095, 0.1639643758535385, 0.1503213495016098, 0.10689502209424973, 0.4102690815925598]
In contrast with bootstrapping, SCL (Blitzer et al,2006) uses the unlabeled target data to learn domain independent features.	[216, 223, 12, 207, 52, 213, 154, 98, 65, 233]	[1, 1, 0, 0, 0, 0, 1, 1, 1, 0]	[0.8497723937034607, 0.7404869794845581, 0.40350744128227234, 0.3185259997844696, 0.30813729763031006, 0.22080327570438385, 0.7536712288856506, 0.5790511965751648, 0.5523486137390137, 0.21449410915374756]
In contrast, our method will produce an exact FSA for many interesting grammars generating regular languages, such as those arising from systematic attachment ambiguities (Church and Patil, 1982).	[222, 174, 21, 80, 47, 70, 92, 221, 209, 135]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.375586599111557, 0.08344082534313202, 0.07429101318120956, 0.0585191585123539, 0.18752466142177582, 0.18512970209121704, 0.346951425075531, 0.06321978569030762, 0.08078458160161972, 0.059331752359867096]
In contrast, the greedy algorithm of Lapata (2003) makes grave search errors.	[225, 155, 71, 68, 220, 15, 7, 10, 30, 47]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7518916130065918, 0.5328963398933411, 0.3239274322986603, 0.24980512261390686, 0.12322569638490677, 0.2366369068622589, 0.2199116200208664, 0.1826668232679367, 0.19797669351100922, 0.09359589219093323]
In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features.	[131, 139, 103, 137, 119, 130, 13, 129, 93, 104]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41469621658325195, 0.8436119556427002, 0.33367589116096497, 0.06419700384140015, 0.29688340425491333, 0.19414374232292175, 0.13095378875732422, 0.27190887928009033, 0.3576732873916626, 0.34952178597450256]
In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000).	[121, 57, 55, 43, 192, 54, 80, 56, 70, 47]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4689108729362488, 0.5308495163917542, 0.4258694648742676, 0.2028595209121704, 0.2533330023288727, 0.1580202877521515, 0.24876779317855835, 0.06576161086559296, 0.4108029305934906, 0.12293702363967896]
In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al, 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988).	[162, 167, 19, 178, 10, 192, 54, 210, 176, 147]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3268539607524872, 0.15246766805648804, 0.07073905318975449, 0.2827135920524597, 0.17050237953662872, 0.06687459349632263, 0.06259742379188538, 0.08544183522462845, 0.08436138182878494, 0.2944399118423462]
In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dec tors: the exact matching between tokens or lemmas, a similarity between tokens based on their edit distance, the derivation ally related form relation and the verb entailment relation in WordNet, and, finally, a WordNet-based similarity (Jiang and Conrath, 1997).	[24, 70, 43, 55, 12, 30, 7, 59, 18, 4]	[0, 0, 0, 1, 0, 1, 0, 0, 0, 0]	[0.4315328299999237, 0.41395390033721924, 0.4019855260848999, 0.7215471863746643, 0.22836922109127045, 0.5713639855384827, 0.324856162071228, 0.1999443620443344, 0.14431802928447723, 0.20302513241767883]
In line with our assumption of raw text to extract over, we use the Brill tagger (Brill, 1995) to automatically tag the WSJ, rather than making use of the manual POS annotation provided in the Penn Treebank.	[216, 142, 168, 179, 154, 46, 129, 16, 108, 24]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5771211385726929, 0.703446626663208, 0.4258689284324646, 0.49488386511802673, 0.23735730350017548, 0.12177475541830063, 0.14467690885066986, 0.3179916739463806, 0.076271191239357, 0.17200113832950592]
In line with our observation, Czech-to-English correlations reported by Callison-Burch et al (2012) are higher: the best metric achieves 0.28 and aver ages 0.25 across four source languages.	[211, 14, 200, 90, 104, 182, 95, 362, 88, 387]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.1961745023727417, 0.25846827030181885, 0.5482710003852844, 0.4783109426498413, 0.26538217067718506, 0.1267455518245697, 0.2719106376171112, 0.08095303177833557, 0.06904297322034836, 0.11085839569568634]
In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability.	[36, 0, 25, 1, 74, 63, 149, 148, 76, 117]	[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]	[0.6402642726898193, 0.788253664970398, 0.7335076928138733, 0.6038897037506104, 0.6901724934577942, 0.5559574961662292, 0.5630141496658325, 0.5279387831687927, 0.25335267186164856, 0.08217024058103561]
In order to attack these problems, some metrics have been proposed to include more linguistic information into the process of matching ,e.g., Meteor (Banerjee and Lavie, 2005) metric and MaxSim (Channad Ng, 2008) metrics, which improve the lexical level by the synonym dictionary or stemming technique.	[1, 2, 5, 7, 8, 3, 6, 4, 0, 9]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.64254230260849, 0.6293933987617493, 0.5398991107940674, 0.44413506984710693, 0.49595531821250916, 0.4656609296798706, 0.1021069586277008, 0.18802590668201447, 0.2371416836977005, 0.08292127400636673]
In order to extract the linguistic features necessary for the ME model, all sentences containing the target word were automatically part of-speech (POS) tagged using the Brill POS tagger (Brill, 1992).	[15, 12, 9, 2, 73, 0, 1, 86, 26, 30]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.5781681537628174, 0.2225039005279541, 0.12319865077733994, 0.3469754159450531, 0.08466536551713943, 0.39678454399108887, 0.12207960337400436, 0.41243264079093933, 0.5332689881324768, 0.0695355236530304]
In order to produce semantic representations we are using an open source HPSG grammar of Japanese: JACY (Siegel and Bender, 2002), which we have extended to cover the dictionary definition sentences (Bond et al, 2004).	[21, 1, 191, 135, 187, 35, 120, 13, 22, 170]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10356950759887695, 0.16733452677726746, 0.287663072347641, 0.05710780993103981, 0.10578502714633942, 0.06864787638187408, 0.08154009282588959, 0.08986906707286835, 0.29253607988357544, 0.05372461676597595]
In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar.	[44, 93, 92, 2, 100, 130, 140, 98, 132, 103]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.8043946027755737, 0.46180039644241333, 0.4503883719444275, 0.132527694106102, 0.16513657569885254, 0.5157938003540039, 0.13318130373954773, 0.16546237468719482, 0.08401014655828476, 0.05406024679541588]
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence.	[2, 133, 10, 63, 152, 139, 40, 47, 126, 67]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6701576709747314, 0.2118002027273178, 0.1731300950050354, 0.13080565631389618, 0.3424376845359802, 0.3450440764427185, 0.15749932825565338, 0.14936165511608124, 0.06891606748104095, 0.13982558250427246]
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence.	[23, 41, 45, 54, 4, 144, 62, 49, 113, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38262298703193665, 0.21541014313697815, 0.26530835032463074, 0.3269081711769104, 0.23956915736198425, 0.23053470253944397, 0.2495172768831253, 0.22628720104694366, 0.09405083954334259, 0.07795501500368118]
In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009).	[94, 2, 216, 237, 57, 110, 129, 91, 60, 72]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3546481430530548, 0.13186180591583252, 0.1602715104818344, 0.14791874587535858, 0.185716912150383, 0.19681768119335175, 0.27504876255989075, 0.09904288500547409, 0.11317950487136841, 0.26783835887908936]
In our experiments we identify SL (Chinese) NEs implicitly found by the word segmentation algorithm stated in Gao et al (2003), and the dictionaries for translating NEs include the same one used for QSL-TFIDF, and the LDC Chinese/English NE dictionary.	[168, 24, 1, 151, 27, 25, 208, 18, 87, 195]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09692010283470154, 0.15639370679855347, 0.21313928067684174, 0.12671661376953125, 0.26067015528678894, 0.0601908341050148, 0.145619735121727, 0.07181945443153381, 0.10821476578712463, 0.17709676921367645]
In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search.	[37, 68, 70, 42, 103, 41, 3, 118, 13, 30]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7873641848564148, 0.6337348818778992, 0.19775298237800598, 0.43545371294021606, 0.35193678736686707, 0.4325278103351593, 0.1227571964263916, 0.36366432905197144, 0.12239183485507965, 0.09213714301586151]
In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al, 2009).	[187, 67, 33, 54, 14, 158, 56, 13, 101, 184]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3899950683116913, 0.11631688475608826, 0.20267274975776672, 0.12431443482637405, 0.11442719399929047, 0.07204582542181015, 0.1747709959745407, 0.09402523189783096, 0.1160910353064537, 0.23017126321792603]
In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004).	[134, 170, 135, 127, 25, 47, 84, 86, 143, 167]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5310701131820679, 0.32245081663131714, 0.14650636911392212, 0.08200179040431976, 0.13587884604930878, 0.07072967290878296, 0.060210101306438446, 0.05929594486951828, 0.0787871852517128, 0.11963263899087906]
In our string-to-tree model, for efficient decoding with integrated n-gram LM, we follow (Zhang et al, 2006) and inversely binarize all translation rules into Chomsky Normal Forms that contain at most two variables and can be incrementally scored by LM.	[7, 40, 153, 114, 22, 115, 28, 112, 30, 119]	[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.32808464765548706, 0.6101346611976624, 0.10935568064451218, 0.08927731215953827, 0.4764074385166168, 0.09280387312173843, 0.059884361922740936, 0.567694365978241, 0.058001261204481125, 0.12670668959617615]
In our study, the first author coded initiative using the annotation scheme of Whittaker and Stenton (1988).	[19, 132, 144, 126, 5, 131, 22, 59, 142, 127]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.569923996925354, 0.2564002573490143, 0.06565254181623459, 0.0900544822216034, 0.18793170154094696, 0.2395285815000534, 0.27041196823120117, 0.06925736367702484, 0.05993637070059776, 0.061848223209381104]
In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation.	[15, 21, 0, 106, 103, 24, 10, 7, 27, 36]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6378958225250244, 0.6211694478988647, 0.196664497256279, 0.23351261019706726, 0.5926399230957031, 0.3008417785167694, 0.2591596841812134, 0.20533068478107452, 0.2910420298576355, 0.08583055436611176]
In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance.	[23, 181, 177, 58, 56, 167, 19, 70, 18, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1290959119796753, 0.1111074835062027, 0.15493403375148773, 0.05312405526638031, 0.14246708154678345, 0.05893336236476898, 0.05955609306693077, 0.05817309767007828, 0.06272681057453156, 0.05498616769909859]
In particular, I will investigate settings that incorporate non syntactic phrases, using methods similar to Liu et al (2006) and Zhang et al (2008).	[33, 8, 14, 61, 4, 127, 17, 60, 121, 136]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8291873931884766, 0.30806753039360046, 0.375775545835495, 0.395520955324173, 0.26372891664505005, 0.23757725954055786, 0.13619130849838257, 0.42579543590545654, 0.40995174646377563, 0.12788543105125427]
In particular, empty nodes (represented as -NONE in the tree bank) were turned into rules that generated the empty string (), and there was no collapsing of categories (such as PRT and ADVP) as is of ten done in parsing work (Collins, 1997, etc.).	[20, 65, 57, 98, 55, 97, 4, 18, 37, 61]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22790804505348206, 0.4731268882751465, 0.44090327620506287, 0.17689596116542816, 0.2927808165550232, 0.0855247750878334, 0.154863640666008, 0.23428115248680115, 0.10669691860675812, 0.12050087749958038]
In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either.	[87, 22, 4, 2, 204, 61, 13, 206, 66, 26]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.21766597032546997, 0.3689369559288025, 0.14807544648647308, 0.10072632133960724, 0.4293150305747986, 0.5800462365150452, 0.38835135102272034, 0.21632671356201172, 0.11382441222667694, 0.0901847630739212]
In particular, this method has been used for word sense disambiguation (Lin, 1997) and thesaurus construction (Lin, 1998).	[167, 5, 23, 8, 134, 138, 95, 19, 14, 3]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6616098284721375, 0.5771430730819702, 0.2187962681055069, 0.26084643602371216, 0.2189774215221405, 0.15199419856071472, 0.21070969104766846, 0.0714457780122757, 0.21954704821109772, 0.0566890686750412]
In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002).	[83, 0, 25, 114, 115, 24, 78, 4, 14, 20]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8233479261398315, 0.7321678400039673, 0.20291483402252197, 0.199442058801651, 0.09127388894557953, 0.0905374214053154, 0.2829899489879608, 0.05605924129486084, 0.054991625249385834, 0.0741787776350975]
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	[24, 277, 319, 282, 48, 19, 50, 303, 305, 271]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5353413224220276, 0.37454405426979065, 0.20689763128757477, 0.1305743306875229, 0.13923101127147675, 0.07385992258787155, 0.38124150037765503, 0.09699665755033493, 0.2879360616207123, 0.11430612206459045]
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale.	[20, 71, 137, 13, 132, 131, 21, 10, 23, 14]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6252340078353882, 0.29651254415512085, 0.24356062710285187, 0.19229963421821594, 0.2885679602622986, 0.25630325078964233, 0.22741663455963135, 0.17245802283287048, 0.1578330248594284, 0.1511116325855255]
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	[27, 261, 28, 54, 222, 120, 264, 4, 145, 15]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.8189363479614258, 0.7403059601783752, 0.7033896446228027, 0.23209144175052643, 0.3927237093448639, 0.6560097336769104, 0.2951447069644928, 0.13164864480495453, 0.27599698305130005, 0.14034077525138855]
In recent years, conditional random fields (CRFs) (Lafferty et al, 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004).	[21, 0, 2, 19, 155, 11, 26, 61, 158, 10]	[1, 1, 1, 1, 0, 0, 1, 0, 0, 0]	[0.848034143447876, 0.7715309858322144, 0.5541427135467529, 0.616861879825592, 0.23407761752605438, 0.33694878220558167, 0.763148307800293, 0.13918189704418182, 0.2653695046901703, 0.08511905372142792]
In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set.	[143, 148, 180, 151, 337, 80, 145, 118, 325, 108]	[1, 1, 1, 1, 1, 0, 0, 0, 1, 0]	[0.6301860809326172, 0.7688031792640686, 0.7376859784126282, 0.5780225396156311, 0.6389396786689758, 0.46681854128837585, 0.3761921525001526, 0.22525599598884583, 0.5008431673049927, 0.0668703094124794]
In some cases, the author may also introduce their own perspective (Lin et al, 2006) through the use of framing (Greene and Resnik, 2009).	[71, 39, 14, 16, 55, 67, 42, 116, 141, 77]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.508278489112854, 0.07953336834907532, 0.2741320729255676, 0.2737293243408203, 0.11799813061952591, 0.1280706226825714, 0.09965807944536209, 0.1413903385400772, 0.09915796667337418, 0.28952711820602417]
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	[19, 197, 46, 0, 54, 1, 43, 36, 32, 28]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6758907437324524, 0.35063284635543823, 0.5012562870979309, 0.33860471844673157, 0.22177107632160187, 0.27441489696502686, 0.1980774998664856, 0.07228294759988785, 0.09584357589483261, 0.16785502433776855]
In supervised approaches, various kinds of models were applied, such as HMM (Jin and Ho, 2009), SVM (Wu et al, 2009) and CRFs (Li et al, 2010).	[7, 96, 151, 149, 102, 171, 180, 39, 28, 111]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06906583905220032, 0.22761191427707672, 0.12034162133932114, 0.2998650074005127, 0.0566471628844738, 0.05473696440458298, 0.05625464394688606, 0.12359049916267395, 0.06009051576256752, 0.19251400232315063]
In that table, TBL stands for Brill &apos s transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al (1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+ (CPD+ENS) Train Test 950 Kw 150 Kw.	[0, 5, 19, 106, 115, 122, 177, 15, 23, 160]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38489586114883423, 0.36865943670272827, 0.4030557870864868, 0.14402195811271667, 0.21300804615020752, 0.18767763674259186, 0.37393918633461, 0.21766389906406403, 0.18605053424835205, 0.12284116446971893]
In the Cause versus Contrast case, their reported performance exceeds ours significantly; however, in a subset of their experiments which test Cause versus Contrast on instances from the human annotated RSTBank corpus (Carlson et al., 2001) where no cue phrase is present, they report only 63% accuracy over a 56% baseline (the baseline is > 50% because the number of input examples is unbalanced).	[47, 122, 121, 66, 31, 29, 51, 139, 12, 123]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36652860045433044, 0.2995184659957886, 0.0882466584444046, 0.12909984588623047, 0.2639530897140503, 0.08352643996477127, 0.09424103796482086, 0.18011555075645447, 0.05753641203045845, 0.0681399554014206]
In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b).	[142, 130, 140, 139, 27, 98, 94, 51, 93, 45]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8229981660842896, 0.8042128682136536, 0.33342868089675903, 0.30537232756614685, 0.12553679943084717, 0.32531142234802246, 0.36370307207107544, 0.05191709101200104, 0.11859268695116043, 0.07186219096183777]
In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level.	[34, 104, 35, 10, 172, 129, 7, 21, 165, 177]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6399127244949341, 0.5189999938011169, 0.5994232892990112, 0.5234463810920715, 0.2584123909473419, 0.2569620609283447, 0.17689615488052368, 0.18709780275821686, 0.40249764919281006, 0.2863702178001404]
In the Penn Treebank (PTB) (Marcus et al, 1994), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges.	[7, 0, 150, 20, 35, 10, 17, 24, 72, 140]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4051629900932312, 0.21782301366329193, 0.5206303596496582, 0.16754268109798431, 0.16754268109798431, 0.1818942427635193, 0.21340614557266235, 0.18461750447750092, 0.08881524205207825, 0.15152187645435333]
In the chart realization tradition (Kay, 1996), the OpenCCG realizer takes logical forms as input and produces strings by combining signs for lexical items.	[20, 130, 22, 132, 1, 89, 138, 16, 126, 28]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.7153886556625366, 0.761532187461853, 0.5905179977416992, 0.5905179977416992, 0.12526705861091614, 0.06648676097393036, 0.07398779690265656, 0.06995122879743576, 0.06995122879743576, 0.06055128946900368]
In the classic natural language generation (NLG) architecture (Reiter, 1994), sentence boundary decisions are made during the sentence planning stage in which the syntactic structure and wording of sentences are decided.	[96, 32, 107, 9, 11, 81, 36, 34, 105, 35]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1632637083530426, 0.8406106233596802, 0.2643366754055023, 0.30704835057258606, 0.19654226303100586, 0.07138513028621674, 0.12039455026388168, 0.0718739926815033, 0.08091506361961365, 0.09727118909358978]
In the corpus of Rhetorical Structure trees built by Carlson et al (2001), for example, we have observed that only 61 of 238 CONTRAST relation sand 79 out of 307 EXPLANATION-EVIDENCE relations that hold between two adjacent clauses were marked by a cue phrase.	[39, 42, 41, 29, 47, 48, 66, 145, 152, 117]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4883189797401428, 0.7086352705955505, 0.31613412499427795, 0.09462691843509674, 0.21572324633598328, 0.12232525646686554, 0.14674724638462067, 0.19065715372562408, 0.20734693109989166, 0.43977174162864685]
In the field of eomputationa.1 linguistics, mutual information [Brown et al, 1988],  2 [Church and Hanks, 1990], or a likelihood ratio test [Dunning, 199a] are suggested.	[72, 11, 79, 32, 191, 161, 30, 73, 120, 184]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.608259916305542, 0.32375413179397583, 0.10571247339248657, 0.09862063080072403, 0.11546975374221802, 0.17563369870185852, 0.11463184654712677, 0.24405725300312042, 0.1536184400320053, 0.1756216436624527]
In the future, we would like to model the biographical fact extraction approach of (Mann and Yarowsky, 2003) in our LDA model.	[36, 148, 31, 22, 14, 2, 62, 141, 34, 30]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2999400794506073, 0.11093587428331375, 0.22930091619491577, 0.1408209502696991, 0.0629170760512352, 0.10681551694869995, 0.06852234899997711, 0.07229285687208176, 0.19896450638771057, 0.10594555735588074]
In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999).	[146, 102, 74, 109, 75, 43, 3, 9, 140, 126]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20730425417423248, 0.8351744413375854, 0.09476107358932495, 0.25458911061286926, 0.1253468543291092, 0.18303759396076202, 0.0941767692565918, 0.0941767692565918, 0.2210012525320053, 0.07806146889925003]
In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006).	[201, 181, 203, 210, 216, 7, 218, 236, 180, 154]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.8186091184616089, 0.09410563111305237, 0.22300177812576294, 0.47808077931404114, 0.4097427725791931, 0.29313990473747253, 0.6160455346107483, 0.29313990473747253, 0.09568484127521515, 0.05901210755109787]
In the meantime, (Brill 1995a) (Brill 1995b) proposed a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning.	[3, 0, 134, 131, 121, 19, 7, 15, 105, 26]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7784737944602966, 0.7130156755447388, 0.6118900775909424, 0.42228084802627563, 0.32763612270355225, 0.4962438941001892, 0.3240582048892975, 0.3044029176235199, 0.20621971786022186, 0.10979358106851578]
In the pipeline approach (Figure 1b), the input word is segmented into letter substrings by an instance-based classifier (Aha et al, 1991), which learns a letter segmentation model from many-to-many alignments (Jiampojamarn et al, 2007).	[96, 134, 29, 121, 0, 37, 157, 27, 57, 87]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.8310235142707825, 0.824070394039154, 0.2190721035003662, 0.32249152660369873, 0.22132126986980438, 0.22716712951660156, 0.17196528613567352, 0.21890610456466675, 0.6071425676345825, 0.16554029285907745]
In the recent years, there have been a number of papers considering this or similar problems: (Brown et al, 1990), (Dagan et al, 1993), (Kay et al, 1993), (Fung et al, 1993).	[5, 12, 10, 20, 45, 7, 72, 42, 33, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.8153970241546631, 0.09888055920600891, 0.10113463550806046, 0.08367350697517395, 0.4247417151927948, 0.060568246990442276, 0.13850262761116028, 0.20136000216007233, 0.5040984749794006, 0.057256489992141724]
In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD.	[35, 32, 33, 169, 120, 74, 34, 75, 135, 155]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6807829737663269, 0.7069972157478333, 0.35498955845832825, 0.33448195457458496, 0.423545241355896, 0.4901786744594574, 0.2908867299556732, 0.49691635370254517, 0.12115593254566193, 0.26363325119018555]
In the system presented in (Mani and Wilson, 2000), weekday name interpretation is implemented as part of a sequence of interpretation rules for temporal expression interpretation more generally.	[23, 88, 43, 31, 53, 2, 80, 111, 47, 45]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2650032341480255, 0.591185450553894, 0.22521424293518066, 0.19088275730609894, 0.3427163064479828, 0.07934380322694778, 0.3453596532344818, 0.0730431079864502, 0.09822715818881989, 0.0634930431842804]
In their study, they first acquire fine-grained SCFs using the unsupervised method proposed by Briscoe and Carroll (1997) and Korhonen (2002).	[49, 181, 35, 42, 59, 25, 130, 173, 61, 27]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5522319078445435, 0.5899446606636047, 0.11525918543338776, 0.448221355676651, 0.18548071384429932, 0.07432727515697479, 0.05275850370526314, 0.448221355676651, 0.12202787399291992, 0.06371515244245529]
In this case, we make use of the out-of-domain data by using features of the source domain tagger's predictions in training and testing the target domain tagger (Florian et al, 2004).	[58, 24, 56, 41, 116, 36, 163, 61, 57, 159]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12391041219234467, 0.13094934821128845, 0.2629601061344147, 0.15605883300304413, 0.23618853092193604, 0.18998168408870697, 0.14752888679504395, 0.05965915694832802, 0.1256571263074875, 0.06945621967315674]
In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner.	[15, 0, 5, 113, 27, 6, 69, 7, 68, 77]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46987542510032654, 0.5535381436347961, 0.24108216166496277, 0.14006344974040985, 0.28464385867118835, 0.2347126603126526, 0.14386054873466492, 0.20215170085430145, 0.3322068154811859, 0.06071209907531738]
In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list.	[22, 19, 64, 15, 31, 0, 77, 28, 42, 65]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34984010457992554, 0.36686399579048157, 0.4164140522480011, 0.14088422060012817, 0.11040852963924408, 0.41195812821388245, 0.11688192188739777, 0.1503809243440628, 0.07351988554000854, 0.21141621470451355]
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	[64, 2, 108, 118, 8, 88, 107, 140, 164, 43]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6101177930831909, 0.3473254442214966, 0.7983525395393372, 0.23128457367420197, 0.297033429145813, 0.1039491668343544, 0.3509396016597748, 0.17746949195861816, 0.17420293390750885, 0.3493824303150177]
In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method.	[55, 0, 3, 44, 15, 16, 1, 8, 20, 37]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5585784316062927, 0.8398639559745789, 0.23481111228466034, 0.48273083567619324, 0.13556911051273346, 0.11567346006631851, 0.1987614631652832, 0.16663908958435059, 0.15381792187690735, 0.07824495434761047]
In this paper, I will review and assess the recent centering approach to the interpretation of Japanese zero pronouns (Walker et al 1994) as a case study.	[0, 62, 17, 47, 113, 75, 292, 9, 163, 117]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3054009675979614, 0.22039180994033813, 0.2623612880706787, 0.1903669536113739, 0.07103058695793152, 0.2702004611492157, 0.26574456691741943, 0.3519929349422455, 0.23779848217964172, 0.22069425880908966]
In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.	[174, 1, 9, 93, 8, 24, 53, 173, 37, 46]	[0, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.31519997119903564, 0.5721398591995239, 0.23369775712490082, 0.5714173316955566, 0.5941047072410583, 0.4521186649799347, 0.2113891839981079, 0.3475768268108368, 0.12799538671970367, 0.42225801944732666]
In this paper, we generate paraphrases adopting the pivot-based method proposed by Bannard and Callison-Burch (2005) in the first round.	[12, 1, 25, 116, 124, 3, 20, 50, 40, 32]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.551848828792572, 0.4692937135696411, 0.23702691495418549, 0.3696315586566925, 0.3771333694458008, 0.1478668600320816, 0.21661484241485596, 0.09994638711214066, 0.19969843327999115, 0.06007590889930725]
In this paper, we use an SVMs-based chunking tool YamCha (Kudo and Matsumoto, 2001).	[189, 151, 21, 22, 90, 150, 192, 1, 121, 8]	[1, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.7706775069236755, 0.445363312959671, 0.5327860713005066, 0.07038440555334091, 0.26026687026023865, 0.6889277696609497, 0.0886273980140686, 0.1912420690059662, 0.24389412999153137, 0.12933841347694397]
In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3.	[157, 206, 230, 160, 166, 62, 228, 168, 159, 162]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47690901160240173, 0.4632103145122528, 0.14494812488555908, 0.23145723342895508, 0.12391982972621918, 0.26241767406463623, 0.1132914125919342, 0.09197966009378433, 0.19843032956123352, 0.3909427225589752]
In this paper, we used the WSD program reported in (Lee and Ng, 2002).	[92, 43, 40, 53, 82, 125, 14, 34, 13, 11]	[1, 0, 0, 1, 1, 1, 0, 0, 0, 0]	[0.8226349949836731, 0.48998603224754333, 0.06923191249370575, 0.5510596036911011, 0.5298798084259033, 0.6336184740066528, 0.38352522253990173, 0.21695806086063385, 0.14760112762451172, 0.11287754774093628]
In this respect it resembles Wu's bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrase based philosophy.	[184, 179, 234, 328, 140, 19, 261, 272, 231, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7173923850059509, 0.21635214984416962, 0.169675812125206, 0.2937716245651245, 0.1314776986837387, 0.17519399523735046, 0.12232525646686554, 0.05216512084007263, 0.07842829823493958, 0.09480315446853638]
In this respect, this is similar to work by Lapata (2003), who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles.	[85, 223, 226, 95, 41, 189, 99, 40, 50, 114]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13028346002101898, 0.6745428442955017, 0.06041565164923668, 0.11947000026702881, 0.2882692813873291, 0.11454691737890244, 0.3183412551879883, 0.21398702263832092, 0.051911547780036926, 0.05739361047744751]
In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue.	[122, 19, 85, 109, 28, 4, 65, 37, 5, 124]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2715722620487213, 0.6587845683097839, 0.06212698295712471, 0.15662069618701935, 0.11883670836687088, 0.17129389941692352, 0.07570911198854446, 0.06933281570672989, 0.06857728958129883, 0.06396897882223129]
In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004).	[6, 30, 1, 2, 23, 24, 20, 9, 21, 22]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7352689504623413, 0.5276582837104797, 0.11825419217348099, 0.10890991985797882, 0.21105128526687622, 0.16312040388584137, 0.10627485811710358, 0.46017584204673767, 0.10999976098537445, 0.056824903935194016]
Incorporating binary and real features yields a rough approximation of generative models in semi supervised CRFs (Suzuki and Isozaki, 2008).	[40, 70, 39, 8, 27, 2, 33, 48, 24, 54]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4876552224159241, 0.4885562062263489, 0.3540215492248535, 0.35376688838005066, 0.20026949048042297, 0.14732931554317474, 0.2945512533187866, 0.11786117404699326, 0.15533263981342316, 0.11660097539424896]
Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000).	[65, 15, 93, 12, 48, 69, 67, 47, 2, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2443605363368988, 0.14585775136947632, 0.3036118447780609, 0.17302869260311127, 0.1608804613351822, 0.12678489089012146, 0.20585447549819946, 0.2677869498729706, 0.06540513783693314, 0.10645262151956558]
Indeed, the COLLAGEN architecture, like that of the Queen's Communicator, manages discourse using a focus stack, a classical idea in the theory of discourse structure (Grosz and Sidner, 1986).	[225, 214, 208, 2, 14, 599, 332, 496, 563, 708]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47748124599456787, 0.25238484144210815, 0.45853278040885925, 0.38255229592323303, 0.38255229592323303, 0.17868632078170776, 0.13639125227928162, 0.20959845185279846, 0.21620535850524902, 0.1471516638994217]
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	[36, 37, 46, 20, 21, 17, 19, 58, 77, 45]	[1, 0, 1, 1, 1, 1, 0, 1, 0, 0]	[0.660381555557251, 0.3294568955898285, 0.7226946949958801, 0.5250272154808044, 0.7727329134941101, 0.5856329202651978, 0.18679587543010712, 0.535922110080719, 0.43239837884902954, 0.3568075895309448]
Insertion of further information such as supertags (Bangalore and Joshi, 1999) or word stems can also be beneficial for further processing.	[96, 227, 4, 14, 99, 28, 161, 34, 83, 178]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6393377184867859, 0.47887906432151794, 0.24617210030555725, 0.24617210030555725, 0.2912449538707733, 0.07947885990142822, 0.13099755346775055, 0.13723431527614594, 0.0918332114815712, 0.2885560691356659]
Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF.	[199, 263, 276, 198, 281, 40, 329, 333, 14, 249]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6331580877304077, 0.42018723487854004, 0.23121270537376404, 0.15425196290016174, 0.30856853723526, 0.09701885282993317, 0.2966833710670471, 0.1575172394514084, 0.1461634635925293, 0.15362638235092163]
Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011).	[29, 42, 48, 34, 43, 14, 141, 1, 66, 15]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15586423873901367, 0.19629202783107758, 0.09930599480867386, 0.08206459134817123, 0.14377257227897644, 0.1645166426897049, 0.07146728783845901, 0.08259956538677216, 0.06286852806806564, 0.060727208852767944]
Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word.	[123, 0, 39, 104, 127, 3, 4, 15, 111, 92]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4129163920879364, 0.8418958187103271, 0.5629752278327942, 0.4924209713935852, 0.25162795186042786, 0.3363860845565796, 0.48968833684921265, 0.05135771259665489, 0.08161300420761108, 0.2619397044181824]
Instead, we develop a new scoring criterion, based on Melamed (1997).	[80, 112, 30, 134, 60, 17, 59, 87, 58, 111]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10095685720443726, 0.30248644948005676, 0.10147784650325775, 0.22335095703601837, 0.11093248426914215, 0.35113629698753357, 0.09137978404760361, 0.13268229365348816, 0.056244686245918274, 0.07754924148321152]
Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information.	[594, 42, 593, 303, 308, 199, 353, 206, 335, 229]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.736860454082489, 0.46117839217185974, 0.10434024035930634, 0.18672725558280945, 0.09469513595104218, 0.07267864793539047, 0.09676685184240341, 0.05201590433716774, 0.38566532731056213, 0.2286747843027115]
It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider.	[15, 40, 13, 107, 2, 125, 42, 70, 34, 135]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6352894306182861, 0.5203430652618408, 0.12092380225658417, 0.4156341552734375, 0.5219072103500366, 0.18732652068138123, 0.12183721363544464, 0.25414803624153137, 0.08714449405670166, 0.07244622707366943]
It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006).	[118, 48, 6, 51, 122, 162, 82, 30, 127, 66]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7143084406852722, 0.06637397408485413, 0.1598859280347824, 0.07651813328266144, 0.13894566893577576, 0.05521006882190704, 0.07099004834890366, 0.05546526983380318, 0.055926613509655, 0.05951905623078346]
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable.	[144, 119, 173, 196, 208, 31, 158, 153, 141, 62]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43629875779151917, 0.20808017253875732, 0.07523567974567413, 0.06601769477128983, 0.06597661972045898, 0.06642771512269974, 0.05595802888274193, 0.23569124937057495, 0.21001440286636353, 0.059558331966400146]
It consists of the second order parsing algorithm of Carreras (2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component.	[51, 52, 3, 12, 58, 63, 33, 46, 114, 73]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5589431524276733, 0.20461216568946838, 0.10431592911481857, 0.27048951387405396, 0.08490458130836487, 0.07967807352542877, 0.13832466304302216, 0.1704699844121933, 0.09915823489427567, 0.6687456369400024]
It could be also interested to test the combination between a better QC system, the current one by Li and Roth's for instance (Li and Roth, 2002), and our machine translation method.	[71, 117, 14, 62, 147, 202, 56, 68, 23, 73]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34025537967681885, 0.8067041039466858, 0.29139360785484314, 0.06150184944272041, 0.07407616823911667, 0.19821757078170776, 0.06713944673538208, 0.09404347836971283, 0.0703297108411789, 0.2475479543209076]
It has also been shown that these techniques prove useful for tasks such as word sense disambiguation (Patwardhan et al, 2003), real-word spelling correction (Budanitsky and Hirst, 2001) and information extraction (Stevenson and Greenwood, 2005), among others.	[32, 77, 142, 23, 154, 70, 72, 14, 150, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7204285264015198, 0.6726822853088379, 0.15216659009456635, 0.055937204509973526, 0.08715391904115677, 0.10781477391719818, 0.07370515912771225, 0.40788301825523376, 0.15343023836612701, 0.07142183929681778]
It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model (Cutting et al, 1992).	[1, 99, 25, 89, 57, 29, 127, 93, 121, 164]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7255221605300903, 0.7560229301452637, 0.12954510748386383, 0.12954510748386383, 0.6583722829818726, 0.11989620327949524, 0.1449570506811142, 0.11989620327949524, 0.49016332626342773, 0.18272051215171814]
It has been previously attempted by Cucerzan and Yarowsky in their language independent NER work which used morphological and contextual evidences (Cucerzan and Yarowsky, 1999).	[0, 40, 2, 196, 7, 9, 4, 163, 17, 197]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.768838107585907, 0.2942900061607361, 0.24641753733158112, 0.20337001979351044, 0.1016249805688858, 0.09334991127252579, 0.09849760681390762, 0.13617710769176483, 0.220948725938797, 0.10750944912433624]
It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994).	[0, 75, 57, 41, 19, 88, 77, 22, 72, 64]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.8212224245071411, 0.47218599915504456, 0.29808276891708374, 0.06700146198272705, 0.34754395484924316, 0.6302110552787781, 0.41722017526626587, 0.09234550595283508, 0.19291634857654572, 0.2583470046520233]
It has been successfully applied for text content such as news articles, scientific papers (Teufel and Moens, 2002) that follow a discourse structure.	[505, 37, 75, 71, 129, 336, 27, 36, 58, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4724321961402893, 0.23642399907112122, 0.13198652863502502, 0.32218703627586365, 0.2357868105173111, 0.3181951642036438, 0.3702779710292816, 0.13519082963466644, 0.11910939961671829, 0.11785681545734406]
It is a very powerful technique already used for NLP applications such as information retrieval (Berry et al, 1995) and text segmentation (Choi et al, 2001) and, more recently, multi and single-document summarization.	[93, 24, 16, 6, 55, 5, 1, 56, 25, 220]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.28866052627563477, 0.33743542432785034, 0.47137466073036194, 0.6977919936180115, 0.44417521357536316, 0.15256786346435547, 0.2874879240989685, 0.058163028210401535, 0.14360815286636353, 0.2708889842033386]
It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996).	[77, 29, 186, 30, 187, 112, 128, 70, 142, 105]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8486379384994507, 0.49513694643974304, 0.49513694643974304, 0.11166372895240784, 0.14051231741905212, 0.3789396286010742, 0.050849549472332, 0.05370945483446121, 0.1589377224445343, 0.052750736474990845]
It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010).	[174, 36, 6, 38, 7, 1, 68, 10, 234, 210]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8106132745742798, 0.5559377670288086, 0.14014752209186554, 0.10369512438774109, 0.07093869894742966, 0.12075507640838623, 0.14751741290092468, 0.09130553901195526, 0.18967027962207794, 0.14329957962036133]
It is difficult to compare these results with results from other studies such as that of Caraballo (1999), as the data used is not the same.	[124, 82, 116, 14, 78, 3, 38, 7, 8, 79]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.8497890830039978, 0.817668616771698, 0.7247239947319031, 0.5443684458732605, 0.5887439250946045, 0.07964117079973221, 0.11802525073289871, 0.1574368178844452, 0.08508521318435669, 0.394436240196228]
It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models.	[25, 3, 12, 130, 82, 93, 1, 10, 8, 17]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8411474823951721, 0.4699384868144989, 0.4699384868144989, 0.7505170702934265, 0.49566760659217834, 0.31257134675979614, 0.2594399154186249, 0.2594399154186249, 0.2869926393032074, 0.2869926393032074]
It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003).	[70, 9, 15, 107, 204, 65, 49, 188, 55, 194]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4428379237651825, 0.1609533578157425, 0.34697046875953674, 0.16301265358924866, 0.13571763038635254, 0.31454432010650635, 0.14547790586948395, 0.13115142285823822, 0.4036152958869934, 0.1414138525724411]
It is not clear how the smoothing technique proposed in (Yarowsky, 1993) could be extended to away ambiguities.	[31, 95, 10, 20, 47, 57, 93, 55, 4, 32]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6581041812896729, 0.5885410308837891, 0.3693886995315552, 0.07742150872945786, 0.07415062189102173, 0.07963069528341293, 0.07853289693593979, 0.3107055127620697, 0.3640298545360565, 0.08933723717927933]
"It is not clear what resources are required to adapt systems to new languages.""It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995)."	[54, 62, 49, 51, 168, 96, 55, 333, 52, 249]	[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]	[0.38733410835266113, 0.1915813833475113, 0.3837519884109497, 0.6015463471412659, 0.40987491607666016, 0.2828635573387146, 0.4820883870124817, 0.5410146713256836, 0.24688720703125, 0.0789308026432991]
It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00).	[112, 140, 65, 117, 118, 142, 128, 144, 141, 35]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6100894808769226, 0.48274874687194824, 0.3091285526752472, 0.13733087480068207, 0.0987560823559761, 0.48985207080841064, 0.353535920381546, 0.30949580669403076, 0.3324122428894043, 0.12611261010169983]
It is not, unfortunately, possible to keep it close to both FUG and PATR (Shieber 1984), but it should be possible for readers familiar with PATR to see roughly what the relation between the two is.	[6, 71, 60, 28, 31, 43, 4, 59, 44, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07235994189977646, 0.1735915094614029, 0.19228409230709076, 0.11636345833539963, 0.06454021483659744, 0.1573486626148224, 0.07491209357976913, 0.17378635704517365, 0.07772381603717804, 0.11084538698196411]
It is therefore necessary to either discard infrequent rules, do manual editing, use a different rule format such as individual dependencies (Collins, 1996) or gain full linguistic control and insight by using a hand written grammar, each of which sacrifices total completeness.	[11, 114, 140, 48, 35, 82, 12, 127, 149, 148]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35285428166389465, 0.40141919255256653, 0.22032830119132996, 0.18769434094429016, 0.09011150151491165, 0.15858592092990875, 0.13086116313934326, 0.10574592649936676, 0.082057423889637, 0.05986563116312027]
It is typically applicable in the text generation field, both for concept-to-text generation and text-to text generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on.	[17, 109, 19, 27, 6, 2, 18, 107, 108, 215]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8393394947052002, 0.6924124956130981, 0.4152315855026245, 0.3506318926811218, 0.28021788597106934, 0.3617023229598999, 0.41844019293785095, 0.3186286985874176, 0.24524278938770294, 0.27420756220817566]
It is well known that two languages are more informative than one (Dagan et al, 1991).	[0, 12, 23, 161, 183, 32, 16, 195, 55, 1]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8510616421699524, 0.3136903643608093, 0.05639135092496872, 0.08898505568504333, 0.07221831381320953, 0.061982400715351105, 0.1669820249080658, 0.07018499076366425, 0.1332394778728485, 0.0699441209435463]
It shows LDA SP performs good correlation with human ratings, where LDASP+Bayes refers to the Bayes prediction method of Ritter et al (2010).	[159, 48, 100, 197, 147, 177, 199, 95, 142, 4]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2163187563419342, 0.538538932800293, 0.25226542353630066, 0.12528780102729797, 0.2158816009759903, 0.17339135706424713, 0.097683846950531, 0.2536473572254181, 0.17467769980430603, 0.17285682260990143]
It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.).	[169, 27, 141, 153, 158, 160, 5, 59, 97, 159]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34829509258270264, 0.2644243836402893, 0.055729810148477554, 0.14250577986240387, 0.1935570389032364, 0.060646902769804, 0.11629840731620789, 0.05386173352599144, 0.15550029277801514, 0.22095701098442078]
It was criticized (Soon et al, 2001) that the features used by McCarthy and Lehnert (1995) are highly idiosyncratic and applicable only to one particular domain.	[317, 313, 314, 309, 242, 193, 173, 44, 13, 195]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.703330397605896, 0.8148273825645447, 0.5111612677574158, 0.7284848093986511, 0.46770164370536804, 0.36218637228012085, 0.26842060685157776, 0.42475348711013794, 0.16461661458015442, 0.07784146815538406]
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English).	[8, 36, 23, 0, 9, 26, 1, 80, 116, 19]	[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.7834629416465759, 0.8351231217384338, 0.38638028502464294, 0.5942031741142273, 0.2960449159145355, 0.3022821247577667, 0.1138981506228447, 0.575266420841217, 0.3131259083747864, 0.18822607398033142]
It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004).	[333, 280, 303, 276, 74, 35, 278, 94, 70, 352]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.506348192691803, 0.49495217204093933, 0.3425179421901703, 0.19243957102298737, 0.3141394257545471, 0.48333457112312317, 0.08994459360837936, 0.08705846220254898, 0.1607242226600647, 0.1555890291929245]
It was observed somewhere around 1990 at Xerox that the rule sets may be composed with the lexicon transducers in an efficient way and that the resulting transducer was roughly similar in size as the lexicon transducer itself (Karttunen et al, 1992).	[51, 47, 112, 113, 78, 17, 116, 89, 70, 6]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6043642163276672, 0.15542802214622498, 0.21416741609573364, 0.46537894010543823, 0.13478250801563263, 0.47721725702285767, 0.07522822171449661, 0.15509431064128876, 0.2378588169813156, 0.12497220188379288]
It would be also interesting to compare our solution with different approaches found in the literature, as for example [Reiter and Dale, 1992] or [Krahmer and Theune, 2000] for the referring expression generation, and the one of Dalianis and Hovy [Dalianis and Hovy, 1996] for the aggregation.	[130, 51, 29, 185, 63, 57, 18, 165, 20, 67]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25277194380760193, 0.33064958453178406, 0.24802300333976746, 0.13623416423797607, 0.15344451367855072, 0.09705157577991486, 0.13977760076522827, 0.22779521346092224, 0.3631139099597931, 0.08527546375989914]
Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics.	[162, 38, 157, 36, 78, 155, 22, 113, 20, 46]	[0, 0, 0, 1, 1, 0, 0, 0, 0, 1]	[0.09171412885189056, 0.15676191449165344, 0.21794599294662476, 0.755608856678009, 0.5413632988929749, 0.06751580536365509, 0.16785070300102234, 0.10344582051038742, 0.19623298943042755, 0.6488354802131653]
Its original PoS tag set is very coarse and the PoS and the word stem information is not very reliable. We therefore decided to retag the tree bank automatically using the Memory-Based Tagger (MBT) (Daelemans et al, 1996) which uses a very fine-grained tag set.	[79, 5, 23, 10, 42, 22, 30, 92, 165, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.39146360754966736, 0.4038858711719513, 0.10046932846307755, 0.1396482288837433, 0.40152621269226074, 0.180623859167099, 0.07363756746053696, 0.2935439646244049, 0.1022050604224205, 0.6614252924919128]
Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006).	[25, 42, 132, 31, 4, 5, 33, 182, 51, 17]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.811863899230957, 0.0739479809999466, 0.36742234230041504, 0.05624891072511673, 0.2150212824344635, 0.09864882379770279, 0.05329637974500656, 0.14809206128120422, 0.0590924434363842, 0.225862517952919]
JACY (Siegel and Bender, 2002) is a broad coverage linguistically precise HPSG-based grammar of Japanese.	[191, 1, 13, 17, 176, 31, 14, 7, 9, 199]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8458698391914368, 0.7840296030044556, 0.4334631562232971, 0.1576734483242035, 0.3645872473716736, 0.4352990984916687, 0.31579890847206116, 0.3594357967376709, 0.3029229938983917, 0.19403241574764252]
Jacquemin (1999) and Barzilay and McKeown (2001) identify phrase level paraphrases, while Lin and Pantel (2001) and Shinyama et al (2002) acquire structural paraphrases encoded as templates.	[121, 61, 62, 58, 1, 41, 9, 91, 32, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2667423486709595, 0.139242023229599, 0.4051348865032196, 0.23586831986904144, 0.10277464240789413, 0.12797582149505615, 0.4610480070114136, 0.24321229755878448, 0.17428742349147797, 0.3202032148838043]
Johnson et al (2007) reduced the phrase table based on the significance testing of phrase pair co-occurrence in bilingual corpus.	[1, 9, 99, 128, 203, 12, 90, 205, 4, 121]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7717165946960449, 0.1397089660167694, 0.24682864546775818, 0.15207546949386597, 0.08399665355682373, 0.1370590776205063, 0.07609815895557404, 0.09374082088470459, 0.10742278397083282, 0.12149583548307419]
KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically.	[1, 17, 9, 3, 190, 5, 21, 168, 6, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21986308693885803, 0.18524996936321259, 0.23639507591724396, 0.1385200023651123, 0.2626839876174927, 0.1824062615633011, 0.1463169902563095, 0.28031623363494873, 0.11024293303489685, 0.15364937484264374]
Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features.	[45, 68, 39, 44, 51, 47, 6, 78, 7, 8]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8526152968406677, 0.31114524602890015, 0.2605988681316376, 0.44000521302223206, 0.2348080575466156, 0.3010657727718353, 0.10564637929201126, 0.15393494069576263, 0.07295180857181549, 0.06382204592227936]
Kanayama and Nasukawa (2006) use syntactic features and context coherency, the tendency for same polarities to appear successively, to acquire polar atoms. Other related work is concerned with subjectivity analysis.	[3, 21, 31, 52, 48, 43, 22, 139, 124, 105]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8478387594223022, 0.7098088264465332, 0.39562222361564636, 0.5258418321609497, 0.2138364315032959, 0.13366863131523132, 0.4147322177886963, 0.09216637164354324, 0.18874578177928925, 0.1295800656080246]
Kaplan et al (1989) discussed such differences in embedding and offered two alternative analyses that rely only on codescriptive specifications.	[145, 129, 19, 150, 120, 121, 124, 3, 68, 1]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8214383125305176, 0.6965881586074829, 0.12796956300735474, 0.07849562913179398, 0.14703406393527985, 0.048587616533041, 0.054532647132873535, 0.058347187936306, 0.06475507467985153, 0.0579199455678463]
Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank.	[94, 15, 70, 26, 16, 98, 71, 33, 95, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46856024861335754, 0.24827530980110168, 0.3261835277080536, 0.14885878562927246, 0.33606860041618347, 0.11878513544797897, 0.17170646786689758, 0.14145195484161377, 0.39171749353408813, 0.2995830178260803]
Kasper (1987) and Eisele and DSrre (1988) have tackled this problem and proposed unification methods for disjunctive feature descriptions.	[12, 0, 5, 1, 4, 16, 135, 125, 13, 36]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5393662452697754, 0.7671142816543579, 0.32354021072387695, 0.45112723112106323, 0.3365595042705536, 0.30224043130874634, 0.3270309269428253, 0.3037079870700836, 0.1899424046278, 0.08585130423307419]
Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words.	[157, 4, 27, 159, 3, 124, 156, 26, 51, 83]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.682957112789154, 0.39601826667785645, 0.39288052916526794, 0.46474000811576843, 0.21586990356445312, 0.19911175966262817, 0.23939889669418335, 0.4262601137161255, 0.32860952615737915, 0.22061359882354736]
Kim and Hovy (2006) use structural features of the language to identify opinion entities.	[40, 95, 15, 19, 97, 2, 98, 34, 23, 155]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7733950018882751, 0.24418871104717255, 0.21568563580513, 0.08212260901927948, 0.05379035323858261, 0.10911770910024643, 0.05172557011246681, 0.07219218462705612, 0.1951221525669098, 0.11823652684688568]
Knight and Chander (1994) and Gamon et al (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification algorithm of choice.	[44, 75, 81, 78, 59, 3, 58, 88, 74, 46]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7399427890777588, 0.33337706327438354, 0.31615856289863586, 0.3066282570362091, 0.24529516696929932, 0.0846986398100853, 0.2182771861553192, 0.07198186218738556, 0.40806373953819275, 0.1707122027873993]
Knight and Graehl (1998) model the transliteration of Japanese syllabic katakana script into English with a sequence of finite-state transducers.	[142, 17, 94, 31, 68, 75, 58, 140, 82, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2494088113307953, 0.3416396677494049, 0.21615636348724365, 0.19470854103565216, 0.10752315074205399, 0.4450038969516754, 0.11553291976451874, 0.23336273431777954, 0.12876710295677185, 0.12332095205783844]
Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search.	[29, 8, 56, 12, 162, 98, 182, 40, 5, 140]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4725230932235718, 0.5322546362876892, 0.32080262899398804, 0.2347652167081833, 0.28532078862190247, 0.23790976405143738, 0.07675690948963165, 0.05584021285176277, 0.10802443325519562, 0.0803867056965828]
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus.	[2, 95, 80, 94, 43, 83, 100, 131, 7, 5]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24630504846572876, 0.6660085320472717, 0.30615437030792236, 0.29576435685157776, 0.16619345545768738, 0.28838273882865906, 0.44319403171539307, 0.05165178328752518, 0.049300666898489, 0.13471578061580658]
Kudo and Matsumoto (2000) also used the same backward beam search together with SVMs rather than ME.	[77, 141, 22, 126, 113, 132, 142, 144, 24, 70]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4047045409679413, 0.632259726524353, 0.10800754278898239, 0.16101694107055664, 0.21213839948177338, 0.12233898788690567, 0.07666526734828949, 0.10486239939928055, 0.07168368995189667, 0.1013851910829544]
Kudo et al (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity.	[27, 14, 15, 171, 168, 22, 4, 16, 12, 3]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.825012743473053, 0.7711688280105591, 0.4645838141441345, 0.31898194551467896, 0.19606685638427734, 0.31770727038383484, 0.3180672526359558, 0.28263360261917114, 0.4410751163959503, 0.17507143318653107]
L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i).	[62, 86, 89, 63, 91, 24, 52, 23, 64, 119]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1025124043226242, 0.07617516815662384, 0.07751985639333725, 0.08151957392692566, 0.06401068717241287, 0.05623767524957657, 0.12023227661848068, 0.10014752298593521, 0.08137258142232895, 0.08200687915086746]
Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus.	[41, 97, 45, 54, 29, 13, 93, 58, 3, 64]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28661954402923584, 0.12954314053058624, 0.3249105215072632, 0.086250901222229, 0.1296045482158661, 0.141326904296875, 0.0688059851527214, 0.09583692997694016, 0.0756470337510109, 0.06615373492240906]
Lappin and Leass (1994), for example, use several heuristics to filter out expletive pronouns, including a check for patterns including modal adjectives.	[67, 179, 39, 173, 286, 341, 212, 289, 263, 82]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7285634279251099, 0.7297399044036865, 0.2506890296936035, 0.5308806896209717, 0.4210060238838196, 0.09871436655521393, 0.3840661942958832, 0.34273937344551086, 0.1076817587018013, 0.1859060823917389]
Latent Dirichlet Allocation and its supervised extensions such as Labeled LDA (LLDA) (Ramage et al, 2009) and supervised LDA (sLDA) (Blei and McAuliffe, 2008) are powerful generative models that capture the underlying semantics of texts.	[37, 35, 30, 3, 31, 16, 20, 32, 38, 24]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7399746775627136, 0.2808024287223816, 0.313303679227829, 0.31503352522850037, 0.14689040184020996, 0.5466320514678955, 0.12583796679973602, 0.13776785135269165, 0.11107272654771805, 0.10647208243608475]
Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG).	[17, 0, 65, 397, 50, 172, 64, 53, 375, 374]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4219473898410797, 0.44913628697395325, 0.49196937680244446, 0.2013247162103653, 0.19347548484802246, 0.10471846163272858, 0.3591628074645996, 0.33787673711776733, 0.2550748288631439, 0.26566430926322937]
Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework.	[133, 116, 98, 112, 16, 131, 114, 106, 18, 135]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5014604926109314, 0.43793216347694397, 0.19228969514369965, 0.1811276078224182, 0.4792933762073517, 0.09652605652809143, 0.18634939193725586, 0.0756232962012291, 0.18316388130187988, 0.05388498306274414]
Lattice represent the system implemented as Dyer et al, (2008).	[42, 90, 38, 143, 25, 29, 91, 51, 30, 43]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.07034926116466522, 0.22632835805416107, 0.06812568008899689, 0.2593311667442322, 0.5874481201171875, 0.05312182754278183, 0.05244682729244232, 0.37551116943359375, 0.06309914588928223, 0.07352267950773239]
Lee (2004) uses a morphologically analyzed and tagged parallel corpus for Arabic English SMT.	[54, 2, 24, 23, 22, 57, 12, 33, 40, 8]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8167261481285095, 0.27185389399528503, 0.40591245889663696, 0.2473974972963333, 0.2853483557701111, 0.15184509754180908, 0.1271810382604599, 0.3207911252975464, 0.15850141644477844, 0.1407727748155594]
Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph.	[39, 40, 88, 114, 81, 102, 79, 55, 41, 181]	[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]	[0.4201693534851074, 0.30345478653907776, 0.17350469529628754, 0.7183213233947754, 0.5787737369537354, 0.3178289532661438, 0.08529146015644073, 0.16287995874881744, 0.28903815150260925, 0.1402011215686798]
Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009).	[29, 78, 22, 27, 81, 84, 33, 23, 28, 31]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8315588235855103, 0.3682003319263458, 0.21188180148601532, 0.123164601624012, 0.1787230372428894, 0.055791571736335754, 0.30993470549583435, 0.1460057646036148, 0.17450375854969025, 0.09556882083415985]
LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality.	[3, 12, 1, 10, 141, 51, 143, 57, 135, 71]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8512319326400757, 0.47800353169441223, 0.2858242392539978, 0.3314717710018158, 0.49330952763557434, 0.32087716460227966, 0.25347596406936646, 0.23810149729251862, 0.11092435568571091, 0.3211166262626648]
Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).	[16, 0, 187, 111, 288, 284, 2, 9, 27, 264]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20261108875274658, 0.6985837817192078, 0.12491991370916367, 0.0734148696064949, 0.0912826806306839, 0.05978861078619957, 0.04764276370406151, 0.04764276370406151, 0.17498332262039185, 0.41830575466156006]
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	[52, 169, 12, 185, 104, 48, 9, 101, 211, 41]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14212799072265625, 0.05420388653874397, 0.07614497095346451, 0.1870088130235672, 0.3522001802921295, 0.2881462574005127, 0.08572971820831299, 0.10637383162975311, 0.08628977835178375, 0.055088430643081665]
Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture.	[3, 199, 14, 16, 89, 45, 21, 73, 161, 118]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7671166062355042, 0.7733049392700195, 0.44076278805732727, 0.08771184831857681, 0.2370486706495285, 0.3266913592815399, 0.26195645332336426, 0.380021870136261, 0.35699570178985596, 0.35496872663497925]
Li et al (2012) and Bohnet and Nivre (2012) use joint models for POS tagging and dependency parsing, significantly outperforming their pipeline counterparts.	[12, 160, 10, 146, 15, 0, 2, 3, 81, 70]	[1, 1, 0, 1, 0, 1, 0, 0, 0, 0]	[0.5537272691726685, 0.5912326574325562, 0.44515150785446167, 0.813010573387146, 0.2775978446006775, 0.5976797342300415, 0.36543190479278564, 0.1165858581662178, 0.1728821098804474, 0.17042623460292816]
Light et al (2004) explore issues with annotating speculative language in biomedicine and out line potential applications.	[69, 1, 47, 154, 141, 100, 4, 0, 70, 157]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.6180087327957153, 0.8490775227546692, 0.6990001797676086, 0.41123297810554504, 0.28010016679763794, 0.5284218788146973, 0.1896270215511322, 0.4459077715873718, 0.08155304938554764, 0.05932379141449928]
Likewise, (Ng and Lee, 1996) report overall accuracy for the noun interest of 87%, and find that that when their feature set only consists of co-occurrence features the accuracy only drops to 80%.	[13, 104, 109, 63, 91, 41, 25, 45, 65, 105]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7902796864509583, 0.7753507494926453, 0.25292375683784485, 0.16335691511631012, 0.29384303092956543, 0.09421678632497787, 0.16313797235488892, 0.3185459077358246, 0.22107113897800446, 0.32625332474708557]
MADA is an SVM based system that disambiguates among different morphological analyses produced by BAMA (Habash and Rambow, 2005).	[100, 42, 41, 26, 15, 16, 2, 30, 148, 78]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.5425800681114197, 0.2568817138671875, 0.1326308697462082, 0.47197654843330383, 0.15005162358283997, 0.6206362843513489, 0.08383737504482269, 0.33532875776290894, 0.05819219723343849, 0.07098191231489182]
Machine learning techniques have been proposed for sentiment classification (Pang et al., 2002; Mullen and Collier, 2004) based on annotated samples from experts, but they have limited performance especially when estimating ratings of a multi-point scale (Pang and Lee, 2005).	[1, 35, 27, 9, 20, 78, 154, 11, 44, 0]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2503306567668915, 0.7223097681999207, 0.16441629827022552, 0.16266553103923798, 0.08017735183238983, 0.14403685927391052, 0.2805674374103546, 0.07089810818433762, 0.08529602736234665, 0.21894264221191406]
Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity.	[147, 137, 153, 140, 112, 150, 143, 148, 146, 160]	[0, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.46859297156333923, 0.7579748034477234, 0.6281633973121643, 0.339443176984787, 0.5134616494178772, 0.26489394903182983, 0.26837971806526184, 0.3590604364871979, 0.3774222433567047, 0.4994039535522461]
Mani and Wilson (2000) worked on news and introduced an annotation scheme for temporal expressions, and a method for using explicit temporal expressions to assign activity times to the entirety of an article.	[1, 10, 149, 4, 30, 141, 24, 38, 142, 63]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8438599705696106, 0.7933727502822876, 0.6267328858375549, 0.3152422606945038, 0.16934795677661896, 0.3047184944152832, 0.260389506816864, 0.07186130434274673, 0.14694984257221222, 0.20941324532032013]
Mann and Yarowsky (2001) applied the stochastic transducer of Ristad and Yianilos (1998) for inducing translation lexicons between two languages, but found that in some cases it offered no improvement over Levenshtein distance.	[114, 126, 45, 118, 121, 166, 124, 178, 128, 50]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7179703712463379, 0.3360218405723572, 0.37911537289619446, 0.2143922746181488, 0.29082173109054565, 0.37911537289619446, 0.1751590371131897, 0.17478404939174652, 0.17590348422527313, 0.24786390364170074]
Mann and Yarowsky (2001) investigated the induction of translation lexicons via bridge languages.	[0, 31, 5, 138, 47, 152, 168, 1, 13, 32]	[1, 1, 0, 0, 1, 1, 1, 0, 0, 0]	[0.8482016921043396, 0.6838637590408325, 0.4454262852668762, 0.4454262852668762, 0.6120981574058533, 0.6838637590408325, 0.6120981574058533, 0.1635298877954483, 0.23038138449192047, 0.31606239080429077]
Mann and Yarowsky (2001) saw little improvement over Edit Distance when applying this transducer to cognates, even when filtering the transducer's probabilities into different weight classes to better approximate Edit Distance.	[118, 105, 121, 51, 172, 34, 50, 171, 126, 21]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8116808533668518, 0.5666806697845459, 0.16559217870235443, 0.17037872970104218, 0.17037872970104218, 0.1486058235168457, 0.28702789545059204, 0.28702789545059204, 0.24791856110095978, 0.2814340591430664]
Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow (Zhang et al, 2001), SVMs (Kudo and Matsumoto, 2001), CRFs (Sha and Pereira, 2003), Maximum Entropy Model (Collins, 2002), Memory Based Learning (Sang, 2002) and SNoW (Munoz et al., 1999).	[20, 10, 12, 154, 22, 151, 155, 90, 3, 5]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7997943162918091, 0.2237224578857422, 0.3219081163406372, 0.4012008607387543, 0.11826411634683609, 0.09916751831769943, 0.22301724553108215, 0.17660781741142273, 0.06851665675640106, 0.0789213627576828]
Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al, 1997), the dictionary HMM model (Kouetal., 2005) and Maximum Entropy Markov Mod els (MEMMs) (Finkel et al, 2004).	[3, 1, 5, 10, 30, 135, 27, 6, 11, 140]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.27266278862953186, 0.14693239331245422, 0.12376797199249268, 0.10004667192697525, 0.17040890455245972, 0.05551397055387497, 0.11545436829328537, 0.12185917049646378, 0.09663090854883194, 0.06278320401906967]
Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another.	[28, 87, 3, 31, 45, 125, 49, 93, 41, 117]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6586704254150391, 0.34345611929893494, 0.2799628973007202, 0.6982617378234863, 0.26852357387542725, 0.42070597410202026, 0.1339423805475235, 0.10528793185949326, 0.1861375868320465, 0.06783755868673325]
Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor.	[44, 132, 32, 156, 16, 69, 94, 5, 42, 90]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11445502936840057, 0.10505089163780212, 0.13616470992565155, 0.23919200897216797, 0.13053250312805176, 0.0687115341424942, 0.07511764764785767, 0.14200952649116516, 0.10469821840524673, 0.06365711987018585]
Marcu and Wong (2002) describes an approximation to O.	[86, 209, 79, 202, 60, 71, 17, 21, 22, 152]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2835063934326172, 0.2835063934326172, 0.1762840300798416, 0.1762840300798416, 0.11733843386173248, 0.0664362832903862, 0.1488877385854721, 0.07651814818382263, 0.0677918866276741, 0.27483847737312317]
Marton and Resnik (2008) find that some constituency types favor matching the source parse while others encourage violations.	[110, 111, 144, 45, 46, 35, 52, 48, 3, 36]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8251015543937683, 0.367385596036911, 0.451321005821228, 0.09847545623779297, 0.1587325781583786, 0.2118636667728424, 0.12084490805864334, 0.0544644370675087, 0.07277018576860428, 0.2020978331565857]
Matsuzaki et al (2005) independently introduce a similar approach and present empirical results that rival ours.	[96, 19, 32, 28, 143, 16, 7, 139, 4, 144]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3467232882976532, 0.320828914642334, 0.16346561908721924, 0.22291497886180878, 0.056851692497730255, 0.18703573942184448, 0.062857486307621, 0.06252779066562653, 0.14745144546031952, 0.12122808396816254]
McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes.	[125, 205, 35, 52, 153, 3, 9, 4, 10, 2]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5064398646354675, 0.7985191345214844, 0.34883543848991394, 0.3755177855491638, 0.3284139037132263, 0.25240522623062134, 0.25240522623062134, 0.3851354718208313, 0.3851354718208313, 0.37945106625556946]
McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD.	[1, 7, 28, 29, 143, 2, 8, 17, 78, 154]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6955527663230896, 0.6955527663230896, 0.27740252017974854, 0.14846542477607727, 0.16545523703098297, 0.15302522480487823, 0.15302522480487823, 0.14036819338798523, 0.16603952646255493, 0.21382944285869598]
McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus.	[76, 157, 116, 27, 62, 0, 121, 79, 101, 8]	[1, 1, 0, 0, 0, 1, 0, 1, 0, 0]	[0.8451196551322937, 0.7143145203590393, 0.20198467373847961, 0.31682318449020386, 0.2542282044887543, 0.5516400337219238, 0.22877119481563568, 0.7390937805175781, 0.3367999792098999, 0.36996081471443176]
McDonald (McDonald et al 2007) has reported some success mixing fine and course labeling in sentiment analysis.	[0, 18, 17, 26, 158, 6, 39, 52, 106, 28]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8008849024772644, 0.28135308623313904, 0.37340715527534485, 0.24796569347381592, 0.28928184509277344, 0.4038970172405243, 0.23767025768756866, 0.25901809334754944, 0.2605554461479187, 0.08767072856426239]
McIntyre and Lapata (2010) create a story generation system that draws on earlier work on narrative schemas (Chambers and Jurafsky, 2009).	[14, 128, 1, 145, 121, 159, 18, 111, 194, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7677404284477234, 0.2200295478105545, 0.07202720642089844, 0.2697101831436157, 0.29132312536239624, 0.10125569999217987, 0.20649486780166626, 0.24460576474666595, 0.08492325246334076, 0.3326106667518616]
Mellish et al (1998) (and subsequently Karamanis and Manurung 2002) advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts.	[4, 135, 77, 55, 121, 53, 85, 51, 35, 12]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7954041957855225, 0.31789231300354004, 0.24582445621490479, 0.1261233538389206, 0.14508110284805298, 0.28998419642448425, 0.2310701459646225, 0.20711851119995117, 0.2654312551021576, 0.0557696707546711]
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses.	[35, 73, 167, 204, 87, 27, 8, 175, 75, 180]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.08828320354223251, 0.2999354302883148, 0.7307265400886536, 0.36632826924324036, 0.11412376165390015, 0.08634072542190552, 0.24620842933654785, 0.1275828778743744, 0.057425376027822495, 0.14515632390975952]
Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used.	[57, 87, 41, 85, 56, 84, 113, 141, 112, 67]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5620166063308716, 0.4414462447166443, 0.4420076608657837, 0.4674679636955261, 0.07562266290187836, 0.4243013858795166, 0.08064710348844528, 0.05989907681941986, 0.0760951042175293, 0.11331556737422943]
Mihalcea et al (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus.	[1, 5, 2, 174, 12, 27, 9, 8, 7, 182]	[1, 1, 1, 1, 1, 0, 0, 1, 1, 0]	[0.8345714211463928, 0.8335447907447815, 0.6900224685668945, 0.6966090202331543, 0.505482017993927, 0.3859906792640686, 0.3183634281158447, 0.5068560242652893, 0.5101221799850464, 0.27947601675987244]
Mikolov et al (2013a) reach top accuracy on the syntactic subset (an syn) with a CBOW predict model akin to ours (but trained on a corpus twice as large).	[24, 79, 34, 68, 12, 19, 18, 21, 23, 58]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5891331434249878, 0.09945603460073471, 0.1593269407749176, 0.12370496988296509, 0.13356634974479675, 0.05202886462211609, 0.11999332904815674, 0.07877355068922043, 0.05904430150985718, 0.5760682821273804]
Miller et al (2004) describe a relevant technique for the latter.	[3, 24, 4, 5, 1, 95, 61, 18, 15, 128]	[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.491167813539505, 0.5148811340332031, 0.12744377553462982, 0.09519985318183899, 0.2090255171060562, 0.07049237936735153, 0.05953929200768471, 0.07878277450799942, 0.5942071676254272, 0.07395271211862564]
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc.	[12, 0, 1, 125, 6, 13, 25, 14, 141, 85]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8460010290145874, 0.8384129405021667, 0.825495719909668, 0.3107627332210541, 0.2667716145515442, 0.16996945440769196, 0.1876872032880783, 0.3998614251613617, 0.10909763723611832, 0.34742265939712524]
Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004).	[0, 1, 12, 85, 125, 84, 152, 107, 151, 108]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7435429096221924, 0.7929513454437256, 0.44184809923171997, 0.27636951208114624, 0.2425289899110794, 0.4053097367286682, 0.11282697319984436, 0.07507834583520889, 0.06842499226331711, 0.28538814187049866]
Mintz et al (2009) use distant supervision to learn to extract relations that are represented in Freebase (Bollacker et al, 2008).	[21, 20, 60, 46, 0, 155, 23, 1, 3, 7]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8382652997970581, 0.7966907620429993, 0.5924831628799438, 0.2543967664241791, 0.28363582491874695, 0.3911168575286865, 0.12480314075946808, 0.12128766626119614, 0.3811708390712738, 0.05629102140665054]
Moldovan et al (2004) also use WordNet.	[153, 89, 123, 136, 144, 80, 125, 34, 75, 74]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7364680767059326, 0.27022865414619446, 0.23642580211162567, 0.14061960577964783, 0.1874774694442749, 0.3049141466617584, 0.16451922059059143, 0.1389777809381485, 0.06256162375211716, 0.1282857358455658]
Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance.	[61, 4, 2, 140, 31, 33, 0, 1, 141, 35]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7796173095703125, 0.08926833420991898, 0.2305179238319397, 0.253446102142334, 0.15519285202026367, 0.11511901021003723, 0.3504793345928192, 0.23038876056671143, 0.10693195462226868, 0.3042609989643097]
Moore (2005) likewise uses this example to motivate the need for models that support arbitrary, overlapping features.	[175, 17, 162, 38, 71, 30, 14, 116, 119, 32]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32024073600769043, 0.09411035478115082, 0.2736383378505707, 0.05708051845431328, 0.11153167486190796, 0.07407081127166748, 0.21258720755577087, 0.3381420969963074, 0.05742407962679863, 0.11755608022212982]
Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data.	[42, 80, 12, 83, 15, 10, 7, 11, 23, 24]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.662464439868927, 0.2400692105293274, 0.13257895410060883, 0.1283554583787918, 0.12636657059192657, 0.2051842361688614, 0.23164482414722443, 0.14706005156040192, 0.07673991471529007, 0.22713685035705566]
Moore and Pollack (1992) gave an example of a simple discourse.	[101, 104, 31, 65, 30, 74, 13, 93, 79, 38]	[1, 0, 0, 0, 1, 0, 0, 0, 1, 0]	[0.6662387847900391, 0.23526278138160706, 0.41808709502220154, 0.45854702591896057, 0.518304705619812, 0.41180315613746643, 0.2704724967479706, 0.2831656038761139, 0.5056266188621521, 0.13406379520893097]
More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006).	[21, 26, 5, 25, 0, 24, 92, 45, 77, 6]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2696952521800995, 0.637316882610321, 0.11781488358974457, 0.13165754079818726, 0.3341505527496338, 0.16583074629306793, 0.22845609486103058, 0.0661264955997467, 0.08195143938064575, 0.09913105517625809]
More recent approaches to compression introduce reordering and paraphrase operations (e.g. ,dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008).	[25, 144, 135, 45, 0, 138, 33, 91, 9, 34]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.701633870601654, 0.6867601871490479, 0.14316189289093018, 0.052315060049295425, 0.8383681774139404, 0.26648586988449097, 0.05200514942407608, 0.28484198451042175, 0.07693714648485184, 0.08376775681972504]
More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.	[97, 60, 59, 119, 164, 31, 70, 19, 37, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23531238734722137, 0.3537272810935974, 0.22526410222053528, 0.1922800987958908, 0.16936559975147247, 0.13210828602313995, 0.09240809828042984, 0.11250361800193787, 0.08414644002914429, 0.16001221537590027]
More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993).	[27, 40, 45, 66, 29, 20, 67, 69, 44, 99]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3352706730365753, 0.14745284616947174, 0.10595673322677612, 0.152150496840477, 0.1031440794467926, 0.1578497290611267, 0.06836574524641037, 0.07117170840501785, 0.11367283761501312, 0.08103065192699432]
More specifically, we use the terms in the lexicon constructed from (Wilson et al, 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3).	[97, 47, 77, 15, 115, 125, 51, 118, 74, 44]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.36328592896461487, 0.1622714251279831, 0.08369050174951553, 0.24037520587444305, 0.136188343167305, 0.10458531975746155, 0.17215773463249207, 0.06413431465625763, 0.49275916814804077, 0.5246878266334534]
Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005).	[186, 99, 162, 47, 13, 160, 16, 116, 166, 184]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5694184899330139, 0.17418892681598663, 0.34280213713645935, 0.24429449439048767, 0.11810224503278732, 0.42822256684303284, 0.0626029223203659, 0.15168924629688263, 0.2663552165031433, 0.135562002658844]
Moreover, as stressed in previous research, using syntactic dependencies seems to be particularly well suited to coping with the problem of linguistic variation across languages (Hwa et al, 2002).	[106, 31, 29, 47, 32, 95, 58, 33, 35, 18]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4577385485172272, 0.2942107915878296, 0.3063402771949768, 0.0740431547164917, 0.15992623567581177, 0.23822149634361267, 0.11370468884706497, 0.06465204805135727, 0.23051640391349792, 0.13482119143009186]
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions).	[93, 0, 39, 18, 8, 2, 25, 219, 47, 29]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.830727756023407, 0.7743692994117737, 0.3892539441585541, 0.7248551249504089, 0.4570798873901367, 0.41028955578804016, 0.17667809128761292, 0.25420811772346497, 0.054442860186100006, 0.05907202884554863]
Moreover, the assigned tag applies to the whole blog post while a finer grained sentiment extraction is needed (McDonald et al, 2007).	[6, 23, 35, 38, 72, 8, 5, 42, 20, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5787678360939026, 0.16196270287036896, 0.36411094665527344, 0.05559050291776657, 0.06261710822582245, 0.1082291454076767, 0.13599345088005066, 0.0521777868270874, 0.052649423480033875, 0.06894279271364212]
Moreover, while some techniques (e.g., Mann and Yarowsky (2001)) use multiple languages, the languages used have resources such as dictionaries between some language pairs.	[47, 168, 31, 43, 152, 123, 55, 176, 9, 6]	[1, 1, 0, 1, 0, 0, 1, 1, 0, 0]	[0.8173760175704956, 0.8173760175704956, 0.452687531709671, 0.5360651016235352, 0.452687531709671, 0.0637616366147995, 0.624363362789154, 0.624363362789154, 0.40961432456970215, 0.46951043605804443]
Most existing studies, such as Nie (1999), Resnik and Smith (2003) and Shi (2006), mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment method.	[147, 346, 23, 3, 7, 328, 111, 384, 387, 119]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17606763541698456, 0.1846303641796112, 0.46472808718681335, 0.1352662891149521, 0.1352662891149521, 0.1725691705942154, 0.12578484416007996, 0.13242629170417786, 0.16392287611961365, 0.1102614477276802]
Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994).	[106, 100, 44, 96, 31, 47, 99, 87, 116, 83]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4485694169998169, 0.40933769941329956, 0.3874422311782837, 0.20780012011528015, 0.25546252727508545, 0.26312389969825745, 0.16418816149234772, 0.2346082627773285, 0.12412837892770767, 0.25374138355255127]
Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008).	[4, 0, 59, 83, 109, 137, 65, 31, 14, 22]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3272694945335388, 0.37046173214912415, 0.3056751489639282, 0.17377470433712006, 0.07019535452127457, 0.08962336927652359, 0.21434277296066284, 0.20800866186618805, 0.2375197857618332, 0.35962873697280884]
Most other researchers take either the HMM alignments (Liang et al, 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.	[145, 147, 146, 149, 128, 161, 167, 9, 164, 148]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36510199308395386, 0.3020974099636078, 0.271330863237381, 0.08272460103034973, 0.1236572340130806, 0.20073524117469788, 0.19416698813438416, 0.26784148812294006, 0.16370399296283722, 0.14443768560886383]
Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007).	[84, 1, 129, 10, 85, 86, 48, 4, 158, 97]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3092975318431854, 0.22700130939483643, 0.06869594007730484, 0.15015782415866852, 0.07790350168943405, 0.07791182398796082, 0.06204177066683769, 0.08102654665708542, 0.10516516864299774, 0.29318878054618835]
Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al (2006).	[82, 62, 7, 135, 114, 33, 13, 18, 137, 119]	[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.3253517150878906, 0.8006199598312378, 0.349113792181015, 0.32139307260513306, 0.13077272474765778, 0.0546187199652195, 0.14467795193195343, 0.5332135558128357, 0.30877766013145447, 0.11184335500001907]
Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005).	[68, 8, 19, 135, 35, 57, 80, 18, 118, 81]	[1, 1, 1, 0, 0, 0, 1, 0, 0, 0]	[0.8285382986068726, 0.8146699666976929, 0.7144255638122559, 0.13678555190563202, 0.3785632848739624, 0.22383008897304535, 0.5267007350921631, 0.10300504416227341, 0.055124085396528244, 0.15541455149650574]
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	[8, 1, 77, 6, 11, 12, 98, 136, 138, 28]	[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]	[0.2535684406757355, 0.1851118505001068, 0.16497375071048737, 0.25381121039390564, 0.5948611497879028, 0.2785099148750305, 0.14770373702049255, 0.5767037272453308, 0.08266305923461914, 0.15519218146800995]
Motivated by the limitations of these previous methods, we propose a new generative alignment model that includes a full semantic parsing model proposed by Lu et al (2008).	[44, 0, 58, 195, 29, 84, 175, 160, 16, 25]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.44695019721984863, 0.7288479208946228, 0.19787117838859558, 0.2945289611816406, 0.4004807472229004, 0.36481496691703796, 0.24568507075309753, 0.25207605957984924, 0.10562817752361298, 0.11565995961427689]
Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOST FALSE, MOST-TRUE, and ALL.	[86, 84, 18, 3, 47, 101, 106, 99, 52, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2226550132036209, 0.41873255372047424, 0.3857263922691345, 0.09064648300409317, 0.21583019196987152, 0.08141214400529861, 0.08867637813091278, 0.07619265466928482, 0.09193749725818634, 0.2966538667678833]
Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary).	[180, 148, 181, 199, 146, 150, 80, 149, 81, 92]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7874983549118042, 0.7802414894104004, 0.6682595014572144, 0.33076003193855286, 0.4901373088359833, 0.1645990014076233, 0.34653639793395996, 0.4197159707546234, 0.23005110025405884, 0.13808980584144592]
Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon.	[1, 0, 163, 3, 20, 36, 162, 126, 121, 39]	[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.48105454444885254, 0.6084175705909729, 0.22189059853553772, 0.26370325684547424, 0.19631381332874298, 0.09637847542762756, 0.26781895756721497, 0.34521734714508057, 0.573355495929718, 0.17356090247631073]
Munteanu and Marcu (2006) proposed a method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora.	[1, 0, 163, 14, 3, 20, 10, 19, 50, 11]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.8366981744766235, 0.8031156659126282, 0.5614737272262573, 0.3563128411769867, 0.2639671862125397, 0.5030534863471985, 0.2547433078289032, 0.23714250326156616, 0.42122483253479004, 0.2010927051305771]
Mutual information (first introduced to computational linguistics by Church and Hanks (1989)) is one of many measures that seems to be roughly correlated to the degree of semantic relatedness be tween words.	[0, 42, 25, 28, 29, 9, 135, 133, 53, 126]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7674180269241333, 0.1804852932691574, 0.17808771133422852, 0.07450897991657257, 0.17474879324436188, 0.10067969560623169, 0.15677796304225922, 0.07152512669563293, 0.29120704531669617, 0.05414331704378128]
My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al, 1993), but that other features might improve parsing of other languages or even other English genres.	[43, 584, 588, 78, 5, 13, 398, 58, 8, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4687899649143219, 0.18322628736495972, 0.2906084954738617, 0.3463646471500397, 0.26323527097702026, 0.26323527097702026, 0.12364507466554642, 0.1938864141702652, 0.13405179977416992, 0.13405179977416992]
NGram Back-off Features: Similar to McDonald et al (2007), we utilize backed-off versions of lexical bi grams and trigrams, where all possible combinations of the words in the ngram are replaced by their POS tags, creating features such as w j POS k, POS j w k, POS j POS k for each lexical bigram and similarly for trigrams.	[121, 80, 122, 81, 83, 58, 82, 134, 77, 85]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09489824622869492, 0.057726968079805374, 0.07220828533172607, 0.08547757565975189, 0.14407698810100555, 0.058863814920186996, 0.07906340062618256, 0.2845514416694641, 0.11998466402292252, 0.07352570444345474]
Naive Bayes models (e.g., Mooney (1996), Chodorow et al (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks.	[109, 37, 31, 120, 34, 33, 93, 124, 39, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4803084433078766, 0.07260290533304214, 0.14118443429470062, 0.22506025433540344, 0.10864289104938507, 0.16750508546829224, 0.17835721373558044, 0.23590494692325592, 0.14588330686092377, 0.20275723934173584]
Named Entities predicted with the Maximum Entropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data.	[11, 64, 134, 38, 0, 9, 39, 123, 16, 60]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4295479655265808, 0.6594851016998291, 0.22917360067367554, 0.283653199672699, 0.6503846645355225, 0.3586645722389221, 0.22702892124652863, 0.35314643383026123, 0.14785142242908478, 0.11540436744689941]
Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003).	[113, 11, 64, 134, 0, 9, 123, 125, 16, 60]	[1, 0, 1, 0, 1, 0, 0, 0, 0, 0]	[0.6905579566955566, 0.4659969210624695, 0.5244479179382324, 0.2251802682876587, 0.5955309271812439, 0.3606518805027008, 0.35868385434150696, 0.17430488765239716, 0.13665136694908142, 0.11790724098682404]
Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages.	[18, 67, 15, 31, 20, 22, 9, 27, 10, 59]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6781180500984192, 0.21823137998580933, 0.29518112540245056, 0.27397090196609497, 0.23077239096164703, 0.11937511712312698, 0.06233559548854828, 0.15219104290008545, 0.057412289083004, 0.05739987641572952]
Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004).	[27, 19, 7, 148, 31, 115, 134, 37, 17, 158]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6749566197395325, 0.688993513584137, 0.38046616315841675, 0.4892672300338745, 0.2634221017360687, 0.05895766615867615, 0.12093851715326309, 0.11049314588308334, 0.1716107428073883, 0.086240753531456]
Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000).	[124, 16, 8, 62, 36, 24, 94, 17, 35, 33]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6741337180137634, 0.09508576989173889, 0.13754017651081085, 0.19531625509262085, 0.12590046226978302, 0.08838730305433273, 0.06072162464261055, 0.1106853261590004, 0.0931275486946106, 0.08366402983665466]
Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases.	[64, 3, 131, 85, 32, 68, 86, 144, 113, 135]	[1, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.8443558812141418, 0.24392221868038177, 0.5524998307228088, 0.3489132225513458, 0.3030272126197815, 0.5417633056640625, 0.2662595212459564, 0.14310692250728607, 0.18403220176696777, 0.08486920595169067]
Non-prepositional NP (Lappin and Leass, 1994).	[144, 57, 95, 97, 63, 157, 40, 131, 238, 61]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6631270051002502, 0.4345555901527405, 0.27316468954086304, 0.2727700173854828, 0.3358593285083771, 0.33952462673187256, 0.45856618881225586, 0.3145759701728821, 0.24134889245033264, 0.1863449513912201]
Nonetheless, the mentioned characteristics are useful indicators to distinguish literal and idiomatic expressions (Fazly and Stevenson, 2006).	[87, 53, 47, 18, 93, 2, 7, 177, 50, 175]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7364922165870667, 0.682784378528595, 0.20850175619125366, 0.41038089990615845, 0.28032922744750977, 0.1219298243522644, 0.2780957818031311, 0.33184367418289185, 0.11704071611166, 0.14582709968090057]
Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule.	[75, 48, 28, 52, 20, 46, 34, 25, 72, 51]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.776772677898407, 0.38408276438713074, 0.4518336355686188, 0.2400246411561966, 0.10947814583778381, 0.2615376114845276, 0.20057794451713562, 0.22710242867469788, 0.3689025640487671, 0.17622382938861847]
Notable examples of corpus construction projects for the biomedical domain are PennBioIE (Kulick et al, 2004) and GENIA (Kim et al, 2003).	[21, 80, 98, 74, 126, 6, 26, 69, 48, 81]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.8015777468681335, 0.2645370066165924, 0.18069995939731598, 0.2320578247308731, 0.1658862978219986, 0.16773027181625366, 0.1137230396270752, 0.774165689945221, 0.09404441714286804, 0.18178172409534454]
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002).	[53, 32, 21, 94, 137, 143, 25, 18, 90, 75]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6094775795936584, 0.15521788597106934, 0.05539413541555405, 0.06732521951198578, 0.059786830097436905, 0.0716191828250885, 0.08111658692359924, 0.05140778049826622, 0.08019379526376724, 0.05617092549800873]
Note that although Lin characterizes his work as detecting non-compositionality, we agree with Bannard et al (2003) that it is better thought of as tapping into productivity.plore whether the light verbs themselves show different patterns in terms of how they are used semi productively in these constructions.	[108, 122, 107, 111, 24, 179, 49, 123, 173, 211]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8356955051422119, 0.6451604962348938, 0.4560011327266693, 0.6611209511756897, 0.4977375566959381, 0.26668065786361694, 0.07843553274869919, 0.15531575679779053, 0.21795940399169922, 0.058130934834480286]
Note that the predicate language representation utilized by Carmel-Tools is in the style of Davidsonian event based semantics (Hobbs, 1985).	[512, 358, 57, 106, 32, 226, 291, 111, 44, 112]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3489122986793518, 0.2890280485153198, 0.17450083792209625, 0.20606030523777008, 0.24852395057678223, 0.179483100771904, 0.20903833210468292, 0.18481183052062988, 0.42246946692466736, 0.0911165103316307]
Note that the reranking may slightly improve the syntactic performance according to (Johansson and Nugues, 2008).	[167, 18, 23, 154, 137, 157, 163, 135, 32, 140]	[1, 0, 0, 1, 0, 0, 1, 0, 0, 1]	[0.7005801796913147, 0.19633165001869202, 0.34768250584602356, 0.5021489262580872, 0.41843608021736145, 0.3105193078517914, 0.7511960864067078, 0.07592257112264633, 0.08082830905914307, 0.6212083697319031]
Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1).	[46, 64, 74, 76, 92, 106, 100, 83, 73, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4245930314064026, 0.10073093324899673, 0.2912765145301819, 0.22557774186134338, 0.2717689275741577, 0.08007372915744781, 0.1894780844449997, 0.27830514311790466, 0.09376558661460876, 0.09935402870178223]
Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words.	[0, 74, 69, 14, 61, 103, 3, 68, 83, 5]	[1, 1, 0, 1, 0, 0, 0, 1, 0, 1]	[0.7747235894203186, 0.8354328274726868, 0.2641858458518982, 0.830162525177002, 0.09485366195440292, 0.30210307240486145, 0.2814919352531433, 0.7745373249053955, 0.4841415286064148, 0.5661893486976624]
On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007).	[124, 8, 1, 154, 180, 7, 11, 157, 83, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32273006439208984, 0.402346670627594, 0.29912611842155457, 0.19255731999874115, 0.10515804588794708, 0.15666817128658295, 0.10974537581205368, 0.0907132476568222, 0.20005948841571808, 0.07448703050613403]
On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.	[25, 55, 199, 205, 4, 224, 187, 217, 234, 212]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5310479998588562, 0.5177165865898132, 0.19984762370586395, 0.23250481486320496, 0.22226251661777496, 0.1017632931470871, 0.09075327217578888, 0.10976626724004745, 0.36505740880966187, 0.09220994263887405]
On the grammar-based side, Bourigault (1992) describes a system for extracting& quot; terminological noun phrases& quot; from French text.	[41, 3, 31, 4, 0, 67, 51, 1, 11, 74]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5023866891860962, 0.37700238823890686, 0.1153688058257103, 0.3895547688007355, 0.14457331597805023, 0.10589157044887543, 0.25431421399116516, 0.1213044747710228, 0.10770164430141449, 0.07282053679227829]
On the other hand, it is quite possible that the WMT-style rankings taken as the gold standard are of a disputable quality themselves, see Section 3.1 or the detailed report on inter annotator agreement and a long discussion on interpreting the rankings in Callison-Burch et al (2012).	[89, 86, 90, 96, 111, 71, 70, 393, 119, 112]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6839466094970703, 0.7471221685409546, 0.6926200985908508, 0.601919412612915, 0.37021440267562866, 0.43793773651123047, 0.3158487379550934, 0.16407838463783264, 0.2456485480070114, 0.29686984419822693]
On the |w| ≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009).	[162, 188, 144, 32, 114, 174, 18, 15, 23, 113]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18278728425502777, 0.21511489152908325, 0.13738082349300385, 0.390558660030365, 0.07689770311117172, 0.10067477822303772, 0.17467601597309113, 0.10928759723901749, 0.12256389111280441, 0.08793975412845612]
One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data.	[69, 95, 1, 84, 112, 108, 70, 96, 63, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7387114763259888, 0.43780723214149475, 0.42965468764305115, 0.16145555675029755, 0.11057838052511215, 0.1436716467142105, 0.179992213845253, 0.1079426184296608, 0.12765461206436157, 0.0945986658334732]
One aspect of VPCs that makes them difficult to extract (cited in ,e.g., Smadja (1993)) is that the verb and particle can be non-contiguous.	[176, 610, 98, 123, 409, 52, 563, 322, 165, 365]	[1, 0, 0, 1, 0, 0, 0, 0, 1, 0]	[0.6836240291595459, 0.49456873536109924, 0.056778356432914734, 0.5146814584732056, 0.05818699672818184, 0.08564676344394684, 0.09987392276525497, 0.10263968259096146, 0.5162140130996704, 0.062485385686159134]
One class of particularly useful features assesses the goodness of the alignment path through the source sentence (Vogel et al., 1996).	[120, 40, 35, 18, 134, 54, 130, 59, 3, 110]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.29095259308815, 0.10902749001979828, 0.2886908948421478, 0.059272926300764084, 0.2530169188976288, 0.18745408952236176, 0.12289028614759445, 0.11661607772111893, 0.09407365322113037, 0.5074658989906311]
One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning.	[13, 1, 3, 47, 106, 98, 87, 49, 96, 18]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6750206351280212, 0.44999727606773376, 0.25982725620269775, 0.4391248822212219, 0.08701472729444504, 0.18571293354034424, 0.10358253866434097, 0.18049509823322296, 0.17147621512413025, 0.3200511932373047]
One may note that the error reductions here are smaller than Palmer (1997)'s error reductions.	[109, 108, 150, 75, 134, 120, 83, 90, 124, 100]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3166802227497101, 0.4003390371799469, 0.23538163304328918, 0.3948034942150116, 0.45231062173843384, 0.23723836243152618, 0.34892040491104126, 0.3599046468734741, 0.21299946308135986, 0.370469331741333]
One of the first works in the area of comparable corpora mining was based on word co-occurrence based approach (Rapp, 1995).	[3, 56, 11, 5, 30, 14, 20, 50, 55, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3082800507545471, 0.28404971957206726, 0.06375233083963394, 0.05030449479818344, 0.15084116160869598, 0.0812670886516571, 0.10739944130182266, 0.1361548900604248, 0.09810516238212585, 0.12715619802474976]
One of the most popular methods leveraging bilingual parallel corpora is proposed by Bannard and Callison-Burch (2005).	[0, 31, 12, 36, 15, 2, 18, 14, 21, 117]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8009285926818848, 0.39157044887542725, 0.2037026286125183, 0.11345759779214859, 0.24242912232875824, 0.22907206416130066, 0.1435600221157074, 0.16878561675548553, 0.12785127758979797, 0.07061941176652908]
One of the most related models is the cascaded chunking model by (Kudo and Matsumoto, 2002).	[42, 109, 52, 41, 1, 44, 56, 119, 47, 108]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39994576573371887, 0.31463733315467834, 0.2595055401325226, 0.27539411187171936, 0.2702350318431854, 0.24912278354167938, 0.3671969175338745, 0.21798992156982422, 0.4494411051273346, 0.29474514722824097]
One of the reasons for this difference is due to the different language pairs under study; (Meyers et al, 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994).	[134, 347, 25, 34, 5, 43, 0, 121, 125, 338]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29371264576911926, 0.261851966381073, 0.14578932523727417, 0.20620408654212952, 0.29842647910118103, 0.37332841753959656, 0.24276798963546753, 0.13488395512104034, 0.1282951831817627, 0.3002305030822754]
One opossibility is the example-based combiner in Brill and Wu (1998, Sec. 3.2).	[82, 59, 32, 1, 29, 45, 26, 54, 49, 38]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3108169436454773, 0.441750168800354, 0.08964903652667999, 0.053594913333654404, 0.2602185010910034, 0.09091494232416153, 0.3977367877960205, 0.06205660104751587, 0.06390617787837982, 0.05355276167392731]
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	[14, 32, 13, 20, 39, 98, 46, 104, 44, 155]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.2113889753818512, 0.20104123651981354, 0.34224408864974976, 0.1990284025669098, 0.23027871549129486, 0.1811731904745102, 0.07750803977251053, 0.055445946753025055, 0.5301295518875122, 0.07400935888290405]
One set are the normal form constraints, as described by Eisner (1996).	[11, 0, 143, 123, 47, 28, 12, 36, 3, 149]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5896468162536621, 0.7760486602783203, 0.26300713419914246, 0.22606465220451355, 0.10043300688266754, 0.13415434956550598, 0.1747024953365326, 0.08734133839607239, 0.1542830616235733, 0.09879263490438461]
Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus.	[13, 14, 12, 11, 2, 146, 108, 125, 73, 118]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.8002256155014038, 0.6345165371894836, 0.49881425499916077, 0.21134185791015625, 0.28300732374191284, 0.2855883836746216, 0.4564969539642334, 0.3025296628475189, 0.7128602266311646, 0.10973060131072998]
Only one of the four is explicitly aimed at personal-pronoun anaphora RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994).	[26, 0, 276, 36, 81, 69, 70, 262, 348, 106]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8511136174201965, 0.8320298194885254, 0.10769148170948029, 0.24706418812274933, 0.2856723368167877, 0.23200729489326477, 0.23250071704387665, 0.24743159115314484, 0.1074453666806221, 0.24515517055988312]
Opinionfinder (Wilson et al, 2005a) is a system for mining opinions from text.	[8, 15, 3, 62, 9, 1, 11, 13, 65, 50]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22756510972976685, 0.1527816355228424, 0.15014246106147766, 0.24772761762142181, 0.0656355619430542, 0.23222333192825317, 0.1491989940404892, 0.10558848083019257, 0.11877031624317169, 0.13036099076271057]
Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008).	[0, 11, 42, 149, 172, 154, 171, 10, 6, 32]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7508787512779236, 0.35649049282073975, 0.0600641705095768, 0.07739150524139404, 0.09707587212324142, 0.06188364326953888, 0.13857634365558624, 0.052496619522571564, 0.13806022703647614, 0.07532862573862076]
Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora.	[1, 0, 163, 14, 50, 19, 146, 158, 29, 20]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5381759405136108, 0.7220339179039001, 0.3849562704563141, 0.47018757462501526, 0.369844526052475, 0.2910003066062927, 0.2939606308937073, 0.2022566944360733, 0.3141055107116699, 0.308270126581192]
Other formalisms aiming to model dependency correctly similarly expand weak generative capacity, notably D-tree Substitution Grammar (Rambow et al, 1995), and consequently end up with much greater parsing complexity.	[7, 3, 79, 270, 128, 75, 155, 69, 110, 0]	[0, 1, 0, 0, 0, 1, 0, 0, 0, 1]	[0.2109384536743164, 0.6771548986434937, 0.1925409734249115, 0.2688048183917999, 0.14428089559078217, 0.5827599763870239, 0.06930472701787949, 0.06504429131746292, 0.059792742133140564, 0.544894814491272]
Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words.	[325, 14, 175, 249, 5, 11, 265, 0, 32, 55]	[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.4535146653652191, 0.6380842924118042, 0.13546819984912872, 0.2227269560098648, 0.2825353145599365, 0.2825353145599365, 0.08014701306819916, 0.5188125371932983, 0.17679467797279358, 0.08878600597381592]
Other proposed methods include paraphrasing (Callison-Burch et al, 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity.	[37, 28, 36, 41, 60, 124, 65, 61, 75, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40574193000793457, 0.056022610515356064, 0.29522186517715454, 0.15475040674209595, 0.2584628760814667, 0.05283327400684357, 0.059035323560237885, 0.07697867602109909, 0.05580487102270126, 0.054489318281412125]
Other recent approaches use Gibbs sampler for learning the SCFG by exploring a fixed grammar having pre-defined rule templates (Blunsom et al, 2008) or by reasoning over the space of derivations (Blunsom et al, 2009).	[22, 48, 4, 111, 42, 26, 40, 32, 38, 89]	[1, 0, 0, 0, 1, 0, 1, 0, 1, 0]	[0.843423068523407, 0.35841143131256104, 0.42531928420066833, 0.19659662246704102, 0.7577191591262817, 0.4102342128753662, 0.8160807490348816, 0.2922542691230774, 0.5424006581306458, 0.18031471967697144]
Others have automatically extracted attribute relations from dictionary definitions (Richardson et al, 1998), structured online sources such as Wikipedia info boxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al, 2008).	[11, 20, 35, 1, 25, 24, 29, 14, 60, 43]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42854025959968567, 0.7568871974945068, 0.26231852173805237, 0.16189278662204742, 0.3162156939506531, 0.11437792330980301, 0.13659383356571198, 0.09625150263309479, 0.21085529029369354, 0.262857049703598]
Our ArEn training data comprises several LDCcorpora, using the same experimental setup as in Blunsom et al (2009a).	[44, 47, 48, 21, 52, 62, 24, 129, 50, 147]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.3824882209300995, 0.14047954976558685, 0.08037802577018738, 0.09446075558662415, 0.09235785901546478, 0.05681139603257179, 0.061923570930957794, 0.5728185772895813, 0.06594515591859818, 0.2627837359905243]
Our annotation guidelines are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al, 2004).	[37, 93, 91, 80, 39, 122, 32, 6, 26, 110]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7378820180892944, 0.15583273768424988, 0.39510050415992737, 0.41831520199775696, 0.5670010447502136, 0.24538621306419373, 0.292692095041275, 0.4029601812362671, 0.27506494522094727, 0.31840771436691284]
Our approach to memory-based all-words WSD follows the memory based approach of (Ng and Lee, 1996), and the work by (Veenstra et al, 2000) on a memory based approach to the English lexical sample task of SENSEVAL-1.	[10, 183, 1, 0, 182, 34, 157, 33, 48, 184]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3636666536331177, 0.3357207775115967, 0.3638809323310852, 0.35168999433517456, 0.1943572461605072, 0.23173226416110992, 0.23978303372859955, 0.21007663011550903, 0.11634238809347153, 0.18430402874946594]
Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).	[186, 42, 368, 332, 409, 8, 18, 53, 333, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10496969521045685, 0.11140727996826172, 0.23229491710662842, 0.20163454115390778, 0.06089065223932266, 0.11307574063539505, 0.10759993642568588, 0.10759993642568588, 0.4633587598800659, 0.1360684633255005]
Our error correction system implements a correction validation mechanism as proposed in (Gamon et al., 2008).	[13, 1, 165, 48, 181, 175, 168, 0, 44, 187]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13370215892791748, 0.14710833132266998, 0.07423556596040726, 0.07986261695623398, 0.2327662706375122, 0.07396277040243149, 0.10607188194990158, 0.3491570055484772, 0.453268438577652, 0.18452800810337067]
Our first pruning technique is broadly similar to Cherry and Lin (2007a).	[80, 154, 147, 106, 111, 165, 121, 2, 35, 71]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7344891428947449, 0.3922044634819031, 0.1999683380126953, 0.32950592041015625, 0.19479230046272278, 0.28102734684944153, 0.08883080631494522, 0.37915900349617004, 0.09791595488786697, 0.06847552955150604]
Our implementation of the NP-based QA system uses the Empire noun phrase finder, which is described in detail in Cardie and Pierce (1998).	[67, 7, 34, 60, 11, 31, 35, 6, 9, 57]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.27840667963027954, 0.429434597492218, 0.11263313889503479, 0.13405269384384155, 0.5409020185470581, 0.12666575610637665, 0.14653919637203217, 0.13646076619625092, 0.1526615470647812, 0.15859819948673248]
Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005).	[119, 7, 109, 111, 136, 103, 17, 148, 42, 115]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7246877551078796, 0.667691707611084, 0.25845327973365784, 0.22768135368824005, 0.16953684389591217, 0.21730177104473114, 0.2185223251581192, 0.13669681549072266, 0.4563698470592499, 0.13788053393363953]
Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006).	[87, 99, 86, 3, 102, 95, 103, 217, 208, 144]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.7951891422271729, 0.6872773766517639, 0.5622751712799072, 0.4100185036659241, 0.3767189085483551, 0.6432746648788452, 0.4621950387954712, 0.18044623732566833, 0.10388744622468948, 0.2739638686180115]
Our results show that grammatical relations based F-score (Riezler et al 2003) correlates reliably with human judgements and could thus be used to measure compression performance automatically.	[109, 166, 154, 26, 123, 29, 4, 149, 5, 132]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6605696082115173, 0.14521227777004242, 0.3055889904499054, 0.09424290806055069, 0.13858197629451752, 0.11610952764749527, 0.057366348803043365, 0.09672916680574417, 0.08763273060321808, 0.07664075493812561]
Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments.	[3, 33, 103, 22, 93, 7, 158, 87, 86, 36]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28983092308044434, 0.1779298335313797, 0.0593474879860878, 0.0732351616024971, 0.06983718276023865, 0.06768381595611572, 0.061896324157714844, 0.05440445989370346, 0.06564414501190186, 0.05070856586098671]
Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets.	[33, 243, 246, 36, 7, 242, 53, 9, 48, 42]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28700846433639526, 0.20763888955116272, 0.28412288427352905, 0.1928100734949112, 0.14114071428775787, 0.14945559203624725, 0.14433817565441132, 0.11444570124149323, 0.10820823907852173, 0.21074354648590088]
Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM.	[153, 19, 118, 2, 146, 5, 140, 157, 151, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36700114607810974, 0.09538163244724274, 0.08097853511571884, 0.14412625133991241, 0.24398039281368256, 0.3626200258731842, 0.43137800693511963, 0.12210892885923386, 0.19006425142288208, 0.09450515359640121]
Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars.	[0, 21, 434, 431, 404, 17, 1, 6, 46, 414]	[1, 1, 1, 0, 1, 1, 0, 0, 1, 0]	[0.8275572657585144, 0.8364658355712891, 0.6044512987136841, 0.3702511191368103, 0.5405935645103455, 0.5221737623214722, 0.2798707187175751, 0.2798707187175751, 0.6475295424461365, 0.2980816662311554]
Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text).	[153, 10, 20, 120, 163, 98, 185, 157, 96, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24533028900623322, 0.3784347176551819, 0.1731855720281601, 0.059838876128196716, 0.11088158935308456, 0.37900641560554504, 0.21173879504203796, 0.1174023225903511, 0.05358695238828659, 0.07354868948459625]
Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers: the Mate parser (Bohnet, 2010), the Easy-First parser (Goldberg and Elhadad, 2010) and the Turbo parser (Martins et al, 2013) Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency.	[24, 21, 220, 252, 16, 17, 235, 25, 13, 167]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23347267508506775, 0.13835257291793823, 0.2672037184238434, 0.12013974785804749, 0.12194621562957764, 0.19018436968326569, 0.1077331155538559, 0.13012194633483887, 0.05386144295334816, 0.10416260361671448]
Our system was tested on the News test set (Callison-Burch et al, 2010) released by the organizers of the 2010 Workshop on Statistical Machine Translation.	[0, 5, 14, 15, 9, 17, 19, 26, 28, 102]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8485943675041199, 0.37379974126815796, 0.16188262403011322, 0.13577225804328918, 0.24778088927268982, 0.2124948501586914, 0.1253940463066101, 0.14167042076587677, 0.11057177186012268, 0.19906830787658691]
Our systems consistently perform better when a mode exists, which makes sense because those are instances in which the annotators are in agreement (McCarthy and Navigli, 2007).	[58, 63, 109, 40, 118, 68, 54, 11, 111, 41]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 1]	[0.7314922213554382, 0.7860456705093384, 0.4841148257255554, 0.28785672783851624, 0.1527976095676422, 0.1719786375761032, 0.2171652764081955, 0.1021176278591156, 0.6000913381576538, 0.6873244643211365]
Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010).	[6, 90, 86, 1, 23, 77, 76, 7, 14, 49]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8241788148880005, 0.5442352294921875, 0.16059201955795288, 0.16706721484661102, 0.13879399001598358, 0.18637964129447937, 0.07955265045166016, 0.29791581630706787, 0.27656224370002747, 0.19860266149044037]
Our work differs from corpus-based work such as Manning (1993) or Kawahara and Kurohashi (2001) in that we are using existing lexical resources rather than a corpus.	[102, 14, 104, 113, 129, 8, 85, 44, 90, 24]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20518897473812103, 0.1446385383605957, 0.1575840711593628, 0.18382209539413452, 0.17792829871177673, 0.2528228163719177, 0.056719984859228134, 0.1281619369983673, 0.08316398411989212, 0.16884025931358337]
Our work extends recent work by Zhu et al (2010) that also examines Wikipedia/Simple English Wikipedia as a data-driven, sentence simplification task.	[34, 53, 3, 35, 56, 10, 244, 180, 29, 312]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8367794752120972, 0.7121940851211548, 0.2057395577430725, 0.16256192326545715, 0.3061542510986328, 0.11949517577886581, 0.15364576876163483, 0.07638750970363617, 0.13701534271240234, 0.22900111973285675]
Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology.	[28, 174, 30, 18, 190, 212, 223, 80, 215, 218]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.6411479711532593, 0.7955844402313232, 0.4350278079509735, 0.3777724802494049, 0.3308901786804199, 0.4299033284187317, 0.6900633573532104, 0.45551779866218567, 0.27733227610588074, 0.10957718640565872]
Our work is also related to that of Riezler et al (2007) where SMT-based query expansion methods are used on data from FAQ pages.	[40, 15, 13, 134, 11, 63, 19, 47, 127, 131]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.4227638840675354, 0.4538973569869995, 0.26490074396133423, 0.46623966097831726, 0.32447919249534607, 0.3760862946510315, 0.5636836290359497, 0.262626975774765, 0.13694576919078827, 0.09185590595006943]
Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006).	[92, 31, 170, 0, 20, 174, 169, 2, 8, 171]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.29910072684288025, 0.570580005645752, 0.43414315581321716, 0.5116074085235596, 0.3739962875843048, 0.21708329021930695, 0.2575012147426605, 0.260475754737854, 0.10517837107181549, 0.2525170147418976]
Our work is partly based on the work done with the Constraint Grammar framework that was originally proposed by Fred Karlsson (1990).	[242, 0, 206, 231, 66, 117, 220, 162, 13, 211]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.8490819931030273, 0.3768453001976013, 0.35143089294433594, 0.45646196603775024, 0.10396093875169754, 0.4544030725955963, 0.30695614218711853, 0.15474677085876465, 0.43426579236984253, 0.6990131735801697]
Over the last several of years, Mechanical Turk, introduced by Amazon as artificial artificial intelligence, has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b).	[121, 38, 4, 5, 150, 27, 118, 11, 125, 198]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6119438409805298, 0.24508824944496155, 0.3507271707057953, 0.2025955617427826, 0.44665491580963135, 0.21001240611076355, 0.10983844101428986, 0.09412102401256561, 0.13342659175395966, 0.26350751519203186]
Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al, 2002), to parse wide-coverage text and assign it deep semantic representations.	[21, 186, 26, 184, 136, 2, 17, 25, 180, 36]	[1, 0, 1, 0, 0, 0, 1, 0, 0, 0]	[0.8206857442855835, 0.22963236272335052, 0.7922888994216919, 0.3501937985420227, 0.31986767053604126, 0.30290117859840393, 0.5693531632423401, 0.1546773612499237, 0.1410924196243286, 0.134840190410614]
Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and up to-date review of the main approaches.	[117, 597, 527, 600, 438, 34, 476, 35, 106, 78]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2950288653373718, 0.5119993686676025, 0.2046755701303482, 0.09740521758794785, 0.2692407965660095, 0.07089865207672119, 0.13063538074493408, 0.07549320161342621, 0.15750829875469208, 0.18333573639392853]
Over the years, several proposals of generic NLG system shave been made: Penman (Matthiessen and Bate man, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al, 2002), etc.	[35, 34, 9, 23, 7, 11, 28, 171, 80, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17114944756031036, 0.14330068230628967, 0.13338594138622284, 0.16282139718532562, 0.11312558501958847, 0.16525641083717346, 0.1782829463481903, 0.11791852861642838, 0.11011146754026413, 0.08345326781272888]
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	[23, 173, 183, 182, 21, 142, 70, 156, 59, 51]	[1, 0, 0, 1, 1, 0, 0, 0, 0, 1]	[0.8464344143867493, 0.3377986550331116, 0.3943212330341339, 0.8311792612075806, 0.5823135375976562, 0.078850157558918, 0.3616042733192444, 0.08900301158428192, 0.06570681184530258, 0.7894364595413208]
P(ht|ps,pos) is estimated from the frequency of ps, and its alignment with ht, in a version of CELEX in which the graphemic and phonemic representation of each word is many-many aligned using the method of Jiampojamarn et al (2007).	[153, 37, 29, 87, 97, 160, 26, 38, 9, 58]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2230251133441925, 0.5512853264808655, 0.1049787849187851, 0.2864428758621216, 0.06981033086776733, 0.24137042462825775, 0.06698139011859894, 0.12214345484972, 0.08823160082101822, 0.11617908626794815]
PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997).	[153, 18, 128, 261, 64, 90, 6, 13, 368, 154]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3805302381515503, 0.21865880489349365, 0.19318988919258118, 0.09580935537815094, 0.07570715993642807, 0.14760197699069977, 0.06411075592041016, 0.06411075592041016, 0.07283985614776611, 0.0945710614323616]
POS Majority lexical type noun count-noun-le c-n-f verb trans-nerg-str-verb-le haben-auxf adj adj-non-prd-le adv intersect-adv-le Table 5: POS tags to lexical types mapping Again for comparison, we have built another simple baseline model using the TnT POS tagger (Brants, 2000).	[40, 18, 4, 141, 14, 133, 1, 27, 20, 17]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21319591999053955, 0.5167322754859924, 0.13015396893024445, 0.3622888922691345, 0.12321972846984863, 0.1202412098646164, 0.3213995099067688, 0.12938913702964783, 0.05873516574501991, 0.12178037315607071]
Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews.	[35, 154, 36, 137, 42, 22, 49, 71, 106, 40]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.614797830581665, 0.6425786018371582, 0.34254321455955505, 0.2732682228088379, 0.26134562492370605, 0.1251429170370102, 0.28413382172584534, 0.6366610527038574, 0.14517946541309357, 0.09272164106369019]
Pang et al (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences.	[3, 20, 83, 15, 96, 95, 0, 13, 14, 26]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.4063881039619446, 0.3146034777164459, 0.14631888270378113, 0.17321835458278656, 0.07800598442554474, 0.058221954852342606, 0.27509045600891113, 0.1398346722126007, 0.16608020663261414, 0.5240904092788696]
Pang et al (Pang et al, 2003) used parallel monolingual corpora built from news stories that had been independently translated several times to learn lattices from a syntax-based alignment process.	[33, 20, 1, 96, 179, 32, 0, 36, 180, 26]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7308130860328674, 0.35500285029411316, 0.12086870521306992, 0.10117249935865402, 0.10000748932361603, 0.09695226699113846, 0.3026857376098633, 0.1638450026512146, 0.09481719136238098, 0.13938255608081818]
Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on.	[237, 152, 397, 28, 27, 206, 33, 225, 71, 376]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6247929930686951, 0.11265102028846741, 0.13118521869182587, 0.10002851486206055, 0.1031942218542099, 0.06250933557748795, 0.14621776342391968, 0.05728282034397125, 0.344812273979187, 0.06674506515264511]
Paraphrase here includes sentences generated in an Information Fusion task (Barzilay et al, 1999).	[15, 55, 0, 7, 37, 60, 85, 14, 12, 84]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.8175203204154968, 0.6048743724822998, 0.48499542474746704, 0.05549952760338783, 0.14311707019805908, 0.3907468318939209, 0.147363543510437, 0.10208676755428314, 0.050555985420942307, 0.6156781315803528]
Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in (1) Lappin and Leass (1994) and (2) Kennedy and Boguraev (1996).	[10, 4, 158, 136, 16, 27, 119, 46, 53, 92]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7957379817962646, 0.11468175798654556, 0.22348947823047638, 0.2083379626274109, 0.20520053803920746, 0.28847283124923706, 0.07108332216739655, 0.6299750208854675, 0.3331535756587982, 0.21701128780841827]
Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000).	[12, 45, 2, 29, 43, 28, 42, 18, 33, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16277262568473816, 0.39523446559906006, 0.13975518941879272, 0.11197724938392639, 0.047303687781095505, 0.06975145637989044, 0.05298502743244171, 0.42984065413475037, 0.365795373916626, 0.0674692764878273]
Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors.	[79, 92, 36, 23, 35, 196, 210, 58, 149, 84]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.8487731218338013, 0.847162127494812, 0.34670472145080566, 0.1568048894405365, 0.40785640478134155, 0.3711127042770386, 0.2727857530117035, 0.5960353016853333, 0.2659696936607361, 0.4191916286945343]
Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size.	[13, 67, 151, 131, 126, 1, 66, 35, 41, 36]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6048833131790161, 0.5293763279914856, 0.4732955992221832, 0.19414888322353363, 0.15983957052230835, 0.22149887681007385, 0.17917877435684204, 0.09130953997373581, 0.14783403277397156, 0.16227014362812042]
Pereira and Warren (1983) and Shieber (1985) present versions of Earley's algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation.	[12, 1, 141, 89, 10, 168, 39, 148, 68, 69]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3029821813106537, 0.16252319514751434, 0.21490013599395752, 0.23186621069908142, 0.3512282073497772, 0.14272841811180115, 0.4039287865161896, 0.16168761253356934, 0.08374117314815521, 0.2388380914926529]
Perspective GATE (Cunningham et al, 2002a) is an architecture, a framework and a development environment for human language technology modules and applications.	[0, 12, 147, 13, 49, 148, 221, 83, 57, 199]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8467667102813721, 0.4040692150592804, 0.31763529777526855, 0.29719725251197815, 0.2618584632873535, 0.29719725251197815, 0.15373200178146362, 0.1619105190038681, 0.22696101665496826, 0.24157997965812683]
Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009).	[62, 2, 26, 49, 19, 48, 0, 102, 148, 32]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.334391325712204, 0.48929691314697266, 0.1167808324098587, 0.1839136779308319, 0.26980262994766235, 0.1796134114265442, 0.3324349522590637, 0.05623272433876991, 0.21962907910346985, 0.21857240796089172]
Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data.	[48, 0, 4, 55, 57, 25, 234, 18, 63, 28]	[1, 1, 0, 1, 0, 1, 0, 0, 1, 0]	[0.8181113004684448, 0.8386531472206116, 0.30040302872657776, 0.5810056924819946, 0.43329140543937683, 0.5119840502738953, 0.3346726894378662, 0.40741029381752014, 0.5936288833618164, 0.1997910588979721]
Preemptive IE (Shinyama and Sekine, 2006) is a paradigm related to Open IE that first groups documents based on pairwise vector clustering, then applies additional clustering to group entities based on document clusters.	[81, 67, 46, 173, 180, 82, 122, 119, 68, 145]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33906397223472595, 0.5707626342773438, 0.3486936390399933, 0.10763858258724213, 0.18964695930480957, 0.25052350759506226, 0.06760798394680023, 0.1585436463356018, 0.0744481012225151, 0.0847010388970375]
Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution.	[102, 80, 88, 119, 135, 45, 86, 138, 116, 72]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17869946360588074, 0.3013969957828522, 0.16329574584960938, 0.2778744399547577, 0.1389368176460266, 0.15879929065704346, 0.19586698710918427, 0.058637890964746475, 0.0720202699303627, 0.09329049289226532]
Previous algorithms for compiling rewrite rules into transducers have followed Kaplan and Kay (1994) by introducing special marker symbols (markers) into strings in order to mark off candidate regions for replacement.	[433, 388, 316, 497, 478, 484, 439, 412, 480, 280]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5374864339828491, 0.3487836420536041, 0.659297525882721, 0.38375577330589294, 0.35011038184165955, 0.29770416021347046, 0.29981693625450134, 0.1608356088399887, 0.38873928785324097, 0.10576082766056061]
Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008).	[11, 44, 2, 20, 14, 34, 35, 33, 10, 49]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.661284327507019, 0.14765408635139465, 0.09602472186088562, 0.20615644752979279, 0.27931898832321167, 0.15173473954200745, 0.15380606055259705, 0.08617622405290604, 0.10162240266799927, 0.07340589910745621]
Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al, 1997).	[180, 198, 95, 22, 184, 100, 0, 134, 24, 67]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7705765962600708, 0.4224308431148529, 0.0715254470705986, 0.11975158005952835, 0.23771753907203674, 0.15682466328144073, 0.4493340849876404, 0.3936927020549774, 0.09771386533975601, 0.05697312951087952]
Previous work on this approach, largely based on Karlsson's original proposal [Karlsson, 1990], is documented in [Karlsson et ai., forthcoming].	[241, 0, 206, 228, 66, 231, 2, 117, 47, 158]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 1]	[0.749373733997345, 0.32425451278686523, 0.3595691919326782, 0.2789889872074127, 0.08458584547042847, 0.34900757670402527, 0.5111337304115295, 0.3401736319065094, 0.14273008704185486, 0.5627519488334656]
Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al (1993) and Rooth et al (1999).	[6, 99, 82, 45, 25, 81, 83, 86, 30, 98]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18385687470436096, 0.7142050266265869, 0.13207218050956726, 0.2097122222185135, 0.14065618813037872, 0.08107005804777145, 0.05726812407374382, 0.0937231183052063, 0.25387898087501526, 0.06141628697514534]
PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data.	[45, 44, 13, 21, 1, 91, 14, 162, 32, 90]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5951554179191589, 0.26430657505989075, 0.2873998284339905, 0.43827155232429504, 0.14478541910648346, 0.33978086709976196, 0.3443343937397003, 0.206186443567276, 0.05847757309675217, 0.5576082468032837]
Proposals for annotating coreference such as (Hirschman, 1998) have been motivated by work on Information Extraction, hence the notion of coreference used is very difficult to relate to traditional ideas about anaphora (van Deemter and Kibble, 2000).	[71, 122, 135, 9, 11, 39, 61, 48, 4, 8]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7876168489456177, 0.287291020154953, 0.21577411890029907, 0.11381996423006058, 0.15769219398498535, 0.15415555238723755, 0.15080304443836212, 0.23371531069278717, 0.1548837423324585, 0.1548837423324585]
Punyakanok et al (2004) formulated an Integer Linear Programming (ILP) model for SRL.	[0, 148, 152, 23, 107, 151, 234, 2, 231, 156]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8099056482315063, 0.832485556602478, 0.12611515820026398, 0.18404872715473175, 0.11169912666082382, 0.22876223921775818, 0.15541133284568787, 0.11540452390909195, 0.08908965438604355, 0.14902819693088531]
Pustejovsky (1991) refers to this kind of relation as co specification i.e. like verb can select for its argument type, an argument also can select its associated predicates.	[238, 272, 187, 333, 303, 208, 305, 234, 276, 390]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8497481346130371, 0.670880913734436, 0.5428673028945923, 0.2934960126876831, 0.44055333733558655, 0.4568074941635132, 0.18082648515701294, 0.16682551801204681, 0.2011580914258957, 0.11451762914657593]
Qualia structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in natural language processing (NLP).	[16, 6, 74, 306, 25, 429, 295, 14, 372, 305]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35808518528938293, 0.1963101625442505, 0.0715787336230278, 0.2514874339103699, 0.2052992731332779, 0.15747958421707153, 0.09081409871578217, 0.14925962686538696, 0.09619464725255966, 0.11098648607730865]
R5 Superset of rules from (Xu et al., 2009).	[26, 213, 130, 68, 188, 208, 152, 180, 170, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19191238284111023, 0.35203129053115845, 0.17246130108833313, 0.11889202147722244, 0.16793952882289886, 0.06200358644127846, 0.049649398773908615, 0.2105647176504135, 0.08353593200445175, 0.07760851085186005]
RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers.	[80, 84, 141, 153, 2, 94, 21, 139, 113, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1959879994392395, 0.320582777261734, 0.11881691217422485, 0.21827004849910736, 0.10718279331922531, 0.09528353065252304, 0.0749884620308876, 0.10502324253320694, 0.13402025401592255, 0.13104867935180664]
ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a).	[178, 3, 112, 171, 149, 81, 145, 156, 179, 135]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3477044105529785, 0.1779775619506836, 0.17156699299812317, 0.24674732983112335, 0.1290755271911621, 0.30637219548225403, 0.38038578629493713, 0.27386751770973206, 0.1768886148929596, 0.19236557185649872]
ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004).	[175, 26, 161, 74, 21, 44, 146, 115, 63, 20]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5371958613395691, 0.2163301408290863, 0.47575870156288147, 0.1247420608997345, 0.12353704124689102, 0.21442653238773346, 0.32118111848831177, 0.17727886140346527, 0.18311862647533417, 0.12306736409664154]
RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures.	[115, 123, 9, 73, 113, 106, 13, 1, 53, 101]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1039867252111435, 0.13972856104373932, 0.10975266993045807, 0.3467996120452881, 0.16796118021011353, 0.05274831876158714, 0.06701341271400452, 0.18016095459461212, 0.07480938732624054, 0.1304699182510376]
Ratnaparkhi et al (1994) created a benchmark dataset of 27,937 quadruples (v, n1, p, n2), extracted from the Wall Street Journal.	[40, 9, 81, 140, 96, 69, 80, 30, 89, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6659188866615295, 0.07540728151798248, 0.07087719440460205, 0.21090485155582428, 0.26347607374191284, 0.30049413442611694, 0.053330790251493454, 0.0805448666214943, 0.0531049445271492, 0.19829049706459045]
Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data.	[50, 161, 124, 19, 128, 162, 147, 169, 83, 45]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6490944027900696, 0.4908589720726013, 0.6222717761993408, 0.22000625729560852, 0.17122292518615723, 0.11044982820749283, 0.10103196650743484, 0.3562808930873871, 0.208507239818573, 0.1134164109826088]
Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web.	[2, 0, 10, 15, 4, 1, 12, 77, 81, 110]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7896166443824768, 0.5488153696060181, 0.2544807195663452, 0.1687435656785965, 0.1900896281003952, 0.24965043365955353, 0.36964166164398193, 0.11820987612009048, 0.1330852508544922, 0.254556804895401]
Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002).	[188, 32, 28, 154, 2, 144, 193, 200, 186, 53]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.807159423828125, 0.17571935057640076, 0.46045297384262085, 0.09716155380010605, 0.20532888174057007, 0.1787603199481964, 0.15105092525482178, 0.14937932789325714, 0.15809479355812073, 0.0947856605052948]
Ravin and Kazi (1999) further refined the method of solving co-reference through measuring context similarity and integrated it into Nominator (Wacholder et al, 1997), which was one of the first successful systems for named entity recognition and co-reference resolution.	[63, 165, 34, 60, 123, 2, 65, 8, 90, 160]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14473524689674377, 0.11950945854187012, 0.10503332316875458, 0.10380712151527405, 0.09476503729820251, 0.11753684282302856, 0.0954972431063652, 0.05810856446623802, 0.2068440020084381, 0.08659639954566956]
Re-ordering effects across languages have been modeled in several ways, including word-based (Brown et al, 1993), template-based (Och et al, 1999) and syntax-based (Yamada, Knight, 2001).	[19, 219, 23, 17, 22, 183, 223, 217, 156, 109]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1872934103012085, 0.1872934103012085, 0.2630114257335663, 0.11528608202934265, 0.13068373501300812, 0.15435613691806793, 0.2630114257335663, 0.11528608202934265, 0.09124403446912766, 0.0666625052690506]
Readers can refer to M. Negri et al 2012.s., for more detailed introduction.	[73, 16, 12, 63, 18, 17, 81, 31, 69, 30]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41449639201164246, 0.05934055149555206, 0.10599793493747711, 0.1111840158700943, 0.0635293573141098, 0.08265943825244904, 0.10457305610179901, 0.1647646576166153, 0.06506876647472382, 0.06825115531682968]
Recent abstractive approaches, such as sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009) and sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009) have focused on rewriting techniques, without consideration for a complete model which would include a transition to an abstract representation for content selection.	[403, 100, 103, 39, 190, 33, 114, 34, 4, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13432317972183228, 0.29493534564971924, 0.1477341204881668, 0.41802263259887695, 0.2969021797180176, 0.12109846621751785, 0.2446686029434204, 0.10008756071329117, 0.09996435791254044, 0.09996435791254044]
Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al, 2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and n-best alignments instead of 1-best alignments (Venugopal et al, 2008).	[135, 138, 32, 38, 117, 118, 76, 119, 79, 28]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6960542798042297, 0.17550420761108398, 0.12168963998556137, 0.07693802565336227, 0.26229798793792725, 0.3271689713001251, 0.08660875260829926, 0.19226118922233582, 0.41550472378730774, 0.20989689230918884]
Recent work has formalised NLG algorithms for referring expression generation in terms of algorithms for finding an appropriate subgraph of a graph representing the domain knowledge [Krahmer et al, 2003].	[72, 301, 4, 8, 151, 19, 165, 23, 15, 87]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6150563955307007, 0.6302085518836975, 0.26522862911224365, 0.26522862911224365, 0.15663883090019226, 0.06145240366458893, 0.25362464785575867, 0.07584848999977112, 0.3225576877593994, 0.15622492134571075]
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	[6, 225, 228, 240, 23, 301, 88, 130, 266, 150]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21479374170303345, 0.18810299038887024, 0.49509286880493164, 0.12885458767414093, 0.13312079012393951, 0.22780285775661469, 0.12898752093315125, 0.12756547331809998, 0.09076379984617233, 0.17434290051460266]
Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008).	[4, 35, 20, 7, 2, 11, 34, 160, 154, 9]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7985919117927551, 0.8165897130966187, 0.26940253376960754, 0.7841591238975525, 0.4037908613681793, 0.3227033317089081, 0.31010693311691284, 0.22704170644283295, 0.2776622474193573, 0.06708115339279175]
Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al, 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al, 2004).	[14, 12, 28, 10, 13, 0, 1, 48, 59, 116]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.14529699087142944, 0.10731085389852524, 0.1602262705564499, 0.1326964646577835, 0.08833378553390503, 0.6153443455696106, 0.12907740473747253, 0.055380865931510925, 0.05173971876502037, 0.16025155782699585]
Recently, since the release of full-text annotations in SemEval 07 (Baker et al, 2007), there has been work on identifying multiple frames and their corresponding sets of arguments in a sentence.	[27, 25, 51, 29, 26, 52, 5, 10, 0, 44]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.4908563196659088, 0.18132063746452332, 0.2312108427286148, 0.0833144262433052, 0.17594510316848755, 0.1778143346309662, 0.11254692077636719, 0.1492423564195633, 0.6782109141349792, 0.14227046072483063]
Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004).	[0, 1, 3, 13, 4, 6, 11, 2, 17, 10]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8329587578773499, 0.4083806872367859, 0.17120856046676636, 0.13727085292339325, 0.23240844905376434, 0.05561703443527222, 0.12567609548568726, 0.07785043865442276, 0.08217457681894302, 0.05563851445913315]
Recognizing textual entailment is to determine whether a sentence (sometimes a short paragraph) can entail the other sentence (Giampiccolo et al, 2007).	[0, 77, 5, 41, 14, 9, 10, 31, 27, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.836637020111084, 0.4054401218891144, 0.2137424647808075, 0.3799961507320404, 0.14309197664260864, 0.17798961699008942, 0.16312628984451294, 0.09360650926828384, 0.0786048173904419, 0.11723286658525467]
Refer to (Roth and Yih, 2004) for more statistics on this data and a list of all the type constraints used.	[2, 8, 6, 215, 210, 148, 22, 100, 63, 44]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3298337161540985, 0.7957673072814941, 0.23832367360591888, 0.23912543058395386, 0.37996670603752136, 0.26547083258628845, 0.14051967859268188, 0.12589387595653534, 0.14957591891288757, 0.06306816637516022]
Regarding (i), Schone and Jurafsky (2001) compare the semantic vector of a phrase p and the vectors of its component words in two ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not.	[152, 129, 184, 157, 137, 14, 9, 48, 151, 127]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6451789736747742, 0.21043147146701813, 0.16101029515266418, 0.2901681363582611, 0.314028263092041, 0.06982055306434631, 0.1254575103521347, 0.053158435970544815, 0.1173626109957695, 0.4886167645454407]
Regular feature grammars can also be compiled into generators using a version of the Semantic Head Driven algorithm (Shieber et al, 1990).	[0, 23, 3, 6, 238, 198, 325, 93, 2, 5]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8347294926643372, 0.19923333823680878, 0.2034495621919632, 0.2034495621919632, 0.10262268036603928, 0.32012882828712463, 0.060708995908498764, 0.058917250484228134, 0.090767040848732, 0.090767040848732]
Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning.	[0, 1, 21, 174, 43, 27, 10, 5, 56, 142]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7260657548904419, 0.5814599394798279, 0.5861994028091431, 0.4940005838871002, 0.13630276918411255, 0.28970256447792053, 0.1379442662000656, 0.12560439109802246, 0.2572426199913025, 0.09834827482700348]
Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007).	[36, 192, 102, 31, 37, 151, 191, 185, 157, 150]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08742944896221161, 0.07470979541540146, 0.20466524362564087, 0.30294767022132874, 0.06972713023424149, 0.07993090897798538, 0.052524227648973465, 0.057245150208473206, 0.34152573347091675, 0.11108702421188354]
Researchers employ the existing SMT models for PG (Quirk et al, 2004).	[17, 12, 8, 1, 102, 95, 80, 7, 155, 162]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4110908508300781, 0.11535748094320297, 0.5027436017990112, 0.0781179741024971, 0.07523944973945618, 0.05491659790277481, 0.24196681380271912, 0.3895476460456848, 0.19714176654815674, 0.06506884098052979]
Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al, 2006), sentiment classification of phrases (Wilson et al, 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a).	[120, 58, 143, 79, 14, 69, 13, 15, 138, 63]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10331672430038452, 0.07074154913425446, 0.09172453731298447, 0.059023257344961166, 0.07618244737386703, 0.06580482423305511, 0.05698345974087715, 0.053519368171691895, 0.06792425364255905, 0.12006931006908417]
Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest.	[122, 67, 17, 7, 112, 59, 106, 121, 63, 64]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4569195508956909, 0.7316709756851196, 0.32726338505744934, 0.111750528216362, 0.325701504945755, 0.13498526811599731, 0.13806721568107605, 0.23511505126953125, 0.21780350804328918, 0.32330140471458435]
Results presented in Harabagiu et al (2001) are higher than those reported here, but assume that all and only the noun phrases involved in coreference relationships are provided for analysis by the coreference resolution system.	[0, 3, 61, 71, 31, 30, 22, 18, 36, 35]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7447959184646606, 0.053498174995183945, 0.053498174995183945, 0.16077342629432678, 0.190840482711792, 0.14191432297229767, 0.12672781944274902, 0.1323995292186737, 0.07366009801626205, 0.0671662837266922]
Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000).	[59, 67, 10, 44, 65, 155, 42, 15, 49, 60]	[1, 0, 0, 0, 1, 0, 0, 1, 0, 0]	[0.5345540642738342, 0.40420591831207275, 0.05727493017911911, 0.1738092303276062, 0.5495756268501282, 0.08957552909851074, 0.08633093535900116, 0.7187081575393677, 0.10237227380275726, 0.07962886244058609]
Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al (2009) recently introduced alternative LP and ILP formulations.	[16, 57, 23, 17, 13, 53, 15, 201, 84, 78]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.689911425113678, 0.5883931517601013, 0.2610146105289459, 0.10347681492567062, 0.19226355850696564, 0.13508562743663788, 0.10704313218593597, 0.3053950369358063, 0.4062967002391815, 0.10110107809305191]
Riezler et al (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) tree bank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f-structure.	[46, 64, 84, 24, 107, 111, 36, 23, 113, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32482409477233887, 0.37628254294395447, 0.09502087533473969, 0.19715063273906708, 0.3171224296092987, 0.11427333205938339, 0.06868518888950348, 0.09919194132089615, 0.10934142768383026, 0.09745930880308151]
Rooth et al (1999) referring to a direct object noun for describing verbs, or used any syntactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003).	[99, 83, 40, 143, 85, 7, 103, 13, 88, 108]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6407532095909119, 0.07396306842565536, 0.1722966432571411, 0.059210505336523056, 0.39123445749282837, 0.0823887288570404, 0.17957629263401031, 0.07695089280605316, 0.07958953827619553, 0.08853522688150406]
Rule based POS tagging methods extract rules from training corpus and use these rules to tag new sentences (Brill, 1992) (Brill, 1994).	[22, 102, 3, 5, 2, 8, 92, 90, 107, 39]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2585272789001465, 0.3805076479911804, 0.18268054723739624, 0.21295392513275146, 0.18779346346855164, 0.27760064601898193, 0.19234295189380646, 0.2985115349292755, 0.17123450338840485, 0.2527247369289398]
S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated.	[0, 27, 29, 4, 2, 12, 1, 23, 10, 26]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8439099192619324, 0.7268970608711243, 0.2024647742509842, 0.21245016157627106, 0.13589435815811157, 0.10121137648820877, 0.08090011030435562, 0.05785753205418587, 0.2134576439857483, 0.06264512240886688]
SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar.	[12, 10, 33, 9, 32, 11, 56, 22, 8, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.44559967517852783, 0.3095243275165558, 0.1284981071949005, 0.2278916984796524, 0.26256653666496277, 0.14006534218788147, 0.12353398650884628, 0.3088427782058716, 0.09395747631788254, 0.13236770033836365]
SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al, 2006b; McClosky et al, 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set.	[31, 32, 14, 8, 72, 102, 16, 139, 67, 18]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3170279264450073, 0.4657972455024719, 0.16422748565673828, 0.4889415502548218, 0.09675368666648865, 0.1100180372595787, 0.07082017511129379, 0.13497281074523926, 0.07208974659442902, 0.3208048939704895]
Same as Dyer et al, (2008), we also extracted rules from a combined bilingual corpus which contains three copies from different segmenters.	[81, 30, 103, 92, 76, 106, 33, 108, 107, 124]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3352114260196686, 0.06060895323753357, 0.08672923594713211, 0.15198849141597748, 0.05705093592405319, 0.2850453555583954, 0.05894250050187111, 0.16591671109199524, 0.07987749576568604, 0.062267057597637177]
Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al, 2003).	[26, 28, 29, 36, 20, 40, 57, 148, 3, 116]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7583498358726501, 0.5293774604797363, 0.15846121311187744, 0.13424985110759735, 0.10155010968446732, 0.66329425573349, 0.15077213943004608, 0.12140908092260361, 0.10836674273014069, 0.04910006746649742]
Schafer and Yarowsky (2002) created lexicons between English and a target local language (e.g. Gujarati) using a related language (e.g. Hindi) as pivot.	[25, 27, 35, 23, 21, 0, 24, 30, 15, 31]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5241387486457825, 0.3247228264808655, 0.2723153829574585, 0.3502656817436218, 0.08064087480306625, 0.3476758599281311, 0.19962364435195923, 0.22379764914512634, 0.1444888412952423, 0.532243549823761]
Schafer and Yarowsky (2002) induced translation lexicons for languages without common parallel corpora using a bridge language that is related to the target languages.	[0, 15, 25, 24, 35, 23, 17, 12, 29, 28]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8048961162567139, 0.7812195420265198, 0.25451505184173584, 0.19529831409454346, 0.09984806925058365, 0.1362726390361786, 0.16000446677207947, 0.1284629851579666, 0.12050949037075043, 0.05600932613015175]
Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English.	[12, 60, 52, 169, 2, 40, 84, 10, 51, 83]	[1, 1, 1, 0, 0, 1, 0, 1, 0, 1]	[0.8279812335968018, 0.8105778098106384, 0.5180736780166626, 0.338190495967865, 0.2575797140598297, 0.7159963250160217, 0.42062658071517944, 0.5891731977462769, 0.40189576148986816, 0.5260406136512756]
Second, we compare our cut-based approach with the five aforementioned approaches to anaphoricity determination (namely, Ng and Cardie (2002a), Ng (2004), Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008)) in terms of their effectiveness in improving a learning-based coreference system.	[12, 16, 23, 19, 57, 58, 60, 18, 31, 30]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7882946729660034, 0.1095389872789383, 0.2080291211605072, 0.30549296736717224, 0.06415557116270065, 0.47355011105537415, 0.09822217375040054, 0.06368593126535416, 0.3314440846443176, 0.10165008157491684]
See (Gildea and Jurafsky, 2000) for some promising initial work in applying statistical techniques to the FrameNet database to automatically label frame elements.	[19, 17, 18, 73, 129, 116, 134, 23, 43, 39]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.3899058699607849, 0.6994562149047852, 0.6319884657859802, 0.2044023722410202, 0.32495763897895813, 0.48505398631095886, 0.16243980824947357, 0.25102463364601135, 0.37346312403678894, 0.0894227996468544]
See Hobbs (1985a) for explanation of this notation for events.	[26, 262, 46, 9, 69, 243, 28, 337, 90, 499]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6631887555122375, 0.6631887555122375, 0.23974882066249847, 0.11562646925449371, 0.3665313422679901, 0.11562646925449371, 0.12427632510662079, 0.19986048340797424, 0.13712280988693237, 0.08035433292388916]
Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002).	[90, 17, 19, 12, 82, 2, 31, 0, 23, 21]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.8323635458946228, 0.6881207227706909, 0.4574083685874939, 0.24610617756843567, 0.2831389605998993, 0.09975031763315201, 0.3050774037837982, 0.7976001501083374, 0.1275094449520111, 0.08932604640722275]
Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a).	[83, 18, 82, 108, 87, 58, 86, 105, 110, 2]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2774522602558136, 0.6517612934112549, 0.2360340803861618, 0.24421128630638123, 0.20075075328350067, 0.13348233699798584, 0.08881834149360657, 0.18726229667663574, 0.05501016974449158, 0.08699353039264679]
Semantic Classifiers Bootstrapping refers to a problem of inducing a classifier given a small set of labeled data and a large set of unlabeled data (Abney, 2002).	[5, 154, 6, 22, 14, 12, 9, 19, 44, 15]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8545013070106506, 0.7230919003486633, 0.24313436448574066, 0.2242969274520874, 0.29790982604026794, 0.17428165674209595, 0.21033678948879242, 0.17768247425556183, 0.15857362747192383, 0.1378176361322403]
Semantic role classifiers rely heavily on lexical features (Johansson and Nugues, 2008b), and this may lead to brittleness; in order to increase robustness, we added features based on hierarchical clusters constructed using the Brown algorithm (Brown et al, 1992).	[74, 125, 151, 43, 135, 53, 170, 75, 42, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3863355219364166, 0.27919259667396545, 0.40300431847572327, 0.11060537397861481, 0.2971048653125763, 0.18206509947776794, 0.05790374428033829, 0.37282970547676086, 0.1401636004447937, 0.07300936430692673]
Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al 2006), fact extraction for information retrieval (Pas?ca et al 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al 2011), among many others (Turney and Pantel, 2010).	[34, 7, 3, 1, 105, 41, 98, 73, 74, 106]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5300642848014832, 0.7022733092308044, 0.16566532850265503, 0.17497888207435608, 0.0588684044778347, 0.08211907744407654, 0.06070679798722267, 0.0505574569106102, 0.1426006704568863, 0.059396643191576004]
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	[65, 2, 27, 61, 63, 6, 21, 25, 55, 42]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21548055112361908, 0.08949150890111923, 0.11938194185495377, 0.40897759795188904, 0.4087037742137909, 0.23538559675216675, 0.23538559675216675, 0.184896782040596, 0.17960095405578613, 0.05651542916893959]
Senseval 3 shared task data (Snyder and Palmer, 2004).	[27, 0, 29, 12, 2, 6, 4, 7, 31, 32]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.658843457698822, 0.778052568435669, 0.13001365959644318, 0.10804786533117294, 0.20882487297058105, 0.1549941897392273, 0.134291872382164, 0.10186772793531418, 0.1549941897392273, 0.10186772793531418]
Sentiment analysis can be dependently or independently done from subjectivity detection, although Pang and Lee (2004) state that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter.	[0, 116, 79, 19, 12, 41, 114, 22, 88, 20]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7737873792648315, 0.40892019867897034, 0.33898359537124634, 0.23091377317905426, 0.09431003034114838, 0.2955699563026428, 0.24081335961818695, 0.21901455521583557, 0.1141681969165802, 0.10387672483921051]
Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007).	[43, 48, 45, 79, 63, 25, 80, 75, 74, 40]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6432345509529114, 0.6046337485313416, 0.46521201729774475, 0.1494724005460739, 0.5246395468711853, 0.3005789518356323, 0.41709285974502563, 0.10911964625120163, 0.06418420374393463, 0.3000684082508087]
Several NLG systems adapt to the user's domain expertize at different levels of generation text planning (Paris, 1987), complexity of instructions (Dale, 1989), referring expressions (Reiter, 1991), and so on.	[4, 103, 7, 99, 8, 10, 19, 36, 41, 6]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6844295859336853, 0.7551743388175964, 0.13850906491279602, 0.360885888338089, 0.31618592143058777, 0.4038829207420349, 0.11996288597583771, 0.08565893769264221, 0.07628656178712845, 0.05944201722741127]
Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004).	[94, 67, 52, 53, 82, 76, 105, 96, 92, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38420045375823975, 0.25876057147979736, 0.13473370671272278, 0.2531065046787262, 0.27674657106399536, 0.05369720235466957, 0.1741396188735962, 0.08227061480283737, 0.07818760722875595, 0.2089257538318634]
Shen and Lapata (2007) developed an answer extraction module that incorporates FrameNet style semantic role information.	[242, 27, 234, 60, 161, 3, 203, 26, 6, 179]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8205851912498474, 0.8401755094528198, 0.4825087785720825, 0.24616165459156036, 0.455562561750412, 0.3647025525569916, 0.17450138926506042, 0.2258901745080948, 0.08188477158546448, 0.330520361661911]
Shen et al (2004) and Och et al (2004) presented approaches to re-rank the output of the decoder using syntactic information.	[191, 55, 40, 48, 120, 37, 149, 58, 31, 38]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7574074864387512, 0.4293793737888336, 0.16195128858089447, 0.23006479442119598, 0.2782055735588074, 0.08814121782779694, 0.1152641549706459, 0.24052079021930695, 0.2085474580526352, 0.13541099429130554]
Shen et al (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first.	[31, 59, 5, 197, 101, 45, 98, 28, 63, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4919569492340088, 0.10605684667825699, 0.09953351318836212, 0.13602183759212494, 0.09461106359958649, 0.09258102625608444, 0.1331178992986679, 0.1710706651210785, 0.1274997442960739, 0.05192510038614273]
Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not.	[57, 53, 162, 87, 31, 98, 156, 127, 302, 32]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6213443279266357, 0.2819836139678955, 0.7128511071205139, 0.1123407781124115, 0.11333467066287994, 0.2440231442451477, 0.11403705924749374, 0.09708154946565628, 0.1299859583377838, 0.09959342330694199]
Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here.	[67, 4, 17, 112, 66, 132, 52, 59, 31, 224]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6313647031784058, 0.1620263308286667, 0.11146608740091324, 0.11385570466518402, 0.07051364332437515, 0.06059518828988075, 0.17161841690540314, 0.05967579409480095, 0.06334710121154785, 0.0592997744679451]
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates.	[56, 55, 37, 58, 168, 205, 191, 20, 17, 19]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.48522889614105225, 0.7629298567771912, 0.6618104577064514, 0.3368532657623291, 0.17843349277973175, 0.247573122382164, 0.08825233578681946, 0.1599942147731781, 0.37643343210220337, 0.1418188065290451]
Similar to a best-first parser (Caraballo and Charniak, 1998), the highest scored hypothesis is expanded first.	[182, 175, 178, 58, 60, 1, 5, 234, 61, 189]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.631016731262207, 0.7510601878166199, 0.4996626377105713, 0.22781802713871002, 0.16410228610038757, 0.16278716921806335, 0.16278716921806335, 0.1816493570804596, 0.11190688610076904, 0.1945420503616333]
Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions.	[28, 43, 15, 14, 46, 29, 72, 32, 71, 13]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7519454956054688, 0.10990813374519348, 0.19120706617832184, 0.18740859627723694, 0.35710635781288147, 0.3206483721733093, 0.27858489751815796, 0.3336072564125061, 0.0722220242023468, 0.1754004955291748]
Similarly as Yarowsky and Wicentowski (2000), we assume that, in any language, vowels are more mutable in inflection than consonants, thus for example replacing a for i is cheaper that replacing s by r.	[74, 113, 102, 83, 11, 15, 117, 37, 109, 39]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8072996735572815, 0.07780668884515762, 0.15957476198673248, 0.15081194043159485, 0.2120470404624939, 0.15931721031665802, 0.06908269971609116, 0.07968395948410034, 0.06542044132947922, 0.060462307184934616]
Similarly to its context-free counterpart, the re-estimation algorithm can be extended to handle partially parsed corpora (Pereira and Schabes, 1992).	[4, 0, 30, 21, 121, 24, 8, 82, 33, 120]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7340263724327087, 0.6546788215637207, 0.3492855131626129, 0.364448219537735, 0.3698391020298004, 0.10008639097213745, 0.37720152735710144, 0.2300221025943756, 0.4961449205875397, 0.20960243046283722]
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	[13, 20, 84, 27, 98, 41, 12, 15, 21, 14]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.51622074842453, 0.34057682752609253, 0.16965043544769287, 0.1585972160100937, 0.18189622461795807, 0.2060725837945938, 0.1499454826116562, 0.5868324041366577, 0.08637282997369766, 0.07398241758346558]
Similarly, Choi et al (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction.	[13, 34, 11, 4, 82, 202, 9, 100, 40, 35]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.3775951862335205, 0.10487984865903854, 0.1817319393157959, 0.13946907222270966, 0.15960325300693512, 0.14230047166347504, 0.627860426902771, 0.36923930048942566, 0.28701844811439514, 0.14907202124595642]
Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes.	[75, 46, 2, 6, 24, 32, 8, 55, 47, 7]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42739683389663696, 0.7148625254631042, 0.319830983877182, 0.319830983877182, 0.40131303668022156, 0.1832626461982727, 0.14738829433918, 0.11172781139612198, 0.21245801448822021, 0.052674293518066406]
Similarly, Snyder and Barzilay (2007) decompose an opinion across several dimensions and capture the sentiment across each dimension.	[11, 47, 15, 166, 8, 30, 168, 160, 151, 99]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4979085624217987, 0.07151300460100174, 0.059846606105566025, 0.09901614487171173, 0.11403390765190125, 0.10219530016183853, 0.06283126771450043, 0.06842252612113953, 0.055236801505088806, 0.10263574868440628]
Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.	[4, 11, 23, 2, 24, 54, 12, 57, 8, 26]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35023051500320435, 0.6092166304588318, 0.23338311910629272, 0.23161794245243073, 0.24408859014511108, 0.25802406668663025, 0.11396003514528275, 0.15732093155384064, 0.156490296125412, 0.15088260173797607]
Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009).	[82, 181, 38, 18, 52, 48, 75, 182, 23, 99]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3284095227718353, 0.461270809173584, 0.24132007360458374, 0.19931955635547638, 0.05071062222123146, 0.25746676325798035, 0.17047065496444702, 0.34099623560905457, 0.08267910033464432, 0.0848316177725792]
Since Schone and Jurafsky (2001) demonstrated similar results whether WordNet or on line dictionaries were used as a gold standard, WordNet was selected.	[130, 195, 20, 194, 129, 85, 90, 91, 92, 21]	[1, 1, 1, 0, 0, 0, 1, 1, 0, 0]	[0.7554060816764832, 0.6619884967803955, 0.5283632278442383, 0.4204361140727997, 0.27375516295433044, 0.24886588752269745, 0.775562047958374, 0.5354446172714233, 0.35862043499946594, 0.17447702586650848]
Since a spelling correction model needs to rank candidate words rather than candidate pronunciations, Toutanova and Moore (2002) derive an error model that determines the probability that a word w was spelled as the non-word r based on their pronunciations.	[157, 52, 129, 29, 34, 123, 1, 3, 169, 0]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32702159881591797, 0.6990200877189636, 0.33569806814193726, 0.23121120035648346, 0.09782995283603668, 0.25059962272644043, 0.2645912170410156, 0.08461106568574905, 0.18421708047389984, 0.2827506959438324]
Since data producing these summaries can be sourced in different documents, summary fusion techniques as proposed in (Radev and McKeown, 1998) can be employed.	[107, 146, 95, 428, 55, 103, 361, 20, 2, 6]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.6343319416046143, 0.4291832447052002, 0.15540210902690887, 0.3046922981739044, 0.1970628798007965, 0.25873714685440063, 0.5228945016860962, 0.1436331421136856, 0.2508588135242462, 0.2508588135242462]
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roark and Charniak, 1998).	[40, 21, 19, 63, 24, 38, 11, 68, 32, 60]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6327818632125854, 0.2428017109632492, 0.24041713774204254, 0.17885522544384003, 0.35901597142219543, 0.06785530596971512, 0.07461260259151459, 0.0769834965467453, 0.11795682460069656, 0.13172650337219238]
Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details.	[6, 12, 33, 10, 138, 93, 57, 7, 74, 101]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42974039912223816, 0.17048576474189758, 0.07138696312904358, 0.06213465705513954, 0.06501580774784088, 0.12244810163974762, 0.24802546203136444, 0.15035471320152283, 0.07284845411777496, 0.10340090095996857]
Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first.	[182, 148, 103, 4, 38, 109, 0, 15, 184, 89]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.624004065990448, 0.47540801763534546, 0.5013148784637451, 0.27710846066474915, 0.35532307624816895, 0.3034753203392029, 0.3582106828689575, 0.16914527118206024, 0.06511792540550232, 0.10158446431159973]
Since the raw Penn Treebank data contains many inconsistencies in its annotations (cf. Ratnaparkhi, 1996), a single inconsistency in a test set tree will very likely yield a zero percent parse accuracy for the particular test set sentence.	[49, 36, 77, 79, 100, 57, 104, 68, 61, 74]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48260632157325745, 0.06919524073600769, 0.4057845175266266, 0.21150320768356323, 0.15428666770458221, 0.29171890020370483, 0.2619662880897522, 0.19809824228286743, 0.0896715447306633, 0.0688546895980835]
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	[15, 85, 10, 119, 19, 76, 75, 63, 139, 112]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4841156005859375, 0.2910400331020355, 0.21081826090812683, 0.29068899154663086, 0.07120097428560257, 0.08615031093358994, 0.17761260271072388, 0.08718714863061905, 0.10879866033792496, 0.1615285575389862]
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004).	[12, 3, 81, 29, 62, 16, 49, 4, 102, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33074405789375305, 0.09647062420845032, 0.09783509373664856, 0.13044482469558716, 0.08661174029111862, 0.1616143137216568, 0.09196564555168152, 0.14786839485168457, 0.2062787264585495, 0.13748864829540253]
Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed.	[116, 61, 12, 174, 11, 84, 41, 64, 60, 0]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4299205541610718, 0.719977617263794, 0.2885585427284241, 0.4392915666103363, 0.2002083659172058, 0.2955557405948639, 0.3812463581562042, 0.14877499639987946, 0.21507979929447174, 0.41867920756340027]
Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner.	[4, 160, 58, 40, 205, 18, 1, 15, 128, 2]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4730702340602875, 0.5144270658493042, 0.31040874123573303, 0.1832171529531479, 0.1786555051803589, 0.32196056842803955, 0.3004474937915802, 0.16634510457515717, 0.09209930151700974, 0.3032107651233673]
Smoothing typically adds considerable computational complexity to the system since multiple models need to be estimated and applied together, and it is often considered a black art (Chen and Goodman, 1996).	[22, 35, 130, 128, 64, 91, 31, 50, 79, 26]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30761632323265076, 0.08306154608726501, 0.19864821434020996, 0.13196027278900146, 0.11610458046197891, 0.0765327587723732, 0.2990753650665283, 0.05706802383065224, 0.13126106560230255, 0.10596611350774765]
Solving the sentence level task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or non speculative.	[19, 76, 57, 15, 112, 78, 91, 36, 30, 46]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7201223969459534, 0.6005228161811829, 0.21291659772396088, 0.20155949890613556, 0.17673946917057037, 0.172489196062088, 0.05248403921723366, 0.06264230608940125, 0.04873298853635788, 0.10543130338191986]
Some clusters of studies have used common test suites, most notably the 2094-word Hne data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992).	[73, 47, 6, 145, 185, 94, 40, 127, 2, 118]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8096411824226379, 0.24373780190944672, 0.07366175949573517, 0.24438691139221191, 0.05452060326933861, 0.11244817823171616, 0.3023231625556946, 0.08898638933897018, 0.06251376867294312, 0.13392408192157745]
Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructions are generated and then filtered based on a statistical model.	[4, 192, 6, 141, 20, 158, 21, 185, 49, 184]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30976563692092896, 0.3089137673377991, 0.20520815253257751, 0.20216046273708344, 0.20428740978240967, 0.1492670476436615, 0.11804643273353577, 0.3250885605812073, 0.11035178601741791, 0.06568049639463425]
Some of the most sophisticated work on this aspect of problem again seems to be that of Melamed (1997).	[199, 216, 109, 111, 1, 10, 60, 173, 157, 211]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7628403902053833, 0.26917362213134766, 0.0621342658996582, 0.07256685942411423, 0.4616856276988983, 0.1984759271144867, 0.11164683103561401, 0.14379972219467163, 0.38929203152656555, 0.31393754482269287]
Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998).	[0, 41, 29, 25, 42, 28, 135, 83, 55, 71]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5560681819915771, 0.24996012449264526, 0.15652208030223846, 0.21983425319194794, 0.1958625316619873, 0.08430466800928116, 0.2005801796913147, 0.12031245976686478, 0.09975189715623856, 0.25184518098831177]
Some refer to the task as cross-document co reference resolution (Bagga and Baldwin, 1998), name discrimination (Pedersen et al, 2005) or Web People Search (WebPS) (Artiles et al, 2007).	[3, 10, 48, 9, 12, 4, 11, 64, 81, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40406158566474915, 0.24479369819164276, 0.12395802140235901, 0.19531148672103882, 0.14295679330825806, 0.25347092747688293, 0.25347092747688293, 0.05047522112727165, 0.06214478239417076, 0.11713912338018417]
Specifically considering Arabic, Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations.	[33, 11, 24, 23, 13, 8, 41, 31, 27, 10]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6309479475021362, 0.5952469706535339, 0.3153863251209259, 0.19329605996608734, 0.24546103179454803, 0.17616869509220123, 0.3767550587654114, 0.19194170832633972, 0.1798078417778015, 0.23165465891361237]
Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup.	[6, 25, 15, 87, 112, 20, 11, 93, 1, 145]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3729816973209381, 0.22391429543495178, 0.17082712054252625, 0.4277685582637787, 0.0911405012011528, 0.09602697938680649, 0.09096264094114304, 0.07772888988256454, 0.09205970168113708, 0.20995493233203888]
Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al, 1998), then the identification of locally uninstantiated roles (NIs).	[44, 19, 17, 65, 32, 30, 0, 27, 5, 10]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.46953168511390686, 0.4481891691684723, 0.37753987312316895, 0.4831453859806061, 0.14857624471187592, 0.1192752867937088, 0.3686583638191223, 0.5347445011138916, 0.16062839329242706, 0.131172776222229]
Speech is often disfluent, and speech repairs are known to repeat large portions of the preceding context (Johnson and Charniak, 2004).	[1, 0, 17, 8, 7, 11, 5, 155, 15, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6063876748085022, 0.641550600528717, 0.47585198283195496, 0.4086287021636963, 0.3382050395011902, 0.19847579300403595, 0.4055737555027008, 0.257933109998703, 0.20573171973228455, 0.27071577310562134]
Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence.	[5, 1, 2, 25, 17, 27, 45, 91, 66, 42]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.8430747389793396, 0.771780252456665, 0.42853033542633057, 0.6539522409439087, 0.7315466403961182, 0.28003743290901184, 0.14096395671367645, 0.32607120275497437, 0.2655050754547119, 0.057044617831707]
Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004).	[152, 11, 2, 195, 180, 177, 13, 118, 155, 170]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6609501838684082, 0.2569926381111145, 0.22503943741321564, 0.4208291172981262, 0.3641221225261688, 0.17399178445339203, 0.14355674386024475, 0.26448512077331543, 0.1541384607553482, 0.0579480305314064]
Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing.	[43, 36, 443, 292, 66, 279, 35, 280, 5, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20233722031116486, 0.08651142567396164, 0.1292228400707245, 0.14656765758991241, 0.25319984555244446, 0.13792449235916138, 0.1575808823108673, 0.14364418387413025, 0.05494437366724014, 0.05494437366724014]
Strube and Hahn (1999) extend the context to more than the last sentence, but switch preference order between the last and the current sentence so that an antecedentes determined in the last sentence, whenever possible.	[478, 168, 281, 306, 481, 321, 297, 129, 477, 127]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4264143109321594, 0.147708460688591, 0.21099752187728882, 0.09107278287410736, 0.15972617268562317, 0.06489606201648712, 0.05225973576307297, 0.07102607190608978, 0.1356867551803589, 0.14817075431346893]
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	[20, 65, 220, 62, 61, 55, 173, 59, 13, 32]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35994502902030945, 0.33780503273010254, 0.4395521283149719, 0.13871586322784424, 0.3646453619003296, 0.16275806725025177, 0.15835539996623993, 0.12574933469295502, 0.47219157218933105, 0.14588500559329987]
Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result.	[341, 332, 18, 334, 337, 311, 4, 9, 26, 314]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6840680837631226, 0.6772274374961853, 0.18924665451049805, 0.34963685274124146, 0.4483700394630432, 0.4518350064754486, 0.2583267092704773, 0.2583267092704773, 0.30307039618492126, 0.3518552780151367]
Such an approach has been taken by Och et al (2004) for integrating sophisticated syntax-informed models in a phrase based SMT system.	[113, 205, 114, 52, 207, 23, 167, 193, 101, 10]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6082063913345337, 0.06725434958934784, 0.07437802851200104, 0.053387098014354706, 0.1530352383852005, 0.5448428392410278, 0.12714512646198273, 0.09256201982498169, 0.12935641407966614, 0.06046370044350624]
Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009).	[185, 133, 41, 64, 36, 66, 63, 49, 37, 43]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3415294885635376, 0.8179929256439209, 0.1686190664768219, 0.2798398435115814, 0.1787293553352356, 0.26572704315185547, 0.16464543342590332, 0.17345215380191803, 0.09449167549610138, 0.06921004503965378]
Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011).	[25, 35, 9, 68, 87, 191, 57, 102, 75, 98]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0551145076751709, 0.05910886451601982, 0.07152702659368515, 0.3770451247692108, 0.3546042740345001, 0.05777933448553085, 0.05972975865006447, 0.05294583737850189, 0.061354462057352066, 0.057659972459077835]
Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009).	[11, 2, 138, 23, 0, 24, 1, 10, 9, 8]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.848004162311554, 0.542974054813385, 0.21038316190242767, 0.05940808728337288, 0.5512450337409973, 0.41819971799850464, 0.35244473814964294, 0.2919227182865143, 0.18517713248729706, 0.09892471879720688]
Sudo et al (2003) extract dependency subtrees within relevant documents as IE patterns.	[55, 13, 61, 0, 47, 21, 35, 6, 4, 123]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6626916527748108, 0.6543314456939697, 0.6738621592521667, 0.3015289902687073, 0.3186642527580261, 0.08576273173093796, 0.11549409478902817, 0.14810067415237427, 0.33917781710624695, 0.47251957654953003]
Supertags are the elementary structures of Lexicalized Tree Adjoining Grammars (LTAGs) (Bangalore and Joshi, 1999).	[87, 35, 73, 39, 303, 7, 17, 74, 344, 80]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7849103808403015, 0.7152036428451538, 0.4869810938835144, 0.36230573058128357, 0.6643287539482117, 0.42981263995170593, 0.42981263995170593, 0.41328075528144836, 0.4873400628566742, 0.16447009146213531]
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maximum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).	[53, 23, 131, 48, 105, 17, 30, 27, 6, 50]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33009520173072815, 0.2201852798461914, 0.19063454866409302, 0.16346971690654755, 0.13048261404037476, 0.09200232475996017, 0.16293412446975708, 0.1504240185022354, 0.06231122463941574, 0.05272652208805084]
Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus.	[11, 165, 120, 13, 25, 12, 21, 9, 1, 133]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.312960684299469, 0.23054490983486176, 0.12608493864536285, 0.09115834534168243, 0.23765257000923157, 0.17865540087223053, 0.12719710171222687, 0.1845005303621292, 0.05785144120454788, 0.058371126651763916]
Swier and Stevenson (2004) were the first to introduce an unsupervised semantic role labeling system.	[13, 0, 1, 63, 9, 205, 15, 27, 196, 28]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3083071708679199, 0.8557147979736328, 0.35258403420448303, 0.26841041445732117, 0.17247052490711212, 0.1623525172472, 0.3315287232398987, 0.18304046988487244, 0.12189379334449768, 0.2522389590740204]
Swier and Stevenson (2004) were the first to introduce unsupervised SRL in an approach that used the VerbNet lexicon to guide unsupervised learning.	[30, 0, 13, 27, 34, 63, 1, 21, 14, 35]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.2187158316373825, 0.7857249975204468, 0.1109057366847992, 0.2949151396751404, 0.5414965748786926, 0.23874999582767487, 0.21658656001091003, 0.10491259396076202, 0.12066037952899933, 0.13187232613563538]
Synchronous binarization (Zhang et al, 2006) solves this problem by simultaneously binarizing both source and target-sides of a synchronous rule, making sure of contiguous spans on both sides whenever possible.	[48, 36, 5, 7, 6, 3, 30, 120, 29, 51]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5978346467018127, 0.15555593371391296, 0.17683342099189758, 0.242152601480484, 0.45088329911231995, 0.2286100536584854, 0.19659407436847687, 0.16051529347896576, 0.4555089771747589, 0.15796932578086853]
Synchronous tree-sequence substitution grammar (STSSG) al lows either side of a rule to comprise a sequence of trees instead of a single tree (Zhang et al, 2008).	[59, 52, 97, 89, 50, 25, 1, 71, 79, 13]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8303400874137878, 0.1930493265390396, 0.5208313465118408, 0.1369427889585495, 0.48422694206237793, 0.12214039266109467, 0.23784013092517853, 0.16025888919830322, 0.06777939945459366, 0.14549702405929565]
Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005).	[19, 102, 20, 17, 31, 22, 111, 4, 97, 87]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7543289661407471, 0.39918866753578186, 0.13030901551246643, 0.1741422563791275, 0.2599448263645172, 0.18115003407001495, 0.19500063359737396, 0.1120387464761734, 0.09098125249147415, 0.07427719235420227]
Syntactic frame as described by Xue and Palmer (2004) Table 3.	[9, 58, 14, 61, 64, 56, 20, 1, 13, 37]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3946344256401062, 0.6227923631668091, 0.1325061023235321, 0.2678327262401581, 0.2728908956050873, 0.15350119769573212, 0.22918491065502167, 0.14354754984378815, 0.07186225801706314, 0.13197565078735352]
System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994).	[69, 25, 9, 88, 14, 64, 13, 28, 8, 58]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6115161180496216, 0.3191474676132202, 0.27451586723327637, 0.5286104083061218, 0.06945455819368362, 0.18220113217830658, 0.08891880512237549, 0.05188070610165596, 0.0778169184923172, 0.07785938680171967]
System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006).	[78, 10, 68, 22, 9, 1, 91, 80, 82, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7738600373268127, 0.4898788034915924, 0.41940972208976746, 0.12288468331098557, 0.18762755393981934, 0.13786302506923676, 0.10025531053543091, 0.11996523290872574, 0.10093622654676437, 0.21636933088302612]
Systems for automatic translation between languages have been divided into transfer-based approaches, which rely on interpreting the source string into an abstract semantic representation from which text is generated in the target language, and statistical approaches, pioneered by Brown et al (1990), which estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.	[32, 130, 141, 138, 211, 37, 27, 7, 135, 219]	[0, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.2908876836299896, 0.3552539050579071, 0.5037393569946289, 0.18717767298221588, 0.381460964679718, 0.6715126037597656, 0.15619070827960968, 0.058756023645401, 0.12259547412395477, 0.16739314794540405]
Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates.	[109, 44, 49, 50, 103, 146, 52, 51, 185, 104]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8451786041259766, 0.8000562191009521, 0.4738437831401825, 0.5053166747093201, 0.219464510679245, 0.13904890418052673, 0.1266704946756363, 0.15984930098056793, 0.10831023007631302, 0.09995432198047638]
TBL is a machine learning approach that has been employed to solve a number of problems in natural language processing; most famously, it has been used for part-of-speech tagging (Brill, 1995).	[36, 0, 1, 8, 124, 6, 13, 43, 3, 10]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8476440906524658, 0.7549891471862793, 0.42609626054763794, 0.42609626054763794, 0.18421226739883423, 0.1891932338476181, 0.1891932338476181, 0.19258500635623932, 0.16613376140594482, 0.16613376140594482]
TBL tagger (Brill, 1992) 7, and a TnT-style trigram tagger (Halacsy et al, 2007).	[101, 0, 21, 3, 20, 23, 97, 2, 24, 29]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11919935792684555, 0.26876962184906006, 0.23661576211452484, 0.16336946189403534, 0.2458326369524002, 0.14358177781105042, 0.22339001297950745, 0.1299377977848053, 0.15226982533931732, 0.13243721425533295]
Table 1 also presents the performance of a typical topic segmentation algorithm, TextTiling (Hearst, 1997).	[370, 442, 31, 439, 436, 241, 19, 27, 271, 77]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7214189767837524, 0.3935474753379822, 0.3221851587295532, 0.2910827100276947, 0.5753001570701599, 0.052634917199611664, 0.33669906854629517, 0.15903818607330322, 0.057425376027822495, 0.16407299041748047]
Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger.	[53, 84, 18, 100, 49, 47, 94, 14, 86, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7726863622665405, 0.4710933268070221, 0.4145407974720001, 0.4852895736694336, 0.1373979151248932, 0.22105064988136292, 0.12461990118026733, 0.16115552186965942, 0.29401499032974243, 0.052948370575904846]
Table 1 shows SRL performance for the local model described above, and the full global CCG-system described by Boxwell et al (2009). We use the method for calculating the accuracy of Propbank verbal semantic roles described in the CoNLL-2008 shared task on semantic role labeling (Surdeanu et al, 2008).	[0, 8, 6, 296, 9, 3, 10, 4, 5, 203]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7724345326423645, 0.19185028970241547, 0.2336578369140625, 0.14229826629161835, 0.4097079932689667, 0.1296149492263794, 0.14467838406562805, 0.08777584135532379, 0.06777995824813843, 0.06295213848352432]
Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009).	[82, 140, 90, 88, 157, 91, 89, 99, 18, 118]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7300330400466919, 0.35128623247146606, 0.347440242767334, 0.1383180022239685, 0.40862804651260376, 0.18747808039188385, 0.11094288527965546, 0.13368132710456848, 0.3101882040500641, 0.111258864402771]
Take the following example from Poon and Domingos (2009), in which the same semantic relation can be expressed by a transitive verb or an attributive prepositional phrase: (1) Utah borders Idaho.	[91, 34, 60, 84, 126, 125, 29, 59, 26, 93]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6382323503494263, 0.5839545130729675, 0.44500604271888733, 0.39032918214797974, 0.1999235451221466, 0.35874271392822266, 0.346046507358551, 0.07327578961849213, 0.3597365617752075, 0.048531051725149155]
Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts.	[51, 72, 111, 15, 133, 9, 122, 40, 5, 123]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6464487314224243, 0.36768704652786255, 0.2824658155441284, 0.25992298126220703, 0.6579352021217346, 0.4016896188259125, 0.4636545181274414, 0.07660027593374252, 0.15196453034877777, 0.33952948451042175]
Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000).	[14, 12, 15, 6, 104, 109, 5, 107, 108, 105]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2772866189479828, 0.4311561584472656, 0.387891560792923, 0.08491461724042892, 0.15902623534202576, 0.175700843334198, 0.05233480781316757, 0.20250819623470306, 0.1807689070701599, 0.1461595594882965]
Text chunking consists of dividing text into syntactically related non overlapping groups of words (Tjong Kim Sang and Buchholz, 2000).	[117, 4, 1, 70, 2, 68, 6, 8, 5, 36]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.8463690876960754, 0.8494634628295898, 0.5025269985198975, 0.5388142466545105, 0.1627853363752365, 0.12071825563907623, 0.4889286756515503, 0.28382784128189087, 0.10538210719823837, 0.19106945395469666]
The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding transformations.	[36, 67, 26, 61, 34, 21, 193, 2, 199, 58]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7452418208122253, 0.1771910935640335, 0.124458909034729, 0.06222403421998024, 0.07683994621038437, 0.09460705518722534, 0.11876075714826584, 0.07153238356113434, 0.07513508945703506, 0.055965207517147064]
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	[117, 116, 240, 68, 14, 51, 27, 38, 110, 124]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6495876312255859, 0.5314515829086304, 0.3634641170501709, 0.6282709240913391, 0.4978291392326355, 0.08227451890707016, 0.3623656928539276, 0.08057568222284317, 0.0930570587515831, 0.057880450040102005]
The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011).	[93, 61, 35, 17, 84, 45, 13, 18, 34, 33]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5733733177185059, 0.44674286246299744, 0.4519105553627014, 0.1612740010023117, 0.15647441148757935, 0.13624833524227142, 0.055502425879240036, 0.05991876497864723, 0.11518542468547821, 0.11962974071502686]
The Bayesian prior can be estimated as per Gale and Church (1991) by assuming that it is equal to the frequency of distinct n-m matches in the training set.	[25, 94, 157, 158, 153, 115, 132, 139, 74, 90]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.27631980180740356, 0.2018427699804306, 0.05710236728191376, 0.058274947106838226, 0.09767844527959824, 0.11525546759366989, 0.05676349997520447, 0.056396156549453735, 0.05730694532394409, 0.06030597910284996]
The BioNLP 2011 Shared Task ((Kim et al, 2011)) series generalized this defining a series of tasks involving more text types, domains and target event types.	[10, 3, 22, 76, 4, 78, 41, 42, 9, 33]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3260280191898346, 0.7414919137954712, 0.27830711007118225, 0.22164535522460938, 0.20940524339675903, 0.1634090095758438, 0.19978857040405273, 0.15921959280967712, 0.21610920131206512, 0.08147882670164108]
The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We use the method described in Clark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.	[17, 68, 21, 25, 138, 46, 115, 6, 18, 4]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.821429431438446, 0.8102972507476807, 0.2843416631221771, 0.4986127018928528, 0.32744503021240234, 0.286112904548645, 0.5407347083091736, 0.157440185546875, 0.10797536373138428, 0.18077675998210907]
The CWM algorithm (Choi et al, 2001) applies the same procedure to a similarity matrix of LSI vectors.	[110, 168, 67, 104, 199, 13, 169, 154, 174, 197]	[1, 1, 1, 1, 0, 0, 1, 0, 0, 0]	[0.7533444762229919, 0.6919698715209961, 0.6362565755844116, 0.6752684712409973, 0.35715100169181824, 0.15541234612464905, 0.5060918927192688, 0.33473581075668335, 0.4387979805469513, 0.22273333370685577]
The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006).	[95, 180, 102, 103, 41, 129, 155, 100, 184, 21]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8072962164878845, 0.5337178111076355, 0.40302959084510803, 0.17317238450050354, 0.09526972472667694, 0.4456402361392975, 0.1644660085439682, 0.33850592374801636, 0.1450413465499878, 0.2329310178756714]
The Czech parser of Collins et al (1999) was run on a different data set and most other dependency parsers are evaluated using English.	[0, 35, 70, 5, 88, 10, 3, 117, 1, 115]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7273952960968018, 0.24195893108844757, 0.1758020669221878, 0.15231209993362427, 0.22718049585819244, 0.27007609605789185, 0.1987326443195343, 0.2679794430732727, 0.15509864687919617, 0.19973336160182953]
The Dirichlet parameters α are drawn independently from a Γ(1, 1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009).	[100, 134, 106, 122, 11, 101, 154, 114, 104, 119]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7803623676300049, 0.4783976376056671, 0.3539820909500122, 0.10388542711734772, 0.23598572611808777, 0.08093328773975372, 0.053008563816547394, 0.18678775429725647, 0.44422009587287903, 0.07022649049758911]
The English POS-tagging has been carried out using freely available TNT tagger (Brants, 2000).	[173, 15, 175, 3, 27, 1, 0, 51, 94, 32]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6225768327713013, 0.36057430505752563, 0.20032279193401337, 0.07419553399085999, 0.24980440735816956, 0.2612254023551941, 0.36267635226249695, 0.20523081719875336, 0.19440951943397522, 0.3880968689918518]
The Espresso algorithm (Pantel and Pennacchiotti, 2006) achieves a precision of 80% in learning part whole relations from the Acquaint (TREC-9) corpus of nearly 6M words.	[121, 27, 36, 32, 42, 126, 133, 151, 114, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24873530864715576, 0.12929730117321014, 0.08884377777576447, 0.10718373209238052, 0.10055870562791824, 0.13355503976345062, 0.13484449684619904, 0.2237911969423294, 0.1519428789615631, 0.09234815090894699]
The Full Brevity algorithm (Dale, 1989) attempts to build a minimal distinguishing description by always selecting the most discriminatory property available;.	[48, 61, 7, 60, 10, 97, 66, 3, 9, 68]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5025268197059631, 0.42538145184516907, 0.14483089745044708, 0.19194191694259644, 0.10708312690258026, 0.08313609659671783, 0.053884107619524, 0.11292366683483124, 0.057870976626873016, 0.07520052045583725]
The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J.	[72, 32, 104, 87, 28, 19, 17, 112, 81, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09524041414260864, 0.0662565529346466, 0.07766520977020264, 0.06826723366975784, 0.15468983352184296, 0.07548696547746658, 0.06201448291540146, 0.150217205286026, 0.08047232776880264, 0.0587393194437027]
The Grammar Matrix was developed initially on the basis of broad coverage grammars for English (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugereid, 2003), ModernGreek (Kordoni and Neu, 2005), and Spanish (Marimon et al, 2007), as well as being applied to 42 other languages from a variety of language families in a classroom context (Bender, 2007).	[13, 176, 17, 105, 15, 4, 1, 191, 196, 202]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.5960747599601746, 0.28989389538764954, 0.1746518313884735, 0.3255631625652313, 0.16457214951515198, 0.3878171145915985, 0.36396080255508423, 0.5134752988815308, 0.29413148760795593, 0.1460738629102707]
The Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two strings is the ratio of the length of their LCS and the length of the longer string.	[147, 146, 423, 302, 77, 204, 44, 144, 234, 45]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.855351984500885, 0.848994255065918, 0.5493538975715637, 0.31358614563941956, 0.3615506887435913, 0.21893270313739777, 0.12003207951784134, 0.07506750524044037, 0.2028188705444336, 0.08322640508413315]
The MDA (Multilingual Document Authoring) system [Brun et al2000] is an instance (descended from Ranta's Grammatical Framework [Ranta 2002]) of a text-mediated interactive natural language generation system, a notion introduced by [Power and Scott 1998] under the name of WYSIWYM.	[2, 23, 124, 36, 43, 26, 9, 33, 22, 41]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5801820755004883, 0.5072514414787292, 0.3679349720478058, 0.16397562623023987, 0.14568263292312622, 0.2488042712211609, 0.11363979429006577, 0.08420634269714355, 0.12248251587152481, 0.09660383313894272]
The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language.	[1, 0, 30, 23, 69, 4, 15, 52, 56, 70]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3719619810581207, 0.5485128164291382, 0.19853951036930084, 0.05866845324635506, 0.12084415555000305, 0.45739710330963135, 0.08588327467441559, 0.17000554502010345, 0.06357409060001373, 0.06736964732408524]
The NLP field has gone through a very long tradition of algorithms designed for solving this problem (Ide and Veronis, 1998).	[13, 448, 6, 71, 4, 453, 372, 449, 37, 332]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.2443322390317917, 0.47645437717437744, 0.3056938946247101, 0.19595132768154144, 0.1272350251674652, 0.1614675670862198, 0.6610077023506165, 0.15042370557785034, 0.3991495370864868, 0.26995187997817993]
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	[127, 1, 12, 224, 223, 0, 6, 5, 8, 211]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7996987700462341, 0.38911697268486023, 0.5452534556388855, 0.36025404930114746, 0.33682382106781006, 0.3145071268081665, 0.3124876916408539, 0.44964635372161865, 0.1866627335548401, 0.26958149671554565]
The SAMT implementation of Zollmann and Venugopal (2006) includes a several-thousand-line Perl script to extract their rules.	[43, 17, 56, 10, 6, 33, 25, 53, 39, 49]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4274798333644867, 0.21221977472305298, 0.09154626727104187, 0.12416427582502365, 0.05979156866669655, 0.0545954592525959, 0.05062597244977951, 0.052975136786699295, 0.058306530117988586, 0.05921889841556549]
The Simplified Clause: In order to extract clauses from the text, we use Lin's parser MINI PAR (Lin, 1994).	[35, 74, 21, 1, 73, 6, 0, 9, 154, 11]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42435795068740845, 0.5893166065216064, 0.10055382549762726, 0.2297924906015396, 0.09744123369455338, 0.12466603517532349, 0.4874759018421173, 0.06729245185852051, 0.08742479234933853, 0.14868925511837006]
The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al, 2005).	[47, 16, 4, 115, 53, 18, 111, 54, 6, 3]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5632602572441101, 0.11546444147825241, 0.20520804822444916, 0.1767503023147583, 0.07966462522745132, 0.05940249189734459, 0.07843539863824844, 0.38950562477111816, 0.34057605266571045, 0.13089844584465027]
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model.	[53, 57, 119, 44, 161, 220, 130, 29, 118, 137]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6499303579330444, 0.3574170172214508, 0.3985742926597595, 0.11818575114011765, 0.11609426140785217, 0.3017584979534149, 0.07253313809633255, 0.12806323170661926, 0.09149803221225739, 0.0927792489528656]
The V-measure (VM) (Rosenberg and Hirschberg, 2007) is an information theoretic metric that reports the harmonic mean of homogeneity (each cluster should contain only instances of a single class) and completeness (all instances of a class should be members of the same cluster).	[33, 221, 24, 28, 34, 25, 45, 76, 40, 60]	[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]	[0.7106680274009705, 0.47906482219696045, 0.739642858505249, 0.3450244963169098, 0.6006423830986023, 0.3446149230003357, 0.5806760191917419, 0.2320292741060257, 0.6102358102798462, 0.4098161458969116]
The Xerox calculus includes the composition, ignore, and substitution operator discussed by Kaplan and Kay (1994) and the priority-union operator of Kaplan and Newman (1997).	[185, 259, 422, 710, 247, 199, 403, 386, 572, 429]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5861817002296448, 0.42158249020576477, 0.3418639898300171, 0.2765401005744934, 0.12933078408241272, 0.10836369544267654, 0.148513525724411, 0.21589061617851257, 0.09768500924110413, 0.3063550591468811]
The above two methods have been used in (Banea et al, 2008) for Romanian subjectivity analysis, but the experimental results are not very promising.	[154, 73, 114, 19, 17, 4, 7, 1, 139, 13]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6830373406410217, 0.4712983965873718, 0.30361029505729675, 0.12377166748046875, 0.7653065919876099, 0.14114652574062347, 0.14893651008605957, 0.14994561672210693, 0.3554956614971161, 0.15692561864852905]
The accuracy is reported in (Collins and Brooks, 1995).	[60, 110, 52, 3, 19, 112, 42, 115, 120, 107]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5786115527153015, 0.5405054092407227, 0.44441157579421997, 0.3674175441265106, 0.3830567002296448, 0.43691113591194153, 0.2909637987613678, 0.46588149666786194, 0.06413569301366806, 0.11405303329229355]
The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)).	[144, 109, 110, 44, 112, 138, 125, 64, 11, 108]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41545340418815613, 0.3718702793121338, 0.23469437658786774, 0.308634489774704, 0.22232478857040405, 0.06116968393325806, 0.07619296759366989, 0.18267446756362915, 0.08107905834913254, 0.21113912761211395]
The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints (Fox, 2002), and (ii) we can model broad syntactic reordering phenomena, such as subject-verb-object constructions translating into subject-object-verb ones, as is generally the case for English and Japanese.	[8, 7, 11, 4, 15, 90, 42, 145, 31, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5841303467750549, 0.6158108711242676, 0.26371899247169495, 0.3126015365123749, 0.055946432054042816, 0.40426087379455566, 0.15985341370105743, 0.13180817663669586, 0.1742931455373764, 0.15646883845329285]
The algorithm in (Bannard and Callison-Burch, 2005) is used for this purpose by pivoting through phrases in the source and the target languages: for each source phrase, all occurrences of its target phrases are found, and all the corresponding source phrases of these target phrases are considered as the potential paraphrases of the original source phrase (Callison Burch et al, 2006).	[107, 102, 99, 32, 57, 33, 129, 115, 3, 40]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.4766068756580353, 0.3240361511707306, 0.1488054096698761, 0.3531889021396637, 0.11002704501152039, 0.5107374787330627, 0.09980388730764389, 0.23560376465320587, 0.12281277775764465, 0.25670742988586426]
The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB.	[91, 2, 8, 189, 25, 35, 1, 188, 193, 22]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8448298573493958, 0.16194796562194824, 0.45219743251800537, 0.16194796562194824, 0.15161281824111938, 0.07329604774713516, 0.12770304083824158, 0.1260100156068802, 0.15161281824111938, 0.06757359206676483]
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	[15, 11, 85, 134, 1, 154, 152, 3, 8, 107]	[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.43292760848999023, 0.17317621409893036, 0.5368214249610901, 0.6265689730644226, 0.2671685516834259, 0.3522786796092987, 0.389630526304245, 0.3034884035587311, 0.13359975814819336, 0.36414390802383423]
The approaches by (Wu, 1994), (Haruno and Yamazaki, 1996), (Ma, 2006) and (Gautam and Sinha, 2007) require an externally supplied bilingual lexicon.	[12, 10, 11, 135, 4, 8, 22, 119, 15, 43]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5433968305587769, 0.1836739033460617, 0.38783904910087585, 0.391172856092453, 0.24771486222743988, 0.26876819133758545, 0.273522287607193, 0.1398896425962448, 0.07814168184995651, 0.06574799120426178]
The approximate string matching algorithm we suggest is essentially that of Brill and Moore (2000), a modified weighted Levenshtein distance, where we allow error operations on character sequences as well as on single characters.	[35, 103, 65, 74, 110, 132, 34, 79, 39, 30]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7977023720741272, 0.6238881349563599, 0.40998250246047974, 0.161224827170372, 0.30774709582328796, 0.6430945992469788, 0.2237289845943451, 0.10784497857093811, 0.357585608959198, 0.3342297077178955]
The automated construction of semantically typed lexicons (terms classified into their appropriate semantic class) from unstructured text is of great importance for various kinds of information extraction (Grishman and Sundheim, 1996), question answering (Moldovan et al, 1999), and ontology population (Suchanek et al, 2007).	[16, 68, 48, 53, 6, 21, 8, 54, 43, 17]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20435890555381775, 0.10627015680074692, 0.11028388887643814, 0.16183389723300934, 0.10715596377849579, 0.0544898621737957, 0.07345125824213028, 0.09086190909147263, 0.06231767311692238, 0.09585858881473541]
The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)).	[63, 10, 19, 116, 61, 120, 54, 62, 1, 86]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8368646502494812, 0.4822230637073517, 0.26185309886932373, 0.1049598827958107, 0.24061743915081024, 0.33168506622314453, 0.10123565047979355, 0.3363572061061859, 0.30392876267433167, 0.07741431891918182]
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	[61, 65, 12, 10, 74, 29, 125, 69, 64, 188]	[1, 0, 0, 0, 0, 0, 1, 1, 1, 0]	[0.8452737927436829, 0.36949944496154785, 0.38410770893096924, 0.09726706147193909, 0.4391767084598541, 0.23884937167167664, 0.5656979084014893, 0.6972773671150208, 0.5035268664360046, 0.2893869876861572]
The basic sense originates derived usages, which are more or less constrained and limited, via metonymy, metaphor, slight sense-shiftings or co-composition (Pustejovsky, 1991, 1995).	[346, 378, 109, 222, 243, 113, 276, 236, 274, 363]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6795490980148315, 0.15705066919326782, 0.2778397500514984, 0.1290736347436905, 0.08580094575881958, 0.15875469148159027, 0.26169028878211975, 0.1895325481891632, 0.13803236186504364, 0.15093259513378143]
The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note.	[163, 157, 165, 167, 177, 175, 156, 179, 31, 6]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7488946318626404, 0.6069502234458923, 0.3963954746723175, 0.2897129952907562, 0.39483726024627686, 0.15191620588302612, 0.14724531769752502, 0.16801586747169495, 0.11020570993423462, 0.1075860932469368]
The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002).	[12, 4, 3, 19, 14, 20, 5, 10, 41, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4223877489566803, 0.3938104212284088, 0.489925354719162, 0.19612379372119904, 0.33921194076538086, 0.3526196777820587, 0.20084092020988464, 0.058684878051280975, 0.16330856084823608, 0.14207033812999725]
The bigram model of Lu et al (2008), which is the one used in this paper, must be trained using parameters previously learned for the IBM Model 1 and unigram model in order to exhibit the best performance.	[71, 161, 159, 12, 160, 29, 9, 70, 1, 10]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.776989221572876, 0.35437923669815063, 0.438403457403183, 0.1297590136528015, 0.4187287390232086, 0.32845640182495117, 0.051950614899396896, 0.06604708731174469, 0.2008565068244934, 0.08372456580400467]
The blocks are simpler than the alignment templates in (Och et al, 1999) in that they do not have any internal structure.	[150, 67, 19, 219, 151, 27, 113, 45, 119, 97]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6581438183784485, 0.5228499174118042, 0.14414790272712708, 0.14414790272712708, 0.4719167947769165, 0.11178941279649734, 0.2165350615978241, 0.15767379105091095, 0.13883349299430847, 0.39853376150131226]
The built-in ambiguity in the hyponymy hierarchy presented in (Caraballo, 1999) is primarily an effect of the fact that all information is composed into one tree.	[105, 2, 18, 32, 31, 109, 28, 78, 133, 44]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22613056004047394, 0.33319902420043945, 0.21639202535152435, 0.1714227944612503, 0.09855784475803375, 0.11544407904148102, 0.09486759454011917, 0.09512607008218765, 0.1629505455493927, 0.08176673203706741]
The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al (2008).	[168, 169, 148, 87, 147, 78, 150, 186, 88, 185]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7523725628852844, 0.11439439654350281, 0.4743538498878479, 0.4635263979434967, 0.08119623363018036, 0.09328760951757431, 0.1904689371585846, 0.07727906852960587, 0.14390762150287628, 0.10839066654443741]
The complexity of this dynamic programming algorithm for g-gram decoding is O (2nn2|V|g-1) where n is the sentence length and |V| is the English vocabulary size (Huang and Chiang, 2007).	[6, 7, 39, 15, 40, 30, 32, 100, 25, 34]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2893081605434418, 0.18895015120506287, 0.16010378301143646, 0.08355613797903061, 0.13421040773391724, 0.1778326779603958, 0.13834087550640106, 0.16066506505012512, 0.05609710142016411, 0.05038882791996002]
The complexity of this dynamic programming algorithm for g-gram decoding is O(2nn 2 |V	[129, 90, 179, 136, 215, 169, 31, 141, 143, 144]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8275386095046997, 0.22565452754497528, 0.0907164067029953, 0.07020914554595947, 0.048909567296504974, 0.19276243448257446, 0.05145902931690216, 0.06542952358722687, 0.05667640268802643, 0.06066128984093666]
The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004).	[90, 98, 24, 3, 262, 105, 20, 77, 102, 47]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8009666800498962, 0.7419652938842773, 0.5573647022247314, 0.16114825010299683, 0.40694713592529297, 0.19077357649803162, 0.09519237279891968, 0.05296994000673294, 0.11700300127267838, 0.29966118931770325]
The current state-of-the-art segmentation software developed by (Low et al, 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track.	[1, 131, 133, 117, 135, 8, 0, 122, 47, 115]	[1, 1, 0, 0, 1, 0, 1, 0, 0, 0]	[0.6337470412254333, 0.5479791760444641, 0.16550064086914062, 0.2851060926914215, 0.5633115768432617, 0.2200617492198944, 0.5669491291046143, 0.2684865891933441, 0.14616206288337708, 0.0681440532207489]
The decoding algorithm that we use is a greedy one - see (Germann et al, 2001) for details.	[171, 175, 4, 174, 169, 18, 172, 166, 165, 1]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.804115355014801, 0.5585696697235107, 0.4994041919708252, 0.2833004593849182, 0.12031830847263336, 0.06072252616286278, 0.2524898052215576, 0.31157776713371277, 0.06433343887329102, 0.3440738618373871]
The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states.	[136, 94, 90, 49, 160, 47, 87, 137, 102, 43]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5249528288841248, 0.43833500146865845, 0.28437840938568115, 0.1685623824596405, 0.07600834220647812, 0.11758279800415039, 0.13667447865009308, 0.16154845058918, 0.1866472065448761, 0.06364332139492035]
The difference in noun attachments between these two sets is striking, but (Ratnaparkhi et al, 1994) do not discuss this (and we also do not have an explanation for this).	[17, 77, 1, 9, 47, 16, 11, 103, 22, 5]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.39684438705444336, 0.12720821797847748, 0.08471941202878952, 0.18359382450580597, 0.605742335319519, 0.16893450915813446, 0.06147678196430206, 0.058716464787721634, 0.45701271295547485, 0.4104653000831604]
The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001).	[90, 96, 108, 29, 28, 50, 49, 103, 32, 86]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7141223549842834, 0.7340003848075867, 0.32038360834121704, 0.3922526240348816, 0.6630783081054688, 0.40884268283843994, 0.08467161655426025, 0.0582277737557888, 0.09750764071941376, 0.15105807781219482]
The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004).	[62, 2, 78, 11, 64, 16, 69, 27, 47, 48]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.830345630645752, 0.41291484236717224, 0.41291484236717224, 0.3802512288093567, 0.3470168709754944, 0.17593370378017426, 0.17073239386081696, 0.19164972007274628, 0.09026415646076202, 0.05354538559913635]
The evaluation data comes from the WSI task of SemEval-2007 (Agirre and Soroa, 2007).	[187, 0, 84, 194, 3, 185, 8, 12, 86, 162]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5138511061668396, 0.8226001858711243, 0.3745519816875458, 0.21040363609790802, 0.3031788468360901, 0.1712535321712494, 0.266463965177536, 0.07853080332279205, 0.08989889174699783, 0.24704387784004211]
The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank.	[21, 14, 157, 2, 44, 49, 22, 0, 55, 47]	[1, 1, 1, 0, 0, 0, 0, 1, 0, 0]	[0.8457852602005005, 0.8477275371551514, 0.7407121658325195, 0.273314505815506, 0.1549447625875473, 0.26585108041763306, 0.256101131439209, 0.5688227415084839, 0.21397365629673004, 0.11953846365213394]
The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results.	[0, 185, 187, 194, 3, 84, 193, 1, 18, 159]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8157772421836853, 0.5244970917701721, 0.1793227642774582, 0.16826102137565613, 0.23959827423095703, 0.19616304337978363, 0.2209957093000412, 0.15244576334953308, 0.14329589903354645, 0.22783857583999634]
The example is taken from the parallel corpus of English and Haitian Kre`yol text messages used in the 2010 Shared Task for the Workshop on Machine Translation (Callison-Burch et al, 2011), which is the corpus used for evaluation in this paper.	[204, 7, 1, 6, 8, 10, 57, 15, 242, 13]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7594749331474304, 0.842711329460144, 0.31935104727745056, 0.4221154749393463, 0.13694575428962708, 0.17723771929740906, 0.09598842263221741, 0.13860784471035004, 0.10538560152053833, 0.33928218483924866]
The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995).	[183, 238, 40, 189, 104, 249, 61, 98, 71, 75]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7208023071289062, 0.4041302800178528, 0.32474425435066223, 0.0853731706738472, 0.3776615560054779, 0.23665346205234528, 0.3476789891719818, 0.6959669589996338, 0.12403938919305801, 0.22909899055957794]
The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training.	[43, 4, 16, 5, 93, 88, 45, 36, 20, 18]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7939470410346985, 0.3440421223640442, 0.30012083053588867, 0.2753754258155823, 0.2526734471321106, 0.19646932184696198, 0.25986137986183167, 0.05599968135356903, 0.09382760524749756, 0.12653852999210358]
The extraction of SSRs is similar to the well known phrase extraction algorithm (Och and Ney, 2004).	[237, 109, 119, 103, 112, 115, 133, 134, 41, 114]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5674021244049072, 0.25946512818336487, 0.09023230522871017, 0.18954068422317505, 0.14580245316028595, 0.05989525839686394, 0.10435530543327332, 0.11898572742938995, 0.07606016099452972, 0.05468234047293663]
The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001).	[112, 5, 23, 15, 30, 98, 106, 110, 18, 26]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.646947979927063, 0.34415796399116516, 0.17973557114601135, 0.17501746118068695, 0.08426420390605927, 0.13331398367881775, 0.3744785785675049, 0.2012772560119629, 0.11258943378925323, 0.15543808043003082]
The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively.	[156, 129, 47, 60, 163, 168, 34, 2, 43, 85]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.2689407765865326, 0.2643495798110962, 0.25631818175315857, 0.2721482217311859, 0.28546372056007385, 0.15693537890911102, 0.3255751430988312, 0.14321279525756836, 0.09988566488027573, 0.5506486892700195]
The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009).	[112, 177, 145, 83, 89, 45, 52, 30, 80, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32007017731666565, 0.2630069851875305, 0.3735521733760834, 0.35577642917633057, 0.05850648134946823, 0.05150720849633217, 0.04836335405707359, 0.08389681577682495, 0.2428591102361679, 0.11408431828022003]
The features are listed as follows: Path The path features are similar to the path feature which is designed by (Gildea and Jurafsky, 2002).	[168, 192, 188, 458, 294, 198, 183, 302, 588, 170]	[1, 1, 1, 0, 0, 0, 0, 0, 1, 0]	[0.7328084707260132, 0.5935792326927185, 0.6487680077552795, 0.48447537422180176, 0.4497392177581787, 0.48610758781433105, 0.34602952003479004, 0.4095206558704376, 0.6006035208702087, 0.33238959312438965]
The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008).	[1, 7, 3, 2, 4, 28, 5, 11, 19, 66]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8459381461143494, 0.8453778028488159, 0.7200366258621216, 0.14935940504074097, 0.22494906187057495, 0.2911396324634552, 0.28795525431632996, 0.06262513250112534, 0.12013930827379227, 0.1058473140001297]
The first evaluation is dependency base devaluation same as Riezler et al (2003).	[98, 160, 4, 32, 135, 130, 19, 107, 29, 89]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1097676157951355, 0.07798881828784943, 0.4101727306842804, 0.32566726207733154, 0.07156045734882355, 0.05106858164072037, 0.053546126931905746, 0.25683557987213135, 0.10801254957914352, 0.17620736360549927]
The first metric, which was introduced by Johnson (2002), has been widely reported by researchers investigating gap insertion.	[38, 18, 5, 78, 35, 98, 42, 36, 17, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1549498438835144, 0.07844018191099167, 0.09070339053869247, 0.05480169132351875, 0.21288418769836426, 0.0637507438659668, 0.06560643017292023, 0.09686333686113358, 0.10951462388038635, 0.0653931125998497]
The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al (2009).	[49, 398, 132, 24, 23, 185, 21, 187, 330, 513]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6030187010765076, 0.44373443722724915, 0.30372655391693115, 0.13775403797626495, 0.10800104588270187, 0.31739434599876404, 0.09031281620264053, 0.1517687886953354, 0.3382338285446167, 0.16628436744213104]
The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al (2007) and Choi et al (2006).	[63, 58, 40, 165, 14, 32, 53, 2, 15, 195]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7341292500495911, 0.19228297472000122, 0.5025449395179749, 0.20533254742622375, 0.18574285507202148, 0.37361598014831543, 0.13146330416202545, 0.20053325593471527, 0.19676846265792847, 0.09483220428228378]
The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004).	[11, 4, 134, 49, 15, 37, 0, 22, 31, 95]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35779547691345215, 0.14704662561416626, 0.23811541497707367, 0.0612967349588871, 0.07284168899059296, 0.12836113572120667, 0.3919425904750824, 0.12634128332138062, 0.10848836600780487, 0.2226826399564743]
The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994).	[5, 15, 4, 44, 61, 122, 105, 12, 16, 22]	[0, 1, 0, 0, 1, 0, 1, 0, 0, 1]	[0.25463566184043884, 0.5521557927131653, 0.47215989232063293, 0.14015431702136993, 0.6663259863853455, 0.1961900144815445, 0.6319758296012878, 0.08616054803133011, 0.08951270580291748, 0.5069350600242615]
The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006).	[48, 21, 61, 1, 20, 65, 43, 116, 10, 109]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.502875804901123, 0.6850295662879944, 0.19019803404808044, 0.17093515396118164, 0.32733988761901855, 0.5336173176765442, 0.06422766298055649, 0.17861688137054443, 0.10532540082931519, 0.07694999873638153]
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	[189, 185, 7, 165, 153, 199, 175, 46, 119, 26]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29889827966690063, 0.35395383834838867, 0.25720280408859253, 0.3422083556652069, 0.40431520342826843, 0.260490745306015, 0.35161304473876953, 0.09142044186592102, 0.1983622908592224, 0.20129583775997162]
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006).	[116, 129, 40, 117, 144, 41, 4, 113, 115, 74]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24906885623931885, 0.5685697197914124, 0.11057131737470627, 0.08697423338890076, 0.0816086158156395, 0.2079690843820572, 0.2653568387031555, 0.3942949175834656, 0.23714835941791534, 0.06831829249858856]
The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006).	[53, 63, 130, 32, 33, 49, 164, 62, 64, 181]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4415375590324402, 0.09531588852405548, 0.1049603596329689, 0.20041126012802124, 0.08662871271371841, 0.09209151566028595, 0.0661955326795578, 0.13557970523834229, 0.25394585728645325, 0.07625062018632889]
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009).	[172, 9, 5, 117, 36, 113, 104, 13, 122, 23]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5296111702919006, 0.5718160271644592, 0.09121073782444, 0.052416205406188965, 0.3547403812408447, 0.056716062128543854, 0.0638154149055481, 0.06453269720077515, 0.07746626436710358, 0.05611853301525116]
The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well.	[1, 37, 0, 6, 183, 186, 31, 5, 83, 187]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8101136088371277, 0.14788591861724854, 0.5050948262214661, 0.30240774154663086, 0.285940945148468, 0.08390185236930847, 0.11351585388183594, 0.1687297523021698, 0.22849497199058533, 0.06689175963401794]
The grammar used for this experiment was developed in the ParGram project (Butt et al, 2002).	[5, 172, 1, 186, 31, 178, 22, 14, 25, 189]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8009264469146729, 0.18955807387828827, 0.3502996563911438, 0.14153353869915009, 0.13265059888362885, 0.1955225020647049, 0.14049303531646729, 0.1037970706820488, 0.15643751621246338, 0.07649186998605728]
The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006).	[22, 62, 55, 72, 53, 25, 9, 54, 29, 57]	[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.720687747001648, 0.8195294141769409, 0.3360358476638794, 0.5859482288360596, 0.4422471821308136, 0.23343026638031006, 0.12282546609640121, 0.5894790887832642, 0.0952427089214325, 0.23596128821372986]
The heuristics are based on those used in Gamon (2010) (personal communication).	[26, 20, 40, 43, 9, 10, 11, 25, 121, 29]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4016522169113159, 0.5769650340080261, 0.2082240730524063, 0.3163374066352844, 0.19487757980823517, 0.11893852055072784, 0.08026115596294403, 0.06769231706857681, 0.2066163420677185, 0.2310713678598404]
The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a).	[116, 109, 17, 18, 16, 4, 2, 70, 136, 157]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8437460660934448, 0.33258193731307983, 0.2116296887397766, 0.10317524522542953, 0.1176283210515976, 0.056061357259750366, 0.0779900848865509, 0.07855642586946487, 0.26079612970352173, 0.06436698138713837]
The hypernyms used to label the internal nodes of that hierarchy are chosen in a simple fashion; pattern-matching as in Hearst (1992) is used to identify candidate hypernyms of the words dominated by a particular node, and a simple voting scheme selects the hypernyms to be used.	[93, 170, 12, 29, 182, 148, 59, 24, 104, 157]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.5493021011352539, 0.35392680764198303, 0.3782406151294708, 0.1863088309764862, 0.6040686368942261, 0.42610421776771545, 0.19715248048305511, 0.3799106180667877, 0.05996149778366089, 0.315939337015152]
The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991).	[179, 184, 59, 358, 370, 0, 136, 85, 84, 133]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5585795640945435, 0.6601237058639526, 0.43083456158638, 0.16471077501773834, 0.23926123976707458, 0.39541444182395935, 0.16490386426448822, 0.24506933987140656, 0.2918270528316498, 0.2669501006603241]
The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006).	[95, 3, 88, 104, 100, 16, 9, 96, 53, 63]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7661348581314087, 0.39535319805145264, 0.09847952425479889, 0.4079866111278534, 0.1632593274116516, 0.10394811630249023, 0.39535319805145264, 0.1400873214006424, 0.11953096836805344, 0.062337737530469894]
The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart.	[73, 181, 97, 180, 15, 184, 99, 106, 154, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2316799908876419, 0.09541618078947067, 0.09148883819580078, 0.06939086318016052, 0.12945422530174255, 0.0999741405248642, 0.07677806913852692, 0.09718567878007889, 0.08068054169416428, 0.09418754279613495]
The iterative training algorithm described above is adopted from Klementiev and Roth (2006).	[100, 102, 142, 147, 91, 70, 6, 34, 148, 134]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39411500096321106, 0.2982388436794281, 0.33220118284225464, 0.15208807587623596, 0.20431315898895264, 0.20708587765693665, 0.4626656174659729, 0.08773409575223923, 0.06441137194633484, 0.17692610621452332]
The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004).	[24, 85, 33, 37, 1, 153, 130, 14, 146, 143]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8351063132286072, 0.7889397144317627, 0.37098681926727295, 0.10001769661903381, 0.15457358956336975, 0.3957703709602356, 0.19338972866535187, 0.3188877999782562, 0.20855361223220825, 0.20723512768745422]
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	[50, 29, 18, 58, 66, 24, 61, 57, 1, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30344077944755554, 0.05206305533647537, 0.2091604620218277, 0.18278051912784576, 0.07573393732309341, 0.12116427719593048, 0.16656778752803802, 0.0614088773727417, 0.16887472569942474, 0.16909289360046387]
The latest state-of-the-art statistical dependency parsers are discriminative, meaning that they are based on classifiers trained to score trees, given a sentence, either via factored whole-structure scores (McDonald et al, 2005a) or local parsing decision scores (Hall et al, 2006).	[8, 43, 167, 46, 22, 54, 38, 97, 1, 66]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7933856248855591, 0.3697645962238312, 0.4422757029533386, 0.4636539816856384, 0.3187212646007538, 0.09512656182050705, 0.15161792933940887, 0.23694643378257751, 0.3368302583694458, 0.20273511111736298]
The learning algorithm in Figure 2 is an instance of augmented-loss training (Hall et al, 2011) which is closely related to the constraint driven learning algorithms of Chang et al (2007).	[92, 0, 109, 115, 189, 2, 1, 43, 17, 16]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4781738817691803, 0.607155442237854, 0.18034474551677704, 0.1485867202281952, 0.1699359267950058, 0.18508334457874298, 0.1463676244020462, 0.10647552460432053, 0.06001608073711395, 0.11432872712612152]
The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b).	[30, 23, 35, 52, 56, 21, 31, 46, 113, 29]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.7531994581222534, 0.26699915528297424, 0.234999418258667, 0.12853316962718964, 0.0945923775434494, 0.1765015572309494, 0.15092454850673676, 0.21439413726329803, 0.6524929404258728, 0.22769545018672943]
The lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms (e.g., LTAGS (Schabes et al, 1988) or HPSG (Pollard& amp; Sag, 1994)) is still constrained to declarative notions.	[24, 52, 115, 34, 29, 33, 222, 50, 78, 67]	[1, 0, 0, 0, 1, 1, 0, 0, 0, 0]	[0.5844129323959351, 0.12813031673431396, 0.233107328414917, 0.3478381931781769, 0.5270882844924927, 0.5270745158195496, 0.36413630843162537, 0.1581818014383316, 0.38439080119132996, 0.11939346790313721]
The linguistic component uses the infrastructure and the following resources from GATE (Cunningham et al, 2002): tokenizer, sentence splitter, part-of-speech tagger, morphological analyzer and VPchunker.	[28, 163, 33, 171, 21, 26, 156, 161, 34, 172]	[1, 1, 0, 0, 0, 1, 0, 1, 0, 0]	[0.8427756428718567, 0.8427756428718567, 0.3130805790424347, 0.3632044494152069, 0.24011775851249695, 0.5399354696273804, 0.24011775851249695, 0.5399354696273804, 0.13222742080688477, 0.19999349117279053]
The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources.	[9, 57, 86, 5, 130, 12, 17, 85, 13, 153]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30636003613471985, 0.33293417096138, 0.4867006540298462, 0.09103599190711975, 0.14801357686519623, 0.08166102319955826, 0.06219860911369324, 0.17093144357204437, 0.4375287890434265, 0.09164831042289734]
The method of manually scoring the 11 submitted Chinese system translations of each segment is the same as that used in (Callison-Burch et al, 2007).	[60, 43, 147, 80, 162, 123, 104, 163, 83, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3991447389125824, 0.1890200674533844, 0.17575526237487793, 0.10636996477842331, 0.08172445744276047, 0.11189214885234833, 0.08708749711513519, 0.06215069815516472, 0.10256057977676392, 0.13623805344104767]
The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990).	[1, 21, 98, 99, 8, 20, 18, 3, 2, 36]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7628548741340637, 0.6550440788269043, 0.6968955993652344, 0.18700885772705078, 0.30246326327323914, 0.1732751578092575, 0.07516330480575562, 0.19641560316085815, 0.09736375510692596, 0.06022517755627632]
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	[32, 94, 130, 75, 51, 93, 11, 84, 154, 91]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.06535732746124268, 0.27519023418426514, 0.11310677975416183, 0.3211410939693451, 0.7478775978088379, 0.4080277681350708, 0.09823113679885864, 0.10534300655126572, 0.23131798207759857, 0.05293804407119751]
The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000).	[342, 275, 350, 508, 270, 19, 66, 448, 135, 98]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10100843757390976, 0.12576837837696075, 0.19970133900642395, 0.4618631601333618, 0.1038898155093193, 0.07654475420713425, 0.07654475420713425, 0.14584290981292725, 0.15721875429153442, 0.2713063657283783]
The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information.	[15, 3, 2, 39, 49, 77, 80, 0, 1, 68]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5239664316177368, 0.513408899307251, 0.2241331934928894, 0.1723642498254776, 0.08618903160095215, 0.1484430730342865, 0.07838957756757736, 0.3943510055541992, 0.0913684293627739, 0.15845367312431335]
The modern Bible is tagged using the C & C maximum entropy tagger (Curran and Clark, 2003), and these tags are transferred from source to target through high-confidence alignments aquired from two alignment approaches.	[18, 60, 1, 21, 4, 137, 53, 95, 129, 141]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3107397258281708, 0.26515868306159973, 0.16495507955551147, 0.2831996977329254, 0.09787780046463013, 0.4875018894672394, 0.08306122571229935, 0.08521385490894318, 0.2897396683692932, 0.13591791689395905]
The monologue side has been annotated with discourse relations, using an adaptation of the annotation guidelines of Carlson and Marcu (2001), whereas the dialogue side has been marked up with dialogue acts, using tags inspired by the schemes of Bunt (2000), Carletta et al. (1997) and Core and Allen (1997).	[29, 1, 2, 52, 61, 302, 18, 5, 3, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0747782438993454, 0.08182281255722046, 0.08182281255722046, 0.06325314939022064, 0.09213794767856598, 0.1915111392736435, 0.18920078873634338, 0.2838086187839508, 0.10510138422250748, 0.20220233500003815]
The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work.	[0, 8, 145, 9, 1, 41, 44, 55, 36, 42]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8507311940193176, 0.7203969955444336, 0.3314369022846222, 0.12749002873897552, 0.21742390096187592, 0.18320146203041077, 0.08499584347009659, 0.05670792981982231, 0.07102855294942856, 0.060950297862291336]
The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997].	[20, 29, 50, 33, 498, 520, 30, 21, 177, 23]	[0, 0, 1, 0, 1, 0, 0, 0, 0, 0]	[0.37727487087249756, 0.37972116470336914, 0.6288201808929443, 0.28703737258911133, 0.5206695199012756, 0.19778041541576385, 0.38113272190093994, 0.43652188777923584, 0.19578199088573456, 0.19629687070846558]
The most common method for obtaining the phrase table is heuristic extraction from automatically word-aligned bilingual training data (Och et al, 1999).	[102, 197, 31, 231, 135, 119, 7, 114, 147, 47]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7683150172233582, 0.563557505607605, 0.14086604118347168, 0.14086604118347168, 0.3116511106491089, 0.3157106637954712, 0.06808045506477356, 0.0597066730260849, 0.19156625866889954, 0.09292852133512497]
The most relevant work is by (Haghighi and Vanderwende, 2009) on exploring content models for multi-document summarization.	[0, 1, 10, 138, 9, 8, 135, 6, 26, 110]	[1, 1, 1, 0, 0, 0, 0, 0, 1, 0]	[0.8208576440811157, 0.8287712335586548, 0.6601248979568481, 0.45794105529785156, 0.43224436044692993, 0.2765007019042969, 0.36535194516181946, 0.4289419651031494, 0.5615503191947937, 0.36268720030784607]
The multi-aspect sentiment (MAS) model (Titov and McDonald, 2008a), which is extended from the multi-grain latent Dirichlet allocation (MG-LDA) model (Titov and McDonald, 2008b), allows sentiment text aggregation for sentiment summary of each rating aspect extracted from MG-LDA.	[39, 37, 36, 65, 2, 156, 183, 28, 189, 16]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.844803512096405, 0.7668400406837463, 0.4273628890514374, 0.503218948841095, 0.1836497187614441, 0.25105157494544983, 0.1776096522808075, 0.3043074905872345, 0.33920249342918396, 0.2719888687133789]
The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation.	[12, 0, 13, 1, 16, 2, 6, 3, 8, 4]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5121473670005798, 0.5898158550262451, 0.11116062849760056, 0.36249276995658875, 0.3943861722946167, 0.40129077434539795, 0.18356020748615265, 0.060185763984918594, 0.13170190155506134, 0.24250665307044983]
The objective of this research is to extend previous work in discrimination by (Pedersen and Bruce, 1997), who developed an approach using agglomerative clustering.	[155, 20, 11, 214, 199, 8, 210, 208, 233, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6646469831466675, 0.2863803505897522, 0.24344894289970398, 0.27692651748657227, 0.3560117483139038, 0.12588278949260712, 0.1578570157289505, 0.15126390755176544, 0.059876564890146255, 0.12440430372953415]
The ones denoted with asterisks (*) were not present in (Toutanova et al, 2005).	[165, 14, 156, 74, 58, 32, 83, 17, 31, 82]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.3898965120315552, 0.20742423832416534, 0.09875411540269852, 0.5286011099815369, 0.43935146927833557, 0.06324796378612518, 0.0708664059638977, 0.18330015242099762, 0.07132203876972198, 0.06959466636180878]
The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words.	[105, 27, 113, 30, 1, 99, 22, 85, 18, 38]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6199098825454712, 0.7075617909431458, 0.3069126307964325, 0.3360004127025604, 0.1384349912405014, 0.04942169785499573, 0.06762754172086716, 0.08732388913631439, 0.07753299921751022, 0.2424672394990921]
The only rule-based approach to German LFG-parsing we are aware of is the hand-crafted German grammar in the ParGram Project (Buttet al, 2002).	[26, 21, 5, 23, 31, 1, 18, 25, 137, 179]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6227016448974609, 0.4761165380477905, 0.7313094139099121, 0.2761719226837158, 0.16803580522537231, 0.3721688985824585, 0.19601811468601227, 0.12037375569343567, 0.3521979749202728, 0.09660990536212921]
The original publications on DATR sought to provide the language with (1) a formal theory of inference (Evans and Gazdar, 1989a) and (2) a model-theoretic semantics (Evans and Gazdar, 1989b).	[11, 0, 4, 19, 68, 69, 8, 53, 46, 6]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.839324414730072, 0.778489351272583, 0.1912660449743271, 0.2296089082956314, 0.21273039281368256, 0.12015446275472641, 0.22368471324443817, 0.17334827780723572, 0.12856803834438324, 0.20983754098415375]
The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy (Ratnaparkhi, Reynar, and Roukos, 1994), and Error-Driven TransformationBased Learning (Brill and Resnik, 1994), which were clearly outperformed by both IB1 and IBI-IG, even though e.g. Brill~ Resnik used more elaborate feature sets (words and WordNet classes).	[78, 98, 110, 116, 121, 82, 76, 129, 60, 91]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7104805111885071, 0.4177308976650238, 0.1190924197435379, 0.16271883249282837, 0.10779737681150436, 0.1336393505334854, 0.0944657102227211, 0.1272856593132019, 0.21010197699069977, 0.12240999191999435]
The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a).	[31, 111, 4, 48, 148, 32, 153, 2, 50, 159]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.8320971727371216, 0.37842226028442383, 0.35892510414123535, 0.13524852693080902, 0.14875255525112152, 0.7164011597633362, 0.4151308834552765, 0.1853387951850891, 0.28025907278060913, 0.2563179135322571]
The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008).	[92, 153, 43, 85, 30, 161, 83, 31, 8, 84]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8309409618377686, 0.3090170621871948, 0.45681294798851013, 0.31614604592323303, 0.1986583024263382, 0.2949918508529663, 0.1891215443611145, 0.10879004746675491, 0.11050476133823395, 0.05919297784566879]
The parser only used a subset of CCG, pureCCG (Eisner, 1996), consisting of the Application and Composition rules.	[22, 126, 40, 64, 147, 30, 95, 105, 94, 21]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30944588780403137, 0.5930598378181458, 0.1821395456790924, 0.2735883295536041, 0.2492903470993042, 0.20696476101875305, 0.2836137115955353, 0.2549755871295929, 0.31978628039360046, 0.15603585541248322]
The parser was trained using 8,000 sentences from the GENIA Treebank (Tateisi et al, 2005), which contains abstracts of papers taken from MEDLINE, annotated with syntactic structures.	[3, 22, 19, 18, 82, 21, 14, 74, 2, 26]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7504228353500366, 0.5100027322769165, 0.4502028524875641, 0.24386407434940338, 0.42086589336395264, 0.14843504130840302, 0.10389652848243713, 0.08734039217233658, 0.19352157413959503, 0.11811326444149017]
The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance.	[22, 151, 174, 168, 54, 140, 175, 143, 159, 30]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0963359847664833, 0.1429082304239273, 0.3094577193260193, 0.0940127968788147, 0.12334293127059937, 0.3179803192615509, 0.25426846742630005, 0.2761172354221344, 0.11995889991521835, 0.07056399434804916]
The performance of the supervised one is obtained by the method of Katz and Giesbrecht (2006).	[27, 47, 8, 99, 68, 36, 32, 115, 109, 31]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5849460959434509, 0.41299930214881897, 0.16766557097434998, 0.4257535934448242, 0.1076095774769783, 0.06206996738910675, 0.14682912826538086, 0.06274330615997314, 0.2653237283229828, 0.07972239702939987]
The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English.	[72, 59, 73, 68, 67, 58, 63, 65, 82, 51]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8399784564971924, 0.7850671410560608, 0.4510301947593689, 0.5967847108840942, 0.26169443130493164, 0.44816264510154724, 0.1550462543964386, 0.12349967658519745, 0.12371973693370819, 0.14928767085075378]
The pioneering work in this area was that of Hindle and Rooth (1993).	[231, 234, 66, 15, 238, 239, 97, 242, 226, 193]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8098381161689758, 0.41708528995513916, 0.20185326039791107, 0.32827791571617126, 0.3970862627029419, 0.3826572000980377, 0.4130817949771881, 0.13818784058094025, 0.1627998948097229, 0.09404269605875015]
The prior probability distributions over alignment operations is estimated from data in the Switchboard in a similar manner to Johnson and Charniak (2004).	[88, 79, 96, 82, 102, 80, 42, 27, 29, 104]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.5806730389595032, 0.7745280861854553, 0.6169862747192383, 0.29584792256355286, 0.6957624554634094, 0.3561522662639618, 0.09252862632274628, 0.08233596384525299, 0.15381385385990143, 0.19778773188591003]
The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency.	[157, 121, 29, 101, 131, 132, 99, 112, 98, 163]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5913506150245667, 0.42014676332473755, 0.4232659637928009, 0.11329840123653412, 0.05630885809659958, 0.2385617196559906, 0.10809649527072906, 0.17335307598114014, 0.08140476047992706, 0.12570424377918243]
The problems of non-nominal terms (Klavans and Kan, 1998), term variation (Jacquemin et al, 1997), and relevant contexts (Maynard and Ananiadou, 1998), can be considered for improving the performance.	[145, 28, 22, 27, 20, 143, 1, 84, 32, 137]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4977359175682068, 0.22718185186386108, 0.19436851143836975, 0.11299574375152588, 0.08633491396903992, 0.05365157127380371, 0.11637071520090103, 0.06584726274013519, 0.058508649468421936, 0.062154147773981094]
The pronunciation variation model is used to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary and these variations are used in the spelling correction approach developed by Toutanova and Moore (2002), which uses statistical models of spelling errors that consider both orthography and pronunciation.	[0, 169, 32, 1, 3, 153, 15, 161, 19, 146]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3598717749118805, 0.15360189974308014, 0.3991640508174896, 0.19730809330940247, 0.1315944343805313, 0.22692987322807312, 0.3427392244338989, 0.14968666434288025, 0.09337834268808365, 0.18725360929965973]
The proposed method employs contextual features based on centering theory (Grosz et al, 1983) as well as conventional syntactic and word-based features.	[19, 158, 76, 218, 121, 265, 114, 257, 20, 111]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11837825179100037, 0.10067330300807953, 0.06071283668279648, 0.05690910667181015, 0.1739998757839203, 0.15369103848934174, 0.057204313576221466, 0.05136501044034958, 0.29283276200294495, 0.05944695323705673]
The proposed method in (Takamura et al, 2005) extracts semantic orientations from a small number of seed words with high accuracy in the experiments on English as well as Japanese lexicons.	[4, 190, 17, 188, 28, 138, 18, 1, 11, 156]	[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]	[0.8525456786155701, 0.8394399881362915, 0.5471516251564026, 0.8057669997215271, 0.751775324344635, 0.5663411617279053, 0.3121712803840637, 0.3038850724697113, 0.09406601637601852, 0.42945733666419983]
The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information.	[1, 181, 0, 78, 77, 162, 17, 14, 170, 5]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7103022336959839, 0.6909855008125305, 0.3950295150279999, 0.24762599170207977, 0.10772620141506195, 0.17480425536632538, 0.07214657217264175, 0.12049727886915207, 0.2139391452074051, 0.1594296544790268]
The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004).	[109, 144, 128, 138, 71, 96, 11, 35, 58, 107]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.19761911034584045, 0.4153335988521576, 0.1106950044631958, 0.05842791497707367, 0.12206970900297165, 0.23779504001140594, 0.06326284259557724, 0.5773453116416931, 0.057931721210479736, 0.05389803275465965]
The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst,1991).	[121, 13, 20, 118, 18, 99, 58, 11, 94, 65]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5401480793952942, 0.35141614079475403, 0.256273090839386, 0.13427956402301788, 0.24422238767147064, 0.15661346912384033, 0.11709524691104889, 0.3245774805545807, 0.11905614286661148, 0.07121990621089935]
The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al, 2007) and ACE2008 (e.g. Baron and Freedman, 2008).	[0, 8, 19, 1, 47, 145, 7, 18, 74, 2]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7701585292816162, 0.6375503540039062, 0.3191579282283783, 0.32986539602279663, 0.1367352157831192, 0.14735645055770874, 0.09600084275007248, 0.09939461946487427, 0.07583137601613998, 0.076455257833004]
The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure of about 62% for the same data set.	[109, 123, 36, 131, 121, 150, 115, 141, 116, 147]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6796340942382812, 0.23886841535568237, 0.1610913723707199, 0.1509348303079605, 0.23895379900932312, 0.13609950244426727, 0.2532733678817749, 0.18402430415153503, 0.1933666318655014, 0.13334695994853973]
The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set.	[109, 123, 36, 121, 115, 147, 141, 116, 3, 102]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.657500147819519, 0.25481146574020386, 0.16759249567985535, 0.25028282403945923, 0.2560420036315918, 0.12982894480228424, 0.19329047203063965, 0.19855071604251862, 0.06069080904126167, 0.07648351788520813]
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	[1, 84, 133, 120, 38, 122, 209, 221, 129, 157]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07513539493083954, 0.5156713128089905, 0.08201087266206741, 0.1791205257177353, 0.05882370099425316, 0.1301034837961197, 0.3127155900001526, 0.1640074998140335, 0.0915169045329094, 0.1049884706735611]
The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores.	[131, 154, 122, 157, 65, 117, 160, 153, 152, 121]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7727632522583008, 0.13063696026802063, 0.289120078086853, 0.2801944315433502, 0.3353393077850342, 0.17894791066646576, 0.18669405579566956, 0.06268909573554993, 0.1866951733827591, 0.05820900574326515]
The role of supervision is to permit some constituents to be built but not others (Pereira and Schabes, 1992).	[43, 124, 82, 126, 109, 108, 50, 113, 31, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05238041281700134, 0.12047366052865982, 0.058085039258003235, 0.0651097372174263, 0.06372732669115067, 0.1271645724773407, 0.07492883503437042, 0.29130735993385315, 0.06584134697914124, 0.43994906544685364]
The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression.	[28, 23, 214, 211, 25, 47, 42, 57, 40, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4070422649383545, 0.4110199511051178, 0.06691013276576996, 0.08819355070590973, 0.26928889751434326, 0.1401207000017166, 0.10808265954256058, 0.07778114825487137, 0.07050589472055435, 0.1580025851726532]
The second benchmark that we adopted is the SIGHAN Bakeoff-2005 dataset (Emerson, 2005) for Chinese word segmentation.	[1, 0, 8, 31, 24, 146, 9, 10, 47, 122]	[1, 1, 1, 1, 0, 0, 1, 0, 0, 0]	[0.6018250584602356, 0.8477964997291565, 0.5795982480049133, 0.5925882458686829, 0.22246550023555756, 0.24358238279819489, 0.5230542421340942, 0.162224680185318, 0.16669152677059174, 0.2304222583770752]
The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs.	[26, 178, 3, 33, 16, 19, 43, 54, 29, 2]	[0, 1, 0, 0, 1, 1, 0, 0, 0, 0]	[0.38910582661628723, 0.6354998350143433, 0.47255587577819824, 0.2663222551345825, 0.5688428282737732, 0.6581519842147827, 0.2424800544977188, 0.3644006550312042, 0.14694078266620636, 0.31636136770248413]
The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1.	[0, 30, 31, 1, 63, 26, 94, 32, 52, 202]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.686320424079895, 0.5227431058883667, 0.48733192682266235, 0.2919376790523529, 0.3428502082824707, 0.5380498766899109, 0.33515092730522156, 0.38990020751953125, 0.1611158847808838, 0.1770508885383606]
The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes.	[1, 29, 0, 5, 52, 181, 32, 77, 6, 84]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5862913727760315, 0.8373249769210815, 0.5225074291229248, 0.4135611951351166, 0.3604160249233246, 0.4711835980415344, 0.42922210693359375, 0.13747544586658478, 0.20466133952140808, 0.07996643334627151]
The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al, 2005).	[80, 103, 4, 34, 45, 100, 50, 88, 59, 78]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08660227805376053, 0.17595067620277405, 0.15911343693733215, 0.2726832330226898, 0.0652775764465332, 0.0758945569396019, 0.2117929756641388, 0.14125816524028778, 0.185638427734375, 0.18850070238113403]
The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997).	[2, 75, 29, 3, 41, 181, 74, 48, 37, 0]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7920767664909363, 0.7119171619415283, 0.346531480550766, 0.3852870762348175, 0.46340256929397583, 0.2627149820327759, 0.1792137324810028, 0.08265336602926254, 0.0559554249048233, 0.6019084453582764]
The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy.	[29, 94, 31, 164, 140, 40, 52, 68, 59, 26]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2494337260723114, 0.2391360104084015, 0.4182889759540558, 0.21806998550891876, 0.15767651796340942, 0.17406156659126282, 0.3894790709018707, 0.1344272345304489, 0.12146341055631638, 0.06720715016126633]
The statistical machine translation framework (SMT) formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability: TM LM =argmaxT p (S|T) p (T), (1) where p (S|T) is called a translation model (TM), rep resenting the generation probability from T into S, p (T) is called a language model (LM) and represents the likelihood of the target language (Brown et al, 1993).	[58, 77, 57, 116, 164, 37, 591, 59, 243, 169]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7447145581245422, 0.3128352463245392, 0.14607909321784973, 0.447212815284729, 0.20189721882343292, 0.1444198340177536, 0.1590385138988495, 0.21010743081569672, 0.07463733106851578, 0.3227618634700775]
The surprisal (Hale, 2001) at word wi refers to the negative log probability of wi given the preceding words, computed using the prefix probabilities of the parser.	[24, 20, 92, 21, 2, 111, 102, 38, 42, 61]	[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.7880314588546753, 0.7086119055747986, 0.3415016531944275, 0.6957830190658569, 0.31058603525161743, 0.19063282012939453, 0.4416022300720215, 0.5106696486473083, 0.43904730677604675, 0.239554762840271]
The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004).	[38, 84, 75, 53, 45, 87, 27, 89, 5, 77]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21139565110206604, 0.13198311626911163, 0.1131645068526268, 0.3369027078151703, 0.23210689425468445, 0.0741785541176796, 0.06582298129796982, 0.08921891450881958, 0.05594271793961525, 0.12104427069425583]
The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006).	[75, 93, 36, 25, 220, 64, 34, 147, 228, 0]	[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.45864930748939514, 0.829339325428009, 0.4381735026836395, 0.29191499948501587, 0.10996633023023605, 0.20365314185619354, 0.5952199101448059, 0.1374903917312622, 0.27818378806114197, 0.3814621865749359]
The system of Mani and Wilson (2000) goes further in using separate sets of hand-crafted rules for recognition and normalization and in separating out several disambiguation tasks.	[2, 12, 150, 69, 92, 23, 21, 105, 14, 143]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7987773418426514, 0.6670608520507812, 0.5052724480628967, 0.24801631271839142, 0.28773054480552673, 0.1986718773841858, 0.16144824028015137, 0.13097034394741058, 0.12475214898586273, 0.1571618914604187]
The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003).	[118, 137, 0, 18, 100, 5, 60, 47, 114, 88]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.8512360453605652, 0.6678180694580078, 0.3633876144886017, 0.31799212098121643, 0.23495614528656006, 0.12102442979812622, 0.18804685771465302, 0.3396023213863373, 0.5606560707092285, 0.15895231068134308]
"The task of automatic NP coreference resolution is to determine ""which NPs in a text [...] refer to the same real-world entity"" (Ng, 2010, p. 1396)."	[3, 153, 4, 77, 26, 23, 80, 223, 192, 29]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8415844440460205, 0.7858990430831909, 0.46830400824546814, 0.2901075482368469, 0.25608813762664795, 0.184944286942482, 0.263221800327301, 0.223398819565773, 0.16346532106399536, 0.28830307722091675]
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	[217, 5, 15, 6, 50, 3, 21, 20, 12, 216]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.11849130690097809, 0.19874629378318787, 0.15863245725631714, 0.09212713688611984, 0.08505336940288544, 0.1986980140209198, 0.11276138573884964, 0.05400530621409416, 0.6027951240539551, 0.11222512274980545]
The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al,2007).	[0, 41, 5, 78, 9, 72, 132, 14, 10, 115]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6857098340988159, 0.4904080927371979, 0.6509177684783936, 0.22571636736392975, 0.1934197098016739, 0.12040236592292786, 0.13518738746643066, 0.23533564805984497, 0.11585955321788788, 0.1377398818731308]
The test data for the experiments consisted of 2,000 sentences, and was the same test set as that used by Collins et al (2005).	[24, 127, 125, 145, 181, 148, 143, 150, 154, 158]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7470307946205139, 0.49419790506362915, 0.21471844613552094, 0.4515203535556793, 0.35573938488960266, 0.26009663939476013, 0.7402171492576599, 0.18240661919116974, 0.21802668273448944, 0.23114237189292908]
The text planner is implemented as a Functional Unification Grammar (Kay 1984) in FUF (Elhadad 1993).	[0, 37, 38, 6, 8, 10, 9, 120, 4, 99]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8344280123710632, 0.6772937178611755, 0.41886526346206665, 0.17483040690422058, 0.132670596241951, 0.13275876641273499, 0.12748344242572784, 0.08574909716844559, 0.09556646645069122, 0.07165798544883728]
The texts were POS-tagged using TnT (Brants,2000).	[27, 1, 0, 17, 23, 4, 34, 15, 44, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21926960349082947, 0.26234886050224304, 0.32475632429122925, 0.1511523276567459, 0.15941770374774933, 0.12112521380186081, 0.15289661288261414, 0.20371150970458984, 0.05278783664107323, 0.13282211124897003]
The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003).	[36, 41, 2, 16, 31, 241, 0, 311, 242, 24]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.716561496257782, 0.7216847538948059, 0.1397227793931961, 0.12227199226617813, 0.26861199736595154, 0.052022844552993774, 0.3023407459259033, 0.19858160614967346, 0.06883540004491806, 0.12463363260030746]
The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003).	[1, 7, 11, 12, 2, 228, 17, 8, 45, 27]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.541559636592865, 0.6056612730026245, 0.17537765204906464, 0.2446674108505249, 0.2954600751399994, 0.19971199333667755, 0.4355716407299042, 0.11942664533853531, 0.06616470217704773, 0.372469425201416]
The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm (Collins, 2002a).	[5, 179, 189, 169, 35, 3, 4, 46, 25, 148]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5958315134048462, 0.5111423134803772, 0.10809098184108734, 0.31882622838020325, 0.053096648305654526, 0.4544216990470886, 0.05932624638080597, 0.09130296856164932, 0.4915396571159363, 0.16635477542877197]
The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006).	[101, 10, 30, 25, 112, 143, 2, 138, 102, 106]	[1, 0, 0, 0, 0, 1, 0, 1, 1, 0]	[0.7839182615280151, 0.3640669584274292, 0.26213517785072327, 0.4075261652469635, 0.4442797601222992, 0.7237994074821472, 0.4590694010257721, 0.6252092123031616, 0.7093961238861084, 0.38338008522987366]
The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004).	[52, 42, 0, 12, 121, 122, 132, 131, 84, 71]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.83573317527771, 0.14359596371650696, 0.25963953137397766, 0.27904969453811646, 0.11819985508918762, 0.11409870535135269, 0.15441974997520447, 0.1651548594236374, 0.11843927949666977, 0.3382869362831116]
The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002).	[13, 0, 15, 17, 1, 16, 94, 104, 52, 39]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8161502480506897, 0.6283971071243286, 0.20375467836856842, 0.20961077511310577, 0.2272884100675583, 0.10681303590536118, 0.15716291964054108, 0.24324262142181396, 0.19107018411159515, 0.17646323144435883]
The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001).	[114, 125, 133, 18, 16, 17, 19, 7, 38, 132]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7494953274726868, 0.3087395429611206, 0.09712117165327072, 0.12391303479671478, 0.3502597510814667, 0.11884450167417526, 0.17364981770515442, 0.1885598599910736, 0.09657122939825058, 0.19557325541973114]
The use of vertex facts indicates that the representation is inspired by the Graph approach to referring expression generation [Krahmer et al, 2003].	[10, 0, 1, 5, 4, 8, 19, 18, 69, 98]	[0, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.3957306742668152, 0.8185293674468994, 0.5815382599830627, 0.5815382599830627, 0.17318731546401978, 0.17318731546401978, 0.08146810531616211, 0.10682525485754013, 0.13749927282333374, 0.15203389525413513]
The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004).	[56, 184, 2, 199, 205, 40, 14, 1, 21, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49898800253868103, 0.4827274680137634, 0.12435290217399597, 0.30066344141960144, 0.2603844702243805, 0.17393112182617188, 0.07133685052394867, 0.1710720658302307, 0.04866405203938484, 0.08412449061870575]
The vector is trained using the perceptron algorithm in combination with the averaging method to avoid over fitting; see Freund and Schapire (1999) and Collins and Duffy (2002) for details.	[5, 4, 0, 6, 1, 12, 17, 9, 13, 24]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7413671016693115, 0.1840185970067978, 0.3140445053577423, 0.22211776673793793, 0.23286718130111694, 0.12783145904541016, 0.20521290600299835, 0.07424859702587128, 0.18903429806232452, 0.07761887460947037]
The very interesting study by Snyder and Barzilay (2008) on multilingual approaches to morphological segmentation was difficult to classify.	[3, 0, 12, 65, 61, 40, 69, 45, 44, 15]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8189903497695923, 0.8362985253334045, 0.529316782951355, 0.4725000858306885, 0.40428078174591064, 0.19363558292388916, 0.11340075731277466, 0.07583603262901306, 0.11500293016433716, 0.1428309679031372]
The word segmenter we built is similar to the maximum entropy word segmenter of (Xue and Shen, 2003).	[31, 52, 0, 68, 72, 46, 38, 44, 10, 54]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7155884504318237, 0.657713770866394, 0.23454570770263672, 0.08717869967222214, 0.10887840390205383, 0.22606106102466583, 0.23972152173519135, 0.23800580203533173, 0.15229247510433197, 0.05923868343234062]
The work (Ratinov and Roth, 2009) also combines their system with several document-level features.	[108, 103, 100, 179, 106, 9, 14, 2, 21, 101]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4903598725795746, 0.8249499201774597, 0.19011464715003967, 0.3071015775203705, 0.07638536393642426, 0.07321427017450333, 0.05651349201798439, 0.05381317809224129, 0.05578738451004028, 0.31935861706733704]
The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration.	[97, 53, 23, 191, 52, 149, 167, 45, 101, 176]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.493757963180542, 0.43541935086250305, 0.13137690722942352, 0.17999953031539917, 0.27463287115097046, 0.4293012320995331, 0.2732604444026947, 0.08324025571346283, 0.4462088644504547, 0.1421084702014923]
The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010).	[71, 20, 35, 58, 79, 77, 102, 32, 4, 84]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2784799337387085, 0.37933701276779175, 0.04788818955421448, 0.07060439139604568, 0.05072569474577904, 0.07496774196624756, 0.16306063532829285, 0.052297867834568024, 0.06320054829120636, 0.056501228362321854]
Their final system had 85.0% precision and 91.8% recall on the Ratnaparkhi et al (1994) dataset.	[146, 68, 107, 26, 22, 77, 81, 69, 65, 60]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7359991669654846, 0.41431838274002075, 0.16883663833141327, 0.05847330763936043, 0.11143254488706589, 0.11716938763856888, 0.08559557795524597, 0.13401734828948975, 0.07567865401506424, 0.07545074820518494]
Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS.	[98, 19, 1, 149, 4, 2, 17, 0, 90, 159]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.8299221992492676, 0.5861470103263855, 0.5274877548217773, 0.41042205691337585, 0.5748316049575806, 0.26943492889404297, 0.24952353537082672, 0.4946872591972351, 0.42746591567993164, 0.25866222381591797]
Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system.	[119, 0, 3, 14, 99, 13, 38, 2, 28, 128]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4967639744281769, 0.47430503368377686, 0.4142986536026001, 0.31322160363197327, 0.23611529171466827, 0.21884776651859283, 0.2596067190170288, 0.3106715679168701, 0.08075170964002609, 0.13185246288776398]
Then, we design a novel shallow semantic kernel which is far more efficient and also more accurate than the one proposed in (Moschitti et al, 2007).	[32, 121, 65, 160, 28, 36, 29, 96, 92, 45]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7255915999412537, 0.06689058244228363, 0.1634446084499359, 0.06525146216154099, 0.18126915395259857, 0.055743612349033356, 0.47366365790367126, 0.1639324426651001, 0.15636663138866425, 0.15947644412517548]
Then, we use a back-off schema (Katz, 1987) to deal with the data sparseness problem when estimating the probability P (L) (Gao et al, 2003).	[144, 152, 60, 133, 122, 138, 14, 13, 72, 126]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8312663435935974, 0.525398850440979, 0.5752540826797485, 0.15921548008918762, 0.19877424836158752, 0.3865220546722412, 0.16221633553504944, 0.15678444504737854, 0.18529261648654938, 0.1391027271747589]
Theoretically, it is possible to use these methods to build a translation lexicon from scratch [Rapp, 1995].	[29, 2, 1, 13, 14, 9, 7, 3, 49, 55]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5262311697006226, 0.2588886320590973, 0.10248193889856339, 0.08277668803930283, 0.07576008141040802, 0.07350253313779831, 0.08649761974811554, 0.11240161210298538, 0.07614752650260925, 0.08169928938150406]
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	[2, 234, 0, 241, 8, 254, 248, 25, 86, 64]	[1, 1, 1, 1, 0, 0, 0, 0, 1, 0]	[0.6343284845352173, 0.5298475623130798, 0.8369458913803101, 0.6859819293022156, 0.49618658423423767, 0.1677982360124588, 0.44686439633369446, 0.25450730323791504, 0.5403218269348145, 0.2677548825740814]
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	[25, 19, 53, 334, 168, 31, 22, 6, 121, 52]	[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.5113217234611511, 0.2723497450351715, 0.5147274732589722, 0.5246134400367737, 0.08516500145196915, 0.15158812701702118, 0.12831860780715942, 0.2537345290184021, 0.21259212493896484, 0.10327858477830887]
There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale and Church 1991 and Brown et al 1991), the lexical approach (Key and Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.).	[4, 10, 43, 64, 122, 15, 53, 2, 66, 20]	[1, 0, 0, 0, 0, 0, 1, 0, 1, 0]	[0.767047107219696, 0.22144727408885956, 0.2275746911764145, 0.46090787649154663, 0.24637964367866516, 0.14954379200935364, 0.7948071956634521, 0.20453965663909912, 0.5873361229896545, 0.37605831027030945]
There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993).	[234, 236, 103, 238, 106, 100, 72, 2, 4, 186]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5263393521308899, 0.5803237557411194, 0.3958393335342407, 0.5398145318031311, 0.3407367765903473, 0.4167899191379547, 0.08180979639291763, 0.05641074851155281, 0.05641074851155281, 0.073367640376091]
There are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings: Johnson et al (2007b) describe a MCMC sampler and Cohen et al (2010) describe a Variational Bayes procedure.	[18, 47, 184, 121, 5, 30, 175, 210, 127, 96]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7885169386863708, 0.19713392853736877, 0.26827356219291687, 0.16211773455142975, 0.1822686344385147, 0.20168299973011017, 0.1327192634344101, 0.2566336691379547, 0.13045130670070648, 0.09474145621061325]
There are various data-driven approaches to this NLP-task (Madnani and Dorr, 2010), but they usually focus on lexical paraphrases and do not address the problem of sentence splitting, either.	[2, 0, 3, 8, 117, 7, 45, 20, 72, 498]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2493295520544052, 0.6852275133132935, 0.20462875068187714, 0.20462875068187714, 0.47729507088661194, 0.2493295520544052, 0.08886580169200897, 0.17619189620018005, 0.26971617341041565, 0.16487768292427063]
There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002).	[14, 4, 27, 52, 111, 73, 50, 51, 103, 8]	[1, 1, 0, 0, 0, 1, 0, 1, 0, 0]	[0.7914975881576538, 0.776176393032074, 0.24147267639636993, 0.2745155692100525, 0.43660399317741394, 0.5460190176963806, 0.12824469804763794, 0.5898563861846924, 0.3613530397415161, 0.36260104179382324]
There exist a general weighted determinization and weight pushing algorithms that can be used to create a deterministic and pushed automaton equivalent to an input word or phone lattice (Mohri, 1997).	[546, 41, 198, 447, 28, 413, 595, 440, 454, 494]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6145012378692627, 0.10774605721235275, 0.6517713665962219, 0.17541249096393585, 0.35851672291755676, 0.18174229562282562, 0.21339485049247742, 0.055211178958415985, 0.1294449418783188, 0.3251841366291046]
There is work on lexicon induction using string distance or other phonetic/orthographic comparison techniques, such as Mann and Yarowsky (2001).	[0, 34, 41, 162, 155, 51, 49, 172, 170, 178]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5909647941589355, 0.5865585803985596, 0.2361311912536621, 0.2361311912536621, 0.4297996461391449, 0.42534273862838745, 0.2740369737148285, 0.42534273862838745, 0.2740369737148285, 0.09061818569898605]
There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004).	[235, 96, 28, 22, 102, 35, 48, 60, 287, 58]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09651894867420197, 0.12384147197008133, 0.09712938219308853, 0.0809023305773735, 0.14519304037094116, 0.05618361383676529, 0.18506015837192535, 0.06670079380273819, 0.10042688995599747, 0.06549213826656342]
There was one training set for each French-English, German-English, Italian-English, Spanish-English language combination (Negri et al, 2011).	[5, 33, 53, 172, 54, 86, 93, 74, 20, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20871378481388092, 0.3149052560329437, 0.21754015982151031, 0.1835188865661621, 0.0957208201289177, 0.09982852637767792, 0.14378735423088074, 0.0752808004617691, 0.07782143354415894, 0.10029282420873642]
Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence.	[11, 72, 54, 108, 113, 116, 24, 47, 13, 175]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6405516266822815, 0.09345395863056183, 0.09605420380830765, 0.13408729434013367, 0.2805706858634949, 0.2742016315460205, 0.09404542297124863, 0.08875623345375061, 0.23043043911457062, 0.09534227102994919]
Therefore, several approaches were proposed to filter these phrase-tables, reducing considerably their size without any loss of the quality, or even achieving improved performance (Johnson et al, 2007).	[6, 29, 171, 0, 90, 33, 203, 9, 12, 166]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4215550422668457, 0.6048352718353271, 0.1047464981675148, 0.4702167212963104, 0.32666218280792236, 0.3196292519569397, 0.15162625908851624, 0.10325237363576889, 0.1489669233560562, 0.24825747311115265]
Therefore, van Deemter (2002) has extended the set of descriptors to boolean combinations of attributes, including negations.	[61, 2, 5, 140, 268, 248, 210, 96, 58, 180]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.714156985282898, 0.19318155944347382, 0.19318155944347382, 0.14288711547851562, 0.4516373872756958, 0.1905401647090912, 0.14126048982143402, 0.0772625133395195, 0.372205913066864, 0.09286234527826309]
These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008).	[167, 14, 177, 13, 156, 15, 175, 0, 5, 166]	[0, 1, 0, 0, 1, 0, 0, 1, 0, 0]	[0.4841381311416626, 0.5407785773277283, 0.38034093379974365, 0.19126035273075104, 0.5124478340148926, 0.25100505352020264, 0.37123700976371765, 0.7833338379859924, 0.1846502274274826, 0.07820142805576324]
These are a superset of those targeted in the BioNLP ST 09 and its repeat, the 2011 GE task (Kim et al, 2011b).	[15, 68, 26, 22, 42, 40, 14, 12, 33, 44]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8429686427116394, 0.16781559586524963, 0.25315672159194946, 0.09220010787248611, 0.15830308198928833, 0.15322624146938324, 0.12452074885368347, 0.10394163429737091, 0.08336223661899567, 0.11911345273256302]
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	[44, 159, 84, 20, 21, 30, 105, 31, 23, 122]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.4093676507472992, 0.24526084959506989, 0.0666186735033989, 0.15405744314193726, 0.3597221374511719, 0.0647435411810875, 0.0926785096526146, 0.1622471809387207, 0.5842115879058838, 0.06244708597660065]
These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category.	[104, 5, 36, 155, 44, 19, 27, 171, 2, 38]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.716209352016449, 0.8013516664505005, 0.06150316074490547, 0.27385908365249634, 0.09988491982221603, 0.5381234884262085, 0.2805800139904022, 0.35851168632507324, 0.20113973319530487, 0.060984838753938675]
These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005).	[31, 93, 32, 95, 90, 3, 46, 30, 82, 135]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6310575008392334, 0.5704101920127869, 0.23392421007156372, 0.20728683471679688, 0.27559512853622437, 0.11189570277929306, 0.147184818983078, 0.10843697190284729, 0.07502441108226776, 0.054446086287498474]
These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004).	[0, 5, 25, 176, 27, 132, 175, 22, 4, 8]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7175253033638, 0.22795824706554413, 0.11478180438280106, 0.23207111656665802, 0.20612341165542603, 0.21739524602890015, 0.3750823438167572, 0.2580825090408325, 0.1971488893032074, 0.15700356662273407]
These improvements could make DBMs quick-and easy to bootstrap directly from any available partial bracketings (Pereira and Schabes, 1992), for example capitalized phrases (Spitkovsky et al2012b).	[29, 78, 37, 136, 129, 81, 61, 0, 86, 65]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.30946746468544006, 0.07101058959960938, 0.09529557824134827, 0.1863681674003601, 0.13145720958709717, 0.1259748786687851, 0.2426449954509735, 0.48211222887039185, 0.5033177733421326, 0.15763436257839203]
These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative mod els, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006).	[5, 144, 171, 108, 174, 13, 9, 98, 7, 17]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7592393159866333, 0.64643794298172, 0.13192187249660492, 0.17588649690151215, 0.1482180505990982, 0.3444427251815796, 0.238913431763649, 0.11270496249198914, 0.44336220622062683, 0.11090725660324097]
These strategies can be seen as transactions made up of conversational games (Carletta et al., 1997).	[291, 31, 25, 29, 123, 287, 26, 139, 151, 269]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5381869077682495, 0.5639179944992065, 0.35807904601097107, 0.1790558397769928, 0.15470919013023376, 0.08021551370620728, 0.18303926289081573, 0.2722999155521393, 0.3137848675251007, 0.22316290438175201]
These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al (2008) uses an algorithm to convert leaves in a parse tree to phrases be fore rule extraction.	[154, 127, 17, 33, 2, 136, 117, 159, 129, 100]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.26191553473472595, 0.21783870458602905, 0.08581793308258057, 0.5768583416938782, 0.3322256803512573, 0.1257982850074768, 0.12055933475494385, 0.05299153923988342, 0.07923510670661926, 0.12731024622917175]
They are used in many applications, e.g. word prediction (Bickel et al, 2005), speech recognition (Rabiner and Juang, 1993), machine translation (Brown et al, 1990).	[64, 112, 56, 146, 10, 0, 108, 145, 2, 4]	[1, 1, 0, 1, 1, 1, 0, 0, 0, 0]	[0.6726841330528259, 0.7352256178855896, 0.15685732662677765, 0.7158704996109009, 0.7237133979797363, 0.5239299535751343, 0.056214865297079086, 0.10054808855056763, 0.07828985899686813, 0.07828985899686813]
They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980).	[73, 93, 89, 10, 54, 9, 26, 23, 29, 35]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30162498354911804, 0.09256439656019211, 0.09746954590082169, 0.07479872554540634, 0.10685966163873672, 0.09006291627883911, 0.18024128675460815, 0.18109986186027527, 0.10663089156150818, 0.13614219427108765]
They employed a CCG (Steedman, 2000) or LTAG (Schabes et al, 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features.	[128, 162, 30, 39, 54, 19, 287, 28, 42, 246]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4344305992126465, 0.10124790668487549, 0.41198715567588806, 0.13667862117290497, 0.08518744260072708, 0.10641958564519882, 0.28047266602516174, 0.13393084704875946, 0.1273982673883438, 0.318266361951828]
They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step.	[1, 3, 75, 102, 27, 38, 35, 105, 71, 84]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4468890428543091, 0.5373778343200684, 0.1560005098581314, 0.16310450434684753, 0.187667116522789, 0.20761430263519287, 0.08451269567012787, 0.07506480813026428, 0.056594934314489365, 0.3158073127269745]
They mention that the resulting shallow parse tags are somewhat different than those used by Ramshaw and Marcus (1995), but that they found no significant accuracy differences in training on either set.	[127, 14, 86, 81, 17, 9, 151, 34, 18, 162]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7945762276649475, 0.26928120851516724, 0.15865695476531982, 0.21840254962444305, 0.21107719838619232, 0.07301371544599533, 0.05228583514690399, 0.08556390553712845, 0.13084863126277924, 0.0727134421467781]
They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004).	[32, 97, 194, 175, 100, 8, 109, 27, 145, 184]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.523887038230896, 0.826345682144165, 0.5429831743240356, 0.22572815418243408, 0.16486448049545288, 0.2461308240890503, 0.15713325142860413, 0.11524492502212524, 0.09008307009935379, 0.28116458654403687]
Third, how the algorithm, presented in (Abney, 2002) for binary classification, can be extended to a multi class problem.	[27, 26, 102, 40, 71, 181, 5, 22, 16, 93]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.505384087562561, 0.24001502990722656, 0.11002952605485916, 0.05834336578845978, 0.05945684388279915, 0.12498573958873749, 0.07360068708658218, 0.10217047482728958, 0.11212598532438278, 0.05985238775610924]
Third, the knowledge of textual structure helps to interpret the meaning of entities in a text (Grosz and Sidner 1986).	[333, 379, 211, 103, 173, 79, 215, 89, 536, 113]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4000401198863983, 0.316053181886673, 0.23218555748462677, 0.33209228515625, 0.36409425735473633, 0.12361332774162292, 0.18802976608276367, 0.12390401214361191, 0.14147032797336578, 0.151624858379364]
Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology (Goldsmith, 2001).	[39, 211, 123, 117, 197, 111, 312, 205, 13, 322]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5943470001220703, 0.76577228307724, 0.28411707282066345, 0.2057495266199112, 0.4630643129348755, 0.192910298705101, 0.20755240321159363, 0.20692037045955658, 0.07912854105234146, 0.18439708650112152]
This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008).	[3, 0, 12, 187, 183, 184, 1, 175, 150, 5]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8336536288261414, 0.7036372423171997, 0.09864625334739685, 0.07801377773284912, 0.057127006351947784, 0.20984689891338348, 0.112784743309021, 0.22605156898498535, 0.05934068560600281, 0.10807020217180252]
This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2.	[86, 138, 99, 89, 80, 2, 68, 13, 105, 129]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5680479407310486, 0.18728578090667725, 0.2717495262622833, 0.3003687560558319, 0.13674414157867432, 0.08528610318899155, 0.07958167046308517, 0.07145381718873978, 0.07903988659381866, 0.07614720612764359]
This alternative, which we have yet to try, has the advantage of fitting into the transformation-based error-driven paradigm (Brill and Resnik, 1994) more cleanly than having a translation stage.	[129, 37, 110, 14, 38, 59, 5, 80, 51, 91]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.12584908306598663, 0.17630456387996674, 0.07622018456459045, 0.5324887037277222, 0.20561735332012177, 0.09008602797985077, 0.06801712512969971, 0.09443297982215881, 0.25906991958618164, 0.12551112473011017]
This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)).	[118, 119, 57, 11, 80, 17, 62, 55, 40, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1420895904302597, 0.19814708828926086, 0.16857819259166718, 0.2730545103549957, 0.059625204652547836, 0.07916102558374405, 0.06382746994495392, 0.12014205008745193, 0.05789598822593689, 0.4135349690914154]
This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006).	[184, 16, 7, 105, 43, 3, 34, 50, 117, 4]	[1, 0, 0, 1, 0, 0, 0, 0, 1, 0]	[0.6774832606315613, 0.24975809454917908, 0.2041025459766388, 0.7589603662490845, 0.38377535343170166, 0.49035942554473877, 0.49035942554473877, 0.10961135476827621, 0.5669480562210083, 0.42327508330345154]
This assumes the existence of a separate higher-level process to produce such a representation, following the canonical pipeline architecture of a full generation system (Reiter, 1994).	[41, 6, 5, 30, 70, 47, 96, 77, 28, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49350130558013916, 0.3010729253292084, 0.20915371179580688, 0.22694332897663116, 0.30002647638320923, 0.11464282125234604, 0.0842987522482872, 0.14155791699886322, 0.17048291862010956, 0.14559721946716309]
This avoids segmentation problems encountered by DeNero et al (2006).	[46, 87, 19, 92, 95, 121, 16, 3, 39, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3640032410621643, 0.2836240231990814, 0.34295427799224854, 0.10701677203178406, 0.17389212548732758, 0.15997186303138733, 0.12367364019155502, 0.12587057054042816, 0.1314341425895691, 0.054120030254125595]
This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack.	[36, 128, 9, 6, 122, 129, 139, 126, 23, 114]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7104594707489014, 0.1329413205385208, 0.2868298888206482, 0.10298759490251541, 0.06630508601665497, 0.050902966409921646, 0.15861371159553528, 0.10975999385118484, 0.2613012194633484, 0.08784424513578415]
This behavior appears to be consistent on the test 2007 and nc-test2007 data sets across systems (Callison-Burch et al, 2007).	[23, 33, 35, 83, 30, 6, 34, 93, 113, 32]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3954831063747406, 0.10935752093791962, 0.26600897312164307, 0.25388848781585693, 0.391729474067688, 0.09681706130504608, 0.16085782647132874, 0.1509747952222824, 0.08173307031393051, 0.08194367587566376]
This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided.	[21, 191, 78, 17, 139, 157, 186, 49, 0, 86]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40269720554351807, 0.10474161803722382, 0.08201352506875992, 0.0925825834274292, 0.13091301918029785, 0.3912530541419983, 0.09522862732410431, 0.1286678910255432, 0.2948915362358093, 0.13058842718601227]
This can happen, if we define features that penalize longer phrase pairs, such as lexical weighting, or if we apply smoothing (Foster et al 2006).	[13, 21, 25, 16, 20, 103, 113, 56, 139, 34]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43477267026901245, 0.27397456765174866, 0.12070506811141968, 0.22007662057876587, 0.2765961289405823, 0.24136294424533844, 0.21301016211509705, 0.06232411041855812, 0.2883487045764923, 0.26059404015541077]
This data is used to train a word alignment model, such as IBM Model 1 (Brown et al, 1993) or HMM-based word alignment (Vogel et al, 1996).	[21, 3, 0, 122, 132, 44, 1, 29, 4, 52]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.838364839553833, 0.3004521429538727, 0.3915803134441376, 0.23768556118011475, 0.22703255712985992, 0.5213683247566223, 0.1499824821949005, 0.17672769725322723, 0.15931309759616852, 0.13296842575073242]
This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure.	[134, 14, 0, 7, 11, 99, 78, 10, 30, 24]	[1, 0, 1, 0, 1, 0, 0, 1, 0, 0]	[0.6079376339912415, 0.16798219084739685, 0.6310009956359863, 0.15012075006961823, 0.6448529362678528, 0.1825285702943802, 0.05629478767514229, 0.5301182270050049, 0.05188465118408203, 0.09666404873132706]
This fact prohibits the feeding of the training algorithms with patterns that have the form: (Tagi_2, Tagi_b Tagi, Tagi.~, Manual_Tagi), which is the ease for similar systems that learn POS disambiguation (e.g., Daelemans et al, 1996).	[79, 56, 164, 10, 184, 39, 146, 26, 2, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14045029878616333, 0.06427408009767532, 0.18599888682365417, 0.21202799677848816, 0.15586362779140472, 0.13475829362869263, 0.18549403548240662, 0.40017837285995483, 0.35897448658943176, 0.18422994017601013]
This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of the-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3evaluations.	[8, 71, 66, 70, 46, 62, 78, 12, 110, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42864954471588135, 0.4750779867172241, 0.10261993110179901, 0.37747371196746826, 0.11718564480543137, 0.05592450127005577, 0.21671834588050842, 0.23759405314922333, 0.0879935547709465, 0.08642524480819702]
This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available.	[222, 193, 194, 176, 199, 195, 65, 184, 186, 220]	[1, 1, 1, 1, 1, 0, 1, 0, 0, 0]	[0.8389221429824829, 0.6817594170570374, 0.6087316274642944, 0.6392331719398499, 0.5572517514228821, 0.3123379349708557, 0.5083807706832886, 0.2809531092643738, 0.3230239450931549, 0.2611289918422699]
This finding is consistent with the results of Mann and Yarowsky (2001), although our experiments show more clear-cut differences.	[4, 137, 20, 17, 146, 113, 7, 3, 140, 136]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8175826668739319, 0.8175826668739319, 0.48845216631889343, 0.3172300457954407, 0.3006882071495056, 0.39292222261428833, 0.07231085002422333, 0.21972475945949554, 0.07231085002422333, 0.21972475945949554]
This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efficient O (n2) exact parsing methods for non projective languages like Czech.	[24, 115, 2, 23, 30, 32, 199, 83, 6, 81]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37562260031700134, 0.26073089241981506, 0.1531834602355957, 0.20163603127002716, 0.1452086716890335, 0.08833891153335571, 0.11393270641565323, 0.04980044066905975, 0.07909779995679855, 0.05227576196193695]
This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia).	[110, 605, 32, 128, 36, 34, 87, 129, 81, 346]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6042239665985107, 0.42615216970443726, 0.3876968026161194, 0.2264503389596939, 0.1609920710325241, 0.20257125794887543, 0.30255037546157837, 0.08026960492134094, 0.22399698197841644, 0.20279450714588165]
This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al, 2008).	[32, 221, 233, 127, 235, 5, 174, 144, 172, 33]	[0, 1, 0, 1, 0, 1, 0, 0, 0, 0]	[0.3387756645679474, 0.6343095302581787, 0.10425635427236557, 0.6266743540763855, 0.07774391025304794, 0.7628085017204285, 0.0737822875380516, 0.13721787929534912, 0.05940356105566025, 0.09985271841287613]
This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008).	[113, 13, 14, 5, 10, 56, 52, 114, 3, 57]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7715559601783752, 0.651917576789856, 0.3457905054092407, 0.11721488833427429, 0.6418905258178711, 0.1507357805967331, 0.349830687046051, 0.11065461486577988, 0.10127096623182297, 0.09164325892925262]
This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006).	[20, 123, 140, 21, 135, 40, 4, 19, 119, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34962502121925354, 0.42996665835380554, 0.3108956515789032, 0.188383087515831, 0.23480048775672913, 0.18467804789543152, 0.3084935247898102, 0.2877199649810791, 0.08862802386283875, 0.09494300931692123]
This is a significant improvement over the 0.58 F-measure score reported by Stevenson and Greenwood (2005) for the same task.	[147, 28, 123, 127, 126, 134, 138, 128, 7, 23]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8402537703514099, 0.6938954591751099, 0.5002694725990295, 0.10306864976882935, 0.3516940176486969, 0.1363920122385025, 0.3547985851764679, 0.0977788120508194, 0.3069564700126648, 0.052259672433137894]
This is achieved by adopting the scoring method of Swier and Stevenson (2004), in which we compute the portion Frame of frame slots that can be mapped to an extracted argument, and the portion% Sent of extracted arguments from the sentence that can be mapped to the frame.	[46, 192, 47, 140, 126, 43, 172, 176, 53, 157]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8470150828361511, 0.06250196695327759, 0.22038958966732025, 0.29643118381500244, 0.18826431035995483, 0.21302250027656555, 0.06254973262548447, 0.13223306834697723, 0.07654208689928055, 0.060963224619627]
This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al (2011).	[48, 106, 13, 158, 164, 167, 170, 3, 169, 165]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5104835033416748, 0.5548028349876404, 0.15775105357170105, 0.20852601528167725, 0.3728681802749634, 0.15513506531715393, 0.07016466557979584, 0.15849091112613678, 0.17026102542877197, 0.170835480093956]
This is an example of a collocation ,i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993).	[137, 130, 28, 210, 185, 197, 148, 306, 248, 615]	[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]	[0.7738147377967834, 0.5943599343299866, 0.5280380845069885, 0.2352403849363327, 0.5016717314720154, 0.3693176805973053, 0.7343303561210632, 0.08823134750127792, 0.07059811800718307, 0.15902960300445557]
This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005).	[32, 30, 31, 2, 5, 110, 118, 27, 3, 39]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7334527373313904, 0.5172236561775208, 0.25245019793510437, 0.5701116323471069, 0.2085402011871338, 0.29367125034332275, 0.10013863444328308, 0.08965208381414413, 0.09589898586273193, 0.05634953826665878]
This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set.	[139, 135, 41, 145, 142, 165, 131, 170, 147, 45]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28338199853897095, 0.34124675393104553, 0.24104641377925873, 0.2066214382648468, 0.19995057582855225, 0.1713806539773941, 0.15018214285373688, 0.05531669780611992, 0.40369656682014465, 0.222051203250885]
This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003).	[20, 21, 4, 98, 33, 89, 83, 31, 112, 34]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2010696828365326, 0.37386977672576904, 0.10326606035232544, 0.23239335417747498, 0.4476567208766937, 0.4673013687133789, 0.2702593505382538, 0.24629952013492584, 0.13215455412864685, 0.10157447308301926]
This is due to the fact that, similar to the question of cue phrase polysemy (Hirschberg and Litman 1993), many Chinese discourse markers have both discourse senses and alternate sentential senses in different utterances.	[25, 24, 85, 19, 5, 429, 23, 261, 360, 42]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7609536051750183, 0.6681461334228516, 0.29057350754737854, 0.22217005491256714, 0.29876187443733215, 0.18259544670581818, 0.14247854053974152, 0.12789151072502136, 0.33864766359329224, 0.1827649176120758]
This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task.	[49, 21, 15, 1, 2, 11, 35, 52, 9, 77]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2056538313627243, 0.34218907356262207, 0.23926502466201782, 0.13283148407936096, 0.15440785884857178, 0.1432720273733139, 0.1998286247253418, 0.13679958879947662, 0.1239948570728302, 0.13283148407936096]
This is in line with Mann and Yarowsky (2003)'s modification, consisting in replacing all numbers in the patterns with the symbol ####.	[95, 35, 34, 54, 120, 92, 29, 96, 93, 108]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1305335909128189, 0.27340617775917053, 0.0925370529294014, 0.09418103843927383, 0.08238912373781204, 0.08747440576553345, 0.12330657988786697, 0.2737181484699249, 0.13040797412395477, 0.13734786212444305]
This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings.	[15, 159, 143, 163, 3, 53, 8, 56, 193, 81]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5676926374435425, 0.2679606080055237, 0.1752280294895172, 0.14621494710445404, 0.3060997724533081, 0.0728873685002327, 0.06698745489120483, 0.45758897066116333, 0.19430311024188995, 0.3050791025161743]
This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009).	[129, 31, 152, 54, 179, 130, 137, 181, 49, 41]	[1, 1, 0, 0, 1, 0, 1, 0, 0, 0]	[0.8527382016181946, 0.7761406898498535, 0.0749584212899208, 0.17974472045898438, 0.6681923866271973, 0.06765349209308624, 0.5516147017478943, 0.06890303641557693, 0.2605505585670471, 0.08342447876930237]
This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories.	[74, 59, 69, 77, 169, 75, 82, 78, 70, 3]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.6310218572616577, 0.6297941207885742, 0.7831571102142334, 0.4215325713157654, 0.2800000309944153, 0.5922014117240906, 0.3390870690345764, 0.21503503620624542, 0.14891350269317627, 0.4695362448692322]
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	[36, 124, 80, 99, 92, 196, 93, 106, 97, 69]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3253701329231262, 0.5483827590942383, 0.27602696418762207, 0.47758474946022034, 0.22832083702087402, 0.256273090839386, 0.12311270087957382, 0.1210385113954544, 0.14582975208759308, 0.22828388214111328]
This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007).	[33, 1, 8, 7, 27, 12, 34, 15, 36, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47266826033592224, 0.10853976756334305, 0.09414249658584595, 0.13576000928878784, 0.1260586380958557, 0.061703313142061234, 0.14517177641391754, 0.13769489526748657, 0.0516180656850338, 0.05402199551463127]
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	[17, 155, 18, 36, 107, 56, 93, 156, 39, 4]	[0, 0, 0, 0, 1, 0, 1, 0, 0, 0]	[0.27766090631484985, 0.2673555314540863, 0.09759184718132019, 0.12398836016654968, 0.6063273549079895, 0.245736226439476, 0.7581727504730225, 0.1850225180387497, 0.05241075158119202, 0.06271614134311676]
This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment.	[171, 213, 216, 55, 20, 182, 156, 196, 11, 170]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7443407773971558, 0.5536693930625916, 0.39209315180778503, 0.17414647340774536, 0.13545557856559753, 0.11689829081296921, 0.2324528843164444, 0.3356761336326599, 0.2794555127620697, 0.17930880188941956]
This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007).	[0, 25, 26, 65, 153, 5, 173, 186, 68, 168]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6033738851547241, 0.3401760458946228, 0.40210801362991333, 0.17133189737796783, 0.5915421843528748, 0.22485095262527466, 0.41836535930633545, 0.25782260298728943, 0.1612052321434021, 0.3633601665496826]
This paper considers differences in texts in the well known Penn TreeBank (hereafter, PTB) and in particular, how these differences show up in the Penn Discourse TreeBank (Prasad et al, 2008).	[0, 69, 1, 54, 13, 10, 6, 17, 120, 81]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8456584811210632, 0.38714098930358887, 0.21341797709465027, 0.05817614495754242, 0.2816604673862457, 0.07809551805257797, 0.05947982519865036, 0.07151246815919876, 0.1731640249490738, 0.05850442126393318]
This paper presents experiments in automatic classification of the animacy of unseen Norwegian common nouns, inspired by the method for verb classification presented in Merlo and Stevenson (2001).	[550, 565, 497, 0, 53, 513, 133, 502, 331, 32]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.43456095457077026, 0.2015419453382492, 0.20122511684894562, 0.6979281306266785, 0.2588765025138855, 0.11036539822816849, 0.1860940307378769, 0.053201332688331604, 0.33607709407806396, 0.06371914595365524]
This position was taken by other computational linguists as well (Carletta et al, 1997, p. 25).	[53, 215, 176, 17, 269, 16, 226, 3, 127, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14691780507564545, 0.25879421830177307, 0.07789848744869232, 0.059512678533792496, 0.23107366263866425, 0.160776749253273, 0.1263582706451416, 0.08700370043516159, 0.0692039281129837, 0.07601839303970337]
This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995) .	[35, 87, 88, 22, 63, 364, 410, 140, 382, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06445801258087158, 0.0701715424656868, 0.05295896530151367, 0.05962454527616501, 0.06457187235355377, 0.07133021950721741, 0.05528317019343376, 0.12576459348201752, 0.08948806673288345, 0.24272401630878448]
This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently byWiddows and Dorow (2002).	[138, 40, 48, 47, 65, 66, 106, 69, 43, 8]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.27743926644325256, 0.6007604598999023, 0.5512348413467407, 0.3158745765686035, 0.11101645976305008, 0.08330614119768143, 0.11023377627134323, 0.11815356463193893, 0.29167935252189636, 0.16944633424282074]
This process not only guarantees that the clusters are hierarchical, it also avoids the state drift discussed by Petrov and Klein (2007).	[39, 93, 98, 68, 5, 90, 70, 11, 1, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38955673575401306, 0.06836205720901489, 0.26400455832481384, 0.2723905146121979, 0.22040247917175293, 0.3714505136013031, 0.22975148260593414, 0.09371455013751984, 0.2575201094150543, 0.06182349845767021]
This result is comparable with or better than most measures reported by McCarthy et al (2003).	[37, 47, 77, 111, 38, 132, 27, 22, 99, 43]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33126920461654663, 0.403462290763855, 0.37060078978538513, 0.21488843858242035, 0.07082831859588623, 0.22538025677204132, 0.2560121715068817, 0.2577056586742401, 0.3883969485759735, 0.2926163971424103]
This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work.	[89, 22, 64, 86, 0, 4, 9, 23, 5, 30]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5144067406654358, 0.7089695334434509, 0.27391862869262695, 0.17312392592430115, 0.3588391840457916, 0.1838381290435791, 0.1399693489074707, 0.08286024630069733, 0.08439231663942337, 0.11286119371652603]
This section gives a description of Collins and Roark's incremental parser (Collins and Roark, 2004) and discusses its problem.	[64, 26, 44, 80, 42, 3, 41, 12, 17, 59]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8490046262741089, 0.7581181526184082, 0.48926883935928345, 0.612289547920227, 0.15607373416423798, 0.15394280850887299, 0.22424636781215668, 0.29898738861083984, 0.19080324470996857, 0.11986439675092697]
This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006).	[12, 52, 40, 131, 8, 2, 80, 27, 132, 6]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7541417479515076, 0.7025573253631592, 0.26844069361686707, 0.7401517629623413, 0.2938222587108612, 0.34337204694747925, 0.2255079299211502, 0.1425480991601944, 0.2208772748708725, 0.1503019481897354]
This suggests that a robust model of discourse structure could complement current robust interpretation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Strube and Muller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al (1998)).	[188, 35, 28, 27, 20, 161, 152, 156, 15, 129]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3485983610153198, 0.3087053894996643, 0.08020319789648056, 0.4781411588191986, 0.47442886233329773, 0.4213588833808899, 0.46858611702919006, 0.10312669724225998, 0.08292761445045471, 0.0875387191772461]
This task is a more advanced and realistic version of the Automatic Semantic Role Labeling task of Senseval-3 (Litkowski, 2004).	[0, 1, 2, 14, 17, 74, 20, 78, 40, 23]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8514242172241211, 0.3762575685977936, 0.3259616494178772, 0.11230268329381943, 0.1894029974937439, 0.2202988862991333, 0.12364044040441513, 0.11277813464403152, 0.12734830379486084, 0.08171407133340836]
This type of reasoning has been identified as a core semantic inference paradigm by the generic Textual Entailment framework (Giampiccolo et al, 2007).	[9, 8, 5, 133, 6, 26, 29, 7, 132, 41]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1800667941570282, 0.29642078280448914, 0.10203548520803452, 0.1602478325366974, 0.11024253070354462, 0.13501746952533722, 0.13024212419986725, 0.09412597119808197, 0.10196199268102646, 0.09955817461013794]
This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012).	[3, 32, 17, 10, 29, 42, 56, 251, 0, 78]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.355644553899765, 0.25299546122550964, 0.1958235800266266, 0.12355462461709976, 0.126011922955513, 0.11216415464878082, 0.12418276071548462, 0.12448501586914062, 0.3789086937904358, 0.09948766231536865]
This work further corroborates Kuhlmann's work on Czech (PDT) for Hindi (Kuhlmann and Nivre, 2006).	[143, 144, 42, 140, 19, 6, 28, 18, 14, 125]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6033686399459839, 0.6027085781097412, 0.13947990536689758, 0.1283368617296219, 0.136076420545578, 0.30641084909439087, 0.08339110016822815, 0.06296336650848389, 0.08980697393417358, 0.11985456943511963]
This work was extended in (Rosti et al, 2007) by introducing system weights for word confidences.	[110, 13, 69, 63, 56, 129, 91, 124, 62, 87]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8168701529502869, 0.4623416066169739, 0.30825430154800415, 0.31219252943992615, 0.24546726047992706, 0.20461788773536682, 0.4792432188987732, 0.2800828814506531, 0.27719855308532715, 0.05833412706851959]
Though several pruning algorithms have been raised (Xue and Palmer, 2004), the policies are all in global style. In this paper, a statistical analysis of Penn Prop Bank indicates that arguments are limited in a local syntax sub-tree rather than a whole one.	[46, 6, 10, 25, 7, 18, 38, 20, 5, 22]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4425448775291443, 0.29414308071136475, 0.06134303659200668, 0.06390064209699631, 0.5901613831520081, 0.11368376761674881, 0.09189829230308533, 0.08729097247123718, 0.11468148976564407, 0.08802101761102676]
Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure.	[5, 7, 1, 3, 2, 8, 6, 4, 9, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7582131624221802, 0.3745611608028412, 0.41711533069610596, 0.17194047570228577, 0.35002997517585754, 0.17746439576148987, 0.1966036558151245, 0.14892098307609558, 0.1556655913591385, 0.25034478306770325]
Thus, Brent (1993) only creates hypotheses on the basis of instances of verb frames that are reliably and unambiguously cued by closed class items (such as pronouns) so there can be no other attachment possibilities.	[34, 176, 31, 224, 248, 76, 123, 257, 174, 236]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7198467254638672, 0.11677520722150803, 0.16818732023239136, 0.15329325199127197, 0.36360445618629456, 0.11774425953626633, 0.39106491208076477, 0.11085740476846695, 0.11694248020648956, 0.062396176159381866]
Thus, dynamic programming based sentence alignment algorithms rely on paragraph anchors (Brown et al 1991) or lexical information, such as cognates (Simard 1992), to maintain a high accuracy rate.	[69, 49, 133, 131, 132, 16, 3, 2, 5, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.372586190700531, 0.4715084135532379, 0.22346821427345276, 0.22509485483169556, 0.3398958444595337, 0.19897706806659698, 0.12062469869852066, 0.18919776380062103, 0.08733002841472626, 0.3673161566257477]
Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY.	[183, 175, 95, 87, 166, 101, 173, 111, 176, 172]	[1, 1, 0, 1, 0, 1, 0, 0, 0, 0]	[0.8535007238388062, 0.5023531317710876, 0.3314417004585266, 0.5268042683601379, 0.2318965196609497, 0.5200095772743225, 0.4933473467826843, 0.16370755434036255, 0.21347233653068542, 0.3288058638572693]
Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004].	[12, 8, 7, 18, 1, 28, 15, 17, 31, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3754548132419586, 0.427324116230011, 0.1217145249247551, 0.11260345578193665, 0.08157505840063095, 0.06433890014886856, 0.09311938285827637, 0.0816640630364418, 0.0637914314866066, 0.1001286655664444]
Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001).	[53, 2, 125, 33, 123, 118, 16, 4, 117, 30]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5821448564529419, 0.5432957410812378, 0.23977623879909515, 0.27308008074760437, 0.446975439786911, 0.14282231032848358, 0.28465721011161804, 0.09874888509511948, 0.4070393145084381, 0.07424047589302063]
Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008).	[172, 156, 161, 9, 127, 178, 200, 93, 28, 201]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3380441665649414, 0.28995105624198914, 0.26064780354499817, 0.2593630254268646, 0.2580142617225647, 0.2205982655286789, 0.16589537262916565, 0.24332675337791443, 0.34845006465911865, 0.3471449613571167]
Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance.	[40, 3, 16, 60, 36, 19, 10, 226, 54, 93]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5544265508651733, 0.2742140591144562, 0.20829632878303528, 0.24244140088558197, 0.2943733036518097, 0.38644763827323914, 0.28744617104530334, 0.15917733311653137, 0.10707177966833115, 0.2573395073413849]
Tillmann (2004) used a lexical reordering model, and Galley et al (2004) followed a syntactic-based model.	[33, 4, 34, 14, 7, 107, 15, 39, 40, 94]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4851977527141571, 0.4144360423088074, 0.22632071375846863, 0.42153123021125793, 0.44503074884414673, 0.2957446873188019, 0.09441976994276047, 0.06824909150600433, 0.11648351699113846, 0.19715900719165802]
Tjong Kim Sang and Buchholz (2000) give an overview of the CoNLL shared task of chunking.	[0, 3, 117, 86, 79, 114, 62, 10, 82, 71]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.8374325633049011, 0.5812682509422302, 0.20717360079288483, 0.14859220385551453, 0.17577636241912842, 0.4550570845603943, 0.07941410690546036, 0.15459352731704712, 0.22079522907733917, 0.5641416907310486]
To POS tag, we use the HMM tagger TnT (Brants, 2000) with the model from http: //corpus.leeds.ac.uk/mocky/.	[0, 4, 173, 37, 1, 15, 27, 17, 36, 41]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3463538587093353, 0.1682737171649933, 0.17487378418445587, 0.12249723821878433, 0.3181850016117096, 0.21034349501132965, 0.14198613166809082, 0.11510779708623886, 0.3552708625793457, 0.24923357367515564]
To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges.	[170, 0, 171, 88, 6, 114, 7, 89, 62, 182]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7285251617431641, 0.6506223678588867, 0.381188303232193, 0.5101028084754944, 0.4401215612888336, 0.24375426769256592, 0.10859637707471848, 0.149386465549469, 0.06175391376018524, 0.10012444853782654]
To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004).	[25, 7, 176, 205, 39, 24, 4, 30, 28, 213]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.369324266910553, 0.3037700355052948, 0.20657016336917877, 0.21455742418766022, 0.13184349238872528, 0.12501993775367737, 0.19774940609931946, 0.12046366930007935, 0.08582434058189392, 0.16075041890144348]
To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD).	[162, 270, 156, 146, 169, 63, 12, 190, 8, 44]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5637510418891907, 0.12412908673286438, 0.11675459891557693, 0.12615464627742767, 0.1389070600271225, 0.18273800611495972, 0.24482658505439758, 0.16601620614528656, 0.11887608468532562, 0.1261630356311798]
To compare with previous works on RSTDT, we use the 18 coarse-grained relations defined in (Carlson et al, 2001).	[152, 106, 26, 92, 25, 23, 80, 43, 13, 22]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34960609674453735, 0.44429662823677063, 0.06935235857963562, 0.23545780777931213, 0.15536165237426758, 0.3493729531764984, 0.2300962507724762, 0.2344651073217392, 0.10131552815437317, 0.08259382843971252]
To continue our example, the resulting entry would be as follows: es gibt? S NP there VP is To give a more formal description of how syn tactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al, 2008).	[136, 91, 128, 134, 121, 43, 46, 42, 80, 88]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.764359712600708, 0.7165239453315735, 0.641175389289856, 0.5308302044868469, 0.2648959457874298, 0.27885866165161133, 0.31549370288848877, 0.11923107504844666, 0.16922344267368317, 0.05499615892767906]
To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990).	[26, 5, 46, 45, 9, 227, 236, 158, 145, 219]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2639828622341156, 0.15471845865249634, 0.1547105461359024, 0.10330021381378174, 0.10769584774971008, 0.2448652982711792, 0.3115004301071167, 0.12229347974061966, 0.13306918740272522, 0.08540309220552444]
To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit.	[7, 6, 1, 14, 37, 49, 39, 5, 86, 76]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.714796245098114, 0.8118921518325806, 0.3935328722000122, 0.3885807693004608, 0.31689614057540894, 0.3021237850189209, 0.16082781553268433, 0.13463027775287628, 0.09518338739871979, 0.0576685331761837]
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	[543, 129, 0, 6, 4, 5, 466, 1, 2, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12792789936065674, 0.3023782968521118, 0.2756099998950958, 0.1825636327266693, 0.24699552357196808, 0.17096512019634247, 0.09762953966856003, 0.15105362236499786, 0.13108855485916138, 0.09443692117929459]
To deal with crossing arcs, Titov et al (2009) and Nivre (2009) designed a SWAP transition that switches the position of the two topmost nodes on the stack.	[66, 117, 48, 52, 16, 46, 62, 63, 24, 49]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3629046678543091, 0.3815526068210602, 0.44729408621788025, 0.42858725786209106, 0.11801762878894806, 0.22599638998508453, 0.19960418343544006, 0.19348348677158356, 0.1236054003238678, 0.13181670010089874]
To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)): Heuristic 1: Whenever the first feedback loop requires the addition of the main verb of the question as a query keyword, generate all verb conjugations as well as its nominalizations.	[26, 29, 15, 28, 34, 25, 30, 41, 24, 68]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7598374485969543, 0.7172142863273621, 0.3063289225101471, 0.32105553150177, 0.40069395303726196, 0.6185490489006042, 0.2619251012802124, 0.26374152302742004, 0.22394095361232758, 0.43636852502822876]
To determine entailment, BIUTEE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al, 2005) and coreference resolution.	[116, 15, 142, 128, 62, 103, 61, 123, 145, 157]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4295770823955536, 0.3559213876724243, 0.18228550255298615, 0.3525535464286804, 0.2679879665374756, 0.05772043764591217, 0.18872134387493134, 0.234279602766037, 0.12381178885698318, 0.11138325184583664]
To do this, we use two unsupervised recursive autoencoders (RAE) (Socher et al, 2011b), one for the source phrase and the other for the target phrase.	[39, 112, 24, 220, 26, 37, 59, 156, 71, 10]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.43281352519989014, 0.25023576617240906, 0.7976537942886353, 0.18469002842903137, 0.09785496443510056, 0.1460372656583786, 0.3549668490886688, 0.11272969841957092, 0.05903787165880203, 0.058804381638765335]
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007).	[22, 16, 25, 54, 58, 162, 62, 161, 26, 120]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7503119111061096, 0.15247707068920135, 0.1685992181301117, 0.18347643315792084, 0.30403977632522583, 0.1556900590658188, 0.129185289144516, 0.14534573256969452, 0.19627057015895844, 0.10304957628250122]
To get the context values and implement the syntactic filters, we parsed our corpora with Minipar (Lin,1994).	[57, 196, 35, 1, 66, 88, 8, 74, 21, 20]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7441989779472351, 0.091783307492733, 0.07781532406806946, 0.1395386904478073, 0.26771876215934753, 0.08930081874132156, 0.2510377764701843, 0.5576356649398804, 0.08488280326128006, 0.0790119469165802]
To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002).	[78, 66, 131, 174, 120, 18, 110, 116, 163, 119]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20071230828762054, 0.12332804501056671, 0.19619418680667877, 0.1490531861782074, 0.07729124277830124, 0.12493500858545303, 0.35065537691116333, 0.0524270161986351, 0.08590075373649597, 0.056057222187519073]
To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993).	[105, 106, 93, 73, 47, 90, 46, 69, 146, 35]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.3855069875717163, 0.3423035144805908, 0.3988892734050751, 0.5412691235542297, 0.31736546754837036, 0.3308549225330353, 0.47332796454429626, 0.20822995901107788, 0.189975768327713, 0.11952414363622665]
To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013).	[62, 13, 68, 7, 9, 1, 21, 65, 48, 54]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11009587347507477, 0.585364580154419, 0.35741639137268066, 0.10610730946063995, 0.08081575483083725, 0.05848023667931557, 0.11990925669670105, 0.1829257309436798, 0.06674553453922272, 0.07405757158994675]
To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008).	[220, 268, 51, 129, 329, 276, 174, 337, 304, 314]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20483826100826263, 0.1582246869802475, 0.19195455312728882, 0.11945320665836334, 0.13326308131217957, 0.12884534895420074, 0.13513660430908203, 0.14450234174728394, 0.10559628903865814, 0.09925410151481628]
To quantify this effect, we learn reordering rules using three sets of alignments: HMM alignments, alignments from a supervised MaxEnt aligner (Ittycheriah and Roukos, 2005), and hand alignments.	[190, 185, 76, 213, 216, 196, 168, 155, 170, 70]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4016066789627075, 0.2273910790681839, 0.18044614791870117, 0.24501489102840424, 0.2580260932445526, 0.2556215822696686, 0.29868072271347046, 0.2807168662548065, 0.22134526073932648, 0.2638547122478485]
To reduce the complexity, Zhao et al (2009) reformulated a pruning algorithm introduced by Xue and Palmer (2004) for dependency structure by considering only direct dependents of a predicate and its ancestors as argument candidates.	[52, 49, 20, 53, 50, 60, 46, 38, 28, 55]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19278745353221893, 0.0987432599067688, 0.11414791643619537, 0.2711581289768219, 0.08531355857849121, 0.05530082806944847, 0.05620706081390381, 0.06450953334569931, 0.054202012717723846, 0.048305023461580276]
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model.	[25, 178, 82, 64, 7, 81, 18, 181, 2, 84]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7932195067405701, 0.7211529612541199, 0.2848861515522003, 0.3589148223400116, 0.18525226414203644, 0.5163655877113342, 0.19872604310512543, 0.47438377141952515, 0.18545864522457123, 0.10621491074562073]
To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case).	[12, 3, 42, 139, 60, 138, 102, 16, 1, 65]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26013675332069397, 0.2024492621421814, 0.11728737503290176, 0.2755877673625946, 0.057953380048274994, 0.058838486671447754, 0.16933774948120117, 0.15128463506698608, 0.19864270091056824, 0.05465603992342949]
To test the reliability of the annotation, we first considered the kappa statistic (Siegel and Castellan, 1988) which is used extensively in empirical studies of dis course (Carletta, 1996).	[23, 58, 64, 81, 79, 49, 1, 4, 15, 3]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8356435894966125, 0.113980732858181, 0.3309936821460724, 0.3030897378921509, 0.1640271693468094, 0.4952937662601471, 0.07785174250602722, 0.07785174250602722, 0.2888004183769226, 0.1587754189968109]
Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features.	[94, 71, 42, 128, 105, 151, 145, 101, 118, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17538298666477203, 0.23046346008777618, 0.39975252747535706, 0.09966631233692169, 0.1157040223479271, 0.13182327151298523, 0.144076406955719, 0.09910554438829422, 0.08052258938550949, 0.056518543511629105]
Towards this, Wiebe and Mihalcea (2006) conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement.	[39, 66, 82, 57, 56, 102, 38, 76, 153, 13]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6742594242095947, 0.7865630984306335, 0.5198412537574768, 0.5439254641532898, 0.2852294445037842, 0.33856749534606934, 0.3986862003803253, 0.1335008442401886, 0.23593805730342865, 0.1491689383983612]
Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al, 2008).	[169, 228, 162, 230, 160, 43, 203, 28, 154, 156]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4985620081424713, 0.25602737069129944, 0.14465148746967316, 0.16817016899585724, 0.1016172394156456, 0.17505453526973724, 0.22134441137313843, 0.06238012760877609, 0.15591348707675934, 0.15614977478981018]
Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000).	[97, 68, 19, 47, 46, 73, 70, 117, 30, 134]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6948999166488647, 0.31373512744903564, 0.1694478839635849, 0.5201402306556702, 0.153844952583313, 0.17696894705295563, 0.26442408561706543, 0.06424013525247574, 0.18553970754146576, 0.08990099281072617]
Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al, 2006).	[6, 78, 18, 60, 1, 19, 107, 138, 13, 49]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31070318818092346, 0.12688098847866058, 0.17171350121498108, 0.22088895738124847, 0.13384422659873962, 0.1430068463087082, 0.11855476349592209, 0.10206559300422668, 0.1265985071659088, 0.1635245382785797]
Transformation Rules: In connection with reference to sets, it has been proposed to use the Q-M algorithm (McCluskey,) to find the shortest formula equivalent to a given input formula (van Deemter, 2002).	[251, 149, 191, 144, 49, 121, 50, 142, 117, 77]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4527812600135803, 0.5925197005271912, 0.14689196646213531, 0.46778663992881775, 0.06888829171657562, 0.10154695808887482, 0.10420829802751541, 0.0844154804944992, 0.07887516915798187, 0.13482844829559326]
Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006) .Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time.	[60, 5, 17, 6, 59, 7, 0, 20, 15, 14]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.44177308678627014, 0.4700816571712494, 0.14122802019119263, 0.21134579181671143, 0.1736719161272049, 0.13973228633403778, 0.5101497173309326, 0.13888020813465118, 0.08696598559617996, 0.14997319877147675]
Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al, 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010).	[16, 0, 1, 55, 48, 61, 19, 5, 34, 3]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22437085211277008, 0.5653121471405029, 0.06298237293958664, 0.10715021938085556, 0.21785178780555725, 0.12955939769744873, 0.2564501166343689, 0.16839537024497986, 0.05592671409249306, 0.05306495353579521]
Translation hyper graphs are generated by each baseline system during the MAPde coding phase, and 1000-best lists used for MERT algorithm are extracted from hyper graphs by the k-best parsing algorithm (Huang and Chiang, 2005).	[228, 11, 13, 308, 22, 67, 71, 18, 10, 294]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21372410655021667, 0.28116756677627563, 0.4620159864425659, 0.43852749466896057, 0.18272437155246735, 0.2535794675350189, 0.10824844986200333, 0.14485277235507965, 0.19220836460590363, 0.17906726896762848]
Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning (Wellington et al, 2006).	[6, 27, 7, 35, 4, 0, 55, 1, 179, 23]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8516526222229004, 0.4845421016216278, 0.17158496379852295, 0.3607946038246155, 0.09356117248535156, 0.4728790819644928, 0.05283714085817337, 0.1658019721508026, 0.09964630752801895, 0.19019357860088348]
Turney (2006) used a corpus-based algorithm.	[166, 128, 103, 171, 168, 50, 579, 98, 97, 581]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4301634132862091, 0.39519572257995605, 0.21982499957084656, 0.4314107596874237, 0.6188876628875732, 0.25851505994796753, 0.4365134537220001, 0.15252864360809326, 0.41581809520721436, 0.4068993330001831]
Two datasets were provided by the organization of SemEval 2012 (Negri et al, 2011): a training set and a test set, both composed by a set of 500 pairs of sentences.	[66, 60, 86, 73, 111, 53, 94, 72, 41, 70]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5456713438034058, 0.6400348544120789, 0.21754616498947144, 0.24853332340717316, 0.08034832030534744, 0.15954899787902832, 0.21492233872413635, 0.18953335285186768, 0.06402235478162766, 0.26533666253089905]
Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007).	[4, 48, 15, 16, 13, 2, 58, 17, 92, 14]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8438096642494202, 0.4819294214248657, 0.5550463795661926, 0.08054059743881226, 0.19384698569774628, 0.19438791275024414, 0.2179218977689743, 0.14548254013061523, 0.09555167704820633, 0.19495734572410583]
Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al, 2008a), which are both based on the Wall Street Journal (WSJ) corpus.	[134, 16, 140, 158, 41, 18, 143, 27, 42, 89]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6913511753082275, 0.6904293894767761, 0.1758396476507187, 0.1061570793390274, 0.08031126856803894, 0.1027463749051094, 0.20097610354423523, 0.14371323585510254, 0.12771640717983246, 0.1514846831560135]
Two other interpretations, the Greedy heuristic interpretation (Dale, 1989) and the local brevity interpretation (Reiter, 1990a) lead to algorithms that have polynomial complexity in the same order of magnitude.	[58, 12, 84, 30, 77, 29, 7, 72, 4, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28848958015441895, 0.22734326124191284, 0.07274163514375687, 0.09581837803125381, 0.05542343854904175, 0.1398760825395584, 0.17913620173931122, 0.08348401635885239, 0.23556652665138245, 0.06175817549228668]
USR is the weakly supervised system of Naseem et al (2010).	[27, 47, 39, 197, 189, 135, 44, 24, 8, 46]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7532382607460022, 0.0671577975153923, 0.4446423351764679, 0.11234856396913528, 0.3549612760543823, 0.08641133457422256, 0.18772615492343903, 0.2256925106048584, 0.1512015014886856, 0.056005578488111496]
Ultimately, however, we are interested in performing context sensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011).	[37, 24, 197, 28, 62, 5, 151, 44, 208, 178]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25928011536598206, 0.4872013330459595, 0.3094259798526764, 0.15283311903476715, 0.1297740787267685, 0.226583331823349, 0.07311078906059265, 0.12109806388616562, 0.3855847418308258, 0.16709062457084656]
Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007).	[116, 127, 18, 38, 28, 31, 22, 20, 17, 199]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7226578593254089, 0.39983490109443665, 0.2785347104072571, 0.19513487815856934, 0.17454108595848083, 0.1292773336172104, 0.1368865966796875, 0.11866161227226257, 0.08691410720348358, 0.0731712356209755]
Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al, 2005).	[175, 135, 184, 0, 6, 109, 186, 42, 61, 36]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.47491833567619324, 0.16286523640155792, 0.2232620269060135, 0.6572265028953552, 0.16129948198795319, 0.061143726110458374, 0.10340514779090881, 0.07802096009254456, 0.06145480275154114, 0.12212147563695908]
Unlike (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs.	[64, 158, 70, 211, 106, 88, 27, 80, 189, 63]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5753225088119507, 0.5437313318252563, 0.21171626448631287, 0.45797353982925415, 0.14541217684745789, 0.09882119297981262, 0.15182442963123322, 0.07070787250995636, 0.10235524922609329, 0.08195202052593231]
Unlike the noun-only test sets used in other studies, (e.g., Koehn and Knight (2002), Haghighi et al (2008)), TS1000 also contains adjectives and verbs.	[176, 177, 4, 40, 136, 151, 22, 132, 108, 128]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4517819285392761, 0.10774305462837219, 0.29847753047943115, 0.05748378112912178, 0.05988640710711479, 0.17446644604206085, 0.08877968788146973, 0.051888495683670044, 0.051940564066171646, 0.07793215662240982]
Unlike the shallow phrases defined for the CoNLL-2000 Shared Task (Tjong Kim Sang and Buchholz, 2000), base phrases correspond directly to constituents that appear in full parses, and hence can provide a straightforward constraint on edges within a chart parser.	[0, 3, 79, 86, 114, 62, 117, 51, 108, 57]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8152926564216614, 0.33220189809799194, 0.10276095569133759, 0.10352349281311035, 0.2434464544057846, 0.09012556821107864, 0.0955323874950409, 0.14167162775993347, 0.348793089389801, 0.16582651436328888]
Unsupervised word clustering techniques of Brown et al (1992) and Clark (2000) are well-suited to dependency parsing with the DMV.	[73, 14, 12, 2, 15, 11, 57, 22, 0, 3]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.543472945690155, 0.7928839921951294, 0.5107139945030212, 0.19026437401771545, 0.06381988525390625, 0.3477543890476227, 0.33561718463897705, 0.08203844726085663, 0.4911375045776367, 0.142804354429245]
Using the MUC co reference scoring algorithm (see Vilain et al 1995).	[80, 1, 29, 46, 67, 5, 8, 59, 111, 114]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.712059736251831, 0.18591588735580444, 0.27144181728363037, 0.26319998502731323, 0.04910150170326233, 0.052337903529405594, 0.05560200661420822, 0.27993327379226685, 0.06551425158977509, 0.062476933002471924]
Using the shortest path within the lattice is reported to have better performance in (Dyer et al, 2008), however we did not implement it due to time constraints.	[78, 156, 84, 13, 77, 80, 118, 154, 144, 155]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7572876214981079, 0.2132561206817627, 0.13299493491649628, 0.11619716882705688, 0.3210580050945282, 0.16719387471675873, 0.4333387017250061, 0.06742838025093079, 0.12352386116981506, 0.15714268386363983]
Using these simple, language agnostic measures allows one to look for divergence types such as those described by Dorr (1994).	[19, 134, 23, 17, 25, 349, 273, 270, 4, 47]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5925920605659485, 0.5181267261505127, 0.6032198667526245, 0.386046439409256, 0.3958079516887665, 0.1527286320924759, 0.3356652557849884, 0.2113790512084961, 0.19006365537643433, 0.19396533071994781]
V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness (Rosenberg and Hirschberg, 2007).	[18, 122, 26, 197, 245, 24, 241, 57, 2, 51]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8437455892562866, 0.8314672112464905, 0.4973653554916382, 0.4290526211261749, 0.19569131731987, 0.18894502520561218, 0.4644191861152649, 0.24002374708652496, 0.2548045814037323, 0.2720613479614258]
Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009).	[179, 18, 192, 182, 68, 189, 2, 13, 130, 159]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6493849754333496, 0.4463910758495331, 0.35575222969055176, 0.3935281038284302, 0.26988983154296875, 0.33204513788223267, 0.3509386479854584, 0.176693394780159, 0.05335145443677902, 0.4172018766403198]
Various experiments of the first strategy are performed in (Banea et al,2008) for the subjective analysis task and an average 65 F1 score was reported.	[18, 31, 90, 22, 84, 100, 122, 36, 50, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20133325457572937, 0.07929681241512299, 0.11524026095867157, 0.20083023607730865, 0.11381511390209198, 0.26376578211784363, 0.14992570877075195, 0.10724635422229767, 0.06423258036375046, 0.08318636566400528]
Vilain et al (1995) introduced the link-based MUC evaluation metric for the MUC-6 and MUC 7 coreference tasks.	[1, 9, 14, 3, 17, 102, 94, 93, 97, 62]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11361188441514969, 0.11170319467782974, 0.08739399909973145, 0.09089350700378418, 0.04872995615005493, 0.13190758228302002, 0.050521690398454666, 0.23389968276023865, 0.06447404623031616, 0.049965232610702515]
Violations of CHEAPNESS (Strube and Hahn, 1999), COHERENCE and SALIENCE (Kibble and Power, 2000).	[32, 0, 50, 26, 188, 150, 375, 59, 60, 380]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11497977375984192, 0.3858495354652405, 0.10514413565397263, 0.11064311861991882, 0.3349006175994873, 0.17608577013015747, 0.28121089935302734, 0.0723358690738678, 0.07515444606542587, 0.18863403797149658]
Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese character.	[70, 58, 79, 49, 46, 72, 13, 131, 77, 142]	[0, 0, 1, 0, 1, 0, 0, 0, 0, 0]	[0.49725741147994995, 0.1392369121313095, 0.5108094811439514, 0.16828350722789764, 0.5450976490974426, 0.2215670496225357, 0.1978590339422226, 0.15751373767852783, 0.3587855398654938, 0.15289951860904694]
Walker et al (1994) proposed forward center ranking for Japanese.	[152, 290, 127, 17, 319, 150, 297, 125, 135, 295]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.3310210704803467, 0.2545105814933777, 0.23635324835777283, 0.20473605394363403, 0.5000669956207275, 0.22628645598888397, 0.14063115417957306, 0.30403995513916016, 0.17808318138122559, 0.2179204672574997]
Walker et al (1997) identified three factors which carry an influence on the performance of SDSs, and which therefore are thought to contribute to its quality perceived by the user: agent factors (mainly related to the dialogue and the system itself), task factors (related to how the SDS captures the task it has been developed for) and environmental factors (e.g. factors related to the acoustic environment and the transmission channel).	[15, 190, 155, 29, 2, 120, 118, 19, 189, 156]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7838456034660339, 0.6096704602241516, 0.2810773551464081, 0.4889449179172516, 0.26831260323524475, 0.324541836977005, 0.20160053670406342, 0.34377968311309814, 0.2512070834636688, 0.2575990855693817]
Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer.	[170, 167, 39, 166, 168, 0, 3, 75, 52, 123]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5521491765975952, 0.7353004217147827, 0.15450847148895264, 0.19167783856391907, 0.46582043170928955, 0.3377423584461212, 0.07310471683740616, 0.20970208942890167, 0.39304518699645996, 0.07227754592895508]
We adopt the problem formulation of Merialdo (1994), in which we are given a dictionary of possible tags for each word type.	[37, 73, 72, 27, 77, 17, 38, 14, 89, 1]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7484001517295837, 0.7881064414978027, 0.7603914737701416, 0.33285173773765564, 0.08155001699924469, 0.20807422697544098, 0.09904009103775024, 0.2765410244464874, 0.43119052052497864, 0.07732248306274414]
We adopted the standard data sets used in the First Web People Search Clustering Task (WePS1) (Artiles et al, 2007) and the Second Web People Search Clustering Task (WePS2) (Artiles et al, 2009).	[8, 0, 1, 145, 28, 36, 30, 148, 35, 7]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.5056357383728027, 0.6326953172683716, 0.22767335176467896, 0.15487341582775116, 0.15765085816383362, 0.08312182128429413, 0.5031225085258484, 0.09996392577886581, 0.07097224146127701, 0.15579500794410706]
We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., super tagging for CCGbank (Hockenmaier and Steedman, 2007)).	[66, 398, 5, 10, 330, 422, 65, 84, 32, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14264002442359924, 0.31266599893569946, 0.19094018638134003, 0.19094018638134003, 0.06197588890790939, 0.11159862577915192, 0.08970292657613754, 0.17628416419029236, 0.1248297318816185, 0.08534098416566849]
We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashiet al (2007).	[1, 2, 3, 28, 9, 12, 4, 16, 26, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2116113156080246, 0.12315438687801361, 0.12431979179382324, 0.19581250846385956, 0.13031166791915894, 0.08829422295093536, 0.06906060874462128, 0.09238141030073166, 0.07863659411668777, 0.058971233665943146]
We also compared the confusion matrix from (Carletta et al, 1997) with the confusion matrix we obtained for our best result on MapTask (FLSA using Game+ Speaker).	[240, 202, 307, 210, 161, 239, 281, 299, 231, 228]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4792538285255432, 0.10936667770147324, 0.09646283835172653, 0.08967119455337524, 0.09235087782144547, 0.063447967171669, 0.10699427872896194, 0.06930192559957504, 0.08842195570468903, 0.08323974907398224]
We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz.	[37, 148, 165, 45, 34, 43, 32, 164, 2, 142]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47275295853614807, 0.4240962266921997, 0.28417280316352844, 0.2667635679244995, 0.49737805128097534, 0.21666601300239563, 0.3172384202480316, 0.10279452055692673, 0.09130493551492691, 0.11346809566020966]
We also used lexical features consulting a dictionary: one is to check if any of the above defined character n-grams appear in a dictionary (Peng et al, 2004), and the other is to check if there are any words in the dictionary that start or end at the current character boundary.	[10, 119, 46, 87, 96, 129, 44, 72, 98, 15]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15417858958244324, 0.6206124424934387, 0.09477826952934265, 0.13459230959415436, 0.4587031900882721, 0.07646313309669495, 0.09625595808029175, 0.0519946925342083, 0.12883859872817993, 0.14682266116142273]
We apply one of the pruning techniques used in Zhang and Gildea (2005).	[95, 96, 25, 1, 131, 68, 78, 118, 73, 120]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8502376675605774, 0.6886541247367859, 0.4945973753929138, 0.5212954878807068, 0.42515990138053894, 0.21791893243789673, 0.4192054569721222, 0.2944568991661072, 0.30527496337890625, 0.14633291959762573]
We are investigating the use of principles of the Generative Lexicon (Pustejovsky 1991) for that purpose.	[0, 14, 6, 97, 41, 310, 427, 191, 137, 417]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8353800177574158, 0.2997623085975647, 0.24862153828144073, 0.10934451967477798, 0.10307420790195465, 0.11014977097511292, 0.10949886590242386, 0.4035276472568512, 0.09049636870622635, 0.2249736338853836]
We argue that even though the rules by Wang et al (2007) exist, it is almost impossible to automatically convert their rules into rules that are applicable to dependency parsers.	[93, 55, 3, 29, 81, 48, 49, 36, 72, 31]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34746578335762024, 0.8241466283798218, 0.14649204909801483, 0.09870413690805435, 0.2318219244480133, 0.25970393419265747, 0.14478905498981476, 0.086818166077137, 0.07717732340097427, 0.05286155641078949]
We base our work partly on previous work done by Bagga and Baldwin (Bagga and Baldwin, 1998), which has also been used in later work (Chen and Martin, 2007).	[71, 8, 4, 11, 124, 101, 25, 0, 3, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3260684907436371, 0.11333837360143661, 0.07239822298288345, 0.07239822298288345, 0.058234356343746185, 0.06614254415035248, 0.06365834176540375, 0.32758066058158875, 0.07033278048038483, 0.0671224519610405]
We believe that a more data-driven type generalization that uses distributional similarity (e.g., (Ritter et al 2010)) may help much more.	[10, 34, 72, 176, 47, 98, 151, 84, 141, 44]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6205080151557922, 0.20977383852005005, 0.0836363211274147, 0.0629451647400856, 0.1329345852136612, 0.10363412648439407, 0.06246073916554451, 0.07104301452636719, 0.14719843864440918, 0.07314565777778625]
We believe that this is an aggressive number of senses for a discrimination system to attempt, considering that (Pedersen and Bruce, 1997) experimented with 2 and 3 senses, and (Schutze, 1998) made binary distinctions.	[132, 198, 13, 111, 26, 124, 107, 85, 115, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3054690361022949, 0.38723236322402954, 0.09543847292661667, 0.09040965139865875, 0.08497073501348495, 0.07161244750022888, 0.06229311227798462, 0.10784615576267242, 0.07070747762918472, 0.38920655846595764]
We believe that this relaxation can be done in that particular case, as adjectives are much more likely to convey opinions a priori than verbs (Wiebe et al 2004).	[305, 153, 522, 303, 334, 147, 284, 201, 177, 52]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6076082587242126, 0.0630907341837883, 0.25773799419403076, 0.1706991195678711, 0.15042616426944733, 0.08531066030263901, 0.09905178844928741, 0.0845988392829895, 0.06485649943351746, 0.06762197613716125]
We benchmark our API by performing knowledge based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al, 2007, Coarse-WSD, hence forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks.	[14, 0, 1, 98, 59, 8, 45, 4, 57, 95]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.27891528606414795, 0.6860794425010681, 0.7224892973899841, 0.18379659950733185, 0.1329898238182068, 0.12533865869045258, 0.10989411175251007, 0.1311413198709488, 0.35391440987586975, 0.1321033090353012]
We borrow the terminology and notation of PATR-II (Shieber, 1984), a minimal constraint-based formalism that extends context-free grammar.	[56, 32, 59, 55, 57, 85, 31, 24, 58, 62]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2621500790119171, 0.2212701290845871, 0.2013005167245865, 0.14698903262615204, 0.2810743451118469, 0.11343934386968613, 0.15442445874214172, 0.1265501230955124, 0.19059565663337708, 0.1743677258491516]
We briefly outline our SCF extraction system for automatically extracting SCFs from corpora, which was based on the design proposed in Briscoe and Carroll (1997).	[0, 181, 49, 35, 25, 166, 50, 182, 20, 156]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7878285646438599, 0.5210206508636475, 0.45790261030197144, 0.110283762216568, 0.08078320324420929, 0.08546055853366852, 0.08837496489286423, 0.08837496489286423, 0.07997165620326996, 0.08078320324420929]
We build the space from a Minipar-parsed version of the British National Corpus with dependency parses obtained from Minipar (Lin, 1993).	[33, 70, 115, 37, 68, 190, 53, 107, 202, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7051164507865906, 0.2777554392814636, 0.0859220027923584, 0.04952269047498703, 0.10710543394088745, 0.07652062177658081, 0.050158049911260605, 0.048005539923906326, 0.048224709928035736, 0.6018247008323669]
We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999).	[0, 18, 130, 45, 40, 20, 15, 83, 9, 22]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7939774394035339, 0.5925607681274414, 0.38713425397872925, 0.2938269078731537, 0.32982364296913147, 0.18111640214920044, 0.24807581305503845, 0.12188197672367096, 0.08751019835472107, 0.06843608617782593]
We can use the technique of restriction (Shieber 1985) to remove these features from our feature structures.	[165, 8, 184, 33, 123, 32, 173, 61, 44, 64]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7651537656784058, 0.3397032618522644, 0.3445625305175781, 0.21122238039970398, 0.24194853007793427, 0.1366439312696457, 0.18051181733608246, 0.3166779577732086, 0.09333992004394531, 0.08848883956670761]
We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT.	[174, 160, 169, 163, 168, 189, 24, 127, 28, 172]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6833166480064392, 0.6447035074234009, 0.24950049817562103, 0.4946954548358917, 0.20902806520462036, 0.21930436789989471, 0.19664092361927032, 0.16170385479927063, 0.10015293955802917, 0.17100834846496582]
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (EisensteinandBarzilay, 2008).	[172, 156, 28, 183, 107, 0, 59, 203, 81, 145]	[1, 0, 0, 0, 1, 1, 0, 0, 0, 0]	[0.7071941494941711, 0.26317960023880005, 0.18171481788158417, 0.28120851516723633, 0.6215522885322571, 0.5060098171234131, 0.05645960196852684, 0.18232402205467224, 0.0640890970826149, 0.08561260253190994]
We compare this improved algorithm to our former algorithm (Schone and Jurafsky (2000)) as well as to Goldsmith's Linguistica (2000).	[98, 37, 102, 61, 32, 19, 31, 21, 121, 108]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8443652391433716, 0.23589830100536346, 0.6085413098335266, 0.3149808943271637, 0.42205584049224854, 0.4605429768562317, 0.43625083565711975, 0.1130262091755867, 0.4623969793319702, 0.17370644211769104]
We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system.	[110, 64, 5, 34, 6, 7, 150, 12, 113, 149]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4695330560207367, 0.10698278248310089, 0.07820145785808563, 0.08385391533374786, 0.0801989883184433, 0.09774276614189148, 0.20312707126140594, 0.06281834840774536, 0.124297596514225, 0.05703366547822952]
We conducted two experiments on Penn Treebank II corpus (Marcus et al, 1994).	[150, 10, 24, 3, 7, 69, 0, 20, 35, 17]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8240258693695068, 0.35555052757263184, 0.3347899317741394, 0.1918475329875946, 0.4512307047843933, 0.16337156295776367, 0.24078576266765594, 0.20119521021842957, 0.20119521021842957, 0.28386393189430237]
We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009).	[22, 162, 53, 218, 21, 167, 60, 29, 111, 108]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6720589399337769, 0.3479428291320801, 0.24227754771709442, 0.16236266493797302, 0.13747383654117584, 0.10669020563364029, 0.2305639386177063, 0.40732043981552124, 0.07499945163726807, 0.11734766513109207]
We constructed a 5-gram language model from the provided English News monolingual training data as well as the English side of the parallel corpus using the SRI language modeling toolkit with modified Kneser Ney smoothing (Chen and Goodman, 1996).	[26, 0, 4, 128, 8, 133, 35, 121, 6, 14]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3641684949398041, 0.6218843460083008, 0.13282452523708344, 0.23986616730690002, 0.2659563720226288, 0.15761421620845795, 0.4193488657474518, 0.08495192229747772, 0.13268952071666718, 0.22374635934829712]
We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997).	[28, 11, 114, 72, 195, 5, 22, 2, 192, 191]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46747514605522156, 0.15441562235355377, 0.35116076469421387, 0.12447383254766464, 0.16294410824775696, 0.20344942808151245, 0.14060382544994354, 0.06595126539468765, 0.21945244073867798, 0.11875955760478973]
We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004).	[107, 1, 0, 93, 91, 156, 125, 85, 109, 108]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.23335140943527222, 0.6981973648071289, 0.7798134088516235, 0.2221182882785797, 0.11603667587041855, 0.16582663357257843, 0.20485463738441467, 0.2597048878669739, 0.16192227602005005, 0.2674676179885864]
We describe the Stanford entry to the BioNLP 2011 shared task on biomolecular event extraction (Kim et al, 2011a).	[1, 0, 6, 5, 27, 28, 69, 30, 18, 11]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7752447724342346, 0.8268853425979614, 0.4759611189365387, 0.23044557869434357, 0.1623000204563141, 0.20527590811252594, 0.1417311429977417, 0.155667245388031, 0.10228376090526581, 0.24523258209228516]
We determine closeness using two similarity measures Jiang and Conrath (1997) and Lin (1997) and two relatedness measures Lesk (Banerjee and Pedersen, 2003) and gloss vector overlap (Pedersen et al, 2004) from the Word Net Similarity package.	[63, 101, 62, 60, 48, 20, 67, 171, 100, 64]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4707655906677246, 0.5539257526397705, 0.48344868421554565, 0.23693013191223145, 0.10894668847322464, 0.05551697686314583, 0.1831880658864975, 0.13512258231639862, 0.4422072470188141, 0.06475908309221268]
We developed a set of augmented context free grammar rules for general English syntactic analysis and the analyzer is implemented using Tomita LR parsing algorithm (Tomita, 1987).	[266, 194, 2, 12, 40, 30, 17, 1, 11, 25]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.7818400859832764, 0.5386620163917542, 0.5975548624992371, 0.5975548624992371, 0.7530848383903503, 0.3450392186641693, 0.3945449888706207, 0.2733021676540375, 0.2733021676540375, 0.26137009263038635]
We do not select German, French and other language pairs as they have already been explored by Callison-Burch (2009).	[98, 8, 42, 76, 87, 100, 86, 77, 2, 170]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.44410213828086853, 0.4165564179420471, 0.1612692028284073, 0.14136409759521484, 0.3556855618953705, 0.1834336668252945, 0.19887520372867584, 0.19814284145832062, 0.05579223483800888, 0.06652604788541794]
We draw on and extend the work of Marcu and Echihabi (2002).	[37, 50, 142, 21, 19, 38, 9, 1, 17, 23]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4912117123603821, 0.8009207844734192, 0.25265175104141235, 0.07494035363197327, 0.06312695890665054, 0.21008650958538055, 0.26043590903282166, 0.10296175628900528, 0.09197226166725159, 0.0865238606929779]
We employ the technique of Support Vector Machines (SVMs) (Vapnik, 1998) as the machine learning technique, which has been successfully applied to various natural language processing tasks including chunking tasks such as phrase chunking (Kudo and Matsumoto, 2001) and named entity chunking (Mayfield et al, 2003).	[0, 7, 21, 189, 8, 180, 15, 1, 10, 68]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7335636019706726, 0.7863271236419678, 0.4734852910041809, 0.3821789026260376, 0.23389258980751038, 0.3522182106971741, 0.8089457154273987, 0.46561068296432495, 0.44526413083076477, 0.3610736131668091]
We evaluated our hidden back off model on the ICSI meeting recorder dialog act (MRDA) corpus (Shriberg et al, 2004).	[0, 12, 10, 3, 28, 1, 14, 9, 63, 19]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.8531933426856995, 0.8440585136413574, 0.4079068601131439, 0.20246970653533936, 0.2416009157896042, 0.1876164674758911, 0.6564800143241882, 0.18252015113830566, 0.18192797899246216, 0.1243349015712738]
We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task.	[34, 21, 133, 159, 87, 6, 5, 27, 31, 43]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5282130241394043, 0.19255369901657104, 0.09068453311920166, 0.11525581777095795, 0.17473068833351135, 0.11913030594587326, 0.08215200901031494, 0.0829419195652008, 0.30413416028022766, 0.058821577578783035]
We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways.	[13, 24, 23, 12, 18, 132, 178, 48, 50, 14]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5477563142776489, 0.46296948194503784, 0.2580399215221405, 0.17715919017791748, 0.26213809847831726, 0.1827886998653412, 0.2939806282520294, 0.07405351847410202, 0.06943665444850922, 0.06681407988071442]
We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006).	[2, 0, 56, 23, 4, 72, 121, 112, 9, 202]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3981917202472687, 0.8141450881958008, 0.3954243063926697, 0.25116756558418274, 0.07481767237186432, 0.13725000619888306, 0.08368705958127975, 0.0683700293302536, 0.08396413177251816, 0.20996691286563873]
We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009).	[127, 100, 65, 99, 15, 138, 136, 8, 76, 52]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6513359546661377, 0.2533608078956604, 0.2668670415878296, 0.12550564110279083, 0.1357525736093521, 0.2685868740081787, 0.07555579394102097, 0.0593966580927372, 0.28069958090782166, 0.07566719502210617]
We follow Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately. We also plot the Receiver Operating Character is tic (ROC) curve to gauge performance at a finer level of discrimination (Swets, 1988).	[39, 145, 10, 112, 49, 91, 64, 89, 94, 107]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.435208261013031, 0.07093141227960587, 0.09462251514196396, 0.1837058961391449, 0.11453448235988617, 0.0932779535651207, 0.11524653434753418, 0.08391499519348145, 0.08357439935207367, 0.08844376355409622]
We follow a previous CCG based approach (Gildea and Hockenmaier, 2003) in using a feature to describe the PARG relationship between the two words, if one exists.	[99, 66, 64, 61, 39, 84, 41, 97, 63, 44]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.45644906163215637, 0.2673659026622772, 0.22643379867076874, 0.2607688307762146, 0.19010965526103973, 0.11980743706226349, 0.14211872220039368, 0.08367419242858887, 0.15303091704845428, 0.36127859354019165]
We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task.	[24, 89, 96, 57, 7, 229, 21, 125, 38, 93]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 1]	[0.5458396673202515, 0.727127194404602, 0.2021251618862152, 0.5560440421104431, 0.11804627627134323, 0.06182653084397316, 0.12953364849090576, 0.21811933815479279, 0.10270801186561584, 0.6261953711509705]
We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem.	[15, 162, 14, 88, 158, 21, 164, 43, 31, 123]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8244686126708984, 0.674859344959259, 0.3076816201210022, 0.5212393999099731, 0.2650860548019409, 0.13103102147579193, 0.3922259211540222, 0.06928128004074097, 0.11194124072790146, 0.43942469358444214]
We follow the notation of Di Eugenio and Glass (2004).	[27, 13, 11, 12, 26, 33, 60, 41, 92, 81]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06857136636972427, 0.08371178060770035, 0.06946133822202682, 0.058122605085372925, 0.07521837949752808, 0.07764732837677002, 0.12431409955024719, 0.18480540812015533, 0.09791041910648346, 0.08122910559177399]
We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting.	[42, 0, 37, 20, 34, 4, 2, 161, 21, 32]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31728309392929077, 0.6269488334655762, 0.06212203577160835, 0.37618735432624817, 0.31645816564559937, 0.19263534247875214, 0.16382093727588654, 0.08682425320148468, 0.16382093727588654, 0.14824487268924713]
We have also used a named entity tagger trained specifically on the Twitter data (Ritter et al, 2011) to directly extract named entities from tweets.	[87, 42, 181, 176, 179, 177, 174, 50, 10, 163]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8088980317115784, 0.5421626567840576, 0.38817495107650757, 0.21123602986335754, 0.21808689832687378, 0.2649160325527191, 0.13622355461120605, 0.2218594253063202, 0.4445337653160095, 0.305766224861145]
We have built a corpus of tweets written in English following the procedure described in (Read, 2005) and (Go et al, 2009).	[46, 45, 21, 63, 42, 58, 2, 4, 67, 53]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5188455581665039, 0.5108731985092163, 0.05379094183444977, 0.11132000386714935, 0.4587341845035553, 0.16371692717075348, 0.06185029447078705, 0.08283992111682892, 0.09854716807603836, 0.056104324758052826]
We have explored using different settings for the seed set size (Steedman et al, 2003).	[16, 143, 59, 79, 15, 25, 104, 35, 37, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28084996342658997, 0.36873659491539, 0.06208669766783714, 0.33811378479003906, 0.22420306503772736, 0.10500950366258621, 0.17451536655426025, 0.07155219465494156, 0.060333363711833954, 0.05865433067083359]
We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002).	[54, 115, 44, 15, 114, 22, 116, 85, 3, 132]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5726931095123291, 0.5936513543128967, 0.12588927149772644, 0.15775080025196075, 0.07417041063308716, 0.08554880321025848, 0.06511464715003967, 0.06452302634716034, 0.05828557163476944, 0.07863998413085938]
We have shown in this paper that the approach to transfer between feature structures introduced in Kaplan et al 1989 can be exploited to deal with the translation of anaphoric dependeneins.	[3, 148, 72, 1, 7, 85, 125, 153, 8, 75]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5013986825942993, 0.21640801429748535, 0.3183800280094147, 0.17299282550811768, 0.12362650036811829, 0.06930770725011826, 0.1123390644788742, 0.06883540004491806, 0.07859908789396286, 0.058449313044548035]
We ignored one-to-one alignments included in the PRONALSYL data sets, and instead induced many-to-many alignments using the method of Jiampojamarn et al (2007).	[160, 37, 136, 28, 57, 147, 140, 6, 65, 139]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7477518320083618, 0.18918173015117645, 0.4011932909488678, 0.4166005849838257, 0.45330724120140076, 0.43064531683921814, 0.6640744209289551, 0.15545199811458588, 0.11900736391544342, 0.09042730927467346]
We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on ,e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002).	[1, 0, 50, 8, 200, 11, 148, 225, 157, 145]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3622590899467468, 0.4290712773799896, 0.10334952175617218, 0.09146692603826523, 0.19998162984848022, 0.11651067435741425, 0.102471262216568, 0.12361279875040054, 0.13075995445251465, 0.1113702803850174]
We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C & C tagger (Curran and Clark, 2003).	[88, 118, 137, 60, 5, 28, 0, 106, 12, 18]	[1, 1, 1, 0, 0, 0, 1, 0, 0, 0]	[0.7799732685089111, 0.8073110580444336, 0.7457807660102844, 0.1762641817331314, 0.14910544455051422, 0.09080604463815689, 0.5198619365692139, 0.1177569329738617, 0.45565515756607056, 0.39149612188339233]
We now describe how we build the syntactic relatedness trie (SRT) that forms the scaffolding for the probabilistic models needed to identify sentiment-bearing words via syntactic constraints extracted from a dependency parse (Kubler et al, 2009). We use the Stanford Parser (de Marneffe and Manning, 2008) to produce a dependency graph and con sider the resulting undirected graph structure over words.	[127, 140, 7, 139, 8, 14, 4, 91, 92, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4694260060787201, 0.1853295862674713, 0.22407495975494385, 0.15584710240364075, 0.06477304548025131, 0.20668336749076843, 0.12415365874767303, 0.38971665501594543, 0.10009574145078659, 0.389267235994339]
We now turn to our method of anaphora resolution, which extends the algorithm presented in Strube (1998), in order to be able to account for discourse deictic anaphora as well as individual anaphora.	[71, 165, 117, 121, 173, 68, 142, 129, 150, 172]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5317770838737488, 0.25363150238990784, 0.4382175803184509, 0.3752281367778778, 0.15609467029571533, 0.2501324415206909, 0.2260715216398239, 0.19989025592803955, 0.17800308763980865, 0.2194293886423111]
We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for.	[116, 106, 1, 101, 93, 108, 115, 51, 107, 26]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.405072957277298, 0.30800262093544006, 0.3770720362663269, 0.5761824250221252, 0.1762852668762207, 0.1836525946855545, 0.2962557077407837, 0.2286178469657898, 0.22554711997509003, 0.12204970419406891]
We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009).	[15, 20, 139, 144, 213, 146, 38, 200, 115, 130]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12854473292827606, 0.1741711050271988, 0.10754191130399704, 0.10202034562826157, 0.05987762287259102, 0.11429765075445175, 0.19219771027565002, 0.13244816660881042, 0.12783104181289673, 0.09716103971004486]
We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set.	[101, 90, 100, 83, 87, 30, 73, 123, 98, 102]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5418777465820312, 0.3400866687297821, 0.22426459193229675, 0.08982370048761368, 0.15734757483005524, 0.15577884018421173, 0.2168091982603073, 0.25551503896713257, 0.15276062488555908, 0.11317374557256699]
We parse all German and English articles with BitPar (Schmid, 2004) to extract verb-argument relations.	[9, 5, 4, 25, 24, 12, 0, 3, 1, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1275913119316101, 0.32073426246643066, 0.07203662395477295, 0.10636232793331146, 0.0815468430519104, 0.11982659995555878, 0.4406734108924866, 0.09891816973686218, 0.189072847366333, 0.15924900770187378]
We parsed the BNC corpus with the RASP parser (Briscoe et al, 2006) and used it for feature extraction.	[4, 1, 41, 3, 20, 46, 72, 48, 6, 40]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.45244553685188293, 0.1443997472524643, 0.38140869140625, 0.05535034462809563, 0.2512367367744446, 0.10705365240573883, 0.4433883726596832, 0.056863535195589066, 0.30000513792037964, 0.301281601190567]
We performed an end-to-end evaluation against a database of 15 million facts automatically extracted from general web text (Fader et al, 2011).	[178, 17, 106, 123, 205, 139, 151, 9, 211, 147]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05637013167142868, 0.1432509869337082, 0.08721277862787247, 0.21232230961322784, 0.14677715301513672, 0.06316220760345459, 0.061101511120796204, 0.051927708089351654, 0.055179305374622345, 0.05542948096990585]
We plan to incorporate the results of this study in an extension of (Reiter and Dale, 1992) algorithm that would take into account other types of properties of the objects like visual salience, temporal attributes (for example time elapsed between mentions), if it participated in an action (like the case of a door opening, or a button being pushed) or its importance to the overall task completion.	[161, 156, 0, 110, 23, 21, 166, 107, 168, 171]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5509534478187561, 0.1654118448495865, 0.23172734677791595, 0.29380953311920166, 0.07896863669157028, 0.06011864170432091, 0.1504913866519928, 0.10600849241018295, 0.1624291092157364, 0.12627774477005005]
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results.	[27, 29, 23, 3, 7, 12, 10, 28, 32, 18]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7450912594795227, 0.2461852878332138, 0.18457213044166565, 0.06724166870117188, 0.0625486746430397, 0.13078345358371735, 0.5285637378692627, 0.06724166870117188, 0.0625486746430397, 0.09375167638063431]
We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004).	[2, 97, 171, 50, 30, 33, 25, 62, 152, 34]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5899078845977783, 0.7598319053649902, 0.5299546718597412, 0.31335359811782837, 0.09738478809595108, 0.2325565367937088, 0.1499299257993698, 0.06756355613470078, 0.17150603234767914, 0.4977649748325348]
We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al (2009)).	[100, 33, 23, 69, 75, 71, 31, 30, 34, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6191492676734924, 0.22777824103832245, 0.3279670476913452, 0.2984451353549957, 0.3159833550453186, 0.18332448601722717, 0.1581553965806961, 0.24921219050884247, 0.1822057068347931, 0.08766185492277145]
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task.	[1, 2, 149, 11, 7, 117, 13, 189, 199, 195]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46167466044425964, 0.4730355441570282, 0.1547640711069107, 0.1561582237482071, 0.11945916712284088, 0.1296404004096985, 0.2631933391094208, 0.14456266164779663, 0.27290406823158264, 0.16822265088558197]
We recommend mate-tools (Bjorkelund et al, 2009) and SuperSenseTagger (Ciaramita and Altun, 2006).	[1, 35, 0, 3, 139, 174, 140, 183, 155, 189]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07876703888177872, 0.0625290498137474, 0.3227617144584656, 0.07452619075775146, 0.07318273931741714, 0.06666473299264908, 0.061716023832559586, 0.18798474967479706, 0.06855712831020355, 0.07785061746835709]
We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models.	[18, 136, 2, 39, 13, 45, 4, 21, 40, 43]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39332741498947144, 0.3582877516746521, 0.17891809344291687, 0.34467360377311707, 0.20319822430610657, 0.4403417110443115, 0.205496683716774, 0.06497978419065475, 0.16914699971675873, 0.08486027270555496]
We refer the reader to (Yamada and Knight, 2001) for more details.	[15, 33, 37, 39, 38, 56, 1, 51, 35, 144]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6403352618217468, 0.5102660655975342, 0.11626443266868591, 0.1439766138792038, 0.09782465547323227, 0.12523633241653442, 0.1104409322142601, 0.08403120189905167, 0.20191821455955505, 0.13919101655483246]
We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error.	[109, 108, 51, 107, 21, 86, 50, 84, 87, 43]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8457298874855042, 0.727347731590271, 0.8014167547225952, 0.393492192029953, 0.35160914063453674, 0.18192024528980255, 0.27965736389160156, 0.2740117013454437, 0.16796323657035828, 0.17081940174102783]
We refer to this method as stacking, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001).	[44, 175, 130, 53, 127, 19, 3, 9, 110, 474]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.775538444519043, 0.3442715108394623, 0.23864281177520752, 0.23930248618125916, 0.1293739229440689, 0.1585199385881424, 0.37397754192352295, 0.37397754192352295, 0.45766231417655945, 0.08878572285175323]
We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data.	[73, 8, 128, 4, 101, 58, 88, 36, 81, 120]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3511489927768707, 0.6055800914764404, 0.18260499835014343, 0.3555945158004761, 0.4628041386604309, 0.14140000939369202, 0.3052902817726135, 0.24245893955230713, 0.1342329978942871, 0.2725317180156708]
We report in the following tables the MUC score (Vilain et al, 1995).	[32, 3, 24, 28, 13, 1, 80, 26, 21, 17]	[0, 0, 1, 0, 0, 0, 1, 0, 0, 0]	[0.4722253978252411, 0.24592436850070953, 0.5083221197128296, 0.43161648511886597, 0.33832207322120667, 0.1318347454071045, 0.7560476064682007, 0.11026935279369354, 0.3939148187637329, 0.058984361588954926]
We rescore the ASR N -best lists with the standard HMM (Vogel et al, 1996) and IBM (Brown et al, 1993) MT models.	[405, 350, 170, 219, 89, 88, 218, 428, 47, 243]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24521124362945557, 0.08936645835638046, 0.09097073972225189, 0.09965383261442184, 0.1272551268339157, 0.1141037568449974, 0.057428691536188126, 0.07358665764331818, 0.06745477020740509, 0.07801249623298645]
We say a schema is a textual schema if it has been extracted from free text, such as the Nell (Carlson et al, 2010) and ReVerb (Fader et al, 2011) extracted databases.	[12, 197, 15, 178, 35, 140, 103, 192, 42, 13]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.789111852645874, 0.5704425573348999, 0.19867342710494995, 0.10968130826950073, 0.30089327692985535, 0.100980244576931, 0.07457801699638367, 0.45783743262290955, 0.2164357751607895, 0.39418140053749084]
We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007).	[55, 169, 3, 17, 6, 24, 131, 53, 56, 0]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.0914938747882843, 0.05935181677341461, 0.12497937679290771, 0.5041130781173706, 0.09465102106332779, 0.057622283697128296, 0.06390081346035004, 0.055895619094371796, 0.0939009040594101, 0.24774417281150818]
We should emphasize that this is due to the difficulty of our test data, and that we have tested against a baseline that has been shown to outperform Knight and Graehl (1998).	[64, 35, 178, 74, 191, 180, 60, 41, 57, 139]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2787635624408722, 0.09710488468408585, 0.1776687353849411, 0.07531614601612091, 0.05908006802201271, 0.14073459804058075, 0.06632502377033234, 0.06976530700922012, 0.06748221814632416, 0.06295391917228699]
We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al, 2006), a document collection that has been used by a variety of authors to evaluate perspective classification.	[135, 124, 66, 68, 34, 139, 99, 105, 49, 94]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34771454334259033, 0.5530316829681396, 0.23901799321174622, 0.39133548736572266, 0.3052307963371277, 0.2893999516963959, 0.4668787121772766, 0.21992669999599457, 0.2275756299495697, 0.15249153971672058]
We stand in a marked contrast to previous 'grafting' approaches which more or less rely on an ad-hoc collection of transformation rules to generate candidates (Riezler et al, 2003).	[26, 141, 13, 142, 16, 110, 45, 47, 159, 165]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38510698080062866, 0.06562826037406921, 0.28667935729026794, 0.05305223912000656, 0.18340083956718445, 0.05657201632857323, 0.05442233756184578, 0.05198304355144501, 0.05486765503883362, 0.06175634264945984]
We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007).	[14, 0, 19, 20, 174, 170, 4, 79, 29, 128]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5591918230056763, 0.5657116174697876, 0.26861777901649475, 0.2866267263889313, 0.2600037753582001, 0.22472144663333893, 0.2692579925060272, 0.10772573202848434, 0.1078190878033638, 0.11498890072107315]
We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009).	[114, 64, 206, 86, 69, 68, 127, 182, 23, 66]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7395859956741333, 0.30851009488105774, 0.4394426941871643, 0.22158661484718323, 0.25139766931533813, 0.09823071211576462, 0.17731082439422607, 0.14100493490695953, 0.3537810742855072, 0.06927704811096191]
We suspect that identities of punctuation marks (Collins, 2003, Footnote 13) - both sentence-final and sentence-initial - could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth.	[222, 64, 218, 234, 130, 72, 254, 446, 18, 201]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7987059354782104, 0.4378325939178467, 0.5018066763877869, 0.453983336687088, 0.244299054145813, 0.1106128990650177, 0.08444061875343323, 0.1020689308643341, 0.11721868813037872, 0.1633484810590744]
We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons: It provides a strong baseline, which ensures the validity of the improvement we would obtain.	[12, 190, 174, 3, 137, 188, 206, 9, 15, 13]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6823512315750122, 0.6031181216239929, 0.7187082767486572, 0.4057237505912781, 0.15137746930122375, 0.18682944774627686, 0.23409466445446014, 0.13383212685585022, 0.24090442061424255, 0.4335779547691345]
We take as our baseline system the work by [Charniak and Johnson 2001].	[104, 13, 12, 73, 116, 114, 53, 174, 185, 173]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42504435777664185, 0.06452838331460953, 0.48274508118629456, 0.28022313117980957, 0.4425884783267975, 0.07584576308727264, 0.07239415496587753, 0.3492904305458069, 0.10018159449100494, 0.2458999752998352]
We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events.	[7, 10, 3, 101, 81, 111, 86, 107, 2, 114]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7827540636062622, 0.2787366807460785, 0.3167511820793152, 0.1766897588968277, 0.44898703694343567, 0.5377782583236694, 0.2532339096069336, 0.16450689733028412, 0.08068876713514328, 0.3789825737476349]
We then describe a novel CCG analysis of NP predicate argument structure, which we implement usingNomBank (Meyers et al, 2004).	[156, 163, 35, 57, 178, 177, 1, 45, 6, 148]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8221968412399292, 0.8383925557136536, 0.2575940489768982, 0.25061866641044617, 0.19250258803367615, 0.47099506855010986, 0.24920834600925446, 0.09450126439332962, 0.2406816929578781, 0.061191923916339874]
We then extract relation instances from each parse and apply the greedy inference algorithm from Hoffmann et al, (2011) to identify the best set of parses that satisfy the distant supervision constraint.	[4, 164, 100, 82, 3, 163, 27, 123, 56, 50]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6194016337394714, 0.32320743799209595, 0.10910280048847198, 0.11135045439004898, 0.1512656807899475, 0.06367354840040207, 0.10547135770320892, 0.24169516563415527, 0.1049153208732605, 0.13516783714294434]
We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text.	[57, 66, 65, 4, 85, 37, 46, 125, 39, 118]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.764165461063385, 0.10808651149272919, 0.15747608244419098, 0.12610867619514465, 0.12610867619514465, 0.08725165575742722, 0.08852211385965347, 0.08852211385965347, 0.0699743703007698, 0.0699743703007698]
We tokenize each tweet with Twitter NLP (Gimpel et al, 2011), remove the @ user and URLs of each tweet, and filter the tweets that are too short (< 7 words).	[30, 9, 85, 40, 60, 22, 97, 96, 24, 19]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7016202211380005, 0.22852487862110138, 0.05635521933436394, 0.23259180784225464, 0.05748314782977104, 0.33401069045066833, 0.20098568499088287, 0.1426008641719818, 0.1878589242696762, 0.4477200508117676]
We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences.	[4, 14, 32, 2, 3, 12, 38, 19, 28, 27]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5133572816848755, 0.4892979860305786, 0.08126730471849442, 0.12217047810554504, 0.11913252621889114, 0.06302783638238907, 0.09878577291965485, 0.2580545246601105, 0.14027215540409088, 0.21296963095664978]
We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system.	[13, 309, 33, 30, 41, 42, 297, 114, 188, 61]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37186098098754883, 0.15268991887569427, 0.15030552446842194, 0.08250287920236588, 0.1351333111524582, 0.0844370648264885, 0.09370721876621246, 0.25434327125549316, 0.05829004570841789, 0.08504960685968399]
We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively.	[87, 308, 174, 317, 226, 126, 125, 149, 234, 130]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7259076237678528, 0.36643552780151367, 0.08612918108701706, 0.07180553674697876, 0.07027343660593033, 0.1531374156475067, 0.10787783563137054, 0.0586853101849556, 0.05892335623502731, 0.2931239604949951]
We use Minipar (Lin, 1993), which produces functional relations for the components in a sentence, including subject and object relations with respect to a verb.	[53, 129, 19, 130, 35, 51, 169, 99, 27, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07348255068063736, 0.18078957498073578, 0.09194953739643097, 0.11000968515872955, 0.057505052536726, 0.10780542343854904, 0.1262616664171219, 0.07679035514593124, 0.07268858700990677, 0.0691177025437355]
We use OpinionFinder (Wilson et al, 2005) which employs negative and positive polarity cues.	[39, 4, 22, 11, 68, 13, 15, 27, 1, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7815832495689392, 0.18523432314395905, 0.07227817922830582, 0.2077021449804306, 0.27013716101646423, 0.13421687483787537, 0.16513171792030334, 0.05422614887356758, 0.2244497388601303, 0.12695160508155823]
We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective.	[0, 41, 25, 135, 28, 29, 42, 15, 91, 87]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7000353336334229, 0.26431596279144287, 0.22005295753479004, 0.1763850450515747, 0.08428002148866653, 0.19782163202762604, 0.22784525156021118, 0.0683244988322258, 0.06850649416446686, 0.08293648064136505]
We use SimFinder (Hatzivassiloglou et al, 1999) for sentence clustering and the f-measure for word overlap to compare noun phrases.	[60, 212, 8, 14, 165, 7, 55, 249, 164, 173]	[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]	[0.27931997179985046, 0.27931997179985046, 0.1374596804380417, 0.5148178339004517, 0.1374596804380417, 0.07735884189605713, 0.15320788323879242, 0.591418981552124, 0.07131633162498474, 0.29569536447525024]
We use SuperSenseTagger (Ciaramita and Altun,2006) as our NER tagger.	[97, 174, 155, 181, 139, 35, 0, 3, 200, 185]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5147099494934082, 0.23637467622756958, 0.13584570586681366, 0.3052421510219574, 0.2954008877277374, 0.16129671037197113, 0.4369666576385498, 0.2201731652021408, 0.10172528773546219, 0.36861228942871094]
We use TinySVM2 along with YamCha3 (Kudo and Matsumoto (2000, 2001)) as the SVM training and test software.	[48, 39, 1, 49, 7, 57, 16, 61, 6, 3]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6254457831382751, 0.49442556500434875, 0.44520992040634155, 0.3558139503002167, 0.08964527398347855, 0.1340111643075943, 0.30233675241470337, 0.13845548033714294, 0.0775960311293602, 0.0980117917060852]
We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model.	[96, 72, 35, 7, 170, 20, 14, 131, 21, 126]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4833569824695587, 0.5310530066490173, 0.19760185480117798, 0.2925179898738861, 0.24947011470794678, 0.3740541338920593, 0.16672980785369873, 0.20041228830814362, 0.23376071453094482, 0.10990109294652939]
We use a frequency-based notation because we use out of-the-box software Bitpar (Schmid, 2004) which implements inside-outside estimation Bitpar reads in frequency models and converts them to relative frequency models.	[9, 1, 24, 30, 25, 5, 16, 3, 4, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16239118576049805, 0.26300719380378723, 0.07652004808187485, 0.23640774190425873, 0.08236780017614365, 0.07126700133085251, 0.06310412287712097, 0.10471609979867935, 0.07704909145832062, 0.07764449715614319]
We use a general-purpose CKY parser (Schmid, 2004) to exhaustively parse the sentences, and we strip off all model-specific information prior to evaluation.	[1, 19, 4, 3, 2, 6, 9, 12, 8, 24]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42798635363578796, 0.536562979221344, 0.14639776945114136, 0.10646148771047592, 0.15280196070671082, 0.13651402294635773, 0.13237400352954865, 0.1323886662721634, 0.061779920011758804, 0.08803454786539078]
We use a mechanism similar to (Collins, 1997) but adapted to Chinese data to find lexical heads in the tree bank data.	[25, 85, 90, 46, 45, 8, 88, 106, 68, 24]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20877791941165924, 0.37401309609413147, 0.16696831583976746, 0.09908592700958252, 0.11659679561853409, 0.3438529670238495, 0.2705761790275574, 0.08460064232349396, 0.2973308861255646, 0.12169960886240005]
We use a sentiment-annotated data set consisting of movie reviews by (Pang and Lee, 2005) and tweets from http: //help.sentiment140.com/for-students.	[81, 22, 36, 40, 1, 67, 0, 6, 78, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34851187467575073, 0.22301122546195984, 0.2899191975593567, 0.11754327267408371, 0.12303192168474197, 0.13868126273155212, 0.27802300453186035, 0.11925604939460754, 0.1286165863275528, 0.06275326013565063]
We use a syntax-based language model which was originally developed for use in speech recognition (Charniak, 2001) and later adapted to work with a syntax-based machine translation system (Charniaket al, 2001).	[10, 139, 134, 7, 124, 1, 145, 40, 73, 70]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3434031307697296, 0.13007110357284546, 0.18320034444332123, 0.20585067570209503, 0.10861729085445404, 0.09714163839817047, 0.11145753413438797, 0.055049266666173935, 0.10417144000530243, 0.12099761515855789]
We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment.	[67, 187, 155, 161, 8, 32, 156, 190, 0, 159]	[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.4916270077228546, 0.7385338544845581, 0.42902714014053345, 0.19978798925876617, 0.11575459688901901, 0.11315024644136429, 0.07607687264680862, 0.08503671735525131, 0.6079744100570679, 0.09830756485462189]
We use different strategies for the identification of the two classes of entities: for the domain-specific ones we use hand-crafted LT TTT rules, while for the non domain-specific ones we use the C&C named entity tagger (Curran and Clark, 2003) trained on the MUC-7 data set.	[14, 8, 21, 10, 22, 4, 62, 25, 27, 73]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2464955449104309, 0.3754948675632477, 0.14113783836364746, 0.095688596367836, 0.2499031275510788, 0.41270533204078674, 0.06959080696105957, 0.12938262522220612, 0.16894610226154327, 0.07177868485450745]
We use the English part of the SemEval-2010 CR task data set, a subset of OntoNotes 2.0 (Hovy et al, 2006).	[33, 66, 3, 42, 1, 55, 0, 65, 60, 8]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6821174621582031, 0.3242741525173187, 0.07991868257522583, 0.05724335089325905, 0.12362232059240341, 0.1409456878900528, 0.2868395447731018, 0.24733856320381165, 0.06723989546298981, 0.0912998616695404]
We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al, 1994) to objectively compare the performances of the systems.	[106, 4, 30, 70, 7, 121, 45, 74, 114, 15]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2763844430446625, 0.18638643622398376, 0.31176021695137024, 0.17625311017036438, 0.08253353834152222, 0.23817339539527893, 0.20208725333213806, 0.07474730908870697, 0.3727487027645111, 0.06970579922199249]
We use the SVM-Light Toolkit version 6.02 (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser version 1.6 (Levy and Manning, 2003) as the constituent parser and the constituent-to-dependency converter.	[144, 60, 155, 61, 29, 64, 134, 44, 98, 40]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42517781257629395, 0.6669917106628418, 0.3366592824459076, 0.20769022405147552, 0.11331471800804138, 0.09583532065153122, 0.06513132154941559, 0.0666135773062706, 0.1286720186471939, 0.06761323660612106]
We use the Tree Tagger (Schmid, 1994) for all POS tagging except for Arabic, where we use the tagger described in Diab et al.	[23, 112, 67, 131, 118, 14, 10, 79, 78, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2793366014957428, 0.06909116357564926, 0.07000963389873505, 0.14085915684700012, 0.10135393589735031, 0.17559091746807098, 0.12599706649780273, 0.10957611352205276, 0.11287068575620651, 0.1482006162405014]
We use the freely available C99 software described in Choi (2000), varying a parameter that allows us to control the average number of sentences per segment and reporting the best result on the test data.	[134, 30, 71, 144, 130, 39, 67, 91, 90, 122]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.49829646944999695, 0.19708621501922607, 0.295092910528183, 0.2639089822769165, 0.3082243800163269, 0.07861565798521042, 0.1477736085653305, 0.09183396399021149, 0.10339617729187012, 0.503328800201416]
We use the log-linear tagger of Toutanova et al (2003), which gives 96.8% accuracy on the test set.	[115, 93, 89, 86, 161, 2, 159, 17, 84, 95]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.803915798664093, 0.8137199282646179, 0.38534310460090637, 0.10668625682592392, 0.23300457000732422, 0.34188511967658997, 0.39398857951164246, 0.4346975088119507, 0.27282655239105225, 0.5027693510055542]
We use the metric described in (Yarowsky, 1994; Golding, 1995).	[216, 271, 231, 212, 54, 51, 248, 255, 69, 96]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7439566850662231, 0.3071778118610382, 0.3220117688179016, 0.28401800990104675, 0.4113976061344147, 0.23451602458953857, 0.4763205945491791, 0.16085484623908997, 0.3534054160118103, 0.23907123506069183]
We use the same alignment data for the five language pairs Chinese-English, Romanian-English, Hindi-English, Spanish-English, and French-English as Wellington et al (2006).	[64, 63, 111, 65, 105, 23, 108, 84, 159, 22]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8250817060470581, 0.8159592747688293, 0.5430975556373596, 0.31367695331573486, 0.10383522510528564, 0.12503811717033386, 0.18491895496845245, 0.2841232120990753, 0.09923263639211655, 0.06698362529277802]
We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit.	[189, 7, 15, 150, 192, 3, 8, 143, 159, 32]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7089873552322388, 0.6025907397270203, 0.10933748632669449, 0.23875385522842407, 0.1820453554391861, 0.45474016666412354, 0.2689552307128906, 0.1566769778728485, 0.17975275218486786, 0.08949832618236542]
We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus.	[2, 116, 40, 17, 19, 22, 12, 30, 100, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1578868329524994, 0.16233116388320923, 0.22135721147060394, 0.08782905340194702, 0.09811640530824661, 0.11966186761856079, 0.05574290081858635, 0.05344541743397713, 0.11644168198108673, 0.11879508942365646]
We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments.	[40, 62, 2, 171, 38, 154, 161, 41, 82, 45]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3079741895198822, 0.44918325543403625, 0.1415482759475708, 0.20816482603549957, 0.1205807626247406, 0.17419803142547607, 0.224872887134552, 0.06168738752603531, 0.16931691765785217, 0.162124902009964]
We used the base feature model defined in (Nivre et al, 2006) for all the languages but Arabic, Chinese, Czech, and Turkish.	[14, 16, 11, 24, 15, 47, 34, 26, 19, 12]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5740463137626648, 0.4074684977531433, 0.37532323598861694, 0.1592303365468979, 0.15514817833900452, 0.47417905926704407, 0.1400398463010788, 0.24873359501361847, 0.08754190057516098, 0.05532862991094589]
We used the dataset created by Liang et al (2009).	[40, 21, 45, 60, 46, 164, 43, 66, 28, 55]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8207725286483765, 0.8119426965713501, 0.16971929371356964, 0.6260319948196411, 0.33648744225502014, 0.3607443571090698, 0.25243014097213745, 0.11553359776735306, 0.07841642200946808, 0.17461080849170685]
We used the list of labeled seeds from (Hatzivassiloglouand McKeown, 1997) and (Stone et al, 1966).	[32, 13, 36, 1, 21, 72, 38, 39, 42, 104]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.145742267370224, 0.056383904069662094, 0.11496226489543915, 0.052000775933265686, 0.13299541175365448, 0.12905482947826385, 0.06683842092752457, 0.27653825283050537, 0.2547525465488434, 0.05868026614189148]
We used the same data set as the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000).	[114, 0, 62, 118, 117, 3, 67, 86, 79, 69]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7548853754997253, 0.8276051878929138, 0.24628925323486328, 0.308927446603775, 0.11141658574342728, 0.5295787453651428, 0.26088765263557434, 0.16470448672771454, 0.19622474908828735, 0.05579192191362381]
We used the training sets, test sets, and evaluation method described in (Resnik, 1997).	[10, 53, 52, 11, 82, 7, 4, 71, 51, 89]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5185696482658386, 0.28664344549179077, 0.40799859166145325, 0.6552277207374573, 0.1755744367837906, 0.11862438917160034, 0.08420268446207047, 0.17751498520374298, 0.10872210562229156, 0.10825543850660324]
We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL 2002 shared task (Tjong Kim Sang, 2002).	[0, 4, 64, 89, 86, 6, 85, 8, 14, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46962863206863403, 0.22594858705997467, 0.29268744587898254, 0.2777160406112671, 0.1903650462627411, 0.1817987710237503, 0.2037513107061386, 0.11160100996494293, 0.18845486640930176, 0.23364804685115814]
We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders.	[57, 37, 116, 66, 65, 4, 38, 85, 117, 76]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7573418617248535, 0.13206644356250763, 0.13206644356250763, 0.1649991124868393, 0.16522431373596191, 0.18255244195461273, 0.23307599127292633, 0.18255244195461273, 0.23307599127292633, 0.20061229169368744]
We uses a mixture of local and global features to train the coefficients of a linear ranking SVM to rank different NE candidates.	[112, 115, 64, 95, 18, 35, 52, 109, 96, 47]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8186089992523193, 0.6793703436851501, 0.29100242257118225, 0.4318898022174835, 0.12003719061613083, 0.10913664102554321, 0.23835043609142303, 0.15457111597061157, 0.059933751821517944, 0.21240346133708954]
We will also study the effect of other window sizes and the combination of this unsupervised approach with minimally-supervised approaches such as (Brill 1995) (Smith and Mann 2003).	[129, 109, 121, 131, 134, 3, 37, 123, 124, 68]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2776736319065094, 0.6849669814109802, 0.24191202223300934, 0.24391748011112213, 0.2554604411125183, 0.17467990517616272, 0.3077281713485718, 0.2519136071205139, 0.34048622846603394, 0.10770762711763382]
We will be reporting on results using PropBank (Kingsbury et al, 2002), a 300k-word corpus in which predicate argument relations are marked for part of the verbs in the Wall Street Journal (WSJ) part of the Penn Tree Bank (Marcus et al, 1994).	[24, 5, 8, 3, 65, 0, 83, 13, 90, 104]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3338307738304138, 0.10417015850543976, 0.09381244331598282, 0.1056513786315918, 0.06224266439676285, 0.1971210539340973, 0.11322054266929626, 0.07197191566228867, 0.22708335518836975, 0.19286410510540009]
We will make use of the Mintz et al (2009) sentence-level features in the expeiments, as described in Section 7.	[136, 102, 113, 99, 119, 80, 81, 32, 131, 128]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47746363282203674, 0.7245166897773743, 0.29236268997192383, 0.14090165495872498, 0.08799057453870773, 0.1602192521095276, 0.17920444905757904, 0.05697208270430565, 0.07319625467061996, 0.29457566142082214]
We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label.	[131, 200, 5, 44, 52, 147, 23, 148, 43, 172]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.34892550110816956, 0.3948194980621338, 0.25594785809516907, 0.4419803321361542, 0.09684402495622635, 0.5308133363723755, 0.05729525163769722, 0.19083240628242493, 0.20541194081306458, 0.14065542817115784]
We'll use a simple example sentence to illustrate how our feature sets are extracted from CONLL formatted data (Nivre et al, 2007).	[71, 3, 16, 189, 96, 21, 144, 118, 111, 160]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8082111477851868, 0.10365008562803268, 0.07965805381536484, 0.4076612889766693, 0.09271314740180969, 0.11786115169525146, 0.08310413360595703, 0.06989126652479172, 0.06972629576921463, 0.0833333283662796]
Weischedel et al (1993) combine several heuristics in order to estimate the token generation prob ability according to various types of information.	[144, 164, 185, 271, 28, 155, 295, 261, 263, 87]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20729725062847137, 0.33496779203414917, 0.25795698165893555, 0.07779630273580551, 0.09185869991779327, 0.10665380954742432, 0.16291429102420807, 0.12327776104211807, 0.20916087925434113, 0.053799085319042206]
Wellington et al (2006) indicate the necessity of introducing discontinuous spans for synchronous parsing to match up with human-annotated word alignment data.	[20, 175, 48, 53, 25, 35, 50, 85, 27, 73]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36455389857292175, 0.11529266834259033, 0.15960383415222168, 0.0895080417394638, 0.2558704912662506, 0.4122714400291443, 0.16149331629276276, 0.2942294776439667, 0.17940670251846313, 0.05571460351347923]
When a parser's grammar can have fewer dimensions than the parser's input, we call it a synchronizer (Melamed, 2004).	[12, 11, 103, 106, 89, 6, 1, 199, 5, 70]	[1, 1, 0, 0, 0, 1, 1, 0, 1, 0]	[0.859759509563446, 0.8588725328445435, 0.3109630048274994, 0.3953655958175659, 0.1053914949297905, 0.7372726798057556, 0.6779606938362122, 0.2666804790496826, 0.5954493880271912, 0.1615144908428192]
When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)).	[20, 71, 1, 21, 10, 132, 23, 22, 139, 32]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7406534552574158, 0.36015698313713074, 0.30422645807266235, 0.29966098070144653, 0.28695377707481384, 0.4244515001773834, 0.15938901901245117, 0.2154812216758728, 0.08658298850059509, 0.06681788712739944]
When trained with a corpus only one-tenth the size of the corpus used in Gale and Church (1991), the algorithm aligns over 80% of word pairs with comparable precision (93%).	[40, 129, 121, 128, 41, 122, 38, 11, 199, 32]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.5309336185455322, 0.3186196982860565, 0.24211308360099792, 0.2893323302268982, 0.09928495436906815, 0.3108784854412079, 0.4028821885585785, 0.17802269756793976, 0.6955838799476624, 0.09191899001598358]
Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown,2007).	[1, 84, 7, 21, 17, 18, 86, 81, 74, 83]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48629486560821533, 0.31783008575439453, 0.25231343507766724, 0.32816609740257263, 0.3476881980895996, 0.2887187600135803, 0.2963702976703644, 0.1517357975244522, 0.25076332688331604, 0.08320456743240356]
Whereas Shieber et al (1990) have discussed similar techniques in the context of semantic head-driven generation, we are concerned here with parsing.	[0, 173, 68, 323, 269, 3, 209, 17, 55, 6]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.8486779928207397, 0.8337459564208984, 0.058932509273290634, 0.4963241517543793, 0.560944139957428, 0.17091691493988037, 0.1068926528096199, 0.17492695152759552, 0.09330358356237411, 0.17091691493988037]
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	[13, 1, 2, 9, 0, 11, 107, 154, 133, 77]	[0, 0, 0, 0, 1, 1, 0, 0, 0, 0]	[0.4675244092941284, 0.47764548659324646, 0.2673015892505646, 0.19768285751342773, 0.5095074772834778, 0.5477129220962524, 0.24920514225959778, 0.3099096715450287, 0.1721908301115036, 0.3243163824081421]
While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information.	[73, 40, 38, 112, 2, 41, 37, 107, 34, 31]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4936838150024414, 0.6609910130500793, 0.244404599070549, 0.19519899785518646, 0.5190610289573669, 0.05872892215847969, 0.41440191864967346, 0.13251100480556488, 0.12168508768081665, 0.15013642609119415]
While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over context insensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions.	[0, 1, 12, 2, 117, 9, 14, 3, 90, 87]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8452118039131165, 0.8421701788902283, 0.1761675775051117, 0.1507728099822998, 0.27677279710769653, 0.1734342724084854, 0.14255014061927795, 0.21943223476409912, 0.05486995354294777, 0.0808216854929924]
While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995).	[135, 498, 39, 65, 43, 520, 491, 398, 109, 59]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35447901487350464, 0.10713254660367966, 0.23174911737442017, 0.0971299484372139, 0.06884025782346725, 0.06630410254001617, 0.06872159987688065, 0.33690348267555237, 0.16421759128570557, 0.059023573994636536]
While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al, 2001)) and synchronous dialogs (e.g., (Galley et al, 2003), (Hsueh et al, 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email).	[70, 50, 51, 224, 199, 11, 192, 169, 37, 133]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5776165723800659, 0.10795018076896667, 0.07543200254440308, 0.10774489492177963, 0.09099162369966507, 0.21779337525367737, 0.10840379446744919, 0.056002311408519745, 0.1048165112733841, 0.06670907139778137]
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	[124, 126, 125, 111, 106, 107, 140, 102, 40, 108]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2330353558063507, 0.7141374349594116, 0.30594369769096375, 0.4040592312812805, 0.40650466084480286, 0.25317785143852234, 0.2158268392086029, 0.3315066993236542, 0.2559017837047577, 0.16785036027431488]
While in principle, increasing the number of rejuvenation steps and particles will make this gap smaller and smaller, we believe the existence of the gap to be interesting in its own right, suggesting a general difference in learning behaviour between batch and incremental learners, especially given the similar results in Johnson and Goldwater (2009).	[110, 140, 8, 106, 124, 121, 1, 20, 87, 28]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7480317950248718, 0.7500433921813965, 0.09740494191646576, 0.5240669846534729, 0.18038420379161835, 0.1853698194026947, 0.16942207515239716, 0.2143896520137787, 0.183925598859787, 0.2759176790714264]
While in the previous sections we have described a tabular method for the transition system of Attardi (2006) restricted to transitions of degree up to two, it is possible to generalize the model to include higher degree transitions.	[31, 91, 36, 5, 18, 13, 8, 67, 2, 25]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24047070741653442, 0.49998611211776733, 0.06332802772521973, 0.07517018914222717, 0.1603444218635559, 0.05572040006518364, 0.06855738908052444, 0.09048058837652206, 0.060215938836336136, 0.08204644173383713]
While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies.	[122, 1, 147, 23, 144, 146, 8, 32, 25, 44]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5553044676780701, 0.6646384596824646, 0.25192174315452576, 0.42997580766677856, 0.2859964072704315, 0.20497390627861023, 0.17080210149288177, 0.2946757376194, 0.2802570164203644, 0.23724088072776794]
While our approach uses a similar hierarchy, McDonald et al (2007) is concerned with recovering the labels at all levels, whereas in this work we are interested in using latent document content structure as a means to benefit task predictions.	[38, 32, 53, 156, 65, 57, 23, 162, 33, 28]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.13258743286132812, 0.3064292371273041, 0.5539451837539673, 0.22670423984527588, 0.27599191665649414, 0.251766562461853, 0.34536001086235046, 0.09893635660409927, 0.06041400879621506, 0.24022772908210754]
While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees.	[102, 100, 9, 142, 107, 5, 28, 20, 50, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11625686287879944, 0.18331637978553772, 0.4199231266975403, 0.3072020709514618, 0.3553266227245331, 0.11867771297693253, 0.16680577397346497, 0.09641552716493607, 0.38288983702659607, 0.10513653606176376]
While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles (Han et al, 2006) and prepositions (Gamon et al, 2008), (Felice and Pullman, 2009), there has been a woeful lack of attention paid to developing best practices for annotation and evaluation.	[44, 6, 34, 38, 46, 61, 163, 200, 173, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7510949969291687, 0.10288555175065994, 0.20291456580162048, 0.4170559346675873, 0.2638440728187561, 0.05735967308282852, 0.2919527292251587, 0.09232910722494125, 0.14391344785690308, 0.44255247712135315]
Wilks (1978) advocates that the typically hard constraints that define a literal semantics should instead be modeled as soft preferences that can accommodate the violations that arise in metaphoric utterances, while Fass (1991) builds on this view to show how these violation scan be repaired to thus capture the literal intent be hind each metaphor.	[119, 112, 101, 268, 115, 262, 215, 2, 9, 653]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.1987983137369156, 0.19817647337913513, 0.11103259772062302, 0.7676060795783997, 0.14779332280158997, 0.124884232878685, 0.2233891636133194, 0.17761173844337463, 0.17761173844337463, 0.22491031885147095]
With Chunking, (Kudo and Matsumoto, 2001) reported the best F-score of 93.91 with the voting of several models trained by Support Vec tor Machine in the same experimental settings and with the same feature set.	[190, 178, 38, 112, 156, 0, 160, 172, 12, 90]	[0, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.43311500549316406, 0.8290103077888489, 0.053753335028886795, 0.29694491624832153, 0.25936686992645264, 0.8276709318161011, 0.0690995380282402, 0.4598239064216614, 0.16256219148635864, 0.061630818992853165]
Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002).	[99, 100, 37, 118, 6, 45, 1, 0, 54, 44]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4848331809043884, 0.47196683287620544, 0.26852846145629883, 0.5991901159286499, 0.06827890872955322, 0.1536049246788025, 0.0986073911190033, 0.3309759497642517, 0.18310748040676117, 0.2143358737230301]
Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.	[202, 2, 201, 0, 87, 88, 18, 126, 30, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15746672451496124, 0.09624946862459183, 0.15720121562480927, 0.4333990216255188, 0.11030314862728119, 0.10497816652059555, 0.07578869163990021, 0.04955856502056122, 0.20649050176143646, 0.08167970180511475]
XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model (Riezler et al, 2002) that works on the packed representations.	[39, 1, 90, 88, 91, 21, 124, 8, 84, 106]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2690967321395874, 0.6744481921195984, 0.06040352210402489, 0.07564669847488403, 0.08057975023984909, 0.11401523649692535, 0.11226028949022293, 0.13875964283943176, 0.164419487118721, 0.08194535970687866]
Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages.	[176, 3, 211, 32, 70, 58, 73, 206, 126, 31]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7018173933029175, 0.23598207533359528, 0.40047186613082886, 0.14628006517887115, 0.2619592845439911, 0.22493287920951843, 0.17632544040679932, 0.16965553164482117, 0.15320159494876862, 0.19051046669483185]
Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task.	[138, 128, 29, 4, 135, 146, 33, 22, 149, 1]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7670262455940247, 0.6600964069366455, 0.16824683547019958, 0.32569053769111633, 0.3357415199279785, 0.42589130997657776, 0.18869777023792267, 0.37475618720054626, 0.11902740597724915, 0.1113038957118988]
Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability.	[95, 68, 34, 93, 76, 23, 81, 77, 24, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24816618859767914, 0.25436750054359436, 0.381094366312027, 0.07919850200414658, 0.16777974367141724, 0.17165198922157288, 0.14466765522956848, 0.12291321903467178, 0.052594833076000214, 0.09324214607477188]
Yarowsky and Wicentowski (2000) report an accuracy of over 99% for their best model and a test set of 3888 pairs.	[121, 116, 111, 132, 112, 53, 88, 120, 76, 113]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7324443459510803, 0.5645562410354614, 0.2819742262363434, 0.24082402884960175, 0.2502571642398834, 0.1618218719959259, 0.3664284944534302, 0.1579550802707672, 0.062225885689258575, 0.1741858869791031]
Yarowsky and Wicentowski (2000) use similar statistics to identify words related by inflection, but they gather their counts from a much smaller corpus.	[69, 33, 39, 77, 67, 83, 112, 65, 47, 73]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6968137621879578, 0.14911052584648132, 0.10241606086492538, 0.07366760820150375, 0.09881337732076645, 0.2549133896827698, 0.10773316025733948, 0.2189961075782776, 0.08969002962112427, 0.0556030310690403]
Yaser Al-Onaizan (Al-Onaizan and Knight, 2002) transliterated an NE in Arabic into several candidates in English and ranked the candidates by comparing their counts in several English corpora.	[68, 57, 89, 59, 202, 114, 63, 110, 53, 58]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5574446320533752, 0.49587300419807434, 0.32681483030319214, 0.40007486939430237, 0.10350286960601807, 0.4386785626411438, 0.15950533747673035, 0.2582472562789917, 0.14422254264354706, 0.2373277097940445]
Yates (2009) considers the output from an open information extraction system (Yates et al, 2007) and clusters predicates and arguments using string similarity and a combination of constraints.	[0, 2, 1, 15, 27, 9, 33, 31, 12, 42]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7091054916381836, 0.20470404624938965, 0.1090606078505516, 0.19530333578586578, 0.13549983501434326, 0.2898659110069275, 0.06361974030733109, 0.21361400187015533, 0.07857336103916168, 0.18780821561813354]
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system.	[11, 65, 9, 55, 40, 1, 42, 17, 16, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23551110923290253, 0.4389108419418335, 0.4021414518356323, 0.10838369280099869, 0.057232052087783813, 0.17186538875102997, 0.13353697955608368, 0.2831435203552246, 0.09592381119728088, 0.1855984479188919]
Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al, 2009).	[80, 49, 89, 5, 47, 27, 86, 92, 104, 99]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1870674192905426, 0.5707677602767944, 0.33663129806518555, 0.16279621422290802, 0.1816539317369461, 0.14517471194267273, 0.15422876179218292, 0.09404649585485458, 0.31132394075393677, 0.06832369416952133]
Zhang and Weld (2013) is based on REVERB (Fader et al, 2011), which uses a regular expression on part-of-speech tags to produce the extractions.	[88, 121, 149, 212, 36, 58, 137, 187, 23, 141]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3847280740737915, 0.2870657742023468, 0.296350359916687, 0.16257202625274658, 0.085291288793087, 0.1401909589767456, 0.25569796562194824, 0.0826716274023056, 0.050477534532547, 0.29791152477264404]
Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009).	[38, 4, 151, 184, 3, 149, 44, 169, 23, 80]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.704257607460022, 0.41152331233024597, 0.4881994426250458, 0.24229080975055695, 0.2463434636592865, 0.2651793658733368, 0.13436885178089142, 0.06456507742404938, 0.11218498647212982, 0.45006468892097473]
Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method.	[70, 54, 22, 23, 78, 5, 31, 97, 36, 14]	[1, 0, 0, 0, 0, 1, 0, 1, 0, 0]	[0.5861955881118774, 0.44839146733283997, 0.36686527729034424, 0.3282686173915863, 0.4317566156387329, 0.5727136731147766, 0.4523402452468872, 0.5727136731147766, 0.2114710509777069, 0.28457537293434143]
Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity.	[4, 275, 286, 337, 299, 310, 301, 241, 296, 281]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3701835572719574, 0.7755348682403564, 0.1648402065038681, 0.24982768297195435, 0.33768415451049805, 0.14984126389026642, 0.4661250412464142, 0.36429646611213684, 0.36308127641677856, 0.08215674012899399]
Zollmann and Venugopal (2006) started with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.	[21, 23, 2, 11, 20, 19, 35, 53, 36, 34]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6936795711517334, 0.21064142882823944, 0.24870339035987854, 0.11691910028457642, 0.1750335693359375, 0.4476226568222046, 0.26397961378097534, 0.26256948709487915, 0.3420308530330658, 0.09278371930122375]
[Hobbs and Shieber 1987] presented an algorithm to generate quantifier scopings from a representation of predicate-argument relations and the relations of grammatical subordination.	[71, 1, 17, 6, 4, 9, 47, 69, 74, 11]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6401728391647339, 0.7051293849945068, 0.41780608892440796, 0.38977518677711487, 0.14483913779258728, 0.14483913779258728, 0.18487370014190674, 0.18324297666549683, 0.16132096946239471, 0.4656144678592682]
a graph-structured stack (Tomita, 1987) was used to efficiently represent ambiguous index operations in a GIG stack.	[105, 3, 13, 141, 66, 26, 112, 108, 4, 14]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.553042471408844, 0.174939826130867, 0.174939826130867, 0.29895925521850586, 0.1543041318655014, 0.13263331353664398, 0.28649237751960754, 0.19657209515571594, 0.17582663893699646, 0.17582663893699646]
classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004).	[46, 15, 44, 47, 56, 6, 110, 31, 196, 16]	[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]	[0.6819972991943359, 0.6374428868293762, 0.5970903038978577, 0.2370583713054657, 0.6472893953323364, 0.30730900168418884, 0.5087084174156189, 0.27494388818740845, 0.39612719416618347, 0.18887385725975037]
coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set.	[73, 87, 106, 2, 102, 118, 71, 52, 110, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33851897716522217, 0.19554966688156128, 0.13666321337223053, 0.28446754813194275, 0.30775752663612366, 0.11139877885580063, 0.20779311656951904, 0.10797812789678574, 0.08508419245481491, 0.1449735313653946]
distant supervision: for each relation in the database D we assume that all the corresponding mentions are positive examples for the corresponding label (Mintz et al 2009).	[21, 0, 19, 3, 10, 29, 137, 28, 20, 23]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7608135938644409, 0.5814723372459412, 0.1619201898574829, 0.20537987351417542, 0.5462618470191956, 0.06389259546995163, 0.1336294710636139, 0.09625767916440964, 0.33890387415885925, 0.2523776888847351]
in (Weischedel et al, 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos, its capitalisation feature and its ending.	[136, 158, 249, 305, 35, 327, 59, 147, 132, 146]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5553503632545471, 0.48235806822776794, 0.18548056483268738, 0.12647365033626556, 0.07100130617618561, 0.3280662000179291, 0.2332320511341095, 0.08592835068702698, 0.23675329983234406, 0.19659115374088287]
paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)).	[124, 8, 23, 17, 133, 37, 18, 119, 108, 10]	[0, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.4660349190235138, 0.7231748104095459, 0.5160450339317322, 0.5847285389900208, 0.4132527709007263, 0.2955711781978607, 0.22563442587852478, 0.21654894948005676, 0.19598808884620667, 0.35735493898391724]
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	[50, 108, 157, 210, 110, 126, 18, 114, 184, 59]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38183173537254333, 0.14445090293884277, 0.17512306571006775, 0.22026681900024414, 0.14901462197303772, 0.17614667117595673, 0.14471766352653503, 0.14849397540092468, 0.14908750355243683, 0.07984453439712524]
the main previous body of work on biographical summarization is that of (Radev and McKeown 1998).	[63, 225, 70, 106, 428, 90, 228, 74, 170, 26]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8323796391487122, 0.5014501214027405, 0.5189218521118164, 0.4294971823692322, 0.39122071862220764, 0.3764790892601013, 0.21510116755962372, 0.2837524116039276, 0.35293594002723694, 0.16529011726379395]
this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions.	[85, 29, 14, 117, 34, 43, 50, 56, 127, 119]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7954743504524231, 0.5678969621658325, 0.08527419716119766, 0.3091038167476654, 0.47669896483421326, 0.10254985094070435, 0.22195741534233093, 0.1339820921421051, 0.08082921802997589, 0.08581623435020447]
was available and/or from the COMLEX Syntax dictionary (Grishman et al, 1994) all the SCFs taken by its member verbs.	[0, 68, 130, 133, 109, 113, 61, 124, 3, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15007659792900085, 0.13161450624465942, 0.20939901471138, 0.20139780640602112, 0.23400373756885529, 0.07537142187356949, 0.13744865357875824, 0.09761859476566315, 0.13594375550746918, 0.14131532609462738]
where d(.) is a WordNet based relatedness measure (Pedersen et al, 2004).	[4, 2, 0, 1, 72, 78, 19, 15, 71, 35]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6283218264579773, 0.5563416481018066, 0.42809513211250305, 0.4380628764629364, 0.5164300203323364, 0.2402966469526291, 0.1487223207950592, 0.15763981640338898, 0.4707527458667755, 0.12324407696723938]
