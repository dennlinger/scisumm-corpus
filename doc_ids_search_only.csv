query	doc_id	label
(Al-Onaizan and Knight 2002) showed that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy.	[110]	[1]
(Brill, 1995) presents a rule-based part-of-speech tagger for unsupervised training corpus.	[132, 1, 21, 30, 0]	[1, 1, 1, 1, 1]
(Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative subcategorization frequency for individual predicates.	[117, 213, 83, 217]	[1, 1, 1, 1]
(Chambers and Jurafsky, 2011) (Poon and Domingos, 2010) (Chen et al 2011) focus on extracting frame-like structures (Baker et al 1998) by defining two types of clusters, event clusters and role clusters.	[11, 103]	[1, 1]
(Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees.	[103, 0]	[1, 1]
(Gale and Church, 1991) has used the ?2 statistics as the correspondence level of the word pairs and has showed that it was more effective than the mutual information.	[101, 199]	[1, 1]
(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents.	[44]	[1]
(Kessler et al, 1997) combine these views by saying that a genre should not be so broad that the texts belonging to it don't share any distinguishing properties.	[24]	[1]
(Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns.	[22, 56]	[1, 1]
(McRoy, 1992) was one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP.	[0]	[1]
(Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging.	[28]	[1]
(Since we used Isozaki's methods (Isozaki and Kazawa, 2002), the run-time complexity is not a problem.) Kudo and Matsumoto (2002) proposed an SVM-based Dependency Analyzer for Japanese sentences.	[39]	[1]
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	[5]	[1]
(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability.	[141, 214]	[1, 1]
(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora.	[2, 125, 25]	[1, 1, 1]
19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006).	[159]	[1]
A Feature based TAG (FTAG, (Vijay-Shanker and Joshi,1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction.	[20, 48]	[1, 1]
"A Gaussian prior also handles the problem of ""pseudo-maximal"" features (Johnson et al, 1999)."	[95]	[1]
A benchmark dataset of 27,937 such quadruples was extracted from the Wall Street Journal corpus by Ratnaparkhi et al (1994) and has been the basis of many subsequent studies comparing machine learning algorithms and lexical resources.	[87]	[1]
A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003).	[199, 34, 26, 85]	[1, 1, 1, 1]
A complete description on WMT-07 evaluation campaign and dataset is available in Callison-Burch et al (2007).	[107]	[1]
A current project at the University of Pennsylvania and the Children's Hospital of Philadelphia (Kulick et al 2004) is producing a corpus that follows many of these basic principles.	[10]	[1]
A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002).	[15]	[1]
A hybrid approach is presented in (Gale and Church, 1993) whose basic hypothesis is that longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences.	[4, 18, 90, 115, 312, 2, 16, 185]	[1, 1, 1, 1, 1, 1, 1, 1]
A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006).	[61, 3, 30]	[1, 1, 1]
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	[25, 9, 18, 333, 262, 53, 258]	[1, 1, 1, 1, 1, 1, 1]
"A number of studies are related to the work we presented, most specifically work on parallel-text based ""information projection"" for parsing (Hwa et al., 2002), but also grammar induction work based on constituent/distituent information (Klein and Manning, 2002) and (language-internal) alignment based learning (van Zaanen, 2000)."	[7]	[1]
A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase (Mintz et al, 2009).	[0, 20]	[1, 1]
A principled solution to this problem is to use an SRL system for nominal predicates trained using NomBank (Meyers et al., 2004).	[156]	[1]
A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)).	[58, 0, 6, 12, 806]	[1, 1, 1, 1, 1]
A recent shared task in biomedical text mining, the BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009), showed that the biomedical natural language processing (BioNLP) community is greatly interested in heading towards the extraction of deep, semantically rich relationships.	[0, 51]	[1, 1]
A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources.	[153]	[1]
A similar observation was reported in the parsing literature, where coarse-to-fine inference with multiple passes of roughly equal complexity produces tremendous speed-ups (Petrov and Klein, 2007).	[0]	[1]
A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g. Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990)).	[0]	[1]
A wide spectrum of tasks have been studied under review mining, ranging from coarse-grained document-level polarity classification (Pang et al,2002) to fine-grained extraction of opinion expressions and their targets (Wu et al, 2009).	[165, 13]	[1, 1]
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	[46]	[1]
Accuracy for XLE is not given, because the results reported by Kaplan et al (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs.	[15]	[1]
Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010).	[77, 49, 50, 26]	[1, 1, 1, 1]
Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al, 2004) achieves an f score of 79.6%.	[157]	[1]
Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see (Litkowski, 2004) for more details).	[23]	[1]
Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee, 2004).	[19]	[1]
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	[9]	[1]
Although it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992).	[0]	[1]
Although most generation systems pipeline decisions (Reiter, 1994), we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions.	[32]	[1]
Although related to Callison-Burch et al (2006) our method is conceptually simpler and more general.	[73]	[1]
Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper.	[0, 56, 223, 11]	[1, 1, 1, 1]
Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003).	[27]	[1]
Although we have not discussed it to this point, (Collins and Roark, 2004) present a perceptron algorithm for use with the Roark architecture.	[125, 173]	[1, 1]
Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000).	[15, 0, 33, 19]	[1, 1, 1, 1]
An existing SCFG parser (Schmid, 2004) was then used, with a simple unknown word heuristic, to generate the Viterbi n-best parses with n= 100, and, after removing the address labels, all equal parses and their probabilities were summed, and the one with highest probability chosen.	[20]	[1]
An obvious first step, which we are currently working on, is to include a linguistically motivated temporal ontology (Moens and Steedman,1988), which will be separate from the existing do main ontology.	[0, 357, 275]	[1, 1, 1]
An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS.	[1, 0, 177]	[1, 1, 1]
Analogous to our prediction task, Lin and Hsin Yihn (2008) and Socher et al (2011) investigated predicting the emotion of a reader from the text that s/he reads.	[222]	[1]
Another difference is that the system in Morante and Daelemans (2009) used shallow syntactic features, whereas this system uses features from both shallow and dependency syntax.	[110]	[1]
Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002).	[276]	[1]
Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy.	[22, 54, 21, 23]	[1, 1, 1, 1]
Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002).	[19]	[1]
Another way for determiner deletion is described in (Lee, 2004).	[36, 15]	[1, 1]
Application of cascades of weighted string transducers (WSTs) has been well-studied (Mohri,1997).	[53, 176]	[1, 1]
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	[39]	[1]
Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011).	[31, 18, 167]	[1, 1, 1]
As Wacholder et al (1997) noted, it is fairly common for one of the mentions of an entity in a document to be a long, typical surface form of that entity (e.g., George W. Bush), while the other mentions are shorter surface forms (e.g., Bush).	[63, 64, 35]	[1, 1, 1]
As a Japanese dependency analyzer based on the cascaded chunking model, we use the publicly available version of CaboCha (Kudo and Matsumoto, 2002), which is trained with the manually parsed sentences of Kyoto text corpus (Kurohashi and Nagao, 1998), that are 38,400 sentences selected from the 1995 Mainichi newspaper text.	[85, 81]	[1, 1]
As a learning model, we use unigram language modelling introduced in (Collins-Thompson and Callan, 2004) to model the reading level of subjects in primary and secondary school.	[0]	[1]
As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected.	[0]	[1]
As an evidence, the CWS evaluation campaign, the Sighan Bakeoff (Emerson, 2005) has been held four times since 2004.	[10]	[1]
As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1.	[0, 161, 126]	[1, 1, 1]
As another example, Denis and Baldridge (2007) and Finkel and Manning (2008) perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output of the anaphoricity classifier and that of the coreference classifier.	[0]	[1]
As described e.g. in Mladova et al. (2009), the annotation framework that we use is based on the knowledge obtained from studying various other systems, especially the Penn Discourse Treebank (Prasad et al., 2008), but naturally it has been adjusted to specific needs of the Czech language and PDT.	[0]	[1]
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	[23, 3, 205]	[1, 1, 1]
As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous.	[70]	[1]
As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein (2007) for joint inference.	[0]	[1]
As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs.	[17]	[1]
As with (Taskar et al, 2005b), we use the large-margin structured prediction model.	[40, 39]	[1, 1]
At ACL 2000, Brill and Moore (2000) introduced a new error model, allowing generic string-to-string edits.	[4, 11, 131, 37]	[1, 1, 1, 1]
Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries.	[20]	[1]
Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection.	[94, 45]	[1, 1]
B3 here is the B3All version of Stoyanov et al (2009).	[78, 97]	[1, 1]
BLEU is smoothed to be more appropriate for sentence level evaluation (Lin and Och, 2004b), and the bi gram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included.	[23, 167, 141]	[1, 1, 1]
Banea et al (2008) demonstrate that machine translation can perform quite well when extending the subjectivity analysis to multilingual environment, which makes it inspiring to replicate their work on lexicon-based sentiment analysis.	[0]	[1]
Bannard and Callison-Burch (2005) defined a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases as: Ppara (p1 ,p2)= sum piv Pt (piv|p1) Pt (p2|piv) where Pt denotes translation probabilies.	[102]	[1]
Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs.	[28, 2, 43, 183]	[1, 1, 1, 1]
Barzilay and McKeown (2001) and Callison Burch et al (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source.	[2, 64, 0]	[1, 1, 1]
Barzilay and McKeown (2001) induced simple POS-based paraphrase rules from paraphrase instances, which can be a good starting point.	[101]	[1]
Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools.	[1]	[1]
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	[3, 108, 202, 62]	[1, 1, 1, 1]
Bergsma and Lin (2006) built a statistical model from paths that include the lemma of the intermediate tokens, but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively.	[147, 79]	[1, 1]
Biemann (2006) described a graph-based clustering methods for word classes.	[1, 53]	[1, 1]
Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data.	[183, 17]	[1, 1]
Blunsom et al (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data.	[2, 22]	[1, 1]
Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features.	[6, 96, 244]	[1, 1, 1]
Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text.	[135]	[1]
Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig.	[0, 1]	[1, 1]
Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy.	[0, 208]	[1, 1]
But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009).	[65, 20, 181, 77]	[1, 1, 1, 1]
By contrast, Schone and Jurafsky (2001) evaluate the identification of phrasal terms without grammatical filtering on a 6.7 million word extract from the TREC databases, applying both WordNet and on line dictionaries as gold standards.	[20]	[1]
By finding semantic differences between the selectional preferences, it can articulate the higher-order structure of conceptual metaphors ((Mason, 2004), p. 24), finding mappings like LIQUID -> MONEY.	[32, 29, 91, 103]	[1, 1, 1, 1]
C99 KA to denote the C99 algorithm (Choi, 2000) when the aver age number of boundaries in the reference data was provided to the algorithm.	[80]	[1]
CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009).	[30]	[1]
Callison-Burch et al (2006) exploited the existence of multiple parallel corpora to learn paraphrases for Phrase-based MT.	[59, 37, 73]	[1, 1, 1]
Callison-Burch et al (2006) point out three prominent factors.	[73, 59]	[1, 1]
Callison-Burch et al (2012) report for several automatic metrics on the whole WMT12 English-to-Czech dataset, the best of which correlates at?= 0.18.	[6]	[1]
Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al (1992).	[1, 62, 64, 63]	[1, 1, 1, 1]
Candidates considered in the semantic tagging process are noun phrases NP, proposition phrases PP, verb phrases VP, adjectives ADJ and adverbs ADV. To gather these candidates we used the Brill transformational tagger (Brill, 1992) for the part-of speech step and the CASS partial parser for the parsing step (Abney, 1994).	[0]	[1]
Cao and Li (2002) propose a new method to translate base noun phrases.	[13, 2]	[1, 1]
Caraballo (1999) let three judges evaluate ten internal nodes in the hyponymy hierarchy that had at least twenty descendants.	[71]	[1]
Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem.	[0]	[1]
Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing co referring arguments, followed by temporal classification to induce partial order.	[20]	[1]
Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data.	[56]	[1]
Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling.	[0]	[1]
Chiang (2010) extended SAMT-style labels to both source and target-side parses, also introducing a mechanism by which SCFG rules may apply at runtime even if their labels do not match.	[44]	[1]
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures.	[19]	[1]
Clustering by committee has also been used to discover concepts from a text by grouping terms into conceptually related clusters (Lin and Pantel, 2002).	[3, 0]	[1, 1]
Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001).	[7, 145]	[1, 1]
Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.	[1, 0, 158, 2]	[1, 1, 1, 1]
Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before.	[134]	[1]
Committee-based approaches to POS tagging have been in focus the last decade: Brill and Wu (1998) combined four different taggers for English using unweighted voting and by exploring contextual cues (essentially a variant of stacking).	[74, 37]	[1, 1]
Compare the systematic study for English-French alignments by (Fox, 2002), who compared (i) tree-bank parser style analyses, (ii) a variant with flattened VPs, and (iii) dependency structures.	[145, 129]	[1, 1]
Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments.	[105]	[1]
Compared to the representation Riedel and Clarke (2006), this bound has the benefit a small polynomial number of constraints.	[177, 200, 97]	[1, 1, 1]
Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004).	[158, 137]	[1, 1]
Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition.	[167, 0]	[1, 1]
Compared with the previous work on katakana to-English transliteration, these accuracies do not look particularly high: both Knight and Graehl (1998) and Bilac and Tanaka (2004) report accuracies above 60% for 1-best transliteration.	[17, 116]	[1, 1]
Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation.	[0]	[1]
Coordination ambiguity is under-explored, despite being one of the three major sources of structural ambiguity (together with prepositional phrase attachment and noun compound bracketing), and belonging to the class of ambiguities for which the number of analyses is the number of binary trees over the corresponding nodes (Church and Patil, 1982), and despite the fact that conjunctions are among the most frequent words.	[125, 72, 67]	[1, 1, 1]
Corpus creation using AMT has numerous precedents now; see i.e. Callison-Burch and Dredze (2010) and Heilman and Smith (2010b).	[163]	[1]
Currently ,word_align depends on char align (Church, 1993) to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet.	[81]	[1]
D-Tree Grammar (DTG) is proposed in (Rambow et al, 1995) to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduce din (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars.	[75, 3, 0]	[1, 1, 1]
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance.	[61, 111]	[1, 1]
DeSR (Attardi, 2006) is an incremental deterministic classifier-based parser.	[18]	[1]
Despite some recent advances in this direction (Bos et al, 2004), it is still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (Lev et al, 2004).	[14, 18]	[1, 1]
Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009).	[79, 69]	[1, 1]
Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees.	[3, 46, 2, 0, 159]	[1, 1, 1, 1, 1]
Distances have been used in e.g. Luo et al (2004).	[146]	[1]
Due to space constraints, details and proof of correctness are available in Lopez (2007a).	[164]	[1]
EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al, 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm.	[17, 0, 1, 15]	[1, 1, 1, 1]
Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary.	[131, 94]	[1, 1]
Early unsupervised approaches to the SRL task include (Swier and Stevenson, 2004), where theVerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.	[34, 0]	[1, 1]
Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations.	[22, 29]	[1, 1]
Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries.	[158, 164, 129]	[1, 1, 1]
Eisenstein et al (Eisenstein et al, 2010) show promising results in identifying an author's geographic location from micro-blogs, but the locations are coarse-grained and rely on a substantial message history per-source.	[175]	[1]
Eisner (1996, section 5) also provides a safe and complete parsing algorithm which can return non-NF derivations when necessary to preseve an interpretation if composition is bounded or the grammar is restricted in other (arbitrary) ways.	[77, 151]	[1, 1]
Eisner (2002) gives a general EM algorithm for parameter estimation in probabilistic finite-state transducers.	[0, 218]	[1, 1]
English to French Lexical-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Dorr, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998).	[96]	[1]
Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus, we continue to integrate the statistical word-alignment techniques (Melamed, 1997) and dictionaries to find the translation candidates for each of the two collocates.	[99]	[1]
Evaluation campaigns like WMT (Callison-Burch et al, 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks.	[5, 147]	[1, 1]
Even though we could not compare our experiment with the probabilistic approach (Lapata, 2003) directly due to the difference of the text corpora, the Kendall coefficient reported higher agreement than Lapata's experiment (Kendall=0.48 with lemmatized nouns and Kendall=0.56 with verb-noun dependencies).	[0]	[1]
Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.	[135, 133, 124, 136]	[1, 1, 1, 1]
Examples include an early statistical method for learning to fill slot-value representations (Miller et al, 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006).	[6]	[1]
Except for the addition of a tag parameter p to the SHIFT transition, this is equivalent to the system described in Nivre (2009), which thanks to the SWAP transition can handle arbitrary non-projective trees.	[120, 114, 115]	[1, 1, 1]
Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training.	[176]	[1]
Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006)).	[12, 66, 5]	[1, 1, 1]
Experiments are performed using all train/test pairs among three conversational speech corpora: the Meeting Recorder Dialog Actcorpus (MRDA) (Shriberg et al, 2004), Switch board DAMSL (Swbd) (Jurafsky et al, 1997), and the Spanish Callhome dialog act corpus (SpCH) (Levin et al, 1998).	[0]	[1]
Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008).	[227]	[1]
Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003).	[89]	[1]
Features like the string ident and substring match features were used by other researchers (Soon et al, 2001), while the features ante med and ana med were used by Strube et al (2002) in order to improve the performance for definite NPs.	[313, 314]	[1, 1]
Filatova and Hovy (2001) infer time values based on the most recently assigned date or the date of the article.	[68, 102, 71, 97, 94, 100, 95, 108, 73, 54]	[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Finally, following (Klein and Manning, 2004) we strip out punctuation from the sentences.	[23]	[1]
Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al, 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods.	[1, 0]	[1, 1]
Finally, we use as a feature the mappings produced in (Navigli, 2006) of WordNet senses to Oxford English Dictionary senses.	[21, 144, 17, 133]	[1, 1, 1, 1]
Finkel et al (2005) proposed a method incorporating non-local structure for information extraction.	[0]	[1]
Finkel et al (2005) used simulated annealing with Gibbs sampling to find a solution in a similar situation.	[39, 121]	[1, 1]
First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process.	[15]	[1]
First, we adopt an ONTOLOGICALLY PROMISCUOUS representation (Hobbs, 1985) that includes a wide variety of types of entities.	[268]	[1]
First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al, 2007).	[21]	[1]
First, we investigate the impact of using different flavours of Covington's algorithm (Covington, 2001) for non projective dependency parsing on the ten different languages provided for CoNLL-X Shared Task (Nivre et al, 2007).	[0]	[1]
Following (Blaheta and Charniak, 2000), incorrectly parsed constituents will be ignored (roughly 11% of the total) in the evaluation of the precision and recall of the function labels, but not in the evaluation of the parser.	[66, 68]	[1, 1]
Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3).	[68]	[1]
Following Galley et al (2006)' s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models.	[133, 127, 42]	[1, 1, 1]
Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300.	[42, 38]	[1, 1]
Following Hale (2001) and Levy (2008), among others, the syntactic processor uses an incremental probabilistic Earley parser to compute a metric which correlates with increased reading difficulty.	[0]	[1]
Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word.	[54]	[1]
Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration c ∈ C, 4. Ct ⊆ C is a set of terminal configurations.	[99, 95, 84, 110, 88]	[1, 1, 1, 1, 1]
Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures.	[114, 84, 86, 106, 164, 93, 112, 148]	[1, 1, 1, 1, 1, 1, 1, 1]
Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters.	[163, 162]	[1, 1]
Following models were applied: n-gram posteriors (Zens and Ney, 2006b), sentence length model, a 6-gram LM and single word lexicon models in both normal and inverse direction.	[43, 56, 0]	[1, 1, 1]
Following phrase-based methods in statistical machine translation (Koehn et al., 2003).	[0, 7, 21]	[1, 1, 1]
Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs.	[4, 0, 206, 19, 75]	[1, 1, 1, 1, 1]
Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2,..., NPj-1.	[51, 50]	[1, 1]
Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.	[14]	[1]
For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conversion.	[345]	[1]
For SemEval 2007, all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al, 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale (T) are set to 1.	[60]	[1]
For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007).	[184, 200, 101]	[1, 1, 1]
For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006).	[27, 91, 32, 3]	[1, 1, 1, 1]
For dependency parsing, McDonald and Pereira (2006) proposed a method which can incorporate some types of global features, and Riedel and Clarke (2006) studied a method using integer linear programming which can incorporate global linguistic constraints.	[76, 0]	[1, 1]
For detailed info. of the corpora and these scores, refer to (Emerson, 2005).	[98, 24]	[1, 1]
For dialogue initiative annotation, I am using Walker and Whittaker's utterance based allocation of control rules (Walker and Whittaker, 1990), which are widely used to identify dialogue initiative.	[27, 0, 54, 46]	[1, 1, 1, 1]
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	[60, 192]	[1, 1]
For each pair of nodes (u, v) in the graph, we compute the semantic similarity score (using WordNet) between every pair of dependency relation (rel: a, b) in u and v as: s (u, v)= reli u ,relj v reli=relj WN (ai ,aj) WN (bi ,bj), where rel is a relation type (e.g., nsubj) and a, b are the two arguments present in the dependency relation (b does not exist for some relations). WN (wi ,wj) is defined as the WordNet similarity score between words wi and wj. The edge weights are then normalized across all edges in the 2There exists various semantic relatedness measures based on WordNet (Patwardhan and Pedersen, 2006).	[80]	[1]
For example transducer determinization (Mohri, 1997) uses a set of pairs of states and weights.	[97, 240, 213]	[1, 1, 1]
For example, (Izumi et al, 2003) reported error rates for English prepositions that were as high as 10% in a Japanese learner corpus.	[0]	[1]
For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags.	[354]	[1]
For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess: :review and accomplish: :complete.	[85, 87, 178]	[1, 1, 1]
For example, Dojchinova and Mihov (2004) mapped their initial tag set of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules.	[217, 185, 180]	[1, 1, 1]
For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy.	[97]	[1]
For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software.	[38, 147, 40]	[1, 1, 1]
For example, Petrov et al (2012) build supervised POS taggers for 22 languages using the TNT tagger (Brants, 2000), with an average accuracy of 95.2%.	[157]	[1]
For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words.	[40]	[1]
For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c.	[141, 82]	[1, 1]
For example, an SVM-based NE-chunker run sat a rate of only 85byte/sec, while previous rule based system can process several kilobytes per second (Isozaki and Kazawa, 2002).	[13, 38, 117]	[1, 1, 1]
For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.	[87]	[1]
For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing.	[0]	[1]
For further details see e.g. (Tillmann and Ney, 2003).	[290, 61, 304, 76]	[1, 1, 1, 1]
For instance, (Cohn and Lapata, 2007) explore the use of triangulation for machine translation, where multiple translation models are learned using multilingual parallel corpora.	[0, 12]	[1, 1]
For instance, (Kay, 1996) proposes to reduce the number of constituents built during realisation by only considering for combination constituents with non overlapping semantics and compatible indices.	[111]	[1]
For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser.	[8, 91]	[1, 1]
For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes.	[106, 5, 32]	[1, 1, 1]
For instance, Teufel and Moens (2002) summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts.	[314, 0, 21, 144]	[1, 1, 1, 1]
For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates.	[149, 124, 0]	[1, 1, 1]
For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007).	[157]	[1]
For language modeling, we use RandLM (Talbot and Osborne, 2007).	[69, 0]	[1, 1]
For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008).	[385]	[1]
For reasons of speed, Yamada and Knight (2002) limited training to sentences of length 30, and were able to use only one fifth of the available Chinese-English parallel corpus.	[188, 120]	[1, 1]
For selecting the best skeleton, two common methods are choosing the hypothesis with the Minimum Bayes Risk with translation error rate (TER) (Snover et al, 2006) (i.e., the hypothesis with the minimum TER score when it is used as the reference against the other hypotheses) (Sim et al, 2007) or choosing the best hypotheses from each system and using each of those as a skeleton in multiple confusion networks (Rosti et al, 2007b).	[25, 105, 177, 116, 115, 38]	[1, 1, 1, 1, 1, 1]
For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus.	[59, 163]	[1, 1]
For the discourse structure analysis, we suggest a statistical model with discourse segment boundaries (DSBs) similar to the idea of gaps suggested for a statistical parsing (Collins (1996)).	[0]	[1]
For the experimental evaluations we use the Bidirectional Tagger with Guided Learning presented in Shen et al (2007).	[0]	[1]
For the predicate identification, we used the features suggested by Johansson and Nugues (2008).	[79, 88, 66, 75]	[1, 1, 1, 1]
For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources.	[0, 244, 55, 33, 116]	[1, 1, 1, 1, 1]
For the task of classifying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al, 2009).	[515]	[1]
For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al, 2006).	[5]	[1]
For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures.	[78, 133]	[1, 1]
For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006).	[30, 33]	[1, 1]
For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004).	[32, 50]	[1, 1]
For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful.	[6, 0]	[1, 1]
For this we adopt the approach of Blunsom et al (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision.	[1]	[1]
For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system.	[6, 0]	[1, 1]
For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts.	[26, 3, 211, 40, 0]	[1, 1, 1, 1, 1]
Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005).	[65, 56, 71]	[1, 1, 1]
Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice.	[89]	[1]
From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000).	[102]	[1]
From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007).	[0]	[1]
Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al (Hasegawa et al, 2004).	[49]	[1]
Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents.	[15, 77]	[1, 1]
Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel.	[33, 153, 24, 38, 0, 16, 18]	[1, 1, 1, 1, 1, 1, 1]
Fung and Yee (1998), for example, proposed to represent the contexts of a word or phrase with a real-valued vector (e.g., a TF-IDF vector), in which one element corresponds to one word in the contexts.	[46]	[1]
Further details of the noisy channel model can be found in Johnson and Charniak (2004).	[0, 152, 28, 142]	[1, 1, 1, 1]
Furthermore, we plan to include the Level 1 translations into the candidate answer generation module in order to do query expansion in the style of Riezler et al (2007).	[98]	[1]
Gamon et al (2008) and Gamon (2010) use a combination of classification and language modeling.	[0, 44, 51]	[1, 1, 1]
Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours.	[150, 91]	[1, 1]
Generative Lexicon (Pustejovsky, 1991), for example have been proposed to facilitate computationally precise description of natural language syntax and semantics.	[0]	[1]
Generative models are also used in unsupervised coreference (Haghighi and Klein, 2010).	[12, 13]	[1, 1]
Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007).	[0]	[1]
Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors.	[314]	[1]
Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003).	[8]	[1]
Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm.	[0]	[1]
Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009).	[74]	[1]
Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation.	[113, 4, 115]	[1, 1, 1]
Graph-based methods have been successfully applied to evaluate word similarity using available ontologies, where the underlying graph included word senses and semantic relationships between them (Hughes and Ramage, 2007).	[2]	[1]
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	[208]	[1]
Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly.	[26, 33]	[1, 1]
Hale (2001) showed that surprisal calculated from a probabilistic Earley parser correctly predicts well 356 known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena.	[4]	[1]
Hasegawa et al (2004)'s method is to use a hierarchical clustering method to cluster pairs of named entities according to the similarity of context words intervening between the named entities.	[4, 190, 71, 82, 72]	[1, 1, 1, 1, 1]
He identified topic boundaries where the LCP score was low (Kozima, 1993).	[59]	[1]
Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011).	[75]	[1]
Hierarchical Phrase-based Machine Translation, proposed by Chiang (Chiang, 2007), uses a general non-terminal label X but does not use linguistic information from the source or the target language.	[0]	[1]
Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns.	[1, 61, 98, 118, 63, 62, 34]	[1, 1, 1, 1, 1, 1, 1]
Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations.	[31, 22, 0]	[1, 1, 1]
However, Zhang et al (2008b) and Sunet al (2009) demonstrate that the additional expressivity gained from non-contiguous rules greatly improves the translation quality.	[30]	[1]
However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus.	[35]	[1]
However, even the sparse-representation version of w tends to be very large: (Isozaki and Kazawa, 2002) report that some of their second degree expanded NER models were more than 80 times slower to load than the original models (and 224 times faster to classify). This approach obviously does not scale well, both to tasks with more features and to larger degree kernels. PKE Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003).	[266, 267, 182, 129, 208]	[1, 1, 1, 1, 1]
However, previous work in machine translation leads us to believe that transferring the correlations between syntax and semantics across languages would be problematic due to argument structure divergences (Dorr, 1994).	[247]	[1]
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	[10]	[1]
However, we attempted to run an experiment as similar as possible in setup to Koehn and Knight (2002), using English Gigaword and German Europarl.	[97]	[1]
However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9].	[16, 118]	[1, 1]
ICTCLAS is developed by Chinese Academy of Science, the precision of which is 97.58% on tagging general words (Huaping Zhang et al, 2003).	[73, 0]	[1, 1]
If it is extended to labeled parsing (a straightforward extension), our formulation fully subsumes that of Riedel and Clarke (2006), since it allows using the same hard constraints and features while keeping the ILP polynomial in size.	[76]	[1]
If we follow Banko and Moore (2004) and construct a full (no OOV) morphological lexicon from the tagged version of the test corpus, we obtain 96.95% precision where theirs was 96.59%.	[110, 105, 30]	[1, 1, 1]
ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al, 1993).	[3]	[1]
Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm.	[105, 12, 1, 35, 69]	[1, 1, 1, 1, 1]
Implicit grammars Goodman (1996, 2003 ) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees.	[20, 19]	[1, 1]
In (Knight,1999) it was proved that the Exact Decoding problem is NP-Hard when the language model is a bigram model.	[0]	[1]
In 2005, Pang and Lee extended their earlier work in (Pang and Lee, 2004) to determine a reviewer's evaluation with respect to multi scales (Pang and Lee, 2005).	[1, 69]	[1, 1]
In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density.	[28, 114, 117]	[1, 1, 1]
In COGEX (Moldovan et al, 2003), a recent QA system, authors used automated reasoning for QA and showed that it is feasible, effective and scalable.	[11]	[1]
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented.	[143]	[1]
In Velikovich et al (2010), the parameters were tuned on a held out dataset.	[46]	[1]
In a different work, Riloff et al (2003) use manually derived pattern templates to extract subjective nouns by bootstrapping.	[0, 3, 18]	[1, 1, 1]
In a second experiment, we applied the feature discovery procedure to the English corpus from CoNLL 2008 (Surdeanu et al, 2008), a dependency corpus converted from the Penn Tree bank and the Brown corpus.	[109, 90]	[1, 1]
In addition, the work of Kahane et al (1998) provides a polynomial parsing algorithm for a constrained class of non projective structures.	[0]	[1]
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score.	[20]	[1]
In addition, we exploit syntactic constructions shown to be useful by other studies - lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997).	[31, 76]	[1, 1]
In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008).	[0, 1]	[1, 1]
In all our models, to simplify we assume that the sentence change information is known (as is common with this corpus (Shriberg et al, 2004)).	[68]	[1]
In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004).	[94, 48]	[1, 1]
In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002).	[0]	[1]
In contrast to Gamon (2010) and Han et al (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers.	[35, 170]	[1, 1]
In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features.	[1, 13, 152, 83]	[1, 1, 1, 1]
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010).	[23]	[1]
In fact, the Stanford coreference resolver (Lee et al 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however.	[6, 0]	[1, 1]
In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al, 2005b).	[63]	[1]
In order to produce the gold standard annotations in GerManC-GS we used the GATE platform, which facilitates automatic as well as manual annotation (Cunningham et al 2002).	[227]	[1]
In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003).	[79, 111]	[1, 1]
In order to test the argument above, and as an attempt to improve the results from the previous experiment, POS-tags were induced using Biemann's unsupervised POS-tagger (Biemann, 2006).	[35]	[1]
In our corpus study (Poesio and Vieira, 1998) we found that our subjects did better at ideutifying discourse-new descriptions all together.	[335]	[1]
In our earlier work, we (Schone and Jurafsky (2000)) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes.	[40]	[1]
In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper.	[45, 32, 124]	[1, 1, 1]
In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).	[80, 87, 97]	[1, 1, 1]
In our implementation we approach these tasks in a two-step approach as proposed in (Gamon et al, 2008).	[24]	[1]
In our model, we adopted the subtree kernel method for the shortest path dependency kernel (Bunescu and Mooney, 2005).	[0]	[1]
In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al (2008a).	[20]	[1]
In our string-to-tree model, for efficient decoding with integrated n-gram LM, we follow (Zhang et al, 2006) and inversely binarize all translation rules into Chomsky Normal Forms that contain at most two variables and can be incrementally scored by LM.	[40]	[1]
In particular, instead of back-off, smoothing techniques could be investigated to reduce the impact of zero probability problems (Chen and Goodman, 1996).	[0]	[1]
In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov + bag-of-words model for language generation.	[0]	[1]
In particular, the first alignment model we will present has already been described in (Melamed, 2000).	[122, 59]	[1, 1]
In particular, we used the method of Collins et al (1999) to simplify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features.	[70]	[1]
In recent years, Akkaya et al (2009) report a successful empirical result where WSD helps improving sentiment analysis, while Wiebe and Mihalcea (2006) study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis.	[16, 19]	[1, 1]
In recent years, NomBank (Meyers et al,2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machine learned approaches such as (Pradhan et al, 2004) and (Liu and Ng, 2007), which perform fairly wellin the overall task of classifying deverbal arguments.	[76]	[1]
In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b).	[142, 130, 139]	[1, 1, 1]
In the OntoNotes project (Hovy et al., 2006), annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses, with the procedure restricted to maintain 90% inter-annotator agreement.	[26]	[1]
In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings.	[0, 1, 58]	[1, 1, 1]
In the current BioNLP 11 Shared Task1 (Kim et al, 2011), we demonstrate its generalizability to different event extraction tasks by applying what is, to a large extent, the same system to every single task and subtask.	[3, 0]	[1, 1]
In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN).	[75, 396, 0]	[1, 1, 1]
In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002).	[0, 18]	[1, 1]
In the graph-based algorithm (Krahmer et al, 2003), which we refer to as Graph, information about domain objects is represented as a labelled directed graph, and REG is modeled as a graph-search problem.	[300, 11, 320, 22]	[1, 1, 1, 1]
In the system by Vieira and Poesio (2000), for example, WordNet is consulted to obtain the synonymy, hypernymy and meronymy relations for resolving the definite anaphora.	[405]	[1]
In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002).	[140]	[1]
In this bakeoff, our basic model is based on the framework described in the work of Ratnaparkhi (1996) which was applied for English POS tagging.	[14, 86]	[1, 1]
In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004).	[15, 13]	[1, 1]
In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables.	[106]	[1]
In this paper, we describe 1) a new dependency conversion (Section 3) of the Penn Treebank (Marcus, et al, 1993) along with the associated dependency label scheme, which is based upon the Stanford parser's popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5).	[12, 25]	[1, 1]
In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine.	[145]	[1]
In this paper, we focus on phrase structure parsing with function labelling as a post-processing step. Integer linear programs have already been successfully used in related fields including semantic role labelling (Punyakanok et al, 2004), relation and entity classification (Roth and Yih, 2004), sentence compression (Clarke and Lapata, 2008) and dependency parsing (Martins et al, 2009).	[151, 0]	[1, 1]
In this paper, we incorporate the MERS model into a state of-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al, 2006).	[5, 22, 1, 24, 126, 28]	[1, 1, 1, 1, 1, 1]
In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006).	[121]	[1]
In this section, we make a comparison of several different TAG parsing algorithms - the CYK based algorithm described at (Vijay-Shanker and Joshi, 1985), Earley-based algorithms with (Alonso et al, 1999) and without (Schabes, 1994) the valid prefix property (VPP), and Nederhof's algorithm (Nederhof, 1999) - on the XTAG English grammar (release 2.24.2001), by using our system and the ideas we have explained.	[89]	[1]
In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT NAACL word-alignment workshop (Mihalcea and Pedersen, 2003).	[98]	[1]
In this work, we focus on Twitter because the labeled corpus by Gimpel et al (2011) allows us to quantitatively evaluate our approach.	[5]	[1]
Incremental refers to the results reported in Seginer (2007).	[23, 0]	[1, 1]
Inspired by Lin (1999), we examine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity.	[69]	[1]
Inverted semrel structure from a definition of motorist Researchers who produced spreading activation networks from MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks.	[0]	[1]
It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet.	[12, 1, 9, 153]	[1, 1, 1, 1]
It has often been proposed that children might make use of information about the contextual distribution of usage of words to induce the parts-of-speech of their native language (e.g. Maratsos and Chalkley, 1980), and work by, e.g., Redington, Chater and Finch (1998) and Clark (2000), showed that parts-of-speech can indeed be induced by clustering together words that are used in similar contexts in a corpus.	[0]	[1]
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).	[0]	[1]
It is a slight variation of the proof given by Rush et al (2010).	[122, 110]	[1, 1]
It is based on the transition-based dependency parsing paradigm (Nivre, 2008).	[110, 164]	[1, 1]
It is interesting to compare our approach with techniques for well-nested dependency trees (Kuhlmann and Nivre, 2006).	[36, 66, 68, 67]	[1, 1, 1, 1]
It is known that PMI gives undue importance to low frequency events (Dunning, 1993), therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus.	[143]	[1]
It is possible to prove that, provided the training set (xi ,zi) is separable with margin > 0, the algorithm is assured to converge after a finite number of iterations to a model with zero training errors (Collins and Roark, 2004).	[53, 52, 57]	[1, 1, 1]
It might be used to rapidly compute approximate outside-probability estimates to prioritize best-first search (e.g., Caraballo and Charniak, 1998).	[192]	[1]
It outperforms most of the systems participating in the task (Pradhan et al., 2007).	[131]	[1]
Its original PoS tag set is very coarse and the PoS and the word stem information is not very reliable. We therefore decided to retag the tree bank automatically using the Memory-Based Tagger (MBT) (Daelemans et al, 1996) which uses a very fine-grained tag set.	[0]	[1]
Jing and McKeown (2000) manually analyzed 30 human-written summaries, and find that 19% of sentences can not be explained by cut-and-paste operations from the source text.	[44, 1, 64, 9, 46, 63, 16, 108, 17]	[1, 1, 1, 1, 1, 1, 1, 1, 1]
KAON Text-To-Onto (Maedche and Staab, 2004) applies text mining algorithms for English and German texts to semi-automatically create an ontology, which includes algorithms for term extraction, for concept association extraction and for ontology pruning. Pattern-based approaches to extract hy ponym/hypernym relationships range from hand-crafted lexico-syntactic patterns (Hearst, 1992) to the automatic discovery of such patterns by e.g. a minimal edit distance algorithm (Pantel et al, 2004).	[31]	[1]
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	[39, 44, 2, 52, 78, 0]	[1, 1, 1, 1, 1, 1]
Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank.	[0]	[1]
Karlgren and Cutting (1994) use a combination of structural markers (e.g., noun count), lexical markers (e.g., it count), and token-level markers (e.g., words per sentence average ,type/token ratio, etc.).	[46]	[1]
Kay and Roscheisen (1993) tried lexical methods for sentence alignment.	[12]	[1]
Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct.	[38, 144, 134]	[1, 1, 1]
Kupiec (1993) applied finite state transducer in his noun phrases recogniser for both English and French.	[65, 75, 50]	[1, 1, 1]
Kupiec proposes an Mgorithm for finding noun phrases in bilingual corpora (Kupiec, 1993).	[0, 88]	[1, 1]
Kurohashi and Nagao (1994) proposed a method to detect conjunctive structures by calculating similarity scores between two sequences of bunsetsus.	[45]	[1]
Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj).	[168, 190, 175, 142]	[1, 1, 1, 1]
Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models.	[195, 175]	[1, 1]
Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation.	[27, 25, 32]	[1, 1, 1]
Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previously assumed).	[62, 95]	[1, 1]
Like Toutanova and Moore (2002), we use the n-gram LTP model from Fisher (1999) to predict these pronunciations.	[77, 24]	[1, 1]
Lin (1999) assumes that a target expression is non-compositional if and only if its (I) J+ value is significantly different from that of any of the variants.	[48, 124, 49]	[1, 1, 1]
Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features.	[17, 216, 217]	[1, 1, 1]
Lu et al (2008) introduced a generative semantic parsing model using a hybrid-tree framework.	[0]	[1]
MULTIR uses features which are based on Mintz et al (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information.	[100, 103, 94]	[1, 1, 1]
Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in (Hwa et al, 2002).	[105, 123]	[1, 1]
Many discourse segmentation techniques (e.g. Hirschberg and Litman, 1993) as well as some topic segmentation algorithms rely on cue words and phrases.	[19]	[1]
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	[138]	[1]
Many of the possible cooccurrences are not observed even in a very large corpus (Church and Mercer, 1993).	[318, 322]	[1, 1]
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	[4, 15, 29, 26, 44, 0]	[1, 1, 1, 1, 1, 1]
Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation.	[0]	[1]
Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores.	[37]	[1]
Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data.	[102]	[1]
Maximum entropy classification (MaxEnt) is a technique which has proven effective in a number of natural language processing applications (Berger et al, 1996).	[0]	[1]
McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint.	[67]	[1]
Meanwhile, we also use a word-level combination framework (Rosti et al, 2007) to combine the multiple translation hypotheses and employ a new rescoring model to generate the final result.	[27, 21, 0]	[1, 1, 1]
Medlock and Briscoe (2007) extended the work of Light et al (2004) by refining their annotation guidelines and creating a publicly available data set (FlyBase data set) for speculative sentence classification.	[22, 25]	[1, 1]
Medlock and Briscoe (2007) proposed a weakly supervised setting for hedge classification in scientific texts where the aim is to minimise human supervision needed to obtain an adequate amount of training data.	[0]	[1]
Melamed (1997b), however, proposes a method for the recognition of multi word compounds in bi texts that is based on the predictive value of a translation model.	[17, 139]	[1, 1]
Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR).	[147, 146]	[1, 1]
Messages are partitioned into multi-paragraph segments using TextTiling, which reportedly has an overall precision of 83% and recall of 78% (Hearst, 1994).	[145, 0, 19]	[1, 1, 1]
Mikheev (1997) suggested a guessing-rule technique, based on prefix morphological rules ,suffix morphological rules, and ending-guessing rules.	[9, 263, 4, 47, 155, 114, 51, 53]	[1, 1, 1, 1, 1, 1, 1, 1]
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc.	[12, 0, 1]	[1, 1, 1]
Minipar outputs dependency trees (Lin, 1999) from the input sentences.	[18]	[1]
Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.	[20, 37, 29, 21]	[1, 1, 1, 1]
Moens and Steedman (1988) describes temporal expressions relating to changes of state.	[90, 59]	[1, 1]
Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)).	[26]	[1]
Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data.	[23]	[1]
Moore and Pollack (1992) gave an example of a simple discourse.	[101, 65, 30, 79]	[1, 1, 1, 1]
More advanced methods like those described by Weischedel et al (1993) incorporate the treatment of unknown words within the probability model.	[146, 147]	[1, 1]
More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006).	[0]	[1]
More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003).	[0, 49]	[1, 1]
More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus.	[197, 3, 98, 195]	[1, 1, 1, 1]
More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories.	[11, 9]	[1, 1]
More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009).	[9]	[1]
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents.	[22]	[1]
More suitable ways could be bilingual chunk parsing, and refining the bracketing grammar as described in [Wu 1997].	[0]	[1]
Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost.	[100, 6, 10]	[1, 1, 1]
Morfessor Baseline (Creutz and Lagus, 2002): This is a public baseline algorithm based on jointly minimizing the size of the morph codebook and the encoded size of all the word forms using the minimum description length MDL cost function.	[3, 144, 238, 74]	[1, 1, 1, 1]
Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively.	[0]	[1]
Most of the features we use are described in more detail in (Toutanova et al, 2005).	[37, 41, 58]	[1, 1, 1]
Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics.	[0]	[1]
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	[0, 13]	[1, 1]
Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al (2002), Turney (2002), Dave et al (2003), Pang and Lee (2004)).	[23]	[1]
Much recent work on temporal relations revolved around the TimeBank and TempEval (Verhagen et al., 2007).	[9, 0, 8]	[1, 1, 1]
Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences.	[0, 2]	[1, 1]
Nivre (2004) investigated the issue of (strict ) incrementality for this type of parsers ;i.e., if at any point of the analysis the processed input forms one connected structure.	[34, 38]	[1, 1]
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	[325]	[1]
Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable.	[128]	[1]
Note that our results for decoding are sharper than that of (Knight, 1999).	[105]	[1]
Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in Hoffmann et al (2011) extracted a relation.	[124, 100]	[1, 1]
Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument.	[26]	[1]
Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.	[31, 27, 137]	[1, 1, 1]
Numerous examples of the utility of word lattices come from the field of finite state automata, language modeling, speech recognition, parsing and machine translation (Mohri, 1997, inter alia).	[0, 447]	[1, 1]
Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words.	[0, 74, 14, 3, 68, 83]	[1, 1, 1, 1, 1, 1]
Of course, it is a known fact that machine learning techniques do not transfer well across different domains (e.g., Blitzer et al (2006)).	[5, 220]	[1, 1]
Of course, other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics.	[0]	[1]
Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel.	[86]	[1]
On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries.	[52, 48]	[1, 1]
On the one hand machine learning is used to automate as much as possible the tasks an IE expert would perform in application development (Cardie 1997) (Yangarber et al 2000).	[69]	[1]
On the other hand, as Abney (1997) points out, the context-sensitive dependencies that unification-based constraints introduce render the relative frequency estimator suboptimal.	[57]	[1]
On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.	[107]	[1]
One of the most important approaches is Lin (1998).	[96]	[1]
One of the most popular methods leveraging bilingual parallel corpora is proposed by Bannard and Callison-Burch (2005).	[0]	[1]
One set was extracted from a hand-tagged corpus (Bruce and Wiebe, 1994) and the other by our algorithm.	[92]	[1]
One system (Hall et al, 2007b) extends this two-stage approach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked.	[8]	[1]
Only one model was used for syntactic parsing in our system, in contrast to the existing work using an ensemble technique for further performance enhancement, e.g., (Hall et al, 2007).	[8]	[1]
Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009).	[37, 19]	[1, 1]
Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs.	[0, 24]	[1, 1]
Other resources for sentiment detection include the Dictionary of Affect in Language (DAL) to score the prior polarity of words, as in Agarwal et al (2011) on social media data.	[117, 127]	[1, 1]
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	[194, 78, 69]	[1, 1, 1]
Other works describe systems that induce structures from corpora, but they use tagged corpora (Brill, 1993), or grammatical informations (Brent, 1993).	[57]	[1]
Our POS results, gathered using a Twitter-specific tagger (Gimpel et al, 2011), echo those of Ashok et al (2013) who looked at predict 14 Of course, simply inserting garbage isn't going to lead to more re-tweets, but adding more information generally involves longer text.	[19]	[1]
Our approach to QC follows that of (Li and Roth, 2002).	[19, 3]	[1, 1]
Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC.	[60, 64, 159]	[1, 1, 1]
Our eventual goal is to develop a set of regular expressions that work on fiat tagged corpora instead of TreeBank parsed structures to allow us to gather information from larger corpora than have been done by the TreeBank project (see Manning 1993 and Gahl 1998).	[0]	[1]
Our general estimation method also has practical applications in cases one uses a probabilistic context-free grammar to approximate strictly more powerful rewriting systems, as for instance probabilistic tree adjoining grammars (Schabes, 1992).	[182, 159, 140]	[1, 1, 1]
Our input has been parsed into Rasp-style tGRs (Briscoe et al, 2006), which facilitates comparison with previous work based on the same data set.	[59]	[1]
Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007).	[0, 4, 26]	[1, 1, 1]
Our method is modeled on the approach developed by Stevenson and Greenwood (2005) but uses a different technique for ranking candidate patterns.	[142, 104]	[1, 1]
Our model is inspired by Centering (Grosz et al, 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences.	[0]	[1]
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005).	[139]	[1]
Our results using 1NN, 72:4%, are comparable to those of Katz and Giesbrecht (2006) using this method on their German data (72%).	[106, 122]	[1, 1]
Our system is based on the GATE natural language processing framework (Cunningham et al, 2002) and it uses the ANNIE IE system included in the standard GATE distribution for text tokenization, sentence splitting and part-of-speech tagging.	[28, 163]	[1, 1]
Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000).	[47, 6]	[1, 1]
Our test set contains the same 107 documents as Culotta et al (2007).	[81]	[1]
Our work focuses on a recent line of exploratory work in the direction of Unrestricted Relation Discovery which is defined as: the automatic identification of different relations in text without specifying a relation or set of relations in advance (Shinyama and Sekine, 2006).	[0]	[1]
Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003).	[25]	[1]
PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears.	[162, 62, 26]	[1, 1, 1]
Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation.	[1]	[1]
Pang et al (2002) and Turney (2002) classified sentiment polarity of reviews at the document level.	[147]	[1]
Pang et al (Pang et al, 2003) used parallel monolingual corpora built from news stories that had been independently translated several times to learn lattices from a syntax-based alignment process.	[33]	[1]
Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she.	[11, 9, 15, 145]	[1, 1, 1, 1]
Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area.	[27]	[1]
Part of speech tags are assigned by Brill's tagger (Brill, 1992).	[0]	[1]
Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006).	[60]	[1]
Patwardhan and Pedersen (2006) evaluate six knowledge-based measures on the task of word sense disambiguation and report the same result.	[127, 126, 124]	[1, 1, 1]
Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest).	[19, 74, 152]	[1, 1, 1]
Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009).	[99, 160, 72]	[1, 1, 1]
Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience.	[0, 76]	[1, 1]
Preliminary experiments with tags derived automatically using distributional clustering (Clark, 2000), have shown essentially the same results.	[72]	[1]
Preposition errors are common among new English speakers (Chodorow et al, 2007).	[12]	[1]
Previous shared tasks have shown that frame-semantic SRL of running text is a hard problem (Baker et al, 2007), partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available.	[62]	[1]
Previous work (Kudo et al, 2004) showed CRFs outperform generative Markov models and discriminative history-based methods in JWS.	[62, 2, 128]	[1, 1, 1]
Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)).	[9]	[1]
Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSA to distinguish between compositional and non compositional verb-particle constructions and noun noun compounds.	[0]	[1]
Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries.	[0]	[1]
QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996).	[91, 49, 62, 154, 52]	[1, 1, 1, 1, 1]
RAP (Kennedy and Boguraev, 1996), Baldwin's pronoun resolution method (Baldwin, 1997) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b).	[46, 0]	[1, 1]
ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a).	[112, 96]	[1, 1]
Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data.	[124]	[1]
Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns.	[0]	[1]
Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)).	[7]	[1]
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	[1, 151, 5, 131]	[1, 1, 1, 1]
Recently, most evaluations of machine translation systems (Callison-Burch et al, 2009) indicate that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method.	[55, 188, 5, 0]	[1, 1, 1, 1]
Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al, 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005).	[113]	[1]
Recently, there is significant research interest in a related task called Web Person Search (WePS) (Artiles et al, 2007), which seeks to determine whether two documents refer to the same person given a person name search query.	[0]	[1]
Recognizing textual entailment is to determine whether a sentence (sometimes a short paragraph) can entail the other sentence (Giampiccolo et al, 2007).	[0]	[1]
Regarding the two state-of-the-art word segmentation systems, one is JUMAN,  a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB, a supervised word segmentation system based on CRFs (Kudo et al, 2004).	[11]	[1]
Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline.	[37, 66]	[1, 1]
Resnik (1999) mined comparable corpora on the assumption that the pages which are comparable of each other share a similar structure (headers, paragraphs, etc.) when text is presented in many languages in the Web.	[0]	[1]
Riezler et al (2003) applied linguistically rich LFG grammars to a sentence compression system.	[159, 34]	[1, 1]
Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision.	[31]	[1]
Riloff et al (2003) explore bootstrapping techniques to identify subjective nouns and subsequently classify subjective vs. objective sentences in newswire text.	[1]	[1]
Roark and Charniak (Roark and Charniak, 1998) followed up on this work by using a parser to explicitly capture these structures.	[24, 45]	[1, 1]
Rosti et al (2007a) collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations.	[82, 22]	[1, 1]
Roth and Yih (2004) also described a classification-based framework in which they jointly learn to identify named entities and relations.	[136]	[1]
Roth and Yih (2004) combined information from named entities and semantic relation tagging, adopting a similar overall goal but using a quite different approach based on linear programming.	[204, 86]	[1, 1]
Roth and Yih (2004) formulated the problem of extracting entities and relations as an integer linear program, allowing them to use global structural constraints at inference time even though the component classifiers were trained independently.	[146, 0]	[1, 1]
Roth and Yih (2004) use log probabilities as weights.	[159, 154]	[1, 1]
SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community.	[19]	[1]
Sample XFST code Finite-state transducers are robust and time and space efficient (Mohri, 1997).	[177, 50, 173, 498, 0]	[1, 1, 1, 1, 1]
Schwarm and Ostendorf (2005) studied four parse tree features (average parse tree height, average number of SBARs, noun phrases, and verb phrases per sentences).	[141]	[1]
Second, using a heuristic proposed in (Och et al, 1999), all the aligned phrase pairs (x?, a?, y?) satisfying the following criteria are extracted: (1) x? and y? consist of consecutive words of x and y, and both have length at most k, (2) a? is the alignment between words of x? and y? induced by a, (3) a? contains at least one link, and (4) there are no links in a that have just one end in x? or y?.	[146]	[1]
See Kuhlmann and Nivre (2006) for the definition of edge degree.	[109, 112, 87]	[1, 1, 1]
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	[0, 151, 5]	[1, 1, 1]
Semantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005).	[39, 113, 0]	[1, 1, 1]
Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al 2006), fact extraction for information retrieval (Pas?ca et al 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al 2011), among many others (Turney and Pantel, 2010).	[7]	[1]
Sentence-level filter: The word-overlap filtering (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two.	[81]	[1]
Settles (2004)'s CRF system deserves special note in the sense that it achieved comparable performance to top ranked systems with a rather simple feature set.	[69]	[1]
Several efforts have been made to develop automatic WSD systems that can provide accurate sense tagging (Ide and Veronis, 1998), with a current emphasis on creating manually sense-tagged data for supervised training of statistical WSD systems, as evidenced by SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Edmonds and Cotton, 2001).	[224]	[1]
Shinyama and Sekine (2006) developed an approach to preemptively discover relations in a corpus and present them as tables with all the entity pairs in the table having the same relations between them.	[18, 33]	[1, 1]
Similar IR features are also used by other WePS systems as they are more robust to the variety of web pages (Artiles et al, 2007).	[30]	[1]
Similar methods to Shen et al (2007) have also been used in Shen and Joshi (2008) and Goldberg and Elhadad (2010).	[56]	[1]
Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2.	[109]	[1]
Similar to the definition of tree sequence used in a single parse tree defined in Liu et al (2007) and Zhang et al (2008a), a tree sequence in a forest also refers to an ordered sub-tree sequence that covers a continuous phrase without overlapping.	[0, 1, 59]	[1, 1, 1]
Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved F measure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation.	[57, 118, 49]	[1, 1, 1]
Similarly to the work by Germann et al (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step.	[58]	[1]
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998).	[31]	[1]
Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al 2011), query logs (Chen et al 2007), and large n-gram corpora (Brants et al., 2007) are employed.	[33, 0]	[1, 1]
Since the original code from [Charniak and Johnson 2001] is not available, we conducted our first experiment to replicate the result of their baseline system described in section 3.	[144, 151, 31, 29]	[1, 1, 1, 1]
Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003).	[12]	[1]
Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between.	[112]	[1]
Smith and Eisner (2006) used a structural locality bias, experimenting on five languages.	[30, 0]	[1, 1]
Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity.	[292]	[1]
Smith and Eisner (2009) propose effective QG features for parser adaptation and projection.	[2]	[1]
Soon et al (2001) include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data.	[204, 240, 178]	[1, 1, 1]
Speech was found to improve inter-annotator agreement in discourse segmentation of monologues (Hirschberg and Nakatani 1996).	[0]	[1]
Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005).	[0]	[1]
Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities.	[102]	[1]
Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns.	[102]	[1]
Stevenson and Wilks (2001) presented a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999).	[440, 315, 201]	[1, 1, 1]
Subsequently, a method was developed to use a special case of the ITGR the aforementioned BTGR for the translation task itself (Wu, 1996).	[77]	[1]
Such a description would be returned by a generalised version of Algorithm 1 proposed by van Deemter (2002).	[64]	[1]
Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008).	[42]	[1]
Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003).	[0, 28]	[1, 1]
Syntactic Parallelism (Lappin and Leass, 1994).	[66]	[1]
TBL is a machine learning approach that has been employed to solve a number of problems in natural language processing; most famously, it has been used for part-of-speech tagging (Brill, 1995).	[36, 0]	[1, 1]
Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger.	[53, 84]	[1, 1]
Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM.	[114, 115, 126, 10, 43, 11, 125]	[1, 1, 1, 1, 1, 1, 1]
Table 1: The patterns we used for entailment acquisition based on (Hearst, 1992) and (Pantel et al, 2004).	[20]	[1]
Table 2 shows the best results among the configurations we have tested (expressed using the official evaluation measures, see (Callison-Burch et al, 2012) for details).	[127]	[1]
Takamura et al (2005) used the Ising model to extract semantic orientations of words (not phrases).	[61, 198, 0, 1, 28, 188]	[1, 1, 1, 1, 1, 1]
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	[29, 334, 0]	[1, 1, 1]
The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding transformations.	[36]	[1]
The BioNLP 09 Shared Task (Kim et al., 2009) was the first shared task that provided a consistent data set and evaluation tools for extraction of such biological relations.	[0]	[1]
The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing.	[25, 32, 22, 0]	[1, 1, 1, 1]
The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG super tagger; and three, a hierarchical level consisting of CCG derivations.	[146]	[1]
The Chinese Nombank extends the general annotation framework of the English Proposition Bank (Palmer et al, 2005) and the English Nombank (Meyers et al, 2004) to the annotation of nominalized predicates in Chinese.	[156]	[1]
The CoNLL 2011 Shared Task (Pradhan et al,2011) is dedicated to modeling unrestricted coreference in OntoNotes.	[0]	[1]
The CoNLL-2007 shared tasks include two tracks: the Multilingual Track and Domain AdaptationTrack (Nivre et al, 2007).	[18, 2, 0]	[1, 1, 1]
The Context-Sensitive extension (Krahmer and Theune, 2002) is able to generate referring expressions for the most salient entity in a context; the Boolean Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups.	[0, 169]	[1, 1]
The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing.	[39, 0]	[1, 1]
The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features.	[1, 151, 37, 44, 34, 66]	[1, 1, 1, 1, 1, 1]
The Korean-English parallel data was collected from news websites and sentence aligned using two different tools described by Moore (2002) and Melamed (1999).	[321]	[1]
The LBJ Tagger is based on a regularized average perceptron (Ratinov and Roth, 2009).	[62]	[1]
The MDA (Multilingual Document Authoring) system [Brun et al2000] is an instance (descended from Ranta's Grammatical Framework [Ranta 2002]) of a text-mediated interactive natural language generation system, a notion introduced by [Power and Scott 1998] under the name of WYSIWYM.	[2, 23]	[1, 1]
The MDL-based tree cut model was originally introduced for handling the problem of generalizing case frames using a thesaurus (Li and Abe, 1998).	[0, 296, 77, 48, 2, 8]	[1, 1, 1, 1, 1, 1]
The OntoNotes 90% solution (Hovy et al 2006) actually means such a degree of granularity that enables a 90% IAA.	[0]	[1]
The RASP parser is a generalized LR parser which builds a non-deterministic generalized LALR parse table from the grammar (Tomita, 1987).	[47, 2, 12]	[1, 1, 1]
The VPs with the verbs start or finish (see Pustejovsky, 1991) can also be accounted for using the qualia structure.	[65]	[1]
The Vieira/Poesio algorithm (Vieira and Poesio, 2000) attempts to classify each definite description as either direct anaphora, discourse-new, or bridging description.	[77, 44, 61, 474, 453, 80, 536]	[1, 1, 1, 1, 1, 1, 1]
The actual realization of the component is based on a constraint-based inheritance algorithm that follows the example of PATR-II (Shieber et al, 1989).	[0]	[1]
The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice.	[174, 132, 154, 140, 234, 150, 14]	[1, 1, 1, 1, 1, 1, 1]
The average senior high school student achieves 57% correct (Turney, 2006).	[411, 58, 7, 19]	[1, 1, 1, 1]
The best F score and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the results using HB-dict+GHM-dict+S-dict and HB-dict+GHMdict is statistically significant (p 0.01), based on the computationally-intensive Monte Carlo method of Yeh (2000), demonstrating the contribution of Sdict.	[184, 185]	[1, 1]
The bootstrapping methods for language independent NER of Cucerzan and Yarowsky (1999) have a similar effect.	[0]	[1]
The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.	[110, 141, 137, 104]	[1, 1, 1, 1]
The core model utilized, extended and evaluated here is based on Cucerzan and Yarowsky (1999).	[159]	[1]
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	[70, 0, 69]	[1, 1, 1]
The data in this corpus is automatically aligned using a technique presented in (Ittycheriah and Roukos, 2005).	[187, 35, 215]	[1, 1, 1]
The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation (Brown et al, 1990).	[0, 120, 3]	[1, 1, 1]
The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies.	[72, 76]	[1, 1]
The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004).	[33, 25, 11, 48, 18, 38]	[1, 1, 1, 1, 1, 1]
The early-update strategy of Collins and Roark (2004) is used so as to improve accuracy and speed up the training.	[161, 18]	[1, 1]
The end result of our selection and aggregation module (see section 6.2) is a fully specified logical form which is to be sent to the Semantic-Head Driven Generation component of Gemini (Shieber et al, 1990).	[0]	[1]
The error model LTR was trained exactly as described originally by Brill and Moore (2000).	[21, 0]	[1, 1]
The evaluations were carried out with the SVMlight-TK software (Moschitti, 2004) available at http: //ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims, 1999).	[134]	[1]
The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011).	[161, 4]	[1, 1]
The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words).	[112, 24]	[1, 1]
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12.	[7]	[1]
The extraction patterns used by both Yangarber et al (2000) and Stevenson and Greenwood (2005) were based on SVO tuples extracted from dependency trees.	[69]	[1]
The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001).	[112, 5]	[1, 1]
The feature structure (adopted from Karttunen, 1984, p. 30) represents disjunctions by enclosing the alternatives in curly brackets ({}).	[49]	[1]
The features that define the constraints on the model are obtained by instantiation of feature templates as in Ratnaparkhi (1996).	[32, 25]	[1, 1]
The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings.	[21, 50]	[1, 1]
The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts.	[414, 412, 206, 205, 70]	[1, 1, 1, 1, 1]
The first approach to predict user judgments on the basis of interaction metrics is the well-known PARADISE model (Walker et al, 1997).	[116, 153]	[1, 1]
The first decoder, Hiero Cube Pruning (HCP), is a k-best decoder using cube pruning implemented as described by Chiang (2007).	[189, 227]	[1, 1]
The first example of this approach was the multi-engine MT system (Frederking and Nirenburg, 1994), which builds a chart using the translation units inside each input system and then uses a chart walk algorithm to find the best cover of the source sentence. Rosti et al (2007a) collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations.	[2]	[1]
The first feature is the absolute difference between ai and ai-1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrase translation models (Koehn, 2004).	[8, 17]	[1, 1]
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	[63, 101]	[1, 1]
The first sentence alignment model used to align English-Chinese bilingual texts is proposed by Wu (1994).	[1, 44, 94]	[1, 1, 1]
The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007).	[75]	[1]
The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011).	[155]	[1]
The following models are used as benchmark: (i) PYTHY (Toutanova et al, 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models.	[3]	[1]
The idea of bidirectional parsing is related to the bidirectional sequential classification method described in (Shen et al, 2007).	[0]	[1]
The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002).	[75, 73, 14]	[1, 1, 1]
The intrinsic evaluation measures used in our experiments are the well-known BLEU (Papineni et al., 2001) and NIST (Doddington, 2002) metrics, and an F-score measure that adapts evaluation techniques from dependency-based parsing (Crouch et al., 2002) and sentence-condensation (Riezler et al, 2003) to machine translation.	[172]	[1]
The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data.	[0]	[1]
The latter utilized several resources for matching hypothesis terms with text terms: WordNet, VerbOcean (Chklovski and Pantel, 2004), utilizing two of its relations, as well as an acronym database ,number matching module, co-reference resolution and named entity recognition tools.	[0]	[1]
The list of polarity words that we use in this component has been taken from the OpinionFinder system (Wilson et al, 2005).	[39]	[1]
The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998).	[97]	[1]
The maximum entropy parser (Ratnaparkhi, 1997) is used in this study, for it offers the flexibility of integrating multiple sources of knowledge into a model.	[0]	[1]
The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified.	[202]	[1]
The morphology algorithm proposed by Kay and Roscheisen (1993) is applied for splitting potential suffixes and prefixes and for obtaining the normalised word forms.	[246]	[1]
The obtained accuracy is around 96% and was computed indirectly by checking disagreement with the Brown sentence aligner (Brown et al, 1991) on randomly selected 500 disagreement cases.	[19, 4]	[1, 1]
The only exception is in (Wacholder et al 1997) where the reported performance for the sole semantic disambiguation task of PNs is 79%.	[57]	[1]
The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper.	[97, 77]	[1, 1]
The other dimensions of both domains were difficult to interpret. We experimented with using the SCL features together with the raw features (n-grams and length), as suggested by (Blitzer et al, 2006).	[16, 42]	[1, 1]
The output of this model is incorporated into the machine translation system by providing the WSD probabilities for a phrase translation as extra features in a log-linear model (Carpuat and Wu, 2007).	[101]	[1]
The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008).	[92]	[1]
The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar.	[3, 0, 29, 38]	[1, 1, 1, 1]
The part-of-speech tagging uses the Curran and Clark POS tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al, 2004), whilst the other preprocessing stages are all rule based.	[15, 28]	[1, 1]
The patterns we used for entailment acquisition based on (Hearst, 1992) and (Pantel et al, 2004).	[30, 120, 36]	[1, 1, 1]
The performance of many natural language processing tasks, such as shallow parsing (Zhang et al., 2002) and named entity recognition (Florianet al, 2004), has been shown to depend on integrating many sources of information.	[35, 9, 118]	[1, 1, 1]
The pioneering work on fusion is Barzilay and McKeown (2005), which introduces the frame work used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output.	[198, 130, 426]	[1, 1, 1]
The present paper deals with five parsers evaluated within the translation frame work: three genuine dependency parsers, namely the parsers described in (McDonald et al, 2005), (Nivre et al, 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs we reconverted to dependency structures by Penn Converter (Johansson and Nugues, 2007).	[15]	[1]
The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work.	[108, 89]	[1, 1]
The proposed method in (Takamura et al, 2005) extracts semantic orientations from a small number of seed words with high accuracy in the experiments on English as well as Japanese lexicons.	[4, 190, 17, 188, 28, 18, 1, 156]	[1, 1, 1, 1, 1, 1, 1, 1]
The recent availability of more corpora has enabled much information about dependency relations to be obtained by using a Japanese dependency analyzer such as KNP (Kurohashi and Nagao, 1994) or CaboCha (Kudo and Matsumoto,2002).	[0]	[1]
The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three.	[90]	[1]
The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999).	[105]	[1]
The same beam-search pruning as described in (Tillmann and Ney, 2003) is used.	[264, 247, 253, 240, 19]	[1, 1, 1, 1, 1]
The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al, 2007), combining six transition-based parsers.	[8]	[1]
The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A* search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity.	[161]	[1]
The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006).	[54, 63]	[1, 1]
The sentence has 42 readings (Hobbs and Shieber, 1987), and it is easy to imagine how the number of readings grows exponentially (or worse) in the length of the sentence.	[24, 21]	[1, 1]
The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes.	[1, 29, 0, 5, 52, 32]	[1, 1, 1, 1, 1, 1]
The similarity measure simwN is based on the proposal in (Lin, 1997).	[158]	[1]
The state space of our model resembles that of Kurohashi and Nagao's Japanese coordination detection method (Kurohashi and Nagao, 1994).	[0]	[1]
The states could be more refined than those shown above: the state for the subject, for example, should probably be not NP but a pair (Npl, NP3s) .STSG is simply a version of synchronous tree adjoining grammar or STAG (Shieber and Schabes, 1990) that lacks the adjunction operation.	[0]	[1]
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008).	[21]	[1]
The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used.	[0]	[1]
The system performance reported in (CM05; (Corley and Mihalcea, 2005)), which is among the best we are aware of, is also included for comparison.	[68]	[1]
The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.	[92]	[1]
The terms alpha and beta are the thresholds of the number and the beam width of lexical entries, and theta is the beam width for global thresholding (Goodman, 1997).	[374, 320, 327]	[1, 1, 1]
The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006).	[49, 130, 156, 163, 173]	[1, 1, 1, 1, 1]
The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool (Gimpel et al, 2011).	[36]	[1]
The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006).	[40, 0, 210]	[1, 1, 1]
The very interesting study by Snyder and Barzilay (2008) on multilingual approaches to morphological segmentation was difficult to classify.	[3, 0]	[1, 1]
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	[133, 37]	[1, 1]
Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008).	[171, 153, 84]	[1, 1, 1]
There are signature terms for different topic texts (Lin and Hovy, 2000).	[137, 60, 42, 236, 150, 145]	[1, 1, 1, 1, 1, 1]
There have been several research papers on using MTurk to help natural language processing tasks, Callison-Burch (2009) used MTurk to evaluate machine translation results.	[124]	[1]
Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002).	[157, 158]	[1, 1]
Therefore, there is nowadays a pressing need to adopt learning approaches to extend the coverage of the FrameNet lexicon by automatically acquiring new LUs, a task we call LU induction, as recently proposed at SemEval-2007 (Baker et al, 2007).	[0, 12]	[1, 1]
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work.	[19]	[1]
These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora.	[132, 22, 77, 74]	[1, 1, 1, 1]
These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001], and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004].	[149, 144, 72]	[1, 1, 1]
These strategies can be seen as transactions made up of conversational games (Carletta et al., 1997).	[31, 25]	[1, 1]
They surpassed their earlier work in 2003 with acyclic dependency network tagger, achieving 97.2% /89.05% (seen/unseen) (Toutanova et al, 2003).	[38, 0]	[1, 1]
They use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004).	[45]	[1]
This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996).	[141]	[1]
This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006).	[184, 105, 3, 34, 117, 4]	[1, 1, 1, 1, 1, 1]
This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a, b), who always identify empties first and determine antecedents later.	[0]	[1]
This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statistical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be (Al-Onaizan and Papineni, 2006).	[0, 9]	[1, 1]
This assumption of similar meaning when multiple phrases map onto a single foreign language phrase is the converse of the assumption made in the word sense disambiguation work of Diab and Resnik (2002) which posits different word senses when a single English word maps onto different words in the foreign language.	[6]	[1]
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	[0]	[1]
This contrasts with Open Information Extraction (Banko and Etzioni, 2008) and On-Demand Information Extraction (Sekine, 2006), which aim to extract large databases of open-ended facts, and with supervised relation extraction, which requires additional supervised data to learn new relations.	[0, 161, 9, 3]	[1, 1, 1, 1]
This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments.	[117]	[1]
This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure.	[19, 21]	[1, 1]
This evaluation was also used in (Sha and Pereira, 2003).	[131]	[1]
This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure.	[0]	[1]
This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available.	[222, 193, 194, 176, 199]	[1, 1, 1, 1, 1]
This idea is demonstrated by Attardi (2006), who proposes a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency.	[0]	[1]
This is a strong constraint on the matching of syntax so it is not surprising that the model has good precision but very low recall on the ACE corpus (Zhao and Grishman, 2005).	[61]	[1]
This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al (2011).	[48]	[1]
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000).	[98]	[1]
This kind of strategy has been widely used in the applications of machine learning to named entity recognition and has also been used in Chinese word segmentation (Xue and Shen, 2003).	[91, 46, 0]	[1, 1, 1]
This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006).	[0]	[1]
This measurement is called longest common subsequence ratio [Melamed, 1995].	[130, 129]	[1, 1]
This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006).	[6, 50, 105]	[1, 1, 1]
This paper shows that the one sense per collocation hypothesis is weaker for fine grained word sense distinctions (e.g. those in WordNet): from the 99% precision mentioned for 2-way ambiguities in (Yarowsky, 1993) we drop to 70% figures.	[98, 1]	[1, 1]
This section presents our results on the GE and the EPI tasks (Kim et al, 2011b; Ohta et al, 2011) respectively. Different experimental methods in processing the obtained event rules are described for the purpose of improving the precision of both tasks and increasing the recall of the EPI task.	[21]	[1]
This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010).	[5, 0]	[1, 1]
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003).	[0]	[1]
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	[138]	[1]
Thus, ? can recursively be an SRK (and evaluate Nested PASs (Moschitti et al, 2007)) or any other potential kernel (over the arguments).	[18, 57]	[1, 1]
Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations.	[2, 178, 5]	[1, 1, 1]
To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007.	[0, 1]	[1, 1]
To address this limitation, a promising approach is distant supervision (DS), which can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base (Mintz et al, 2009).	[0]	[1]
To continue our example, the resulting entry would be as follows: es gibt? S NP there VP is To give a more formal description of how syn tactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al, 2008).	[136, 128]	[1, 1]
To find the minimum value, we can use a subgradient method (Rush et al 2010).	[167]	[1]
To handle the ASR results of disfluent utterances, we employ SRI's Gemini robust language parser (Dowding et al, 1993).	[0]	[1]
To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures.	[8, 0]	[1, 1]
To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them.	[9, 0]	[1, 1]
To improve this performance, we plan to enrich the Arabic lexicon with more proper names, using either name recognition (Maloney and Niv, 1998) or a back translation approach after name recognition in English texts (Al-Onaizan and Knight, 2002).	[0, 11, 173]	[1, 1, 1]
To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers.	[49, 2]	[1, 1]
To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a).	[20]	[1]
To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the devset and test set.	[0]	[1]
To save time, we use a pruning stage (Xue and Palmer, 2004) to filter out the constituents that are clearly not semantic arguments to the predicate.	[49, 50, 51, 43, 63]	[1, 1, 1, 1, 1]
To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al, 1995), B3 (Bagga and Baldwin, 1998), and 3 -CEAF (Luo, 2005).	[59]	[1]
To test this hypothesis, we tried two quite different training data sets, one from the cell phone domain and the other from the DVD player domain, both used in (Wu et al, 2009).	[142, 159, 143]	[1, 1, 1]
To this end, the GATE Gazetteer (Cunningham et al., 2002) was used, and only entities recognized by it automatically were considered.	[28]	[1]
Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004).	[64, 69, 0]	[1, 1, 1]
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	[0]	[1]
Toutanova and Moore (2002) improve the model by incorporating pronunciation information.	[1, 169, 0]	[1, 1, 1]
Towards this, Wiebe and Mihalcea (2006) conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement.	[66, 57]	[1, 1]
Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)).	[4, 36]	[1, 1]
Training data are normally drawn from sizeable corpora of native English text (British National Corpus for DeFelice and Pulman (2007, 2008), Wall Street Journal in Knight and Chander (1994), a mix of Reuters and Encarta in Gamon et al (2008, 2009).	[69]	[1]
Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported.	[7, 2, 134, 138, 108, 182, 158]	[1, 1, 1, 1, 1, 1, 1]
Transliterations can be generated for tokens in a source phrase (Knightand Graehl, 1997), with o (f, e) calculating phonetic similarity rather than orthographic.	[18, 9]	[1, 1]
Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding.	[72, 0]	[1, 1]
Turney (2002) described a way to automatically build such a lexicon based on looking at co-occurrences of words with other words whose sentiment is known.	[12]	[1]
Twitter has previously been proposed as a candidate for modeling conversations, see for example Ritter et al (2010).	[0]	[1]
Two different evaluation measures are reported as described by the task: F=0.5 is a harmonic mean of purity and inverse purity of the clustering result, and F? =0.2 is a version of F that gives more importance to inverse purity (Artiles et al, 2007).	[91, 101, 92, 143]	[1, 1, 1, 1]
Two evaluation tasks for Barzilay and Lapata (2008)'s entity-based model are sentence ordering and summary coherence rating.	[189, 395, 194]	[1, 1, 1]
Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hyper graphs).	[173, 136]	[1, 1]
Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010).	[156]	[1]
Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features.	[81, 80, 27, 124, 134]	[1, 1, 1, 1, 1]
Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems are not required to translate the whole context but just the target word.	[0]	[1]
Unsupervised HMMs were applied to conversational data by Ritter et al (2010) who experimented with Twitter conversations.	[0]	[1]
Using the prefix probability, we can compute the word-by-word Surprisal (Hale, 2001), by taking the log ratio of the previous word's prefix probability against this word's prefix probability.	[92, 38]	[1, 1]
Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus.	[228, 1, 11, 216]	[1, 1, 1, 1]
Variations of SCFGs go back to Aho and Ullman (1972)'s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multi text Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al (2004).	[0]	[1]
Velikovich et al (2010) employed a different label propagation method, as described in Figure 3.	[34]	[1]
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t.	[218]	[1]
We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system.	[113, 64, 29, 136]	[1, 1, 1, 1]
We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier, 2003) , which has a similar motivation as this paper, except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Tree bank.	[8, 0, 33]	[1, 1, 1]
We also compare with the multi parse system of (Toutanova et al, 2008) which uses a global joint model using multiple parse trees.	[11, 496]	[1, 1]
We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context.	[111, 14, 17]	[1, 1, 1]
We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004).	[0, 14]	[1, 1]
We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser, implementing Collins' parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008).	[3, 9, 0]	[1, 1, 1]
We begin by outlining the general process of learning extraction patterns, similar to one presented by (Yangarber, 2003).	[62, 14]	[1, 1]
We begin by summarizing the model of Yamada and Knight (2001), which can be thought of as representing translation as an Alexander Calder mobile.	[0]	[1]
We build two classifiers based on the work of Pang and Lee (2004) to measure the polarity and objectivity of article edits.	[16]	[1]
We compare our P-Mod algorithm against the t-test measure, which, of all standard measures, yields the best results in general-language collocation extraction studies (Evert and Krenn, 2001), and also against the widely used C-value, which aims at enhancing the common frequency of occurrence measure by making it sensitive to nested terms (Frantzi et al, 2000).	[39, 89, 83]	[1, 1, 1]
We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG.	[11, 84, 183, 10, 165]	[1, 1, 1, 1, 1]
We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004).	[63]	[1]
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008).	[30]	[1]
We compare this improved algorithm to our former algorithm (Schone and Jurafsky (2000)) as well as to Goldsmith's Linguistica (2000).	[98, 102]	[1, 1]
We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set.	[131, 109, 18, 150, 4, 19]	[1, 1, 1, 1, 1, 1]
We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system.	[110]	[1]
We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al, 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus.	[11]	[1]
We describe the Stanford entry to the BioNLP2011 shared task on biomolecular event extraction (Kim et al, 2011a).	[3, 25, 0]	[1, 1, 1]
We did not explicitly test the utility of CRF-type features for improving recall on out-of-vocabulary items, but we note that in the Bakeoff, the model of Tseng et al (2005), which was very similar to our CRF-only system (only containing a few more feature templates), was consistently among the best performing systems in terms of test OOV recall (Emerson, 2005).	[117]	[1]
We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006)) that contain w1 or w2.	[44]	[1]
We evaluated the alignment performance of the proposed models with two tasks: Japanese English word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al, 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003).	[58]	[1]
We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively.	[172, 162, 7, 167]	[1, 1, 1, 1]
We extend our previous work (Chodorowetal., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994).	[16]	[1]
We find the exact top N consistent most likely local model labelings using a simple dynamic program described in (Toutanova et al, 2005).	[79, 47, 73]	[1, 1, 1]
We follow a bottom-up chart generation approach (Kay, 1996) for production systems similar to (Varges, 2005).	[0]	[1]
We follows the formulation by Rush et al (2010).	[72]	[1]
We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)).	[16, 0, 100]	[1, 1, 1]
We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes.	[129]	[1]
We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions.	[19]	[1]
We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007).	[0, 1]	[1, 1]
We identify a third approach through the use of selectors, first introduced by (Lin, 1997), which help to disambiguate a word by comparing it to other words that may replace it within the same local context.	[39, 22, 11]	[1, 1, 1]
We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002).	[51]	[1]
We intend to investigate any potential linkages between the word groups in the texts and other theories that provide pre-determined structures of text, such as Rhetorical Structure Theory (Marcu, 1997).	[0, 11]	[1, 1]
We investigated four individual approaches for the syntax-features, a regular-expression-based quasi-parser, a system based on Dekang Lin's Mini Par (Lin, 1993), a system based on the Collins parser (Collins, 1999), and one based on the CMU Link Grammar Parser (Sleator and Temperley, 1993), as well as a family of voting-based combination schemes.	[0]	[1]
We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common.	[0]	[1]
We note that while these results fall notably below the best result reported for Phosphorylation events in the BioNLP shared task, they are comparable to the best results reported in the task for Regulation and Binding events (Kim et al, 2009a), suggesting that the dataset allows the extraction of the novel PTM events with Theme and Site arguments at levels comparable to multi-argument shared task events.	[0]	[1]
We now turn to the concise integer LP formulation of Martins et al (2009).	[0, 50, 144, 126, 145, 154]	[1, 1, 1, 1, 1, 1]
We omit discussion here of the corpus currently in production by the University of Pennsylvania and the Children's Hospital of Philadelphia (Kulick et al 2004), since it is not yet available in finished form.	[10, 9]	[1, 1]
We parse questions and candidate sentences with MiniPar (Lin, 1994), a fast and robust parser for grammatical dependency relations.	[196]	[1]
We participated in the BioNLP-ST 2011 (Kim et al, 2011a), and applied a graph matching-based approach (Liu et al, 2010) to tackling the Task 1 of the GE NIA event extraction (GE) task (Kim et al, 2011b), and the core task of the Epigenetics and Post-translational Modifications (EPI) task (Ohta et al, 2011), two main tasks of the BioNLP-ST 2011.	[15, 21]	[1, 1]
We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al, 2004) automatically extracted from tree banks, maximising the probability of a tree given an f-structure.	[22, 0, 182, 1, 29]	[1, 1, 1, 1, 1]
We propose SG-ITG that follows Wellington et al (2006)'s suggestion to model at most one gap.	[46, 172]	[1, 1]
We ran the C & C parser using the normal-form model (we reproduced the numbers reported in Clark and Cur ran (2007)), and copied the results of the hybrid model from Clark and Curran (2007), since the hybrid model is not part of the public release.	[796, 624, 623]	[1, 1, 1]
We refer the reader to (Yamada and Knight, 2001) for more details.	[15, 33, 35]	[1, 1, 1]
We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001).	[133]	[1]
We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006).	[11, 7]	[1, 1]
We say a schema is a textual schema if it has been extracted from free text, such as the Nell (Carlson et al, 2010) and ReVerb (Fader et al, 2011) extracted databases.	[12, 13]	[1, 1]
We solve SAT analogies with a simplified version of the method of Turney (2006).	[130]	[1]
We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b).	[80]	[1]
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	[174]	[1]
We train a baseline phrase-based French-English system using WMT-09 corpora (Callison-Burchetal., 2009) for training and evaluation.	[14]	[1]
We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012).	[120]	[1]
We use OpinionFinder (Wilson et al, 2005) which employs negative and positive polarity cues.	[39]	[1]
We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in Word Net as the training corpus.	[43, 82]	[1, 1]
We use TnT (Brants, 2000), a second order Markov Model tagger.	[27, 0]	[1, 1]
We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations.	[78, 14, 72]	[1, 1, 1]
We use a support vector machine (SVM) based chunker yamcha (Kudo and Matsumoto, 2001) for the chunking process.	[189, 0]	[1, 1]
We use all of the Treelet models we described in Quirk et al (2005) namely: Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting.	[82, 149]	[1, 1]
We use one-count smoothing (Chen and Goodman, 1996), where (ti) is based on the number of words that occur with ti once: (ti)= |wi: C (ti ,wi)= 1|.	[68, 67]	[1, 1]
We use the co-occurrence based graph clustering framework introduced in (Biemann, 2006).	[123, 139, 158, 18]	[1, 1, 1, 1]
We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence.	[117, 60]	[1, 1]
We use the same alignment data for the five language pairs Chinese/English, Romanian/English, Hindi/English, Spanish/English, and French/English (Wellington et al, 2006).	[64, 63, 111, 65, 84]	[1, 1, 1, 1, 1]
We use the standard splits of the data used in semi-supervised tagging experiments (e.g.Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test.CCG-TUT.	[59]	[1]
We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets.	[10]	[1]
We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)).	[41, 42, 87, 52, 67, 96]	[1, 1, 1, 1, 1, 1]
We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011).	[117, 119, 118]	[1, 1, 1]
We used the JNLPBA-2004 training data, which is a set of tokenized word sequences with IOB2 (Tjong Kim Sang and Veenstra, 1999) protein labels.	[65]	[1]
We used the dv package1 to compute type vectors from a Minipar (Lin, 1993) parse of the BNC.	[178]	[1]
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004).	[1]	[1]
We weigh each context f using point wise mutual information (Church and Hanks 1989).	[0, 41]	[1, 1]
We will also use our measures in applications, to check their effectiveness in supporting various tasks, e.g. in mapping frames across Text and Hypothesis in RTE, in linking related frames in discourse, or in inducing frames for LU which are not in FrameNet (Baker et al, 2007).	[13, 28]	[1, 1]
We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002).	[43, 71, 61, 70, 67]	[1, 1, 1, 1, 1]
We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars.	[0, 45]	[1, 1]
When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ tree bank together with additional WSJ newswire (Klein and Manning,2002).	[15]	[1]
While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information.	[40]	[1]
While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction.	[111]	[1]
While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether.	[56]	[1]
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	[126]	[1]
While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007).	[227]	[1]
While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices.	[5, 144]	[1, 1]
While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them: maximum recall can be achieved without resembling the gold standard text decomposition; and maximum precision can be achieved with only a few overlapped elements.BLEU (Papineni et al, 2001a) computes the n gram precision while the metric ROUGE (LinandOch, 2004a) computes the n-gram recall.	[13, 20]	[1, 1]
While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case.	[11, 31]	[1, 1]
While there are related tags for dialogue act tagging schema? like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al, 2004) which includes a committag these classes are too general to allow identification of action items specifically.	[36, 67]	[1, 1]
While these results are worse than those obtained previously for this model, the experiments in Klein and Manning (2004) only used sentences of 10 words or fewer, without punctuation, and with gold-standard tags.	[23, 28]	[1, 1]
Wilson et al (2009) use conjunctive and dependency relations among polarity words.	[472, 498, 496]	[1, 1, 1]
Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification.	[42]	[1]
Word features are introduced primarily to help with unknown words, as in (Weischedel et al 1993).	[113, 0]	[1, 1]
Works aimed at discovering parallel sentences include (Utiyama and Isahara, 2003), who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus.	[2, 8, 1, 66]	[1, 1, 1, 1]
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	[3]	[1]
Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules.	[13, 2]	[1, 1]
Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration.	[18]	[1]
Yarowsky and Wicentowski (2000) report an accuracy of over 99% for their best model and a test set of 3888 pairs.	[121]	[1]
Yarowsky and Wicentowski (2000) use similar statistics to identify words related by inflection, but they gather their counts from a much smaller corpus.	[69]	[1]
Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus).	[182, 187, 223, 225, 217]	[1, 1, 1, 1, 1]
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	[167, 133]	[1, 1]
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	[130, 17, 6, 35]	[1, 1, 1, 1]
Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT.	[39, 3, 217, 83]	[1, 1, 1, 1]
Zollmann and Venugopal (2006) started with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.	[21]	[1]
[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs.	[23]	[1]
a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)).	[0, 48, 29, 37]	[1, 1, 1, 1]
delta can be efficiently computed with the algorithm proposed in (Collins and Duffy, 2002).	[7]	[1]
distant supervision: for each relation in the database D we assume that all the corresponding mentions are positive examples for the corresponding label (Mintz et al 2009).	[21, 0]	[1, 1]
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures.	[1, 55]	[1, 1]
reported accuracies of 93% and 93.74% on CTB-I (Xue et al, 2002) (100K words) and CTB 5.0 (500K words), respectively, each using a Maximum Entropy approach.	[5]	[1]
see Klein and Manning (2003c) for details.	[141, 70]	[1, 1]
their relation edges are obtained from the Spade system described in Soricut and Marcu (2003).	[143, 90]	[1, 1]
tree-to-tree translation model based on tree sequence alignment (Zhang et al 2008a) without losing of generality to most syntactic tree based models.	[0, 11, 50, 28, 30]	[1, 1, 1, 1, 1]
used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text.	[181, 188, 0]	[1, 1, 1]
