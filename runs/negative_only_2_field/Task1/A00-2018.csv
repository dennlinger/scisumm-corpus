Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)",'17',"<S sid=""17"" ssid=""6"">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>",
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank",'101',"<S sid=""101"" ssid=""12"">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>",
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus",'176',"<S sid=""176"" ssid=""3"">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>",
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)",'81',"<S sid=""81"" ssid=""50"">Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.</S>",
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article",'58',"<S sid=""58"" ssid=""27"">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S>",
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)",'174',"<S sid=""174"" ssid=""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>",
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)",'133',"<S sid=""133"" ssid=""24"">One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.</S>",
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis",'95',"<S sid=""95"" ssid=""6"">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>",
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus",'56',"<S sid=""56"" ssid=""25"">In max-entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4.</S>",
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows",'187',"<S sid=""187"" ssid=""14"">Though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing.</S>",
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees",'81',"<S sid=""81"" ssid=""50"">Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.</S>",
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically",'38',"<S sid=""38"" ssid=""7"">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>",
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents",'171',"<S sid=""171"" ssid=""62"">As already noted, our best model uses a Markov-grammar approach.</S>",
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))",'81',"<S sid=""81"" ssid=""50"">Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.</S>",
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions",'17',"<S sid=""17"" ssid=""6"">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>",
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size",'99',"<S sid=""99"" ssid=""10"">Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S>",
