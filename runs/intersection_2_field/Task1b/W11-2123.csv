Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","'12','0','199','244','1','27','281'","<S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""27"" ssid=""5"">Hash tables are a common sparse mapping technique used by SRILM&#8217;s default and BerkeleyLM&#8217;s hashed variant.</S><S sid=""281"" ssid=""2"">Hieu Hoang named the code &#8220;KenLM&#8221; and assisted with Moses along with Barry Haddow.</S>",['method_citation']
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","'223','203','0','26','1','244','45'","<S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S><S sid=""203"" ssid=""22"">As noted in Section 4.4, disk cache state is controlled by reading the entire binary file before each test begins.</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""26"" ssid=""4"">We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S><S sid=""45"" ssid=""23"">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>","['method_citation','results_citation']"
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","'135','0','1','131','142'","<S sid=""135"" ssid=""7"">Therefore, we want state to encode the minimum amount of information necessary to properly compute language model scores, so that the decoder will be faster and make fewer search errors.</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""131"" ssid=""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S><S sid=""142"" ssid=""14"">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore, when the model is queried for p(wnjwn&#8722;1 1 ) but the longest matching suffix is wnf , it may return state s(wn1) = wnf since no longer context will be found.</S>",['method_citation']
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","'1','4','21','130','0','199','6','52'","<S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""4"" ssid=""4"">Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.</S><S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid=""130"" ssid=""2"">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""6"" ssid=""1"">Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.</S><S sid=""52"" ssid=""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>",['method_citation']
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","'254','12','199','244'","<S sid=""254"" ssid=""73"">We used this data to build an unpruned ARPA file with IRSTLM&#8217;s improved-kneser-ney option and the default three pieces.</S><S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>",['method_citation']
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","'108','133','131','52','7','68'","<S sid=""108"" ssid=""12"">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S><S sid=""133"" ssid=""5"">When two partial hypotheses have equal state (including that of other features), they can be recombined and thereafter efficiently handled as a single packed hypothesis.</S><S sid=""131"" ssid=""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S><S sid=""52"" ssid=""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S><S sid=""7"" ssid=""2"">This paper presents methods to query N-gram language models, minimizing time and space costs.</S><S sid=""68"" ssid=""46"">The trie data structure is commonly used for language modeling.</S>",['method_citation']
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"'274','1','129','68','25'","<S sid=""274"" ssid=""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""129"" ssid=""1"">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S><S sid=""68"" ssid=""46"">The trie data structure is commonly used for language modeling.</S><S sid=""25"" ssid=""3"">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>",['method_citation']
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","'199','0','1','52'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""52"" ssid=""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>",['method_citation']
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"'1','244','45','223','52','131'","<S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S><S sid=""45"" ssid=""23"">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S><S sid=""52"" ssid=""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S><S sid=""131"" ssid=""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>",['method_citation']
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"'204','149','81'","<S sid=""204"" ssid=""23"">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S><S sid=""149"" ssid=""21"">RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid=""81"" ssid=""59"">Values in the trie are minimally sized at the bit level, improving memory consumption over trie implementations in SRILM, IRSTLM, and BerkeleyLM.</S>","['method_citation','results_citation']"
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","'21','4','236','103','37','12','102','52'","<S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid=""4"" ssid=""4"">Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.</S><S sid=""236"" ssid=""55"">Statistics are printed before Moses exits and after parts of the decoder have been destroyed.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""37"" ssid=""15"">Linear probing hash tables must have more buckets than entries, or else an empty bucket will never be found.</S><S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S><S sid=""102"" ssid=""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S sid=""52"" ssid=""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>",['method_citation']
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","'0','1','281','27'","<S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""281"" ssid=""2"">Hieu Hoang named the code &#8220;KenLM&#8221; and assisted with Moses along with Barry Haddow.</S><S sid=""27"" ssid=""5"">Hash tables are a common sparse mapping technique used by SRILM&#8217;s default and BerkeleyLM&#8217;s hashed variant.</S>",['method_citation']
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","'1','103','6','40','67','7','154','137','22'","<S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""6"" ssid=""1"">Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.</S><S sid=""40"" ssid=""18"">The fraction of buckets that are empty is m&#8722;1 m , so average lookup time is O( m 1) and, crucially, constant in the number of entries.</S><S sid=""67"" ssid=""45"">While sorted arrays could be used to implement the same data structure as PROBING, effectively making m = 1, we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid=""7"" ssid=""2"">This paper presents methods to query N-gram language models, minimizing time and space costs.</S><S sid=""154"" ssid=""26"">To optimize left-to-right queries, we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S><S sid=""137"" ssid=""9"">The state function is integrated into the query process so that, in lieu of the query p(wnjwn&#8722;1 1 ), the application issues query p(wnjs(wn&#8722;1 1 )) which also returns s(wn1 ).</S><S sid=""22"" ssid=""17"">Our open-source (LGPL) implementation is also available for download as a standalone package with minimal (POSIX and g++) dependencies.</S>",['method_citation']
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","'130','0','12','244','199','1','4'","<S sid=""130"" ssid=""2"">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""4"" ssid=""4"">Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.</S>",['method_citation']
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","'0','1','199','7','68'","<S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""7"" ssid=""2"">This paper presents methods to query N-gram language models, minimizing time and space costs.</S><S sid=""68"" ssid=""46"">The trie data structure is commonly used for language modeling.</S>",['method_citation']
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","'199','1'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>",['method_citation']
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","'131','0','1','52','7','47','103'","<S sid=""131"" ssid=""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""52"" ssid=""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S><S sid=""7"" ssid=""2"">This paper presents methods to query N-gram language models, minimizing time and space costs.</S><S sid=""47"" ssid=""25"">For 2 &lt; n &lt; N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>",['method_citation']
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","'1','17','223','12'","<S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""17"" ssid=""12"">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code.</S><S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S><S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>",['method_citation']
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","'199','205','0','1','218'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""205"" ssid=""24"">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S>",['method_citation']
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","'199','0','1','103'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>",['method_citation']
