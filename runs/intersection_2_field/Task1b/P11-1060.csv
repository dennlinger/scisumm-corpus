Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1060,D11-1039,0,2011,0,"Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","'132','2','8','9','156','0','6'","<S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S><S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S sid=""8"" ssid=""4"">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S><S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""156"" ssid=""41"">There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid=""0"">Learning Dependency-Based Compositional Semantics</S><S sid=""6"" ssid=""2"">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S>","['hypothesis_citation','method_citation']"
2,P11-1060,P13-1092,0,2011,0,"In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","'2','132','13','12','106','22'","<S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S><S sid=""13"" ssid=""9"">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S><S sid=""12"" ssid=""8"">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S sid=""106"" ssid=""82"">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S><S sid=""22"" ssid=""18"">The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S>",['method_citation']
3,P11-1060,P13-1092,0,2011,0,"To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","'3','9','21','0','48','25'","<S sid=""3"" ssid=""3"">In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S sid=""0"">Learning Dependency-Based Compositional Semantics</S><S sid=""48"" ssid=""24"">In addition, trees enable efficient computation, thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>","['hypothesis_citation','method_citation']"
4,P11-1060,P13-1092,0,2011,0,"More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","'25','21','0','45','171','47'","<S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S><S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S sid=""0"">Learning Dependency-Based Compositional Semantics</S><S sid=""45"" ssid=""21"">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S><S sid=""171"" ssid=""56"">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S><S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>","['hypothesis_citation','method_citation']"
5,P11-1060,P13-1092,0,"Liang et al, 2011",0,"GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","'21','154'","<S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S sid=""154"" ssid=""39"">The restriction to present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009).</S>",['hypothesis_citation']
6,P11-1060,W12-2802,0,2011,0,"Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","'141','0','7','49'","<S sid=""141"" ssid=""26"">Rather than using lexical triggers, several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid=""0"">Learning Dependency-Based Compositional Semantics</S><S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S><S sid=""49"" ssid=""25"">Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language.</S>",['method_citation']
7,P11-1060,P13-2009,0,"Liang et al, 2011",0,"It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","'0','21','55','25','110','18','32','8'","<S sid=""0"">Learning Dependency-Based Compositional Semantics</S><S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S sid=""55"" ssid=""31"">The tree structure still enables us to compute denotations efficiently based on (1) and (2).</S><S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S><S sid=""110"" ssid=""86"">Specifically, we truncate each Ci,j to a maximum of K candidates sorted by decreasing score based on parameters &#952;.</S><S sid=""18"" ssid=""14"">CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005).</S><S sid=""32"" ssid=""8"">Define a special predicate &#248; with w(&#248;) = V. We represent functions by a set of inputoutput pairs, e.g., w(count) = {(S, n) : n = |S|}.</S><S sid=""8"" ssid=""4"">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>","['hypothesis_citation','method_citation']"
8,P11-1060,D12-1069,0,Liangetal2011,0,"One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)","One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Lianget al 2011) or even a binary correct/incorrect signal (Clarke et al2010)","'9','117','146','15','148','112','142','24'","<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""117"" ssid=""2"">In each dataset, each sentence x is annotated with a Prolog logical form, which we use only to evaluate and get an answer y.</S><S sid=""146"" ssid=""31"">We find that only for a small fraction of training examples do the K-best sets contain any trees yielding the correct answer (29% for DCS on GEO).</S><S sid=""15"" ssid=""11"">Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z.</S><S sid=""148"" ssid=""33"">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S><S sid=""112"" ssid=""88"">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S><S sid=""142"" ssid=""27"">This option is not available to us since we do not have annotated logical forms, so we must instead rely on lexical triggers to define the search space.</S><S sid=""24"" ssid=""20"">Our system outperforms all existing systems despite using no annotated logical forms.</S>","['implication_citation','method_citation']"
9,P11-1060,N12-1049,0,2011,0,"For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","'10','35','47','2','13'","<S sid=""10"" ssid=""6"">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid=""35"" ssid=""11"">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S><S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S><S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S sid=""13"" ssid=""9"">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>",['method_citation']
10,P11-1060,P12-1045,0,2011,0,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,"'2','132','1','9','13','112','148'","<S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S><S sid=""1"" ssid=""1"">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""13"" ssid=""9"">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S><S sid=""112"" ssid=""88"">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S><S sid=""148"" ssid=""33"">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>",['method_citation']
11,P11-1060,P14-1008,0,"Liang et al,2011",0,"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","'25','21','0','36','35','173'","<S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S><S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S sid=""0"">Learning Dependency-Based Compositional Semantics</S><S sid=""36"" ssid=""12"">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid=""35"" ssid=""11"">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S><S sid=""173"" ssid=""58"">In DCS, the mark-execute and Tom Kwiatkowski for providing us with data construct provides a flexible framework for dealing and answering questions.</S>","['hypothesis_citation','method_citation']"
12,P11-1060,P14-1008,0,"Liang et al, 2011",0,"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","'25','49','47','154','94','152','48'","<S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S><S sid=""49"" ssid=""25"">Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language.</S><S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S><S sid=""154"" ssid=""39"">The restriction to present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009).</S><S sid=""94"" ssid=""70"">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S><S sid=""152"" ssid=""37"">(DCS on GEO), we find that the feature [area, area] The idea of using CSPs to represent semantics is has a much higher weight than [area, city].</S><S sid=""48"" ssid=""24"">In addition, trees enable efficient computation, thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S>","['hypothesis_citation','method_citation']"
13,P11-1060,P14-1008,0,"Liang et al, 2011",0,"are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries","In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries","'12','2','132','106','167','31','13'","<S sid=""12"" ssid=""8"">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S><S sid=""106"" ssid=""82"">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S><S sid=""167"" ssid=""52"">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S><S sid=""31"" ssid=""7"">Conceptually, a world is a relational database where each predicate is a relation (possibly infinite).</S><S sid=""13"" ssid=""9"">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>",['method_citation']
14,P11-1060,P14-1008,0,"Liang et al, 2011",0,"as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","'84','51','46','42','43','10'","<S sid=""84"" ssid=""60"">There are three cases: Extraction (d.ri = E) In the basic version, the denotation of a tree was always the set of consistent values of the root node.</S><S sid=""51"" ssid=""27"">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid=""46"" ssid=""22"">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees, where each z &#8712; Z consists of (i) a predicate for each child i, the ji-th component of v must equal the j'i-th component of some t in the child&#8217;s denotation (t &#8712; JciKw).</S><S sid=""42"" ssid=""18"">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid=""43"" ssid=""19"">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter, 2003).</S><S sid=""10"" ssid=""6"">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>",['method_citation']
15,P11-1060,D11-1140,0,2011,0,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,"'1','9','172','106','132','7'","<S sid=""1"" ssid=""1"">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""172"" ssid=""57"">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid=""106"" ssid=""82"">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S><S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S><S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S>",['method_citation']
16,P11-1060,D11-1140,0,"Liang et al, 2011",0,"and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","'138','24','45'","<S sid=""138"" ssid=""23"">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid=""24"" ssid=""20"">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid=""45"" ssid=""21"">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>",['method_citation']
17,P11-1060,P13-1007,0,2011,0,"In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","'138','161','94','172','1'","<S sid=""138"" ssid=""23"">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid=""161"" ssid=""46"">CCG (Steedman, 2000), in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.</S><S sid=""94"" ssid=""70"">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S><S sid=""172"" ssid=""57"">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid=""1"" ssid=""1"">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S>",['method_citation']
18,P11-1060,D11-1022,0,2011,0,"DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","'25','141','0','17','1','36'","<S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S><S sid=""141"" ssid=""26"">Rather than using lexical triggers, several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid=""0"">Learning Dependency-Based Compositional Semantics</S><S sid=""17"" ssid=""13"">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S><S sid=""1"" ssid=""1"">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid=""36"" ssid=""12"">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>","['hypothesis_citation','method_citation']"
19,P11-1060,P12-1051,0,2011,0,"In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","'20','17','171'","<S sid=""20"" ssid=""16"">At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.</S><S sid=""17"" ssid=""13"">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S><S sid=""171"" ssid=""56"">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>","['hypothesis_citation','method_citation']"
