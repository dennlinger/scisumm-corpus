Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1043,C10-1045,0,2008,0,"Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew","Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew","'33','153','161','21','83'","<S sid=""33"" ssid=""12"">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S><S sid=""153"" ssid=""31"">Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima&#8217;an, 2007; Cohen and Smith, 2007).</S><S sid=""161"" ssid=""39"">We report the F1 value of both measures.</S><S sid=""21"" ssid=""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid=""83"" ssid=""15"">Each token may admit multiple analyses, each of which a sequence of one or more lexemes (we use li to denote a lexeme) belonging a presupposed Hebrew lexicon LEX.</S>",
2,P08-1043,P11-1141,0,2008,0,Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models,Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models,"'4','0','19','3','189','186'","<S sid=""4"" ssid=""4"">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""19"" ssid=""15"">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid=""3"" ssid=""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid=""189"" ssid=""3"">Better grammars are shown here to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones.</S><S sid=""186"" ssid=""24"">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>",
3,P08-1043,P10-1074,0,2008,0,"Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of He brew, based on lattice parsing","Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing","'0','21','52','3','51'","<S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""21"" ssid=""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid=""52"" ssid=""10"">Cohen and Smith (2007) later on based a system for joint inference on factored, independent, morphological and syntactic components of which scores are combined to cater for the joint inference task.</S><S sid=""3"" ssid=""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid=""51"" ssid=""9"">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>",
4,P08-1043,P11-1089,0,2008,0,Goldberg and Tsarfaty (2008) pro pose a generative joint model,Goldberg and Tsarfaty (2008) propose a generative joint model,"'0','3','19','162','53','186'","<S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""3"" ssid=""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid=""19"" ssid=""15"">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid=""162"" ssid=""40"">Finally, our U (unparsed) measure is used to report the number of sentences to which our system could not propose a joint analysis.</S><S sid=""53"" ssid=""11"">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S><S sid=""186"" ssid=""24"">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>",
5,P08-1043,W10-1404,0,2008,0,Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach,Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach,"'4','47','0','188','49','48','46'","<S sid=""4"" ssid=""4"">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S sid=""47"" ssid=""5"">Sima&#8217;an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation.</S><S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""188"" ssid=""2"">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid=""49"" ssid=""7"">Tsarfaty and Sima&#8217;an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid=""48"" ssid=""6"">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid=""46"" ssid=""4"">The development of the very first Hebrew Treebank (Sima&#8217;an et al., 2001) called for the exploration of general statistical parsing methods, but the application was at first limited.</S>",
6,P08-1043,P11-2124,0,2008,0,Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrewtext,Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text,"'48','49','0','46','90','159'","<S sid=""48"" ssid=""6"">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid=""49"" ssid=""7"">Tsarfaty and Sima&#8217;an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""46"" ssid=""4"">The development of the very first Hebrew Treebank (Sima&#8217;an et al., 2001) called for the exploration of general statistical parsing methods, but the application was at first limited.</S><S sid=""90"" ssid=""22"">Since the lattice L for a given sentence W is determined by the morphological analyzer M we have which is precisely the formula corresponding to the so-called lattice parsing familiar from speech recognition.</S><S sid=""159"" ssid=""37"">Our parsing performance measures (SY N) thus report the PARSEVAL extension proposed in Tsarfaty (2006).</S>",
7,P08-1043,P11-2124,0,"Goldberg and Tsarfaty, 2008",0,"Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice","Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice","'14','94','2','105','22','87'","<S sid=""14"" ssid=""10"">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S><S sid=""94"" ssid=""26"">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid=""2"" ssid=""2"">These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid=""105"" ssid=""37"">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S><S sid=""22"" ssid=""1"">Segmental morphology Hebrew consists of seven particles m(&#8220;from&#8221;) f(&#8220;when&#8221;/&#8220;who&#8221;/&#8220;that&#8221;) h(&#8220;the&#8221;) w(&#8220;and&#8221;) k(&#8220;like&#8221;) l(&#8220;to&#8221;) and b(&#8220;in&#8221;). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.</S><S sid=""87"" ssid=""19"">We define the lattice L to be the concatenation of the lattices Li corresponding to the input words wi (s.t.</S>",
8,P08-1043,P12-2002,0,2008,0,2The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008),The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008),"'85','8','140','12','7','47'","<S sid=""85"" ssid=""17"">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S><S sid=""8"" ssid=""4"">The Hebrew token &#8216;bcl&#8217;1, for example, stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima&#8217;an et al., 2001).</S><S sid=""140"" ssid=""18"">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid=""12"" ssid=""8"">This leads to word- and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid=""7"" ssid=""3"">In Modern Hebrew (Hebrew), a Semitic language with very rich morphology, particles marking conjunctions, prepositions, complementizers and relativizers are bound elements prefixed to the word (Glinert, 1989).</S><S sid=""47"" ssid=""5"">Sima&#8217;an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation.</S>",
9,P08-1043,D12-1046,0,"Goldberg and Tsarfaty, 2008",0,"A study that is closely related toours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew","A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew","'0','3','19','186','188','18','13','53'","<S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""3"" ssid=""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid=""19"" ssid=""15"">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid=""186"" ssid=""24"">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S><S sid=""188"" ssid=""2"">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid=""18"" ssid=""14"">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S><S sid=""13"" ssid=""9"">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)).</S><S sid=""53"" ssid=""11"">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>",
10,P08-1043,D12-1133,0,2008,0,"Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing","Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing","'43','49','0','141','108','54'","<S sid=""43"" ssid=""1"">Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000), Yona and Wintner (2005), and recently by the knowledge center for processing Hebrew (Itai et al., 2006).</S><S sid=""49"" ssid=""7"">Tsarfaty and Sima&#8217;an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""141"" ssid=""19"">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid=""108"" ssid=""40"">Secondly, some segments in a proposed segment sequence may in fact be seen lexical events, i.e., for some p tag Prf(p &#8212;* (s, p)) &gt; 0, while other segments have never been observed as a lexical event before.</S><S sid=""54"" ssid=""1"">A Hebrew surface token may have several readings, each of which corresponding to a sequence of segments and their corresponding PoS tags.</S>",
11,P08-1043,E09-1038,0,"Goldberg and Tsarfaty, 2008",0,"4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec","Parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008)","'0','141','49','144','89','13'","<S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""141"" ssid=""19"">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid=""49"" ssid=""7"">Tsarfaty and Sima&#8217;an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid=""144"" ssid=""22"">In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as 10Lattice parsing can be performed by special initialization of the chart in a CKY parser (Chappelier et al., 1999).</S><S sid=""89"" ssid=""21"">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer, we look for the most probable parse tree &#960; s.t.</S><S sid=""13"" ssid=""9"">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)).</S>",
12,P08-1043,E09-1038,0,"Goldberg and Tsarfaty, 2008",0,"It is the same grammar as described in (Goldberg and Tsarfaty, 2008)","It is the same grammar as described in (Goldberg and Tsarfaty, 2008)","'136','28','37','63','169','77','102','4'","<S sid=""136"" ssid=""14"">Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.</S><S sid=""28"" ssid=""7"">The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.</S><S sid=""37"" ssid=""16"">The same form fmnh can be segmented as f-mnh, f (&#8220;that&#8221;) functioning as a reletivizer with the form mnh.</S><S sid=""63"" ssid=""10"">When the same token is to be interpreted as a single lexeme fmnh, it may function as a single adjective &#8220;fat&#8221;.</S><S sid=""169"" ssid=""7"">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid=""77"" ssid=""9"">Segments with the same surface form but different PoS tags are treated as different lexemes, and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).</S><S sid=""102"" ssid=""34"">In order to pass these constraints onto the parser, the lexical rules in the grammar are of the form pi &#8212;* (si, pi) Parameter Estimation The grammar probabilities are estimated from the corpus using simple relative frequency estimates.</S><S sid=""4"" ssid=""4"">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>",
14,P08-1043,E09-1038,0,2008,0,"Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task","The most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task","'3','52','47','0','21','94'","<S sid=""3"" ssid=""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid=""52"" ssid=""10"">Cohen and Smith (2007) later on based a system for joint inference on factored, independent, morphological and syntactic components of which scores are combined to cater for the joint inference task.</S><S sid=""47"" ssid=""5"">Sima&#8217;an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation.</S><S sid=""0"">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid=""21"" ssid=""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid=""94"" ssid=""26"">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>",
15,P08-1043,E09-1038,0,2008,0,Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank,Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank,"'134','136','47','4','191','133','127','21','192'","<S sid=""134"" ssid=""12"">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S><S sid=""136"" ssid=""14"">Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.</S><S sid=""47"" ssid=""5"">Sima&#8217;an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation.</S><S sid=""4"" ssid=""4"">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S sid=""191"" ssid=""5"">In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go.</S><S sid=""133"" ssid=""11"">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid=""127"" ssid=""5"">Data We use the Hebrew Treebank, (Sima&#8217;an et al., 2001), provided by the knowledge center for processing Hebrew, in which sentences from the daily newspaper &#8220;Ha&#8217;aretz&#8221; are morphologically segmented and syntactically annotated.</S><S sid=""21"" ssid=""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid=""192"" ssid=""6"">Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S>",
16,P08-1043,E09-1038,0,2008,0,The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token,The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token,"'133','89','112','75','70','85','83'","<S sid=""133"" ssid=""11"">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid=""89"" ssid=""21"">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer, we look for the most probable parse tree &#960; s.t.</S><S sid=""112"" ssid=""44"">We first make use of our morphological analyzer to find all segmentation possibilities by chopping off all prefix sequence possibilities (including the empty prefix) and construct a lattice off of them.</S><S sid=""75"" ssid=""7"">Every token is independent of the others, and the sentence lattice is in fact a concatenation of smaller lattices, one for each token.</S><S sid=""70"" ssid=""2"">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S><S sid=""85"" ssid=""17"">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S><S sid=""83"" ssid=""15"">Each token may admit multiple analyses, each of which a sequence of one or more lexemes (we use li to denote a lexeme) belonging a presupposed Hebrew lexicon LEX.</S>",
17,P08-1043,E09-1038,0,"Goldberg and Tsarfaty, 2008",0,"Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parse val to use characters instead of space-delimited tokens as its basic units","Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units","'149','73','1','155','133','135','36','162','10'","<S sid=""149"" ssid=""27"">We use a patched version of BitPar allowing for direct input of probabilities instead of counts.</S><S sid=""73"" ssid=""5"">We use double-circles to indicate the space-delimited token boundaries.</S><S sid=""1"" ssid=""1"">Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.</S><S sid=""155"" ssid=""33"">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S><S sid=""133"" ssid=""11"">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid=""135"" ssid=""13"">We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.</S><S sid=""36"" ssid=""15"">Furthermore, the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.</S><S sid=""162"" ssid=""40"">Finally, our U (unparsed) measure is used to report the number of sentences to which our system could not propose a joint analysis.</S><S sid=""10"" ssid=""6"">This token may further embed into a larger utterance, e.g., &#8216;bcl hneim&#8217; (literally &#8220;in-the-shadow the-pleasant&#8221;, meaning roughly &#8220;in the pleasant shadow&#8221;) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S>",
