Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.	1
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	However there is no global pruning.	0
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.	0
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	Here, the pruning threshold t0 = 10:0 is used.	0
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	The effect of the pruning threshold t0 is shown in Table 5.	0
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	(f1; ;mg n fl1; l2; l3g ;m) German to English the monotonicity constraint is violated mainly with respect to the German verbgroup.	0
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed.	0
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijeiô1 iô2) max bI 1 I Yi=1 [p(bijbiô1; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijeiô1 iô2) p(bijbiô1; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijeiô1 iô2) is the trigram language model probability.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	1
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	This algorithm can be applied to statistical machine translation.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	The goal of machine translation is the translation of a text given in some source language into a target language.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.	0
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	In this paper, we have presented a new, eÃcient DP-based search procedure for statistical machine translation.	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	A position is presented by the word at that position.	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	A search restriction especially useful for the translation direction from German to English is presented.	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	This approach is compared to another reordering scheme presented in (Berger et al., 1996).	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed.	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	0
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	Although the ultimate goal of the Verbmobil project is the translation of spoken language, the input used for the translation experiments reported on in this paper is the (more or less) correct orthographic transcription of the spoken sentences.	0
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.	1
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	This approach leads to a search procedure with complexity O(E3 J4).	1
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.	0
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	In German, the verbgroup usually consists of a left and a right verbal brace, whereas in English the words of the verbgroup usually form a sequence of consecutive words.	0
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The complexity of the quasimonotone search is O(E3 J (R2+LR)).	0
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The resulting algorithm has a complexity of O(n!).	0
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	3) A tight coupling with the speech recognizer output.	0
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.	0
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	While processing the source sentence monotonically, the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position.	0
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	The resulting algorithm has a complexity of O(n!).	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	The complexity of the quasimonotone search is O(E3 J (R2+LR)).	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	This approach leads to a search procedure with complexity O(E3 J4).	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	The approach assumes that the word reordering is restricted to a few positions in the source sentence.	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	In the second and third translation examples, the IbmS word reordering performs worse than the QmS word reordering, since it can not take properly into account the word reordering due to the German verbgroup.	0
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	The alignment model uses two kinds of parameters: alignment probabilities p(aj jajô1; I; J), where the probability of alignment aj for position j depends on the previous alignment position ajô1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÃcient to consider only the best 50 words.	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	The final score is obtained from: max e;e0 j2fJôL;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	The sentence length probability p(JjI) is omitted without any loss in performance.	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	(1), Pr(eI 1) is the language model, which is a trigram language model in this case.	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	For the inverted alignment probability p(bijbiô1; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	A modified language model probability pÃ(eje0; e00) is defined as follows: pÃ(eje0; e00) =  1:0 if Ã = 0 p(eje0; e00) if Ã = 1 : We associate a distribution p(Ã) with the two cases Ã = 0 and Ã = 1 and set p(Ã = 1) = 0:7.	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model.	0
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	For Ã = 1, a new target language word is generated using the trigram language model p(eje0; e00).	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	Table 4 shows translation results for the three approaches.	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In the following, we assume that this word joining has been carried out.	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	Word Re-ordering and DP-based Search in Statistical Machine Translation	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In Section 4, we present the performance measures used and give translation results on the Verbmobil task.	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.	0
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	In this section, we brie y review our translation approach.	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	We apply a beam search concept as in speech recognition.	1
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	In this paper, we have presented a new, eÃcient DP-based search procedure for statistical machine translation.	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	Word Re-ordering and DP-based Search in Statistical Machine Translation	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	A search restriction especially useful for the translation direction from German to English is presented.	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	Table 5: Effect of the beam threshold on the number of search errors (147 sentences).	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	Here, we process only full-form words within the translation procedure.	0
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.	1
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	Our approach uses word-to-word dependencies between source and target words.	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	For Ã = 0, no new target word is generated, while an additional source sentence position is covered.	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	The goal of machine translation is the translation of a text given in some source language into a target language.	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.	0
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	A position is presented by the word at that position.	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.	1
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	However there is no global pruning.	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	Here, the pruning threshold t0 = 10:0 is used.	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	The effect of the pruning threshold t0 is shown in Table 5.	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	To be short, we omit the target words e; e0 in the formulation of the search hypotheses.	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	Here, we process only full-form words within the translation procedure.	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.	0
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Ã = 0 or Ã = 1 new target words.	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	We apply a beam search concept as in speech recognition.	1
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	The advantage is that we can recombine search hypotheses by dynamic programming.	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	The monotone search performs worst in terms of both error rates mWER and SSER.	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	The quasi-monotone search performs best in terms of both error rates mWER and SSER.	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	Word Re-ordering and DP-based Search in Statistical Machine Translation	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) !	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	We have tested the translation system on the Verbmobil task (Wahlster 1993).	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series, we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0, and this number is reported as the number of search errors.	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	For Ã = 1, a new target language word is generated using the trigram language model p(eje0; e00).	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition.	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	The search starts in the hypothesis (I; f;g; 0).	0
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction.	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Ã;e00 j02Cnfjg fp(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once.	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	= p(fj je) max Ã;e00 j02Cnfjg np(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j).	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and eiô1 = e0.	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Ã;e00 np(jjj0; J) p(Ã) pÃ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The final score is obtained from: max e;e0 j2fJôL;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.	0
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	The inverted alignment probability p(bijbiô1; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	3) A tight coupling with the speech recognizer output.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	The alignment model uses two kinds of parameters: alignment probabilities p(aj jajô1; I; J), where the probability of alignment aj for position j depends on the previous alignment position ajô1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	2.1 Inverted Alignments.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	A straightforward way to find the shortest tour is by trying all possible permutations of the n cities.	0
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	(1), Pr(eI 1) is the language model, which is a trigram language model in this case.	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	In this case, we have no finite-state restrictions for the search space.	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	Otherwise for the predecessor search hypothesis, we would have chosen a position that would not have been among the first n uncovered positions.	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	While processing the source sentence monotonically, the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position.	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	We have tested the translation system on the Verbmobil task (Wahlster 1993).	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet.	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition.	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	The German finite verbs 'bin' (second example) and 'konnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions).	0
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	In this paper, we have presented a new, eÃcient DP-based search procedure for statistical machine translation.	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	We have tested the translation system on the Verbmobil task (Wahlster 1993).	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	This approach is compared to another reordering scheme presented in (Berger et al., 1996).	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	Word Re-ordering and DP-based Search in Statistical Machine Translation	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	We apply a beam search concept as in speech recognition.	1
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	In this case, we have no finite-state restrictions for the search space.	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	Table 5: Effect of the beam threshold on the number of search errors (147 sentences).	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	Otherwise for the predecessor search hypothesis, we would have chosen a position that would not have been among the first n uncovered positions.	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	The advantage is that we can recombine search hypotheses by dynamic programming.	0
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	To be short, we omit the target words e; e0 in the formulation of the search hypotheses.	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	1
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	The advantage is that we can recombine search hypotheses by dynamic programming.	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Ã;e00 np(jjj0; J) p(Ã) pÃ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	A dynamic programming recursion similar to the one in Eq. 2 is evaluated.	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	= p(fj je) max Ã;e00 j02Cnfjg np(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j).	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	The complexity of the quasimonotone search is O(E3 J (R2+LR)).	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	A simple extension will be used to handle this problem.	0
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	This approach leads to a search procedure with complexity O(E3 J4).	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	Such a classification can be seen as a not-always-correct summary of global features.	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	This is because in many cases, the use of hyphens can be considered to be optional (e.g., third-quarter or third quarter).	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	The features we used can be divided into 2 classes: local and global.	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	Another attempt at using global information can be found in (Borthwick, 1999).	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	4.2 Global Features.	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	Multiple features can be used for the same token.	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	Each feature group can be made up of many binary features.	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	Sentence (2) and (3) help to disambiguate one way or the other.	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier.	0
In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.	The effect of a second reference resolution classifier is not entirely the same as that of global features.	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance.	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997).	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	3.1 Maximum Entropy.	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	In the maximum entropy framework, there is no such constraint.	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	However, their system is a hybrid of hand-coded rules and machine learning methods.	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	Named Entity Recognition: A Maximum Entropy Approach Using Global Information	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	This paper presents a maximum entropy-based named entity recognizer (NER).	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	We have used the Java-based opennlp maximum entropy package1.	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	At most one feature in this group will be set to 1.	0
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	It uses a maximum entropy framework and classifies each word given its features.	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance.	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	This paper presents a maximum entropy-based named entity recognizer (NER).	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	As such, global information from the whole context of a document is important to more accurately recognize named entities.	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities.	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier.	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources.	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	Named Entity Recognition: A Maximum Entropy Approach Using Global Information	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	Context from the whole document can be important in classifying a named entity.	0
The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.	Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	We have shown that the maximum entropy framework is able to use global information directly.	1
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier.	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	3.1 Maximum Entropy.	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	The probability distribution that satisfies the above property is the one with the highest entropy.	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	In the maximum entropy framework, there is no such constraint.	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	Named Entity Recognition: A Maximum Entropy Approach Using Global Information	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	This paper presents a maximum entropy-based named entity recognizer (NER).	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	We have used the Java-based opennlp maximum entropy package1.	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	It uses a maximum entropy framework and classifies each word given its features.	0
modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).	He used an additional maximum entropy classifier that tries to correct mistakes by using reference resolution.	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	It uses a maximum entropy framework and classifies each word given its features.	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique.	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	By using the output of a hand-coded system such as Proteus, MENE can improve its performance, and can even outperform IdentiFinder (Borthwick, 1999).	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997).	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	Sequence of Initial Caps (SOIC): In the sentence Even News Broadcasting Corp., noted for its accurate reporting, made the erroneous announcement., a NER may mistake Even News Broadcasting Corp. as an organization name.	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	Ltd., then organization will be more probable.	0
More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).	This paper presents a maximum entropy-based named entity recognizer (NER).	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	We have used the Java-based opennlp maximum entropy package1.	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	As we will see from Table 3, not much improvement is derived from this feature.	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	Such constraints are derived from training data, expressing some relationship between features and outcome.	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1.	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	This paper presents a maximum entropy-based named entity recognizer (NER).	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	Local features are features that are based on neighboring tokens, as well as the token itself.	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used.	0
They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).	Multiple features can be used for the same token.	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.	1
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	Sentence (2) and (3) help to disambiguate one way or the other.	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	These results are achieved by training on the official MUC6 and MUC7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task (Bikel et al., 1997; Bikel et al., 1999; Borth- wick, 1999).	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	Information from a sentence is sometimes insufficient to classify a name correctly.	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task.	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	Hence we decided to restrict ourselves to only information from the same document.	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	However, their system is a hybrid of hand-coded rules and machine learning methods.	0
Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.	If is one of Monday, Tuesday, . . .	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	Global features are extracted from other occurrences of the same token in the whole document.	1
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	Multiple features can be used for the same token.	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	For each token , zero, one, or more of the features in each feature group are set to 1.	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	However, it is unlikely that other occurrences of News Broadcasting Corp. in the same document also co-occur with Even.	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	Previous work deals with this problem by correcting inconsistencies between the named entity classes assigned to different occurrences of the same entity (Borthwick, 1999; Mikheev et al., 1998).	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	The secondary classifier in (Borthwick, 1999) uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document.	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	Hence, for each token, one of the four features zone-TXT, zone- HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0.	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	This group contains a large number of features (one for each token string present in the training data).	0
Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.	The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	The zone to which a token belongs is used as a feature.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	Multiple features can be used for the same token.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	This group of features attempts to capture such information.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	We group the features used into feature groups.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	The effect of a second reference resolution classifier is not entirely the same as that of global features.	0
(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.	With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997).	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	The use of global features has improved the performance on MUC6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on MUC7 test data from 85.22% to 87.24% (14% reduction in errors).	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	4.2 Global Features.	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	Such a classification can be seen as a not-always-correct summary of global features.	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	The features we used can be divided into 2 classes: local and global.	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	The effect of a second reference resolution classifier is not entirely the same as that of global features.	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	Global features are extracted from other occurrences of the same token in the whole document.	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	This paper presents a maximum entropy-based named entity recognizer (NER).	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	In (Bikel et al., 1997) and (Bikel et al., 1999), performance was plotted against training data size to show how performance improves with training data size.	0
Such global features enhance the performance of NER (Chieu and Ng, 2002b).	A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.	0
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	Location list is processed into a list of unigrams and bigrams (e.g., New York).	1
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	For all lists except locations, the lists are processed into a list of tokens (unigrams).	1
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	Hence, there is a total of 29 classes (7 name classes 4 sub-classes 1 not-a-name class).	0
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	Each name class is subdivided into 4 sub-classes, i.e., N begin, N continue, N end, and N unique.	0
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.	0
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix- List.	0
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	Dictionaries: Due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task.	0
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	A Person-Prefix-List is compiled in an analogous way.	0
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	For corporate suffixes, a list of tokens cslist that occur frequently as the last token of an organization name is collected from the training data.	0
Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.	A secondary reference resolution classifier has information on the class assigned by the primary classifier.	0
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	The features we used can be divided into 2 classes: local and global.	1
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	Local features are features that are based on neighboring tokens, as well as the token itself.	1
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).	0
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).	0
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	Multiple features can be used for the same token.	0
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	Such a classification can be seen as a not-always-correct summary of global features.	0
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	4.1 Local Features.	0
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.	0
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability.	0
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	Another attempt at using global information can be found in (Borthwick, 1999).	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.	1
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	This group contains a large number of features (one for each token string present in the training data).	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword.	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	Suffixes and Prefixes: This group contains only two features: Corporate-Suffix and Person-Prefix.	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	Local features are features that are based on neighboring tokens, as well as the token itself.	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	If is a number string (such as one, two, etc), then the feature NumberString is set to 1.	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1.	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones).	0
Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).	Lexicon Feature: The string of the token is used as a feature.	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: if = true, previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972).	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability.	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	This might be because our features are more comprehensive than those used by Borthwick.	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	The system described in this paper is similar to the MENE system of (Borthwick, 1999).	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	The baseline system in Table 3 refers to the maximum entropy system that uses only local features.	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	It uses a maximum entropy framework and classifies each word given its features.	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3.	0
In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).	Multiple features can be used for the same token.	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.	1
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	Same for . In the case where the next token is a hyphen, then is also used as a feature: (init- Caps, ) is set to 1.	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	The secondary classifier in (Borthwick, 1999) uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document.	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities.	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	(1998) did make use of information from the whole document.	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	The effect of a second reference resolution classifier is not entirely the same as that of global features.	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided.	0
Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).	We have shown that the maximum entropy framework is able to use global information directly.	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities.	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	However, their system is a hybrid of hand-coded rules and machine learning methods.	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	(1998) did make use of information from the whole document.	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	3.1 Maximum Entropy.	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	In the maximum entropy framework, there is no such constraint.	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	The probability distribution that satisfies the above property is the one with the highest entropy.	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	Named Entity Recognition: A Maximum Entropy Approach Using Global Information	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	This paper presents a maximum entropy-based named entity recognizer (NER).	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	We have used the Java-based opennlp maximum entropy package1.	0
AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)	It uses a maximum entropy framework and classifies each word given its features.	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	Global features are extracted from other occurrences of the same token in the whole document.	1
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities.	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	Multiple features can be used for the same token.	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	Although we have not done any experiments on other languages, this way of using global features from a whole document should be applicable to other languages.	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	The zone to which a token belongs is used as a feature.	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	The effect of a second reference resolution classifier is not entirely the same as that of global features.	0
Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).	(1999) did not report using any dictionaries, but mentioned in a footnote that they have added list membership features, which have helped marginally in certain domains.	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.	1
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	Recently, statistical NERs have achieved results that are comparable to hand-coded systems.	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	Named Entity Recognition: A Maximum Entropy Approach Using Global Information	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	It uses a maximum entropy framework and classifies each word given its features.	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	We have shown that the maximum entropy framework is able to use global information directly.	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	MUC7 has also seen hybrids of statistical NERs and hand-coded systems (Mikheev et al., 1998; Borthwick, 1999), notably Mikheev' s system, which achieved the best performance of 93.39% on the official NE test data.	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	The baseline system in Table 3 refers to the maximum entropy system that uses only local features.	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	This group of features attempts to capture such information.	0
To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;	If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Our method is not able to find 43 (329 + 205) Ã 4499 = 362words in all 12 pe these translations.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	So we estimate that recall (for M = 10) is approximately 115 / 362 = 31.8% . pruning.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Accuracy of our system in each period (M = 10) In Table 1, period 1 is Jul 01 â Jul 15, period 2 is Jul 16 â Jul 31, â¦, period 12 is Dec 16 â Dec 31.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation.	0
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	It is phonetic-based.	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	We call Section 4), which was used to rank the English candidate words based on transliteration.	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	Mining New Word Translations from Comparable Corpora	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004).	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new, i.e., not occurring from Jan to Jun 1995.	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	For transliteration, we estimate P(e | c) as follows: P(e | c) = P(e | pinyin) = â P(e, a | pinyin) a For the Chinese corpus, we used the Linguistic Data Consortium (LDC) Chinese Gigaword Corpus from Jan 1995 to Dec 1995.	0
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Comparable corpora refer to texts that are not direct translation but are about the same topic.	1
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	1
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	But parallel corpora are scarce resources, especially for uncommon lan guage pairs.	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency them English translation candidate words.	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Mining New Word Translations from Comparable Corpora	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).	0
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	If an English word e is the translation of a Chinese word c , they will have similar contexts.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.	0
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	Hence, our method of using both sources of information outperforms using either information source alone.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information.	0
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	But parallel corpora are scarce resources, especially for uncommon lan guage pairs.	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	Mining New Word Translations from Comparable Corpora	0
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	Chinese Giga- word corpus consists of news from two agencies: = ââ P(l a a i | pi ) Xinhua News Agency and Central News Agency.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new, i.e., not occurring from Jan to Jun 1995.	0
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	On the other hand, using our method of combining both sources of information and setting M = â, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except å·´ä½äº,å©å,æ®å©æ³) have their correct English translations at rank one position.	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Mining New Word Translations from Comparable Corpora	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	During the whole December period, we only managed to find English translations which were present in the English side of the comparable corpora for 43 Chinese words.	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm.	1
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	The work that is most similar to ours is the recent research of (Huang et al., 2004).	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	But parallel corpora are scarce resources, especially for uncommon lan guage pairs.	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Comparable corpora refer to texts that are not direct translation but are about the same topic.	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	Chinese Giga- word corpus consists of news from two agencies: = ââ P(l a a i | pi ) Xinhua News Agency and Central News Agency.	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service.	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency them English translation candidate words.	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	Comparable corpora such as news documents of the same period from different news agencies are readily available.	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected.	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general.	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004).	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form.	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	0
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) âdT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c tâTc (C ( e )) lables to English letter sequences.	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	On the other hand, using our method of combining both sources of information and setting M = â, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except å·´ä½äº,å©å,æ®å©æ³) have their correct English translations at rank one position.	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	They attempted to improve named entity translation by combining phonetic and semantic information.	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	The name could have multiple translations in English.	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	If an English word e is the translation of a Chinese word c , they will have similar contexts.	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus.	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).	1
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	â t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	rank these words e1 , e2 ,..., ek according to the P (Q | D ) = â P (t | D ) c t average of their rank positions in the two lists.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	Rankâ is the transliteration rank.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	rankâ is the context rank, âTrans.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	Rank of correct translation for period Dec 01 â Dec 15 and Dec 16 â Dec 31.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	#e is the total number of English translation candidates in the period.	0
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	1
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Mining New Word Translations from Comparable Corpora	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	The work that is most similar to ours is the recent research of (Huang et al., 2004).	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	The name could have multiple translations in English.	0
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm.	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.	1
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	That is, Chinese is the source language and English is the target language.	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	AlOnaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters.	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Mining New Word Translations from Comparable Corpora	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information.	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.	0
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases.	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	They attempted to improve named entity translation by combining phonetic and semantic information.	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	Each of the two individual methods provides a P(Q | D) is the one that best matches the query.	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	We translated Chinese words into English.	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	AlOnaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters.	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	8 of the 43 words are translated to English multi-word phrases (denoted as âphraseâ in Table 3).	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	â P(tc tcâC ( c ) | T (C (e)))q (tc ) For our task, the query is C (c) , the context Term tc is a Chinese word.	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping.	0
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	If we are willing to spend more time on searching, then in principle we can find these translations.	0
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.	1
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.	1
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.	1
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	0
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	The additional information may also help to disambiguate the (base) part of speech.	0
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	0
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.	0
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.	0
Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).	8 In German, the genitive case of arguments is more and.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	Our tagger was first evaluated on data from the German Tiger treebank.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	We also evaluated our tagger on the Czech Academic corpus (HladkaÂ´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	The best accuracy of the TnT tagger was 88.2% with a maximal suffix length of 5.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	The accuracy of our tagger is lower than on the development data.	0
For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German	The tagging accuracy reported by Kempe was below that of a traditional trigram tagger.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	A supplementary lexicon was created by analyzing a word list which included all words from the faÎ± paÎ± (t) log paÎ±(t) < 1 training, development, and test data with a German computationa l morphology.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	Note that only the words, but not the POS tags from the test and development data were used, here.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	The data was divided into 80% training data, 10% development data and 10% test data.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	Hence there is no need to put aside training data for parameter tuning.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	The lexical probabilities of unknown words are obtained as follows: The unknown words are divided into four disjoint classes6 with numeric expressions, words starting with an uppercase letter, words starting with a lowercase letter, and a fourth class for the other words.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data.	0
For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.	1
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (GimeÂ´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	MorcËeâs tagging accuracy was 95.12%, 0.3% better than the n-gram tagger.	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	(2007) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (MorcËe), and evaluated them on the Prague Dependency Treebank (PDT 2.0).	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.	0
We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;	Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0).	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	This strategy returned the best results on the development data.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	The best results are obtained with a context size of 10.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	8 In German, the genitive case of arguments is more and.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	Table 3 shows the results of an evaluation based on the plain STTS tagset.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	A threshold of 6 consistently produced optimal or near optimal results for pre-pruning.	0
For German, we show results for RFTagger (Schmid and Laws, 2008).	Results for 2 and for 10 preceding POS tags as context are reported for our tagger.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	Therefore dropping the factor p(word) has no influence on the ranking of the different tag sequences.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	A hybrid system based on four different tagging methods reached an accuracy of 95.68%.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	We also evaluated our tagger on the Czech Academic corpus (HladkaÂ´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	All context attributes other than the base POS are always used in combination with the base POS.	0
The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).	Furthermore, our tagger uses no corpus-specific heuristics, whereas MorcËe e.g. is optimized for Czech POS tagging.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0).	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	In case of the SVM- Tool, we were not able to successfully integrate the supplementary lexicon.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	Relative to the TnT tagger, however, the accuracy is quite similar for test and development data.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	3 We also experimented with a pruning criterion based on binomial tests, which returned smaller trees with a slightly lower accuracy, although the difference in accuracy was never larger than 0.1% for any context size.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	We also evaluated our tagger on the Czech Academic corpus (HladkaÂ´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.	0
However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.	We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train the SVMTool (or other discriminatively trained tag- gers) without such a restriction given the difficulties to train it with the standard context size.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	The analyses gener |TaÎ±| tâTaÎ± pÎ±(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	All context attributes other than the base POS are always used in combination with the base POS.	0
With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)	The first attribute of a POS tag is the main category.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	Given a threshold of 6, the node is therefore not pruned.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	The corresponding figures for the test data are.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	These two-class trees can be pruned with a fixed pruning threshold.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	The third row gives the corresponding figures for our tagger.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	4 5 84.11 89.14 8 5.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	Thus, pre-pruning with a threshold of 6 was used in the experiments.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.	0
Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).	The main differences of our tag- ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.The smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag).5 4.2 Unknown Words.	0
(iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008)	The additional information may also help to disambiguate the (base) part of speech.	0
(iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008)	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
(iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008)	A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.	0
(iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008)	â¢ All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	1
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	The best tag sequence is computed with the Viterbi algorithm.	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|tiâ1 ) is internally computed as a product of attribute probabili ties.	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	This test, which maximizes the information gain1 wrt.	0
These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.	These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.	1
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	6 In earlier experiments, we had used a much larger number of word classes.	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das RealitaÂ¨ t?	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens.	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	Note that only the words, but not the POS tags from the test and development data were used, here.	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	8 In German, the genitive case of arguments is more and.	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.	We also evaluated our tagger on the Czech Academic corpus (HladkaÂ´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	1
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	Thus, pre-pruning with a threshold of 6 was used in the experiments.	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.	0
For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).	The best tag sequence is computed with the Viterbi algorithm.	0
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	1
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.	1
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.	0
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	We also evaluated our tagger on the Czech Academic corpus (HladkaÂ´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.	0
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	0
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.	0
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.	The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.	1
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	Note that only the words, but not the POS tags from the test and development data were used, here.	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das RealitaÂ¨ t?	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23.	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	The probability of an attribute (such as âNomâ) is always conditioned on the respective base POS (such as âNâ) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns.	0
For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.	Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in).	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in).	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	The analyses gener |TaÎ±| tâTaÎ± pÎ±(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	xical prob HMM taggers are fast and were successfully applied to a wide range of languages and training corpora.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Note that only the words, but not the POS tags from the test and development data were used, here.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	These values were optimal on the development data.	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.	1
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.	0
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das RealitaÂ¨ t?	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (GimeÂ´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	3 We also experimented with a pruning criterion based on binomial tests, which returned smaller trees with a slightly lower accuracy, although the difference in accuracy was never larger than 0.1% for any context size.	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features.	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model.	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.	0
Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).	Decision trees are incrementally built by first selecting the test which splits the manually annotated training sample into the most homogeneous subsets with respect to the class.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in).	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	The analyses gener |TaÎ±| tâTaÎ± pÎ±(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	xical prob HMM taggers are fast and were successfully applied to a wide range of languages and training corpora.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Note that only the words, but not the POS tags from the test and development data were used, here.	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	These values were optimal on the development data.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	Note that only the words, but not the POS tags from the test and development data were used, here.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (GimeÂ´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	Table 3 shows the results of an evaluation based on the plain STTS tagset.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	A hybrid system based on four different tagging methods reached an accuracy of 95.68%.	0
In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.	A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	The analyses gener |TaÎ±| tâTaÎ± pÎ±(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	Results for 2 and for 10 preceding POS tags as context are reported for our tagger.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	The tagger builds a suffix trie for each class of unknown words using the known word types from that class.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.	0
The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.	Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	1
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	â¢ All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	The additional information may also help to disambiguate the (base) part of speech.	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).	As an example, take the German sentence Das zu versteuernde Einkommen sinkt (âThe to be taxed income decreasesâ; The tËN N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	Furthermore, our tagger uses no corpus-specific heuristics, whereas MorcËe e.g. is optimized for Czech POS tagging.	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	Czech POS tagging has been extensively studied in the past (HajicË and VidovaÂ´-HladkaÂ´, 1998; HajicË et al., 2001; Votrubec, 2006).	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.	0
6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.	This restriction improved the tagging accuracy for large contexts.	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data.	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	We also evaluated our tagger on the Czech Academic corpus (HladkaÂ´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (HladkaÂ´ et al., 2007) and compared to the TnT tag- ger.	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	The additional information may also help to disambiguate the (base) part of speech.	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	Czech POS tagging has been extensively studied in the past (HajicË and VidovaÂ´-HladkaÂ´, 1998; HajicË et al., 2001; Votrubec, 2006).	0
In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).	The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS.	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.	1
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	The first result was obtained with TnT trained on Tiger data which was mapped to STTS before.	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	The data was divided into 80% training data, 10% development data and 10% test data.	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (GimeÂ´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.	0
Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	This restriction improved the tagging accuracy for large contexts.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	Table 2: Tagging accuracies on development data in percent.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	The tagging accuracy reported by Kempe was below that of a traditional trigram tagger.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	With athreshold of 10â3 or lower, the influence of prun ing on the tagging accuracy was negligible.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	By far the most frequent tagging error was the confusion of nominative and accusative case.	0
Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)	Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	Thus, the simpler pruning strategy presented here was chosen.	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	Note that only the words, but not the POS tags from the test and development data were used, here.	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	The analyses gener |TaÎ±| tâTaÎ± pÎ±(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	A similar idea was previously presented in Kempe (1994), but apparently never applied again.	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (GimeÂ´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	The tagger builds a suffix trie for each class of unknown words using the known word types from that class.	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.	0
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	1
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	The additional information may also help to disambiguate the (base) part of speech.	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	By far the most frequent tagging error was the confusion of nominative and accusative case.	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	(2003) describe a more complex backoff smoothing method.	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class.	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	â¢ All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	Our tagger combines two ideas, the decomposition of the probability of complex POS tags into a product of feature probabilities, and the estimation of the conditional probabilities with decision trees.	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	The suffix tries are pruned until (i) all suffixes have a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous.	0
So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)	An object is classified by evaluating the test of the top node on the object, following the respective edge to a daughter node, evaluating the test of the daughter node, and so on until a terminal node is reached whose class is assigned to the object.	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	6 Joint Segmentation and Parsing.	1
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	Better Arabic Parsing: Baselines, Evaluations, and Analysis	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic.	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words.	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	To our knowledge, ours is the first analysis of this kind for Arabic parsing.	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	We propose a limit of 70 words for Arabic parsing evaluations.	0
Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).	We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	6 Joint Segmentation and Parsing.	1
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation.	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	Variants of alif are inconsistently used in Arabic texts.	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (Â§6).	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	But for eign learners are often surprised by the verbless predications that are frequently used in Arabic.	0
Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).	11 taTweel (-) is an elongation character used in Arabic script to justify text.	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	input token, the segmentation is then performed deterministically given the 1-best analysis.	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	Each model was able to produce hypotheses for all input sentences.	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (Â§6).	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	Moreover, the Stanford parser achieves the most exact Leaf Ancestor matches and tagging accuracy that is only 0.1% below the Bikel model, which uses pre-tagged input.	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	The Stanford parser includes both the manually annotated grammar (Â§4) and an Arabic unknown word model with the following lexical features: 1.	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	In the initial release of the ATB, inter-annotator agreement was inferior to other LDC treebanks (Maamouri et al., 2008).	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design.	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words.	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	In particular, the decision to represent arguments in verb- initial clauses as VP internal makes VSO and VOS configurations difficult to distinguish.	0
One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).	Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	Diacritics can also be used to specify grammatical relations such as case and gender.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	Variants of alif are inconsistently used in Arabic texts.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	Until now, all evaluations of Arabic parsingâincluding the experiments in the previous sectionâhave assumed gold segmentation.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	But for eign learners are often surprised by the verbless predications that are frequently used in Arabic.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	11 taTweel (-) is an elongation character used in Arabic script to justify text.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply âArabicâ) because of the unusual opportunity it presents for comparison to English parsing results.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.	0
This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)	Crucially, the conventional orthographic form of MSA text is unvocalized, a property that results in a deficient graphical representation.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	6 Joint Segmentation and Parsing.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	Certainly these linguistic factors increase the difficulty of syntactic disambiguation.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	Nonetheless, parse quality is much lower in the joint model because a lattice is effectively a long sentence.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Simaâan, 2008), and the effect of variable word order (Collins et al., 1999).	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.	0
Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).	Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% F1.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length â¤ 40.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	F1 85 Berkeley 80 Stanford.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	English parsing evaluations usually report results on sentences up to length 40.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	The Stanford parser includes both the manually annotated grammar (Â§4) and an Arabic unknown word model with the following lexical features: 1.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	Until now, all evaluations of Arabic parsingâincluding the experiments in the previous sectionâhave assumed gold segmentation.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (Â§5).	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	However, when we pre- tag the inputâas is recommended for Englishâ we notice a 0.57% F1 improvement.	0
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length â¤ 40.	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	The Stanford parser includes both the manually annotated grammar (Â§4) and an Arabic unknown word model with the following lexical features: 1.	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	We are unaware of prior results for the Stanford parser.	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	English parsing evaluations usually report results on sentences up to length 40.	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	Table 6: Incremental dev set results for the manually annotated grammar (sentences of length â¤ 70).	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	We propose a limit of 70 words for Arabic parsing evaluations.	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.	0
The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).	Arabic sentences of up to length 63 would need to be.	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply âArabicâ) because of the unusual opportunity it presents for comparison to English parsing results.	1
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007).	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	Crucially, the conventional orthographic form of MSA text is unvocalized, a property that results in a deficient graphical representation.	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation.	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length â¤ 40.	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (AlBatal, 1990).	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing.	0
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (Â§4).	1
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	In our grammar, features are realized as annotations to basic category labels.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	Annotation consistency is important in any supervised learning task.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008).	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.	How should the absence of vowels and syntactic markers influence annotation choices and grammar development?	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	We propose a limit of 70 words for Arabic parsing evaluations.	1
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (Â§5).	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	English parsing evaluations usually report results on sentences up to length 40.	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	Arabic sentences of up to length 63 would need to be.	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	(2006) developed a technique for splitting and chunking long sentences.	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	Nonetheless, parse quality is much lower in the joint model because a lattice is effectively a long sentence.	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	Table 9: Dev set results for sentences of length â¤ 70.	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (AlBatal, 1990).	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	Better Arabic Parsing: Baselines, Evaluations, and Analysis	0
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.	Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering.	0
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	We start with noun features since written Arabic contains a very high proportion of NPs.	1
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	In our grammar, features are realized as annotations to basic category labels.	1
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	The Stanford parser includes both the manually annotated grammar (Â§4) and an Arabic unknown word model with the following lexical features: 1.	0
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	segmentation (Table 2).	0
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	But Arabic contains a variety of linguistic phenomena unseen in English.	0
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	Figure 2: An ATB sample from the human evaluation.	0
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	Contains digits.	0
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	Process nominals name the action of the transitive or ditransitive verb from which they derive.	0
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages.	0
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	8 We use head-finding rules specified by a native speaker.	1
For Arabic, we use the head-finding rules from Green and Manning (2010).	We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (Â§4).	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	We use the default inference parameters.	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	We use the log-linear tagger of Toutanova et al.	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead).	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	Since our objective is to compare distributions of bracketing discrepancies, we do not use heuristics to prune the set of nuclei.	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	We retain segmentation markersâwhich are consistent only in the vocalized section of the treebankâto differentiate between e.g. ï¿½ âtheyâ and ï¿½ + âtheir.â Because we use the vocalized section, we must remove null pronoun markers.	0
For Arabic, we use the head-finding rules from Green and Manning (2010).	Unlike the WSJ corpus which has a high frequency of rules like VP âVB PP, Arabic verb phrases usually have lexi calized intervening nodes (e.g., NP subjects and direct objects).	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.	1
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	We start with noun features since written Arabic contains a very high proportion of NPs.	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing.	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	We showed in Â§2 that lexical ambiguity explains the underperformance of these categories.	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	We propose a limit of 70 words for Arabic parsing evaluations.	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	We also collapse unary chains withidentical basic categories like NP â NP.	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	However, when we pre- tag the inputâas is recommended for Englishâ we notice a 0.57% F1 improvement.	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead).	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus.	0
We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).	Better Arabic Parsing: Baselines, Evaluations, and Analysis	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.	1
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% F1.	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	We showed in Â§2 that lexical ambiguity explains the underperformance of these categories.	0
We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).	We propose a limit of 70 words for Arabic parsing evaluations.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	Modifying the Berkeley parser for Arabic is straightforward.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	(2006).	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	The Berkeley parser gives state-of-the-art performance for all metrics.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	The Stanford parser includes both the manually annotated grammar (Â§4) and an Arabic unknown word model with the following lexical features: 1.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	We showed in Â§2 that lexical ambiguity explains the underperformance of these categories.	0
We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets	We quantify error categories in both evaluation settings.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	Annotation consistency is important in any supervised learning task.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (Â§3).	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	This annotation choice weakens splitIN.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a).	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals.	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	How should the absence of vowels and syntactic markers influence annotation choices and grammar development?	0
Recently, Green and Manning (2010) analyzed the PATB for annotation consistency	We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (Â§4).	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	6 Joint Segmentation and Parsing.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	Nonetheless, parse quality is much lower in the joint model because a lattice is effectively a long sentence.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	We propose a limit of 70 words for Arabic parsing evaluations.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	In general, several gross corpus statistics favor the ATB, so other factors must contribute to parsing underperformance.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.	0
We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).	Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	12 For English, our Evalb implementation is identical to the most recent reference (EVALB20080701).	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	This paper is based on work supported in part by DARPA through IBM.	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	It has no syntactic function.	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	This feature has a linguistic justification.	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	Acknowledgments We thank Steven Bethard, Evan Rosen, and Karen Shiells for material contributions to this work.	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.	0
Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)	A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (AlBatal, 1990).	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (Â§3).	1
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	Annotation consistency is important in any supervised learning task.	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a).	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals.	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (Â§4).	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	Instead, we extend the variation n-gram method of Dickinson (2005) to compare annotation error rates in the WSJ and ATB.	0
Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)	We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).	1
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models.	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ.	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric.	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length â¤ 40.	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.	0
Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.	Even with vocalization, there are linguistic categories that are difficult to identify without semantic clues.	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.	1
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	Better Arabic Parsing: Baselines, Evaluations, and Analysis	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	To our knowledge, ours is the first analysis of this kind for Arabic parsing.	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	We propose a limit of 70 words for Arabic parsing evaluations.	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	5.1 Parsing Models.	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	6 Joint Segmentation and Parsing.	0
Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.	To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply âArabicâ) because of the unusual opportunity it presents for comparison to English parsing results.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	We are unaware of prior results for the Stanford parser.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	Moreover, the Stanford parser achieves the most exact Leaf Ancestor matches and tagging accuracy that is only 0.1% below the Bikel model, which uses pre-tagged input.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	The Stanford parser includes both the manually annotated grammar (Â§4) and an Arabic unknown word model with the following lexical features: 1.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	Modifying the Berkeley parser for Arabic is straightforward.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	evaluated to account for the same fraction of the data.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers.	0
The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).	4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun ï¿½ '.i . Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (Â§6).	1
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	The ATB segmentation scheme is one of many alternatives.	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (HajicË and ZemaÂ´nek, 2004; Habash and Roth, 2009).	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	We are unaware of prior results for the Stanford parser.	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	6 Joint Segmentation and Parsing.	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	The resulting structural differences between tree- banks can account for relative differences in parsing performance.	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings.	0
One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	The pre terminal morphological analyses are mapped to the shortened âBiesâ tags provided with the tree- bank.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	But Arabic contains a variety of linguistic phenomena unseen in English.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	In application settings, this may be a profitable strategy.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.	0
2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).	Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (Â§6).	1
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	This paper is based on work supported in part by DARPA through IBM.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	Aside from adding a simple rule to correct alif deletion caused by the preposition J, no other language-specific processing is performed.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	more frequently than is done in English.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	We use the log-linear tagger of Toutanova et al.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	6 Joint Segmentation and Parsing.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	For each terminal, the Leaf Ancestor metric extracts the shortest path to the root.	0
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	However, in practice, unknown word models also make the distribution improper.	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text.	1
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences.	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	We also mark all nodes that dominate an SVO configuration (containsSVO).	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	Arabic sentences of up to length 63 would need to be.	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	(2006) developed a technique for splitting and chunking long sentences.	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	Although these are technically nominal, they have become known as âequationalâ sentences.	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	Table 9: Dev set results for sentences of length â¤ 70.	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	Each model was able to produce hypotheses for all input sentences.	0
Following Green and Manning (2010) and others, sentences headed by X nodes are deleted	English parsing evaluations usually report results on sentences up to length 40.	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.	1
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	Our baseline for all sentence lengths is 5.23% F1 higher than the best previous result.	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	F1 85 Berkeley 80 Stanford.	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% F1.	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages.	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.	0
Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).	Table 8a shows that the best model recovers SBAR at only 71.0% F1.	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.	1
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct ï¿½ ?f iDafa.	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length â¤ 40.	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (Â§5).	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007).	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing.	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	The range of the score is between 0 and 1 (higher is better).	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	English parsing evaluations usually report results on sentences up to length 40.	0
Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.	The pre terminal morphological analyses are mapped to the shortened âBiesâ tags provided with the tree- bank.	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-.	1
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	pre-processing.	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	The Berkeley parser gives state-of-the-art performance for all metrics.	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	Moreover, the Stanford parser achieves the most exact Leaf Ancestor matches and tagging accuracy that is only 0.1% below the Bikel model, which uses pre-tagged input.	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	phrase (markContainsVerb).	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order.	0
The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)	Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	At the phrasal level, we remove all function tags and traces.	1
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	7 Unlike Dickinson (2005), we strip traces and only con-.	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	We map the ATB morphological analyses to the shortened âBiesâ tags for all experiments.	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	Finally, we add âDTâ to the tags for definite nouns and adjectives (Kulick et al., 2006).	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	We also mark all nodes that dominate an SVO configuration (containsSVO).	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead).	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization.	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	We retain segmentation markersâwhich are consistent only in the vocalized section of the treebankâto differentiate between e.g. ï¿½ âtheyâ and ï¿½ + âtheir.â Because we use the vocalized section, we must remove null pronoun markers.	0
We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.	Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% F1.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).	1
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	But Arabic contains a variety of linguistic phenomena unseen in English.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	11 taTweel (-) is an elongation character used in Arabic script to justify text.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	evaluated to account for the same fraction of the data.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	We start with noun features since written Arabic contains a very high proportion of NPs.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	of Arabic.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.	0
Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)	Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings.	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	Copying sharable parts is called redundant copying.	1
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721.	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	A FORWARD slot value represents an eternal relationship while a COPY slot value represents a temporary relationship.	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	The above factors can be examined by inspecting failure tendency information, from which the efficiency gain from the SING method can be predicted.	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.	0
While an improvement over simple destructive unification, Tomabechi&aposs approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.	(2) Number of features FSs have: if each FS has only a small number of features, the efficiency gain from the SING unification method is small.	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	Strategic Lazy Incremental Copy Graph Unification	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	However, such cases do not occur or are very rare, and for example, in many cases of natural language analysis, FS unification failures occur in treating only limited kinds of features related to grammatical agreement such as number and/or person agreement and semantic selectional constraints.	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	In such cases, application of the EFF strategy, that is, treating features tending to fall in unification first, reduces unnecessary computation when the unification finally fails.	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	To achieve this, I, he LING unification method, which uses copy dependency information, was developed.	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	Method In a system where FS unification is applied, there are features whose values fail relatively often in unification with other values and there are features whose values do not fail so often.	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	(2) Number of features FSs have: if each FS has only a small number of features, the efficiency gain from the SING unification method is small.	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.	0
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.	The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	In Section 5, a method which uses this generalized strategy is proposed.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	That is, an FS unification method is proposed that introduces a strategy called the e_arly failure Â£inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).	0
This observation is the basis for a reordering method proposed by Kogure [1990].	Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	Section 3 and 4 introduce the LING method and the SING method, respectively.	0
This observation is the basis for a reordering method proposed by Kogure [1990].	These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	By using learned failure tendency information, feature value unification is applied in an order that first treats features with the greatest tendency to fail.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	This order is related to the unification failure tendency.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system.	0
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).	The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test.	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.	1
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	Copying sharable parts is called redundant copying.	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	Strategic Lazy Incremental Copy Graph Unification	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	Previous research identified DG copying as a significant overhead.	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	A better method would nfinimize the copying of sharable varts.	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721.	0
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.	One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	Copying sharable parts is called redundant copying.	1
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	Structure sharing avoids memory wastage'.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	This paper proposes an FS unification method that allows structure sharing with constant m'der node access time.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	Pereira's structure sharing FS unification method can avoid this problem.	0
• Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.	The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	For example, in Japanese sentence analysis, unification of features for conjugation forms, case markers, and semantic selectional restrictions tends to fail but unification of features for semantic representations does not fail.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	For example, a spoken Present.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	To achieve this, I, he LING unification method, which uses copy dependency information, was developed.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	These formalisms were applied in the field of natural language processing and, based on these formalisms, ~:~ystems such as machine translation systems were developed [l<ol;u, e et a l 8gJ.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	This causes O(log d) graph node access time overhead in assembling the whole DG from the skeleton and environments where d is the number of nodes in the DG.	0
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).	The combined method Inakes each FS unification efficient and also reduces garbage collection and page swapping occurrences by avoiding memory wastage, thus increasing the total efficiency of li'S unification-based natural language processing systems such aa analysis and generation systems based on IlI'SG.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	The revised procedure uses a newly introduced slot COPY-DEPENDENCY.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	3'he skeleton part is shared by one of the input FSs and the result FS.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	Copying sharable parts is called redundant copying.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	,',:opyArcs applies CopyNode to each arc value with node l' as the new ancestor node and returns the set of new arcs for non-Nil_ CopyNode results.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).	Structure sharing avoids memory wastage'.	0
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	Copying sharable parts is called redundant copying.	1
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	A better method would nfinimize the copying of sharable varts.	1
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.	1
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).	0
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.	0
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	Pereira's structure sharing FS unification method can avoid this problem.	0
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	5, the subgraphs of the result DG surrounded by the dashed rectangle can be shared with subgraphs of input structures G1 and G2, Section 4 proposes a method t.hat avoids this problem, Wroblewski's method first treats arcs with labels that exist in both input nodes and then treats arcs with unique labels.	0
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	3'he skeleton part is shared by one of the input FSs and the result FS.	0
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).	Previous research identified DG copying as a significant overhead.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	Strategic Lazy Incremental Copy Graph Unification	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	To achieve this, I, he LING unification method, which uses copy dependency information, was developed.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	The LING unification procedure uses a revised CopyNode procedure which does not copy structures immediately.	0
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug	The revised procedure uses a newly introduced slot COPY-DEPENDENCY.	0
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	A better method would nfinimize the copying of sharable varts.	1
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	Copying sharable parts is called redundant copying.	1
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.	0
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.	0
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	Pereira's structure sharing FS unification method can avoid this problem.	0
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).	0
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.	0
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short).	0
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].	These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).	0
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	Copying sharable parts is called redundant copying.	1
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.	1
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.	0
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	ENDPROCEDURE Figure 7: The revised CopyNode procedure has the disadvantage of treating copy dependency information.	0
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.	0
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	The LING unification procedure uses a revised CopyNode procedure which does not copy structures immediately.	0
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	That is, antecedent nodes in the COPY-DEPENDENCY values are also copied.	0
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	The revised procedure uses a newly introduced slot COPY-DEPENDENCY.	0
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure&aposs approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.	The procedure incrementally copies nodes and ares on the subgraphs of each input 1)G until a node with an empty ARCS value is found.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	Complex FSs can have complex FSs as their feature values and can share certain values among features.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	The advantages of such a system include: (1)rule writers are not required to describe control infimnation such as eonstraiut application order in a rule, and (12)rule descriptions can be used iu different processing directions, i.e., analysis and general,ion.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	Ile proposed an incremental copy graph unification method to avoid over copying and early copying.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	When a new copy of a node is needed later, the LING unification procedure will actually copy structures using the COPY-DEPENDENCY slot value of the node (in GetOutNode procedure in lJ'ig.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	When a NODE's GENERATION value is equal to the global value specifying the current unit]cation process, the structure has been created in the current process or that the structure is currel~l. The characteristics which allow nondestructive incremental copy are the NODE's two different slots, FORWARD and COPY, for representing forwarding relationships.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	Copying sharable parts is called redundant copying.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	That is, antecedent nodes in the COPY-DEPENDENCY values are also copied.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	However, these advantages in describing rules are disadvantages in applying them because of tt~e lack of control information.	0
PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);	Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.	1
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	The Troll system, which is based on this idea, effectively inqflements type resolution.	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	ig (i,.L(~sti,.malfl(~ wh('thef such a.d ditional (:()mpl(~xit.y would I)e justified to 'Our CXl)ericl~(:c is derived l,'imarily flora test-i.I" Ihc 'l'loll system (m a tat, her lar<e,e e, ramul;G for (',(!l>lll;lll imfiial vcrh I>lHases, which was wiit-t('n I)y I'hhard Ilillrichs a.d Tsum:ko Na, kazawa aud iinl)lclncut,cd by I)clmar McuH_:J's.	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	(;lea['ly the re ~olv;-~nl.~, o[ such a. recursiv(~ l.yl)(', could Not I)(~ l,reCOmlfiled a.s r,.~quiI'oxl in Troll.	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	Then in ~3, we discuss how such type cons|fronts linty be mainta.ined under unification as exemplilied in the na.tura.1 language D~rs- ing/generation system '.l'ro]l [7].	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities.	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability.	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	i ;ill(| (i(~t,z's Troll are ex:-t.niples o| illilllenienl.a.Lions o| a,pF, ro]) ria, Loliess |or illa.[iSlil,s. l low an a.i)ln'oprhi.teness [orniaJisnl enco<les a conjunctive I:(',R is ob\.'i<>us~ bll(.	0
This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).	In this pal)er, we will first in §2 survey the range of type eonstra.ints tha.t ma.y be expressed with just a. type hiera.rchy and *']'he resea.rch pl'eS(!lllL('d ill |,his; paper was pay tia.lly sponsored hy '[kfilprojekt B4 "(;onsl.rahH.s on Grammar fl~r Efficient Ck:neration" of the Soi,der forschungsbereich 340 of the Deutsche ["orschungsgemeinscha, ft. "VVe would also like to thank 'l'hilo GStz for helph,l comments ou thc ideas present.ed here.	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities.	1
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	The Troll system, which is based on this idea, effectively inqflements type resolution.	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch.	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	type inferencing fails?	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	This strategy a.ctua.lly ma.inta.ins apl)ropri~tteness conditions in some ca.ses in which a. type in-ferencing stra.tegy would fa.il, l)'inMly, in §4, we discuss the possibilities for genera lizillg this a.pl)roa.ch to ha.ndle a bro~Mer r~tnge of constra.ints, including constraints inw)lving reentran cies.	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	Uni/ication of sets of fca.ture structures is defined here ill the standard way: S t2 ,S" = {1"[ I"' 6 S and l"" G S" and 1" = 1"' H 1""}.	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	(;lea['ly the re ~olv;-~nl.~, o[ such a. recursiv(~ l.yl)(', could Not I)(~ l,reCOmlfiled a.s r,.~quiI'oxl in Troll.	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	is a constra.int of the following fornl : i[' a.n object is of ;~ cert;fin kind then ill deserves certa.in fea.tures with wdues of cert~till kinds An FCI~ stat:ing tha,2: a. verb must h~we v and N t'eatures with values A- and -respectively is a.ll example of a. conjunctive FCI{.	0
The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.	This, h.w(wer, wa.s a. simple ca.se iu which a.I1 of the named dis.jun(:tion could ho removed.	0
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.	1
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl" the feature structure is satisfiable.	1
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.	0
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	It would, of course, not be very efficient to work with such large disjunctions of feature structures.	0
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed.	0
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES	0
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively.	0
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability.	0
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	Consider again the encoding of p and the feature structure 9~.	0
449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,&aposOaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.	!['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	Collocations were automatically located in a text by looking up pairwise words in this lexicon.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	The combination of word repetition with another linguistic feature improved on its individual result, where less troughs were placed per text.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	word repetition 7.1 3.16 41 collocation (97.6%) word repetition 7.3 5.22 41 relation weights (97.6%) 41 Collocations are located by looking up word pairs in the collocation lexicon.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	Due to that success, these five approaches were applied in this experiment.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.	These texts were presented to seven test subjects who were instructed to identify the sentences at which a new subject area commenced.	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	collocation 6.3 3.83 35 (83.3%) Table 1.	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	Another approach to text segmentation is the detection of semantically related words.	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	Text Segmentation Using Reiteration and Collocation	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result.	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	Collocations were automatically located in a text by looking up pairwise words in this lexicon.	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	Therefore, many of the troughs placed by the segmentation algorithm represented valid subject Table 2.	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	Comparison of segmentation algorithm using different linguistic features.	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994).	0
Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.	word repetition 7.1 3.16 41 collocation (97.6%) word repetition 7.3 5.22 41 relation weights (97.6%) 41 Collocations are located by looking up word pairs in the collocation lexicon.	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.	1
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows.	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	Relation weights are word repetition 8.5 3.62 (97.6%) calculated between pairwise words according to their location in RT.	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text.	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995).	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	Collocations were automatically located in a text by looking up pairwise words in this lexicon.	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result.	0
In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.	In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	Ponte and Croft ( 1997) used word co-occurrences to expand the number of terms for matching.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994).	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	The lexical cohesion relations of reiteration and collocation are used to identify related words.	0
Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.	Lexical cohesion is divided into three classes: general noun, reiteration and collocation.	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	Another approach extracted semantic information from Roget's Thesaurus (RT).	1
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	All semantic relations not classified under the class of reiteration are attributed to the class of collocation.	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	Identifying semantic relations in a text can be a useful indicator of its conceptual structure.	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	The lexical cohesion relations of reiteration and collocation are used to identify related words.	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995).	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented.	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured.	0
When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.	Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.	1
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	A window size of three sentences was found to produce the best results.	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	The first two methods represent lexical cohesion relations.	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	Hence, each generated text consisted of two articles.	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	Halliday and Hasan classified cohesion under two types: grammatical and lexical.	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows.	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score.	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	An instance of cohesion between a pair of elements is referred to as a tie.	0
The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).	In this investigation, an error margin of two sentences was considered.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	Text segmentation could also be used as a pre-processing step in automatic summarisation.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	Adjacent segmentation points were treated as one point because it is likely that they refer to the same subject change.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	Ponte and Croft ( 1997) used word co-occurrences to expand the number of terms for matching.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	Word repetitions are identified between identical words and words derived from the same stem.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	An error margin of two sentences either side of a segmentation point was used by Hearst (1993) and Reynar ( 1994) allowed three sentences.	0
This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).	Each text was only 500 words in length and was related to a specific subject area.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	Word repetitions are identified between identical words and words derived from the same stem.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	The first two methods represent lexical cohesion relations.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	Reynar ( 1994) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	Hence, each generated text consisted of two articles.	0
In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.	Method: Seven topical articles of between 250 to 450 words in length were extracted from the World Wide Web.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	Text segmentation could also be used as a pre-processing step in automatic summarisation.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	Segmentation points, indicating a change of subject, were determined by the agreement of three or more test subjects (Litman ami Passonneau, 1996).	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	An investigation was conducted to determine whether the segmentation algorithm could reliably locate subject change in text.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	Previous work on text segmentation has used term matching to identify clusters of related text.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	An error margin of two sentences either side of a segmentation point was used by Hearst (1993) and Reynar ( 1994) allowed three sentences.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	No restriction was placed on the number of subject changes that could be identified.	0
This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).	Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel).	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	Many examples of heterogeneous data can be found in daily life.	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation.	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	Word repetitions are identified between identical words and words derived from the same stem.	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	Cohesion concerns how words in a text are related.	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction.	0
In other words, meaning of UW can be found generally through co- occurrence words [5].	For example, the pairwise words orange and peel form a collocation.	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved.	1
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes.	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	Each segment could be summarised individually and then combined to provide an abstract for a document.	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	To evaluate the results, the information retrieval metrics precision and recall were used.	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994).	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	In this investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments (4.1 per text) than the test subjects (4.5).	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	Yaari ( 1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments.	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	For 9 out of the 20 texts segmented, all troughs were relevant.	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	Identifying semantic relations in a text can be a useful indicator of its conceptual structure.	0
In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].	Method: Seven topical articles of between 250 to 450 words in length were extracted from the World Wide Web.	0
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.	1
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	Identifying semantic relations in a text can be a useful indicator of its conceptual structure.	1
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	The first two methods represent lexical cohesion relations.	0
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).	0
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	The lexical cohesion relations of reiteration and collocation are used to identify related words.	0
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	Another approach extracted semantic information from Roget's Thesaurus (RT).	0
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	All semantic relations not classified under the class of reiteration are attributed to the class of collocation.	0
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995).	0
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows.	0
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	Halliday and Hasan classified cohesion under two types: grammatical and lexical.	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219â228, Singapore, 67 August 2009.	1
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	2.2 N -gram Language Model N -gram language models have become standard in machine translation systems.	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	Translations Classical lexical translation features depend on s and t and the alignment a between them.	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment.	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	Our QDG decoder has no way to enforce coverage; it does not track any kind of state in Ïs apart from a single recently aligned word.	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	After discarding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words Ã phrase conditional and âlexical smoothingâ probabilities Ã two conditional directions.	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	We therefore optimize pseudo-likelihood instead, making the following approximation (Be 10 Alignments could be supplied by automatic word alignment algorithms.	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001).	0
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).	We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the âgrow-diag-final-andâ heuristic, and extract phrases up to length 3.	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, Ït, a | s, Ïs).	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	Feature-Rich Translation by Quasi-Synchronous Lattice Parsing	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	We filter sentences of length more than 15 words, which only removes 6% of the data.	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993).	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	Decoding as QG parsing (Â§3â4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary ânon- localâ features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (Â§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	In this work, we focus on syntactic features of target-side dependency trees, Ït, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features.	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	(2005) use features involving phrases and source- side dependency trees and Mi et al.	0
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).	We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001).	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	Decoding as QG parsing (Â§3â4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary ânon- localâ features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (Â§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	Feature-Rich Translation by Quasi-Synchronous Lattice Parsing	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, Ït, a | s, Ïs).	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	2.2 N -gram Language Model N -gram language models have become standard in machine translation systems.	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist.	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in Ït (or a deliberate choice is made by the decoder to translate it to NULL).	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice.	0
Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).	Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	Feature-Rich Translation by Quasi-Synchronous Lattice Parsing	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, Ït, a | s, Ïs).	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	2.2 N -gram Language Model N -gram language models have become standard in machine translation systems.	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in Ït (or a deliberate choice is made by the decoder to translate it to NULL).	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001).	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	Decoding as QG parsing (Â§3â4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary ânon- localâ features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (Â§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.	0
We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.	4.1 Translation as Monolingual Parsing.	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219â228, Singapore, 67 August 2009.	1
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, Ït, a | s, Ïs).	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	We denote this grammar by Gs,Ïs ; its (weighted) language is the set of translations of s. Each word generated by Gs,Ïs is annotated with a âsense,â which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in Ït and nodes in Ïs. In principle, any portion of Ït may align to any portion of Ïs, but in practice we often make restrictions on the alignments to simplify computation.	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	Given a source sentence s and its parse Ïs, a QDG induces a probabilistic monolingual dependency grammar over sentences âinspiredâ by the source sentence and tree.	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	It equates to finding the most probable derivation under the s/Ïs-specific grammar Gs,Ïs . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known.	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	If, for example, we require that, for all j, a(Ït(j)) = Ïs(a(j)) or a(j) = 0, and that the root of Ït must align to the root of Ïs or to NULL, then strict isomorphism must hold between Ïs and Ït, and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005).	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in Ït (or a deliberate choice is made by the decoder to translate it to NULL).	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible.	0
We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).	There have been many features proposed that consider source- and target-language syntax during translation.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	It equates to finding the most probable derivation under the s/Ïs-specific grammar Gs,Ïs . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	Given the lattice and Gs,Ïs , lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms Ït , and the alignments aâ that are most probable, (Eisner, 1997).	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	Given a sentence s and its parse Ïs, at decoding time we seek the target sentence tâ, the target tree For a QDG model, the decoding problem has not been addressed before.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	Given a sentence s and its parse tree Ïs, we formulate the translation on the feasibility of inference, including decoding.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	We denote this grammar by Gs,Ïs ; its (weighted) language is the set of translations of s. Each word generated by Gs,Ïs is annotated with a âsense,â which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in Ït and nodes in Ïs. In principle, any portion of Ït may align to any portion of Ïs, but in practice we often make restrictions on the alignments to simplify computation.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	Given a source sentence s and its parse Ïs, a QDG induces a probabilistic monolingual dependency grammar over sentences âinspiredâ by the source sentence and tree.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	4 problem as finding the target sentence tâ (along with its parse tree Ï â source tree) such that3 and alignment aâ to the 2.1 Lexical.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	5.2 Summing over Ït and a. For the summation over dependency trees and alignments given fixed t, required for p(Ït | t, s, Ïs), we perform âinsideâ lattice parsing with Gs,Ïs . The technique is the summing variant of the decoding method in Â§4, except for each state j, the sausage lattice only includes arcs from j â 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3) time and requires O(a2) space.	0
For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.	We decode by performing lattice parsing on a lattice encoding the set of possible translations.	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	We use the GermanEnglish portion of the Basic Travel Expression Corpus (BTEC).	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	, m} â 2{1,...,n} Î¸ source and target language vocabularies, respectively function mapping each source word to target words to which it may translate source language sentence (s0 is the NULL word) target language sentence, translation of s dependency tree of s, where Ïs (i) is the index of the parent of si (0 is the root, $) dependency tree of t, where Ït (i) is the index of the parent of ti (0 is the root, $) alignments from words in t to words in s; â denotes alignment to NULL parameters of the model gtrans (s, a, t) f lex (s, t) j f phr (si , tk ) lexical translation features (Â§2.1): word-to-word translation features for translating s as t phrase-to-phrase translation features for translating sj as t i k glm (t) j f N (tjâN +1 ) language model features (Â§2.2): N -gram probabilities gsyn (t, Ït ) f att (t, j, tl, k) f val (t, j, I ) target syntactic features (Â§2.3): syntactic features for attaching target word tl at position k to target word t at position j syntactic valence features with word t at position j having children I â {1, . . .	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	Features that consider only target-side syntax and words without considering s can be seen as âsyntactic language modelâ features (Shen et al., 2008).	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	That is, p(t, Ït, a | s, Ïs) = exp{Î¸Tg(s, Ïs, a, t, Ït)} plest are word-to-word features, estimated as the conditional probabilities p(t | s) and p(s | t) for s â Î£ and t â T. Phrase-to-phrase features generalize these, estimated as p(tl | sl) and p(sl | tl) where sl (respectively, tl) is a substring of s (t).	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	For models without syntactic features, we constrained the decoder to produce dependency trees in which every wordâs parent is immediately to its right and ignored syntactic features while scoring structures.	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	In this work, we focus on syntactic features of target-side dependency trees, Ït, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features.	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into ï¿½al,tl,Ï l exp{Î¸Tg(s, Ïs, al, tl, Ï l)} (2) disjoint parts of the source and target sentences t t 4 There are two conventional definitions of feature func-.	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	There have been many features proposed that consider source- and target-language syntax during translation.	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	Here we take first steps toward such a âuniversalâ decoder, making the following contributions:Arbitrary feature model (Â§2): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.	0
(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.	Figure 1 gives an example, showing a German sentence and dependency tree from an automatic parser, an English reference, and a lattice repre 7 Arguably, we seek argmax.	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.	1
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	Decoding as QG parsing (Â§3â4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary ânon- localâ features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (Â§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	Feature-Rich Translation by Quasi-Synchronous Lattice Parsing	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in Ït (or a deliberate choice is made by the decoder to translate it to NULL).	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, Ït, a | s, Ïs).	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	The lattice is a weighted âsausageâ lattice that permits sentences up to some maximum length Â£; Â£ is derived from the source sentence length.	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	We presented feature-rich MT using a principled probabilistic framework that separates features from inference.	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible.	0
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	Here we take first steps toward such a âuniversalâ decoder, making the following contributions:Arbitrary feature model (Â§2): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.	1
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	4.1 Translation as Monolingual Parsing.	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	Given a source sentence s and its parse Ïs, a QDG induces a probabilistic monolingual dependency grammar over sentences âinspiredâ by the source sentence and tree.	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	selected at each position and a dependency tree over them.	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	Feature-Rich Translation by Quasi-Synchronous Lattice Parsing	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism.	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	There have been many features proposed that consider source- and target-language syntax during translation.	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	Given the lattice and Gs,Ïs , lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms Ït , and the alignments aâ that are most probable, (Eisner, 1997).	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993).	0
Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.	â¢ A counter of uncovered source words: of the source sentence during translation: all parts f sunc (a) = ï¿½n Î´(|aâ1(i)|, 0).	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	Here we take first steps toward such a âuniversalâ decoder, making the following contributions:Arbitrary feature model (Â§2): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.	1
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	Feature-Rich Translation by Quasi-Synchronous Lattice Parsing	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this work that s is parsed.	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993).	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	We presented feature-rich MT using a principled probabilistic framework that separates features from inference.	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	Our first set of experiments compares feature sets commonly used in phrase- and syntax-based trans when using a very small k, due to their reliance on non-local language model and phrase features.	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	We find large gains in BLEU by adding more features, and find that gains obtained through phrase features and syntactic features are partially additive, suggesting that these feature sets are making complementary contributions to translation quality.	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	Figure 2: Comparison of size of k-best list for cube decoding with various feature sets.	0
Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).	Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism.	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab.	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible.	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	Features that consider only target-side syntax and words without considering s can be seen as âsyntactic language modelâ features (Shen et al., 2008).	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	11â13: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008).	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	Given a sentence s and its parse Ïs, at decoding time we seek the target sentence tâ, the target tree For a QDG model, the decoding problem has not been addressed before.	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	There have been many features proposed that consider source- and target-language syntax during translation.	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	form (t (i) , Ï (i), s (i) , Ï (i)), for i = 1, ..., T , max The solution is to introduce a set of coverageimum likelihood estimation for this model con 9 features gcov (a).	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research.	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	In particular, we compare the effects of combining phrase features and syntactic features.	0
On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).	We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219â228, Singapore, 67 August 2009.	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.	1
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	This causes decoding to proceed left- to-right in the lattice, the way phrase-based decoders operate.	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	Given a sentence s and its parse Ïs, at decoding time we seek the target sentence tâ, the target tree For a QDG model, the decoding problem has not been addressed before.	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	There have been many features proposed that consider source- and target-language syntax during translation.	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice.	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	6.5 Varying k During Decoding.	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	We have validated cube summing and decoding as practical methods for approximate inference.	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	Given a sentence s and its parse tree Ïs, we formulate the translation on the feasibility of inference, including decoding.	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001).	0
Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.	The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist.	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	Gibbs sampling for the fertility IBM Model 1 is similar but simpler.	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	In the testing stage, the sampling algorithm is the same as above except that we keep the alignments 1 that maximize P (a1 , f1 |e2I +1).	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	This Gibbs sampling method updates parameters constantly, so it is an âonline learningâ algorithm.	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic.	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |ajâ1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.	0
Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.	We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596â605, MIT, Massachusetts, USA, 911 October 2010.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	Qc 2010 Association for Computational Linguistics estimation.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	This work was supported by NSF grants IIS0546554 and IIS0910611.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	Our work is different from others in essential ways.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.	We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, Ïi = j=1 Î´(aj , i) which has nice probabilistic guarantees.	1
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596â605, MIT, Massachusetts, USA, 911 October 2010.	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003).	0
, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).	Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |ajâ1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	Following Brown et al.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	Therefore, we assume that ÏÇ« follows a Poisson distribution with parameter I Î»(Ç«).	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	This work was supported by NSF grants IIS0546554 and IIS0910611.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	Our work is different from others in essential ways.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.	We choose t = 1, 5, and 30 for the fertility HMM.	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	We solve the problem in the following way: estimate the parameter Î»(enon empty ) for all nonempty words, all infrequent words share this parameter.	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	One may try to solve it by forcing all these words to share a same parameter Î»(einfrequent).	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Therefore, we assume that ÏÇ« follows a Poisson distribution with parameter I Î»(Ç«).	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Words from position I + 1 to 2I + 1 in the target sentence are all empty words.	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	We consider words that appear less than 10 times as infrequent words.	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter Î»(e).	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	where Î´ is the Kronecker delta function: ( 1 if x = y Î´(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility.	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Our work is different from others in essential ways.	0
Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	, eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	This work was supported by NSF grants IIS0546554 and IIS0910611.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	Our work is different from others in essential ways.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	(1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	For the fertility IBM Model 1, we do not need to estimate the distortion probability.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	Gibbs sampling for the fertility IBM Model 1 is similar but simpler.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	Initially, the fertility IBM Model 1 and fertility HMM did not perform well.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.	0
Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.	We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	Results are shown in Table 2; we see that better word alignment results do not lead to better translations.	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	Model BLEU HMM 19.55 HMMF30 19.26 IBM4 18.77 Table 2: BLEU results	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	This is the same complexity as the HMM if t is O(I ), and it has lower complexity if t is a constant.	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . .	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003).	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	Table 1, Figure 1, and Figure 2 shows the AER results for different models.	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model.	0
Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.	Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	IBM Model 1 and the HMM are both generative models, and both start by defining the probability of alignments and source sentence given the In order to compute the âjump probabilityâ in the target sentence: P (aJJ 1 ); the data likeli HMM model, we need to know the position of the 1 , f1 |e2I +1 hood can be computed by summing over alignments: aligned target word for the previous source word.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |ajâ1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	After we add I + 1 empty words to the target sentence, the alignment is a mapping from source to target word positions: a : j â i, i = aj where j = 1, 2, . . .	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	Without loss of generality, P (aJ , f J |e2I +1) can bility of jumping to an empty word is either 0 or p0, and the third equation implies that the probability of jumping from a nonempty word is the same as the probability of jumping from the corespondent empty 1 1 1 be decomposed into length probabilities, distortion probabilities (also called alignment probabilities), and lexical probabilities (also called translation probabilities): P (aJ , f J |e2I +1) 1 1 1 J word.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	We evaluated our model by computing the word alignment and machine translation quality.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	between f J and eI . When a word fj is not aligned 1 1 with any word e, aj is 0.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.	0
For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).	There have been many years of research on word alignment.	0
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596â605, MIT, Massachusetts, USA, 911 October 2010.	1
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.	1
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.	0
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.	0
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase).	0
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	Gibbs sampling for the fertility IBM Model 1 is similar but simpler.	0
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	Initially, the fertility IBM Model 1 and fertility HMM did not perform well.	0
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.	0
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1.	0
Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.	We need more the HMM do in the EM algorithms.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	Surprisingly, we can achieve better results than the HMM by computing as few as 1 sample for each alignment, so the fertility hidden Markov model is much faster than the HMM.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003).	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	Initially, the fertility IBM Model 1 and fertility HMM did not perform well.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM.	0
Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.	In this case, the fertility hidden Markov model is not faster than the HMM.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	This work was supported by NSF grants IIS0546554 and IIS0910611.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	Our work is different from others in essential ways.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	Initially, the fertility IBM Model 1 and fertility HMM did not perform well.	0
Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.	We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |ajâ1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	Following Brown et al.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	This work was supported by NSF grants IIS0546554 and IIS0910611.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	Our work is different from others in essential ways.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	We choose t = 1, 5, and 30 for the fertility HMM.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	We solve the problem in the following way: estimate the parameter Î»(enon empty ) for all nonempty words, all infrequent words share this parameter.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.	0
Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.	We developed a fertility hidden Markov model that runs faster and has lower AER than the HMM.	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, Ïi = j=1 Î´(aj , i) which has nice probabilistic guarantees.	1
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	For the fertility IBM Model 1, we do not need to estimate the distortion probability.	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	The absolute position in the HMM is not important, because we re-parametrize the distortion probability in terms of the distance between adjacent alignment points (Vogel et al., 1996; Och and Ney, 2003): = P (J |e2I +1) n P (aj , fj |f jâ1, ajâ1, e2I +1) c(i â iâ²) 1 j=1 1 1 1 P (i|iâ², I ) = "Â£ iâ²â² c(iâ²â² â iâ²) J = P (J |e2I +1) n P (aj |f jâ1, ajâ1, e2I +1) Ã where c( ) is the count of jumps of a given distance.	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (ÏI , ÏÇ«, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I Ïi 1 Î»(ei ) summing over fertilities and alignments: n Î»(ei) eâ Ã P (f J |e2I +1) = "Â£ I J P (ÏI , ÏÇ«, aJ , f J |e2I +1).	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	Now P (ÏI , ÏÇ«, aJ , f J |e2I +1) can be decomposed 1 (2I + 1)J n P (fj | j=1 eaj ) (1) 1 1 1 1 in the following way: P (ÏI , ÏÇ«, aJ , f J |e2I +1) In the fertility HMM, we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence, and that 1 1 1 1 = P (ÏI |e2I +1)P (ÏÇ«|ÏI , e2I +1) Ã 1 1 1 1 J the lexical probability depends only on the aligned target word: n P (aj , fj |f jâ1, ajâ1, e2I +1, ÏI , ÏÇ«) j=1 1 1 1 1 P (ÏI , ÏÇ«, aJ , f J |e2I +1) = n Î»(ei) eâÎ»(ei ) 1 1 1 I Ï 1 Î»(e ) Ïi! Ã = n Î»(ei) i eâ i i=1 (I Î»(Ç«))ÏÇ« eâI Î»(Ç«) ÏÇ«!	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), Î»(e), Î¾1(e) , Î¾2(a )) 1 1 1 = P (ÏI , ÏÇ«, aJ , f J |e2I +1) = PË â² aJ e 2I +1, f J ) log â² P (aJ , f J | e2I +1) 1 1 ,ÏÇ« 1 1 1 1 1 1 J 1 1 1 1 â P (ÏI , ÏÇ«, aJ , f J |e2I +1) Ã â Î¾1(e)( P (f |e) â 1) 1 1 1 1 I ï£« J ï£¶ e f n Î´ ï£­ i=1 j=1 Î´(aj , i), Ïiï£¸ Ã â Î¾2(aâ²)( aâ² a P (a|aâ²) â 1) ï£« 2I +1 J ï£¶ Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 Î´ ï£­ i=I +1 j=1 Î´(aj , i), ÏÇ«ï£¸ (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, ÏÇ« and each P (f |e) = "Â£s c (f |e; f (s), e(s)) (4) Ïi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.	0
I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).	Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	We solve the problem in the following way: estimate the parameter Î»(enon empty ) for all nonempty words, all infrequent words share this parameter.	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	One may try to solve it by forcing all these words to share a same parameter Î»(einfrequent).	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Words from position I + 1 to 2I + 1 in the target sentence are all empty words.	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Therefore, we assume that ÏÇ« follows a Poisson distribution with parameter I Î»(Ç«).	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	We consider words that appear less than 10 times as infrequent words.	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter Î»(e).	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	where Î´ is the Kronecker delta function: ( 1 if x = y Î´(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility.	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Our work is different from others in essential ways.	0
Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).	Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	, eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	This work was supported by NSF grants IIS0546554 and IIS0910611.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	Our work is different from others in essential ways.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.	0
The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).	(1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	It is similar in some ways to IBM Model 4, but is much easier to understand.	1
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596â605, MIT, Massachusetts, USA, 911 October 2010.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	Most models have limited ability to model fertility.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	Our model is also easier to understand than IBM Model 4.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	, aJ the alignments one reason that our model is easier to understand.	0
The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).	It is easier to understand than IBM Model 4 (see Section 3).	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	Gibbs sampling for the fertility IBM Model 1 is similar but simpler.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	This Gibbs sampling method updates parameters constantly, so it is an âonline learningâ algorithm.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596â605, MIT, Massachusetts, USA, 911 October 2010.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.	0
Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.	Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993).	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	We report results for the best and median hyperparameter settings obtained in this way.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	Specifically, for both settings we report results on the median run for each setting.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	Our model outperforms theirs on four out of five languages on the best hyperparameter setting and three out of five on the median setting, yielding an average absolute difference across languages of 12.9% and 3.9% for best and median settings respectively compared to their best EM or LBFGS performance.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	Across all languages, +PRIOR consistently outperforms 1TW, reducing error on average by 9.1% and 5.9% on best and median settings respectively.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	(2010) reports the best unsupervised results for English.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	The difference between the featureless model (+PRIOR) and our full model (+FEATS) is 13.6% and 7.7% average error reduction on best and median settings respectively.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	Overall, the difference between our most basic model (1TW) and our full model (+FEATS) is 21.2% and 13.1% for the best and median settings respectively.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings.	0
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	Their best model yields 44.5% one-to-one accuracy, compared to our best median 56.5% result.	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.	1
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type.	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus.	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags.	0
This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.	We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings.	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.	1
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	(2010) reports the best unsupervised results for English.	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	In contrast to results reported in Johnson (2007), we found that the per P (Ti|T âi, Î²) n (f,v)âWi P (v|Ti, f, W âi, T âi, Î²) formance of our Gibbs sampler on the basic 1TW model stabilized very quickly after about 10 full it All of the probabilities on the right-hand-side are Dirichlet, distributions which can be computed analytically given counts.	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	(2009) also report results on English, but on the reduced 17 tag set, which is not comparable to ours).	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	This assumption, however, is not inherent to type-based tagging models.	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity.	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	The P (T |Ï) distribution, in English for instance, should have very low mass for the DT (determiner) tag, since determiners are a very small portion of the vocabulary.	0
More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.	This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model.	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	(2009) also report results on English, but on the reduced 17 tag set, which is not comparable to ours).	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	In our work, we demonstrate that using a simple naÂ¨Ä±veBayes approach also yields substantial performance gains, without the associated training complexity.	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	On several languages, we report performance exceeding that of state-of-the art systems.	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	On several languages, we report performance exceeding that of more complex state-of-the art systems.1	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	encodes the one tag per word constraint and is uni form over type-level tag assignments.	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	Even without features, but still using the tag prior, our median result is 52.0%, still significantly outperforming GracÂ¸a et al.	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.	0
As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.	A promising direction for future work is to explicitly model a distribution over tags for each word type.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	Parameter Component As in the standard Bayesian HMM (Goldwater and Griffiths, 2007), all distributions are independently drawn from symmetric Dirichlet distributions: 2 Note that t and w denote tag and word sequences respectively, rather than individual tokens or tags.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	While possible to utilize the feature-based log-linear approach described in Berg-Kirkpatrick et al.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	Specifically, we assume each word type W consists of feature-value pairs (f, v).	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	(2010), we adopt a simpler naÂ¨Ä±ve Bayes strategy, where all features are emitted independently.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	Statistics for all data sets are shown in Table 2.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	Across all languages, high performance can be attained by selecting a single tag per word type.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	Î² is the shared hyperparameter for the tag assignment prior and word feature multinomials.	0
2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).	In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.	0
Following Lee et al.(2010) we used only the training sections for each language.	Following the setup of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English.	0
Following Lee et al.(2010) we used only the training sections for each language.	We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, evaluate three variants: The first model (1TW) only 4 Typically, the performance stabilizes after only 10 itera-.	0
Following Lee et al.(2010) we used only the training sections for each language.	On each language we investigate the contribution of each component of our model.	0
Following Lee et al.(2010) we used only the training sections for each language.	(2010)âs richest model: optimized via either EM or LBFGS, as their relative performance depends on the language.	0
Following Lee et al.(2010) we used only the training sections for each language.	For each language and setting, we report one-to-one (11) and many- to-one (m-1) accuracies.	0
Following Lee et al.(2010) we used only the training sections for each language.	We can only compare with GracÂ¸a et al.	0
Following Lee et al.(2010) we used only the training sections for each language.	Its only purpose is 3 This follows since each Î¸t has St â 1 parameters and.	0
Following Lee et al.(2010) we used only the training sections for each language.	We train and test on the CoNLL-X training set.	0
Following Lee et al.(2010) we used only the training sections for each language.	During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, Î±, Î²) â P (T , t, W , w|Î±, Î²) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, Ï, Î¸, Ï, w|Î±, Î²)dÏdÎ¸dÏ Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior.	0
Following Lee et al.(2010) we used only the training sections for each language.	We refer to (T , W ) as the lexicon of a language and Ï for the parameters for their generation; Ï depends on a single hyperparameter Î².	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.	1
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus.	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	5.1 Data Sets.	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits âone tag per discourseâ sparsity â words are likely to select a single predominant tag in a corpus, even when several tags are possible.	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	Statistics for all data sets are shown in Table 2.	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags.	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	With the exception of the Dutch data set, no other processing is performed on the annotated tags.	0
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type.	0
vised POS induction algorithm (Lee et al., 2010)	We consider the unsupervised POS induction problem without the use of a tagging dictionary.	1
vised POS induction algorithm (Lee et al., 2010)	Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process.	0
vised POS induction algorithm (Lee et al., 2010)	Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).	0
vised POS induction algorithm (Lee et al., 2010)	In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (MeÂ´rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; GracÂ¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
vised POS induction algorithm (Lee et al., 2010)	This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007).	0
vised POS induction algorithm (Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
vised POS induction algorithm (Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (GracÂ¸a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.	0
vised POS induction algorithm (Lee et al., 2010)	We tokenize MWUs and their POS tags; this reduces the tag set size to 12.	0
vised POS induction algorithm (Lee et al., 2010)	On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).	0
vised POS induction algorithm (Lee et al., 2010)	Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus.	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	We consider the unsupervised POS induction problem without the use of a tagging dictionary.	1
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Recent work has made significant progress on unsupervised POS tagging (MeÂ´rialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson,2007; Goldwater and Griffiths, 2007; Gao and John son, 2008; Ravi and Knight, 2009).	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (MeÂ´rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; GracÂ¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Simple Type-Level Unsupervised POS Tagging	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process.	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, the lexicon is generated as: P (T , W |Ï) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.	0
Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Even without features, but still using the tag prior, our median result is 52.0%, still significantly outperforming GracÂ¸a et al.	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	For all languages we do not make use of a tagging dictionary.	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, the lexicon is generated as: P (T , W |Ï) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	In this paper, we make a simplifying assumption of one-tag-per-word.	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	The final model tions.	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Conditioned on T , features of word types W are drawn.	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	We evaluate our model on seven languages exhibiting substantial syntactic variation.	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, our full model takes advantage of word features not present in GracÂ¸a et al.	0
Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, in existing systems, this expansion come with a steep increase in model complexity.	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	We consider the unsupervised POS induction problem without the use of a tagging dictionary.	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (MeÂ´rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; GracÂ¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, the lexicon is generated as: P (T , W |Ï) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	For all languages we do not make use of a tagging dictionary.	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	On several languages, we report performance exceeding that of state-of-the art systems.	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	On several languages, we report performance exceeding that of more complex state-of-the art systems.1	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Simple Type-Level Unsupervised POS Tagging	0
Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007).	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	encodes the one tag per word constraint and is uni form over type-level tag assignments.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	to explore how well we can induce POS tags using only the one-tag-per-word constraint.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	As is standard, we report the greedy one-to-one (Haghighi and Klein, 2006) and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	In contrast to results reported in Johnson (2007), we found that the per P (Ti|T âi, Î²) n (f,v)âWi P (v|Ti, f, W âi, T âi, Î²) formance of our Gibbs sampler on the basic 1TW model stabilized very quickly after about 10 full it All of the probabilities on the right-hand-side are Dirichlet, distributions which can be computed analytically given counts.	0
Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)â€™s one-class HMM.	In contrast to the Bayesian HMM, Î¸t is not drawn from a distribution which has support for each of the n word types.	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process.	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	The resulting model is compact, efficiently learnable and linguistically expressive.	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	For this experiment, we compare our model with the uniform tag assignment prior (1TW) with the learned prior (+PRIOR).	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	(2010)âs richest model: optimized via either EM or LBFGS, as their relative performance depends on the language.	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	9 66.4 47.	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	8 66.4 52.	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness.	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	Hyperparameters Our model has two Dirichlet concentration hyperparameters: Î± is the shared hyperparameter for the token-level HMM emission and transition distributions.	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see Section 3).	0
It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%	(2009), who also incorporate a sparsity constraint, but does via altering the model objective using posterior regularization.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.	1
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	â similar results have been observed across multiple languages.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits âone tag per discourseâ sparsity â words are likely to select a single predominant tag in a corpus, even when several tags are possible.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Similar behavior is observed when adding features.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	In contrast to these approaches, our method directly incorporates these constraints into the structure of the model.	0
Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	The token-level term is similar to the standard HMM sampling equations found in Johnson (2007).	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	Conditioned on T , features of word types W are drawn.	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	Once HMM parameters (Î¸, Ï) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from Ï.	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters Î¸.	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	P St = n. Î² T VARIABLES Ï Y W : Word types (W1 ,.	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	Specifically, the lexicon is generated as: P (T , W |Ï) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then Î¸t is drawn from DIRICHLET(Î±, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4).	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure.	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	to represent the ith word type emitted by the HMM: P (t(i)|Ti, t(âi), w, Î±) â n P (w|Ti, t(âi), w(âi), Î±) (tb ,ta ) P (Ti, t(i)|T , W , t(âi), w, Î±, Î²) = P (T |tb, t(âi), Î±)P (ta|T , t(âi), Î±) âi (i) i i (âi) P (Ti|W , T âi, Î²)P (t |Ti, t , w, Î±) All terms are Dirichlet distributions and parameters can be analytically computed from counts in t(âi)where T âi denotes all type-level tag assignment ex cept Ti and t(âi) denotes all token-level tags except and w (âi) (Johnson, 2007).	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	In contrast to the Bayesian HMM, Î¸t is not drawn from a distribution which has support for each of the n word types.	0
Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).	The corresponding token words w are drawn conditioned on t and Î¸.2 Our full generative model is given by: K P (Ï, Î¸|T , Î±, Î²) = n (P (Ït|Î±)P (Î¸t|T , Î±)) t=1 The transition distribution Ït for each tag t is drawn according to DIRICHLET(Î±, K ), where Î± is the shared transition and emission distribution hyperparameter.	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.	1
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	Recent work has made significant progress on unsupervised POS tagging (MeÂ´rialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson,2007; Goldwater and Griffiths, 2007; Gao and John son, 2008; Ravi and Knight, 2009).	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity.	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy.	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	Second, the reduced number of hidden variables and parameters dramatically speeds up learning and inference.	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	(2010) reports the best unsupervised results for English.	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process.	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus.	0
Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.	Simple Type-Level Unsupervised POS Tagging	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus.	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998).	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words.	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	Discovering Corpus-Specific Word Senses	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	This approach to disambiguation combines the benefits of both Yarowsky's (1995) and Schtitze's (1998) approaches.	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	We used the simple graph model based on co-occurrences of nouns in lists (cf.	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora.	0
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph.	0
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).	1
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	1
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	We used the simple graph model based on co-occurrences of nouns in lists (cf.	0
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. "genomic DNA from rat, mouse and dog".	0
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted.	0
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph.	0
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	In contrast to pure Markov clustering, we don't try to find a complete clustering of G into senses at once.	0
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	The algorithm is based on a graph model representing words and relationships between them.	0
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.	0
Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.	The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the "best" cluster, it suffices to build a relatively small graph in 1.	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	1
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	The algorithm is based on a graph model representing words and relationships between them.	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words.	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998).	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	Let G, denote the local graph around the ambiguous word w. The adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph of the word wing of a graph G, is defined by setting (111G) pq equal to the weight of the edge between nodes v and v q . Normalizing the columns of A/G results in the Markov Matrix Taw whose entries (Thi,)pq can be interpreted as transition probability from v q to vv . It can easily be shown that the k-th power of TG lists the probabilities (TL )pq of a path of length k starting at node vq and ending at node V. The MCL-algorithm simulates flow in Gw by iteratively recomputing the set of transition probabilities via two steps, expansion and inflation.	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.	0
The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times.Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.	The output of the MCL-algorithm strongly depends on the inflation and expansion parameters r and k as well as the size of the local graph which serves as input to MCL.	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	1
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	We used the simple graph model based on co-occurrences of nouns in lists (cf.	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted.	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	Our algorithm was applied to each word in the list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) in order to extract the top two sense clusters only.	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. "genomic DNA from rat, mouse and dog".	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the "best" cluster, it suffices to build a relatively small graph in 1.	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	The algorithm is based on a graph model representing words and relationships between them.	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	An extract of the results is listed in table 1.	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another.	0
Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.	The expansion step corresponds with taking the k-th power of TG as outlined above and allows nodes to see new neighbours.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	This paper presents an unsupervised algorithm which automatically discovers word senses from text.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	In section 2, we present the graph model from which we discover word senses.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	Discovering Corpus-Specific Word Senses	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	Take the "best" cluster (the one that is most strongly connected to w in Gw before removal of w), add it to the final list of clusters L and remove/devalue its features from F. 5.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.	0
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	Instead we link each word to its top n neighbors where n can be determined by the user (cf.	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning.	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	Discovering Corpus-Specific Word Senses	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998).	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	Automatic word sense discovery has applications of many kinds.	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora.	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters.	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	This paper presents an unsupervised algorithm which automatically discovers word senses from text.	0
This method, as the ones presented in (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.	In section 2, we present the graph model from which we discover word senses.	0
As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.	Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.	0
As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.	Therefore, even after removal of the wing-node, the two areas of meaning are still linked via tail.	0
As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.	However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning.	0
As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.	To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).	0
As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.	Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering (van Dongen, 2000).	0
As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.	Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse.	0
As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.	Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them.	0
As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland a merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output of word sense clustering.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	1
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	We will soon carry out and report on a more thorough analysis of our algorithm.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	section 2), and we plan to evaluate the uses of our clustering algorithm for unsupervised disambiguation more thoroughly.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	Our algorithm was applied to each word in the list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) in order to extract the top two sense clusters only.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	Here we only mention a few direct results of our work.	0
In our case, we chose a more general approach by working at the level of a simi­larity graph: when the similarity of two words is given by their relation of cooccurrence, our situa­tion is comparable to the one of (Véronis, 2003) and (Dorow and Widdows, 2003)	We prepare an evaluation of our algorithm as applied to the collocation relationships (cf.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	This paper presents an unsupervised algorithm which automatically discovers word senses from text.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	In section 2, we present the graph model from which we discover word senses.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters.	0
From a global viewpoint, these two differences lead (Véronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.	Therefore, even after removal of the wing-node, the two areas of meaning are still linked via tail.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	Instead we link each word to its top n neighbors where n can be determined by the user (cf.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	Take the "best" cluster (the one that is most strongly connected to w in Gw before removal of w), add it to the final list of clusters L and remove/devalue its features from F. 5.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	2. Recursively remove all nodes of degree one.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	Let G, denote the local graph around the ambiguous word w. The adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph of the word wing of a graph G, is defined by setting (111G) pq equal to the weight of the edge between nodes v and v q . Normalizing the columns of A/G results in the Markov Matrix Taw whose entries (Thi,)pq can be interpreted as transition probability from v q to vv . It can easily be shown that the k-th power of TG lists the probabilities (TL )pq of a path of length k starting at node vq and ending at node V. The MCL-algorithm simulates flow in Gw by iteratively recomputing the set of transition probabilities via two steps, expansion and inflation.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.	0
The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).	In section 2, we present the graph model from which we discover word senses.	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	The benefits of automatic, data-driven word sense discovery for natural language processing and lexicography would be very great.	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998).	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	Instead we link each word to its top n neighbors where n can be determined by the user (cf.	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	Automatic word sense discovery has applications of many kinds.	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	Discovering Corpus-Specific Word Senses	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	Discrimination against previously extracted sense clusters enables us to discover new senses.	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	This paper presents an unsupervised algorithm which automatically discovers word senses from text.	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	In section 2, we present the graph model from which we discover word senses.	0
This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).	This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated.	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering (van Dongen, 2000).	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	Instead we link each word to its top n neighbors where n can be determined by the user (cf.	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	2. Recursively remove all nodes of degree one.	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	Then remove the node corresponding with w from G. 3.	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. "genomic DNA from rat, mouse and dog".	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus.	0
We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.	However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.	1
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the "best" cluster, it suffices to build a relatively small graph in 1.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	An appropriate choice of the inflation param 80 eter r can depend on the ambiguous word w to be clustered.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	In section 2, we present the graph model from which we discover word senses.	0
Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.	It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (âx)(x â L â§ (ây)(y â L â§ (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to â and â§, and a kind of concatenative meaning to the predicate S(t1, t2).	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	As the source code is available, collaboration is encouraged.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	10 } Here, instead of calling the fsm regex() function to construct the machine from a regular expressions, we could instead have accessed the beforementioned low-level routines and built the network entirely without regular expressions by combining low-level primitives, as follows, replacing line 5 in the above: network = fsm_concat( fsm_kleene_plus( fsm_symbol("a")), fsm_kleene_plus( fsm_symbol("b"))); The API is currently under active development and future functionality is likely to include conversion of networks to 8-bit letter transducers/automata for maximum speed in regular expression matching and transduction.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	One of Fomaâs design goals has been compatibility with the Xerox/PARC toolkit.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	It has been successfully compiled on Linux, Mac OS X, and Win32 operating systems, and is likely to be portable to other systems without much effort.	0
Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.	Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Tâs fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).	1
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes.	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	Foma is largely compatible with the Xerox/PARC finite-state toolkit.	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	N/A N/A âignoresâ, left quotient, right quotient, âinsideâ quotient â â/ = /= N/A language membership, position equivalence âº < > precedes, follows â¨ âª â§ â© - .P. .p. | & â .P. .p. union, intersection, set minus, priority unions => -> (->) @-> => -> (->) @-> context restriction, replacement rules <> shuffle (asynchronous product) Ã â¦ .x. .o. cross-product, composition Table 1: The regular expressions available in Foma from highest to lower precedence.	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (âx)(x â L â§ (ây)(y â L â§ (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to â and â§, and a kind of concatenative meaning to the predicate S(t1, t2).	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	Foma: a finite-state compiler and library	0
The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).	The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata.	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.	1
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	Foma is free software and will remain under the GNU General Public License.	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	The compiler and library are implemented in C and an API is available.	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	The others are scripts that can be run on both Xerox/PARC and Foma.	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions.	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers.	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	For example, one can either say: ContainsX = Î£* X Î£*; MyWords = {cat}|{dog}|{mouse}; MyRule = n -> m || p; ShortWords = [MyLex1]1 â© Î£Ë<6; or: Proceedings of the EACL 2009 Demonstrations Session, pages 29â32, Athens, Greece, 3 April 2009.	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possibleânon-deterministic automata naturally being easier to inspect and analyze.	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	Foma: a finite-state compiler and library	0
The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.	For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (âx)(x â L â§ (ây)(y â L â§ (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to â and â§, and a kind of concatenative meaning to the predicate S(t1, t2).	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers.	1
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	10 } Here, instead of calling the fsm regex() function to construct the machine from a regular expressions, we could instead have accessed the beforementioned low-level routines and built the network entirely without regular expressions by combining low-level primitives, as follows, replacing line 5 in the above: network = fsm_concat( fsm_kleene_plus( fsm_symbol("a")), fsm_kleene_plus( fsm_symbol("b"))); The API is currently under active development and future functionality is likely to include conversion of networks to 8-bit letter transducers/automata for maximum speed in regular expression matching and transduction.	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (âx)(x â L â§ (ây)(y â L â§ (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to â and â§, and a kind of concatenative meaning to the predicate S(t1, t2).	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators.	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	The others are scripts that can be run on both Xerox/PARC and Foma.	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	The Foma API gives access to basic functions, such as constructing a finite-state machine from a regular expression provided as a string, performing a transduction, and exhaustively matching against a given string starting from every position.	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton.	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	For example, one can either say: ContainsX = Î£* X Î£*; MyWords = {cat}|{dog}|{mouse}; MyRule = n -> m || p; ShortWords = [MyLex1]1 â© Î£Ë<6; or: Proceedings of the EACL 2009 Demonstrations Session, pages 29â32, Athens, Greece, 3 April 2009.	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possibleânon-deterministic automata naturally being easier to inspect and analyze.	0
Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.	Foma: a finite-state compiler and library	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Tâs fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	The others are scripts that can be run on both Xerox/PARC and Foma.	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately.	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	Foma is largely compatible with the Xerox/PARC finite-state toolkit.	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	However, all the low-level functions that operate directly on automata/transducers are also available (some 50+ functions), including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms.	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	One of Fomaâs design goals has been compatibility with the Xerox/PARC toolkit.	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions.	0
Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.	For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	Foma is largely compatible with the Xerox/PARC finite-state toolkit.	1
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Tâs fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	Foma: a finite-state compiler and library	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata.	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	The compiler and library are implemented in C and an API is available.	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	N/A N/A âignoresâ, left quotient, right quotient, âinsideâ quotient â â/ = /= N/A language membership, position equivalence âº < > precedes, follows â¨ âª â§ â© - .P. .p. | & â .P. .p. union, intersection, set minus, priority unions => -> (->) @-> => -> (->) @-> context restriction, replacement rules <> shuffle (asynchronous product) Ã â¦ .x. .o. cross-product, composition Table 1: The regular expressions available in Foma from highest to lower precedence.	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions.	0
This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).	The following basic snippet illustrates how to use the C API instead of the main interface of Foma to construct a finite-state machine encoding the language a+b+ and check whether a string matches it: 1.	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	Foma is largely compatible with the Xerox/PARC finite-state toolkit.	1
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (âx)(x â L â§ (ây)(y â L â§ (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to â and â§, and a kind of concatenative meaning to the predicate S(t1, t2).	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	10 } Here, instead of calling the fsm regex() function to construct the machine from a regular expressions, we could instead have accessed the beforementioned low-level routines and built the network entirely without regular expressions by combining low-level primitives, as follows, replacing line 5 in the above: network = fsm_concat( fsm_kleene_plus( fsm_symbol("a")), fsm_kleene_plus( fsm_symbol("b"))); The API is currently under active development and future functionality is likely to include conversion of networks to 8-bit letter transducers/automata for maximum speed in regular expression matching and transduction.	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Tâs fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	One of Fomaâs design goals has been compatibility with the Xerox/PARC toolkit.	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	N/A N/A âignoresâ, left quotient, right quotient, âinsideâ quotient â â/ = /= N/A language membership, position equivalence âº < > precedes, follows â¨ âª â§ â© - .P. .p. | & â .P. .p. union, intersection, set minus, priority unions => -> (->) @-> => -> (->) @-> context restriction, replacement rules <> shuffle (asynchronous product) Ã â¦ .x. .o. cross-product, composition Table 1: The regular expressions available in Foma from highest to lower precedence.	0
In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).	GNU AT&T Foma xfst flex fsm 4 Î£âaÎ£15 0.216s 16.23s 17.17s 1.884s Î£âaÎ£20 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately.	1
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton.	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	Below is a simple example of the format: Multichar_Symbols +Pl +Sing LEXICON Root Nouns; LEXICON Nouns cat Plural; church Plural; LEXICON Plural +Pl:%Ës #; +Sing #;	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	For example, one can either say: ContainsX = Î£* X Î£*; MyWords = {cat}|{dog}|{mouse}; MyRule = n -> m || p; ShortWords = [MyLex1]1 â© Î£Ë<6; or: Proceedings of the EACL 2009 Demonstrations Session, pages 29â32, Athens, Greece, 3 April 2009.	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	It has specific support for many natural language processing applications such as producing morphological and phonological analyzers.	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	North Sami is a large lexicon (lexc file) for the North Sami language available from http://divvun.no.	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators.	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	The others are scripts that can be run on both Xerox/PARC and Foma.	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.	0
This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).	For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	1
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	3.4 Experiments with topic-sensitive LexRank.	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentenceto the input question via IDFweightedword overlap.	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	An intuitive interpretation of the stationary distribution can be under- 917 stood by the concept of a random walk on the graphrepresentation of the Markov chain.With probability d, a transition is made from the current node (sentence) to the nodes that are similar to the query.	0
Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).	Using Random Walks for Question-focused Sentence Retrieval	0
Its weight twij is calculated by tf · idf (Otterbacher et al., 2005).	We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap).	0
Its weight twij is calculated by tf · idf (Otterbacher et al., 2005).	Therefore,we examined properties of the questions within eachcluster in order to see what effect they might have onsystem performance.We hypothesized that the baseline system, which compares the similarity of each sentence to the question using IDF-weighted word overlap, should perform well on questions that provide many contentwords.	0
Its weight twij is calculated by tf · idf (Otterbacher et al., 2005).	(5:8) The storm was expected to hit with its full fury today, slamming intothe North Carolina coast with 105mph winds and 45-foot wave crests, before moving through Virginia and bashing thecapital with gusts of about 60 mph.	0
Its weight twij is calculated by tf · idf (Otterbacher et al., 2005).	For instance, if a sentence thatgets a high score in our baseline model is likely tocontain an answer to the question, then a related sentence, which may not be similar to the question itself, is also likely to contain an answer.This idea is captured by the following mixture model, where p(s|q), the score of a sentence s givena question q, is determined as the sum of its relevance to the question (using the same measure asthe baseline described above) and the similarity tothe other sentences in the document cluster: p(s|q) = d rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) where C is the set of all sentences in the cluster.	0
Its weight twij is calculated by tf · idf (Otterbacher et al., 2005).	Each systems extract file lists the document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train What is the condition under whichthreat GIA will take its action?Milan plane MSNBC, CNN, ABC, 9 15 train How many people were in thecrash Fox, USAToday building at the time of the crash?Turkish plane BBC, ABC, 10 12 train To where was the plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train How many people were killed inattack the most recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train Who was to blame forclub fire Fox, BBC, Ananova the fire?FBI most AFP, UPI 3 14 train How much is the State Department offeringwanted for information leading to bin Ladens arrest?Russia bombing AP, AFP 2 11 train What was the cause of the blast?Bali terror CNN, FoxNews, ABC, 10 30 train What were the motivationsattack BBC, Ananova of the attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train What kinds of equipment or weaponssniper BBC, Washington Times, CBS were used in the killings?GSPC terror Newstracker 8 29 train What are the charges againstgroup the GSPC suspects?China Novelty 43 25 18 train What was the magnitude of theearthquake earthquake in Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test How many people FoxNews, Washington Post were on board?David Beckham AFP 20 28 dev/test How long had Beckham been playing fortrade MU before he moved to RM?Miami airport Newstracker 12 15 dev/test How many concourses doesevacuation the airport have?US hurricane DUC d04a 14 14 test In which places had the hurricane landed?EgyptAir crash Novelty 4 25 29 test How many people were killed?Kursk submarine Novelty 33 25 30 test When did the Kursk sink?Hebrew University bombing Newstracker 11 27 test How many people were injured?Finland mall bombing Newstracker 9 15 test How many people were in the mall at the time of the bombing?Putin visits Newstracker 12 20 test What issue concerned BritishEngland human rights groups?	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	Newstracker clusters were collected automatically by our Web-based news summarization system.	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004).	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary.	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization.	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	(7:6) Figure 1: Question tracking interface to a summarization system.	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	In (Erkan and Radev, 2004), we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization.	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	As previously mentioned, the original LexRank method performed wellin the context of generic summarization.	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	To this end, we propose to use a stochastic, graph-based method.	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries.	0
includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).	While evaluations of question answeringsystems are often based on a shorter list of rankedsentences, we chose to generate longer lists for several reasons.	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	1
and sentence retrieval for question answering (Otterbacher et al., 2005).	Using Random Walks for Question-focused Sentence Retrieval	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	Since we are interested in a passage retrieval mechanism that findssentences relevant to a given question, providing input to the question answering component of our system, the improvement in average TRDR score isvery promising.	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	Currently, we address the question-focused sentence retrieval task.	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time.	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	Therefore, we are working towards developing a systemfor question answering from clusters of complex stories published over time.	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	To evaluate our sentence retrieval mechanism, weproduced extract files, which contain a list of sentences deemed to be relevant to the question, for thesystem and from human judgment.	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	In contrast to the classical question answering setting (e.g. TREC-style Q&A (Voorhees and Tice, 2000)), in which the userpresents a single question and the system returns acorresponding answer (or a set of likely answers), inthis case the user has a more complex informationneed.	0
and sentence retrieval for question answering (Otterbacher et al., 2005).	For each sentence and question pair in a givencluster, the judges were asked to indicate whetheror not the sentence contained a complete answerto the question.	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	1
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	3.4 Experiments with topic-sensitive LexRank.	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	This model hasproven to be successful in query-based sentence retrieval (Allan et al., 2003), and is used as our competitive baseline in this study (e.g. Tables 4, 5 and7).	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	This time, all four LexRank systems outperformed the baseline, both in terms of average MRRand TRDR scores.	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	To contrast, LexRank might perform better when the question provides fewer content words,since it considers both similarity to the query andinter-sentence similarity.	0
These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).	Table 4 shows the configurationsof LexRank that performed better than the baselinesystem on the training data, based on mean TRDRscores over the 184 training questions.	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences.	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	Using Random Walks for Question-focused Sentence Retrieval	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	3.2 Relevance to the question.	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing.	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively.	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap).	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	When using any metric to comparesentences and a query, there is always likely to bea tie between multiple sentences (or, similarly, theremay be cases where fewer than the number of desired sentences have similarity scores above zero).LexRank effectively provides a means to break suchties.	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	The specific problem we consider differsfrom the classic task of PR for a Q&A system ininteresting ways, due to the time-sensitive nature ofthe stories in our corpus.	0
Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.	For instance, if a sentence thatgets a high score in our baseline model is likely tocontain an answer to the question, then a related sentence, which may not be similar to the question itself, is also likely to contain an answer.This idea is captured by the following mixture model, where p(s|q), the score of a sentence s givena question q, is determined as the sum of its relevance to the question (using the same measure asthe baseline described above) and the similarity tothe other sentences in the document cluster: p(s|q) = d rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) where C is the set of all sentences in the cluster.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	We also considered several thresholdvalues for inter-sentence cosine similarities, wherewe ignored the similarities between the sentencesthat are below the threshold.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the inputquery, and not the similarity between the candidatesentences themselves, is likely to miss some important sentences.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	To apply LexRank, a similarity graph is producedfor the sentences in an input document set.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	An intuitive interpretation of the stationary distribution can be under- 917 stood by the concept of a random walk on the graphrepresentation of the Markov chain.With probability d, a transition is made from the current node (sentence) to the nodes that are similar to the query.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	When using any metric to comparesentences and a query, there is always likely to bea tie between multiple sentences (or, similarly, theremay be cases where fewer than the number of desired sentences have similarity scores above zero).LexRank effectively provides a means to break suchties.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	the same sentence score (similarity to the query), yetthe top ranking two sentences are not actually relevant according to the judges.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15.	0
A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.	This is intuitive because if the threshold is toohigh, such that only the most lexically similar sentences are represented in the graph, the method doesnot find sentences that are related but are more lex3A threshold of -1 means that no threshold was used suchthat all sentences were included in the graph.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	1
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	To apply LexRank, a similarity graph is producedfor the sentences in an input document set.	1
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	3.4 Experiments with topic-sensitive LexRank.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	As previously mentioned, the original LexRank method performed wellin the context of generic summarization.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).	In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	1
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	3.4 Experiments with topic-sensitive LexRank.	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentenceto the input question via IDFweightedword overlap.	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	3.1 The LexRank method.	0
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	Recent work has motivated the need for systemsthat support Information Synthesis tasks, in whicha user seeks a global understanding of a topic orstory (Amigo et al., 2004).	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	1
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	3.4 Experiments with topic-sensitive LexRank.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	To apply LexRank, a similarity graph is producedfor the sentences in an input document set.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	As previously mentioned, the original LexRank method performed wellin the context of generic summarization.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline.	0
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).	In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	1
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	3.4 Experiments with topic-sensitive LexRank.	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	The stationary distributionof a Markov chain can be computed by a simple iterative algorithm, called power method.1 A simpler version of Equation 5, where A is auniform matrix andB is a normalized binary matrix,is known as PageRank (Brin and Page, 1998; Pageet al., 1998) and used to rank the web pages by theGoogle search engine.	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentenceto the input question via IDFweightedword overlap.	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.	0
Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.	In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences.	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	The work described here also makes use of a hidden Markov model.	1
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a "hidden" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	The implementation of the hidden Markov model is based on that of Rabiner, Levinson and Sondhi (1983).	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	The statistical methods can be described in terms of Markov models.	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	Training a hidden Markov model having this topology corrected all nine instances of the error in the test data.	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions.	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	In practice, models have been limited to second-order, and smoothing methods are normally required to deal with the problem of estimation with limited data.	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.	0
This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989	The "augmented network" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}).	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	An alternative to uniformly increasing the order of the conditioning is to extend it selectively.	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	A straightforward method of extending the context is to use second-order conditioning which takes account of the previous two word categories.	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	The "augmented network" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}).	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	ADJECTIVE DETERMINER To all states NOUN in Basic Network "Transitions to  To all states all states in in Basic Network Basic Network except NOUN and ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING ALL STATES EXCEPT DETERMINER Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network.	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	In a first order model, Ci and Ci_l are random variables denoting the categories of the words at position i and (i - 1) in a text.	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	Mixed higher- order context can be modeled by introducing explicit state sequences.	0
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	In practice, models have been limited to second-order, and smoothing methods are normally required to deal with the problem of estimation with limited data.	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	The training was sentence-based, and the model was trained using 6,000 sentences from the corpus.	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	Increasing the order of the conditioning requires exponentially more parameters.	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	For an n category model this requires n 3 transition probabilities.	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	"Temperatures in the upper mantle range apparently from....".	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text.	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	A word sequence is considered as being generated from an underlying sequence of categories.	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	For example, 9 errors are from 3 instances of "... as well as ..." that arise in the text.	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	Methods have ranged from locally-operating rules (Greene and Rubin, 1971), to statistical methods (Church, 1989; DeRose, 1988; Garside, Leech and Sampson, 1987; Jelinek, 1985) and back-propagation (Benello, Mackie and Anderson, 1989; Nakamura and Shikano, 1989).	0
adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].	This has the disadvantage that they cannot share training data.	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.	1
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	The determination of part-of-speech categories for words is an important problem in language modeling, because both the syntactic and semantic roles of words depend on their part-of-speech category (henceforth simply termed "category").	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	In the document, 142 words were tagged as unknown (their possible categories were not known).	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	2.	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	As in the previous section, the corrections are not programmed into the model.	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	As an example, we may consider a systematic error made by the basic model.	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a "hidden" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	The performance of a tagging program depends on the choice and number of categories used, and the correct tag assignment for words is not always obvious.	0
We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)	For example, 9 errors are from 3 instances of "... as well as ..." that arise in the text.	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	Mixed higher- order context can be modeled by introducing explicit state sequences.	1
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	Modeling Dependencies across Phrases Linguistic considerations can be used to correct errors made by the model.	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	The "augmented network" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}).	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	The model has the advantage that a pre-tagged training corpus is not required.	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	To model such dependency across the phrase, the networks shown in Figure 2 can be used.	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	The tagger could be extended by further category refinements (e.g. inclusion of a gerund category), and the single pronoun category currently causes erroneous tags for adjacent words.	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	ADJECTIVE DETERMINER To all states NOUN in Basic Network "Transitions to  To all states all states in in Basic Network Basic Network except NOUN and ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING ALL STATES EXCEPT DETERMINER Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network.	0
Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.	This has the disadvantage that they cannot share training data.	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	In this regard, word equivalence classes were used (Kupiec, 1989).	1
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner.	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	However they are both members of the equivalence class noun-or-verb, and so are considered to behave identically.	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	A word at position i is represented by the random variable Wi, which ranges over the vocabulary {w~ ...wv} (v is the number of words in the vocabulary).	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	Whenever a transition is made to a state, the state-dependent probability distribution P(Eqvi I Ci) is used to obtain the probability of the observed equivalence class.	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	A word sequence is considered as being generated from an underlying sequence of categories.	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	Thus the words "play" and "touch" are considered to behave identically, as members of the class noun-or-verb, and "clay" and "zinc"are members of the class noun.	0
The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).	The implementation of the hidden Markov model is based on that of Rabiner, Levinson and Sondhi (1983).	0
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.	1
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.	1
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text.	0
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	The tagger could be extended by further category refinements (e.g. inclusion of a gerund category), and the single pronoun category currently causes erroneous tags for adjacent words.	0
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a "hidden" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.	0
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	This is not surprising, because "to" is the only member of the to-inf category, P(Wi = "to" [ Ci = to-in]) = 1.0.	0
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	For instance, the word "dog" can be seen in the states noun and verb, and only has a nonzero probability in those states.	0
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.	0
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	It can be seen that only simple forms of prepositional phrase are modeled in the networks; a single noun may be optionally preceded by a single adjective and/or determiner.	0
In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.	Only context has been supplied to aid the training procedure, and the latter is responsible for deciding which alternative is more likely, based on the training data.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	The model has the advantage that a pre-tagged training corpus is not required.	1
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	For example, 9 errors are from 3 instances of "... as well as ..." that arise in the text.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	The basic model tagged these sentences correctly, except for- "range" and "rises" which were tagged as noun and plural-noun respectively 1.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimate the model parameters.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	In a first order model, Ci and Ci_l are random variables denoting the categories of the words at position i and (i - 1) in a text.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	For an n category model this requires n 3 transition probabilities.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	The initial values of the model parameters are calculated from word occurrence probabilities, such that words are initially assumed to function equally probably as any of their possible categories.	0
The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.	Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.	1
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	The determination of part-of-speech categories for words is an important problem in language modeling, because both the syntactic and semantic roles of words depend on their part-of-speech category (henceforth simply termed "category").	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	For example, 9 errors are from 3 instances of "... as well as ..." that arise in the text.	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	It minimizes the resources required for high performance automatic tagging.	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	These are exemplified by the following sentence fragments: 1.	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	Application areas include speech recognition/synthesis and information retrieval.	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	Of all the possible category sequences from which a given word sequence can be generated, the one which maximizes the probability of the words is used.	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	there are two noun states, and two adjective states: one of each in the augmented network, and in the basic network).	0
One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].	The basic network cannot model the dependency of the number of the verb on its subject, which precedes it by a prepositional phrase.	0
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.	1
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	In this regard, word equivalence classes were used (Kupiec, 1989).	1
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.	0
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.	0
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model.	0
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary.	0
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	The replacement of the auxiliary category by the following categories greatly improved this: Category Name Words included in Category Be be Been been Being being Have have Have* has, have, had, having be* is, am, are, was, were do* do, does, did modal Modal auxiliaries Unique Equivalence Classes for Common Words Common words occur often enough to be estimated reliably.	0
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	Equivalence classes {Eqvl ...Eqvm} replace the words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci).	0
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	Editing the Transition Structure A common error in the basic model was the assignment of the word "to" to the to-infcategory ("to" acting as an infinitive marker) instead of preposition before noun phrases.	0
Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)	A word at position i is represented by the random variable Wi, which ranges over the vocabulary {w~ ...wv} (v is the number of words in the vocabulary).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	1
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	In IE, creating the patterns which express the requested scenario, e.g. âmanagement successionâ or âcorporate merger and acquisitionâ is regarded as the hardest task.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	This research was supported in part by the Defense Advanced Research Projects Agency as part of the Translingual Information Detection, Extraction and Summarization (TIDES) program, under Grant N66001001-18917 from the Space and Naval Warfare Systems Center, San Diego, and by the National Science Foundation under Grant IIS00325657.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Hereafter, each pair of NE categories will be called a domain; e.g. the âCompany â Companyâ domain, which we will call CC- domain (Step 2).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Out of those 15 links, 4 are errors, namely âbuy - payâ, âacquire - payâ, âpurchase - stakeâ âacquisition - stakeâ.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	For example, in Information Retrieval (IR), we have to match a userâs query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâs question even if the formulation of the answer in the document is different from the question.	1
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	If two phrases can be used to express the same relationship within an information extraction application (âscenarioâ), these two phrases are paraphrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	If the expression is longer or complicated (like âA buys Bâ and âAâs purchase of Bâ), it is called âparaphraseâ, i.e. a set of phrases which express the same thing or event.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	One possibility is to use n-grams based on mutual information.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Using structural information As was explained in the results section, we extracted examples like âSmith estimates Lotusâ, from a sentence like âMr.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	So, there is a limitation that IE can only be performed for a predefined task, like âcorporate mergersâ or âmanagement successionâ.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Automatic paraphrase discovery is an important but challenging task.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Limitations There are several limitations in the methods.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	[Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link âIBMâ and âLotusâ) (Step 4).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	It is natural that the larger the data in the domain, the more keywords are found.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	1
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	There have been other kinds of efforts to discover paraphrase automatically from corpora.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	This limits the number of NE category pairs to 2,000 and the number of NE pair instances to 0.63 million.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	It is not easy to make a clear definition of âparaphraseâ.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	The results, along with the total number of phrases, are shown in Table 1.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Automatic paraphrase discovery is an important but challenging task.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	All the sentences have been analyzed by our chunker and NE tag- ger.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	There have been other kinds of efforts to discover paraphrase automatically from corpora.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Limitations There are several limitations in the methods.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	These results are promising and there are several avenues for improving on these results.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].	1
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	This can be repeated several times to collect a list of author / book title pairs and expressions.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Link phrases based on instance pairs Using NE instance pairs as a clue, we find links between sets of phrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	In total, for the 2,000 NE category pairs, 5,184 keywords are found.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	They cluster NE instance pairs based on the words in the contexts using a bag- of-words method.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	The similar explanation applies to the link to the âstakeâ set.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	For each pair we also record the context, i.e. the phrase between the two NEs (Step1).	1
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	The data is sorted based on the frequency of the context (âa unit ofâ appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. âNBCâ and âGeneral Electric Co.â appeared 10 times with the context âa unit ofâ).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	First, from a large corpus, we extract all the NE instance pairs.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, we donât know how many such paraphrase sets are necessary to cover even some everyday things or events.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	As was explained in the results section, âstrengthâ or âaddâ are not desirable keywords in the CC-domain.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	shows some keywords with their scores.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	As we shall see, most of the linked sets are paraphrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	One is that smaller sets sometime have meaningless keywords, like âstrengthâ or âaddâ in the CC-domain, or âcompareâ in the PC-domain.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Although this is not a precise criterion, most cases we evaluated were relatively clear-cut.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Out of those 15 links, 4 are errors, namely âbuy - payâ, âacquire - payâ, âpurchase - stakeâ âacquisition - stakeâ.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	The links can solve the problem.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.	1
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Extract NE pair instances with contexts From the four years of newspaper corpus, we extracted 1.9 million pairs of NE instances.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	There have been other kinds of efforts to discover paraphrase automatically from corpora.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	All the sentences have been analyzed by our chunker and NE tag- ger.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	For example, the two NEs âEastern Group Plcâ and âHanson Plcâ have the following contexts.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Limitations There are several limitations in the methods.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	The availability of comparable corpora is limited, which is a significant limitation on the approach.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Out of those 15 links, 4 are errors, namely âbuy - payâ, âacquire - payâ, âpurchase - stakeâ âacquisition - stakeâ.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	In IE, creating the patterns which express the requested scenario, e.g. âmanagement successionâ or âcorporate merger and acquisitionâ is regarded as the hardest task.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One possibility is to use n-grams based on mutual information.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	For example, the phrase â's New York-based trust unit,â is not a paraphrase of the other phrases in the âunitâ set.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	For example, in Information Retrieval (IR), we have to match a userâs query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâs question even if the formulation of the answer in the document is different from the question.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One obvious application is information extraction.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	It is not easy to make a clear definition of âparaphraseâ.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Automatic paraphrase discovery is an important but challenging task.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.	1
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Limitations There are several limitations in the methods.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	As can be seen in Figure 3, the phrases in the âagreeâ set include completely different relationships, which are not paraphrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	So, we set a threshold that at least two examples are required to build a link.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	We can make several observations on the cause of errors.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	The procedure using the tagged sentences to discover paraphrases takes about one hour on a 2GHz Pentium 4 PC with 1GB of memory.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	There have been other kinds of efforts to discover paraphrase automatically from corpora.	1
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	However, those methods need initial seeds, so the relation between entities has to be known in advance.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	For example, in the CC-domain, 96 keywords are found which have TF/ITF scores above a threshold; some of them are shown in Figure 3.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	However, it is desirable if we can separate them.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	A total of 13,976 phrases were grouped.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	shows some keywords with their scores.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	At this step, we will try to link those sets, and put them into a single cluster.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	The accuracies for link were 73% and 86% on two evaluated domains.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Although this is not a precise criterion, most cases we evaluated were relatively clear-cut.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Cluster phrases based on Links We now have a set of phrases which share a keyword.	1
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	âAgreeâ is a subject control verb, which dominates another verb whose subject is the same as that of âagreeâ; the latter verb is generally the one of interest for extraction.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	It is a relatively frequent word in the domain, but it can be used in different extraction scenarios.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	As can be seen in the example, the first two phrases have a different order of NE names from the last two, so we can determine that the last two phrases represent a reversed relation.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Sometimes extracted phrases by themselves are not meaningful to consider without context, but we set the following criteria.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Now we have sets of phrases which share a keyword and we have links between those sets.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Among these 32 sets, we found the following pairs of sets which have two or more links.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Here, the term frequency (TF) is the frequency of a word in the bag and the inverse term frequency (ITF) is the inverse of the log of the frequency in the entire corpus.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	The data is sorted based on the frequency of the context (âa unit ofâ appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. âNBCâ and âGeneral Electric Co.â appeared 10 times with the context âa unit ofâ).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, we donât know how many such paraphrase sets are necessary to cover even some everyday things or events.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, âIBM plans to acquire Lotusâ.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	For example, the phrase â's New York-based trust unit,â is not a paraphrase of the other phrases in the âunitâ set.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	As was explained in the results section, âstrengthâ or âaddâ are not desirable keywords in the CC-domain.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, the method of using keywords rules out phrases which donât contain popular words in the domain.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	For example, in the CC-domain, 96 keywords are found which have TF/ITF scores above a threshold; some of them are shown in Figure 3.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	However, there are phrases which express the same meanings even though they do not share the same keyword.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	shows some keywords with their scores.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	As we shall see, most of the linked sets are paraphrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	1
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	1
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Evaluation of links A link between two sets is considered correct if the majority of phrases in both sets have the same meaning, i.e. if the link indicates paraphrase.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	It is not easy to make a clear definition of âparaphraseâ.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Automatic paraphrase discovery is an important but challenging task.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	For example, the phrase â's New York-based trust unit,â is not a paraphrase of the other phrases in the âunitâ set.	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here.	1
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	Neural networks are worth investigating since they offer potential advantages over decision trees.	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	We investigated several modeling alternatives for the components of the model (backoff n-grams and maximum entropy models for discourse grammars, decision trees and neural networks for prosodic classification) and found performance largely independent of these choices.	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	We have the apparent difficulty that decision trees (as well as other classifiers, such as neural networks) give estimates for the posterior probabilities, P(Ui[Fi).	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error.	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	Unfortunately, the event classification accuracy on the Switchboard corpus was considerably poorer than in the Map Task domain, and DA recognition results when coupled with a discourse grammar were substantially worse than with decision trees.	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	Levin et al. (1999) couple DA classification with dialogue game classification; dialogue games are units above the DA level, i.e., short DA sequences such as question-answer pairs.	0
(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.	3.1 Dialogue Act Likelihoods.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Our approach to dialogue modeling has two major components: statistical dialogue grammars modeling the sequencing of DAs, and DA likelihood models expressing the local cues (both lexical and prosodic) for DAs.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	3.1 Dialogue Act Likelihoods.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	3.3 Dialogue Act Decoding.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	2.3 Major Dialogue Act Types.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Previous research on DA modeling has generally focused on task-oriented dialogue, with three tasks in particular garnering much of the research effort.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram.	0
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Our approach to dialogue modeling has two major components: statistical dialogue grammars modeling the sequencing of DAs, and DA likelihood models expressing the local cues (both lexical and prosodic) for DAs.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Previous research on DA modeling has generally focused on task-oriented dialogue, with three tasks in particular garnering much of the research effort.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	The only DA types with higher uncertainty were BACKCHANNELS and AGREEMENTS, which are easily confused with each other without acoustic cues; here the rate of change was no more than 10%..	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Eventually, it is desirable to integrate dialogue grammar, lexical, and prosodic cues into a single model, e.g., one that predicts the next DA based on DA history and all the local evidence.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	In an automatic labeling of word boundaries as either utterance or nonboundaries using a combination of lexical and prosodic cues, we obtained 96% accuracy based on correct word transcripts, and 78% accuracy with automatically recognized words.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	2.3 Major Dialogue Act Types.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	DA modeling has mostly been geared toward automatic DA classification, and much less work has been done on applying DA models to automatic speech recognition.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	5.2 Dialogue Act Classification Using Prosody.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	We believe that dialogue-related tasks have much to benefit from corpus-driven, automatic learning techniques.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	DA modeling has mostly been geared toward automatic DA classification, and much less work has been done on applying DA models to automatic speech recognition.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	It seems to have been first employed by Nagata (1992), and in follow-up papers by Nagata and Morimoto (1993, 1994) on the ATR Dialogue database.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	Our primary purpose in adapting the tag set was to enable computational DA modeling for conversational speech, with possible improvements to conversational speech recognition.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	Woszczyna and Waibel (1994) and Suhm and Waibel (1994), followed by ChuCarroll (1998), seem to have been the first to note that such a combination of word and dialogue n-grams could be viewed as a dialogue HMM with word strings as the observations.	0
There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).	Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	The relation between utterances and speaker turns is not one-to-one: a single turn can contain multiple utterances, and utterances can span more than one turn (e.g., in the case of backchanneling by the other speaker in midutterance).	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	Dialogue grammars for conversational speech need to be made more aware of the temporal properties of utterances.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	As a preliminary experiment to test the integration of prosody with other knowledge sources, we trained a single tree to discriminate among the five most frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, and AGREEMENT,totaling 79% of the data) and an Other category comprising all remaining DA types.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	The most common types of utterances were STATEMENTS and OPINIONS.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	One frequent example in our corpus was the distinction between BACKCHANNELS and AGREEMENTS (see Table 2), which share terms such as right and yeah.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	Their task is to help one speaker reproduce a route drawn only on the other speaker's map, all without being able to see each other's maps.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	Our primary purpose in adapting the tag set was to enable computational DA modeling for conversational speech, with possible improvements to conversational speech recognition.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	Abandoned utterances are those that the speaker breaks off without finishing, and are followed by a restart.	0
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Our approach to dialogue modeling has two major components: statistical dialogue grammars modeling the sequencing of DAs, and DA likelihood models expressing the local cues (both lexical and prosodic) for DAs.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Previous research on DA modeling has generally focused on task-oriented dialogue, with three tasks in particular garnering much of the research effort.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	The only DA types with higher uncertainty were BACKCHANNELS and AGREEMENTS, which are easily confused with each other without acoustic cues; here the rate of change was no more than 10%..	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Eventually, it is desirable to integrate dialogue grammar, lexical, and prosodic cues into a single model, e.g., one that predicts the next DA based on DA history and all the local evidence.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	In an automatic labeling of word boundaries as either utterance or nonboundaries using a combination of lexical and prosodic cues, we obtained 96% accuracy based on correct word transcripts, and 78% accuracy with automatically recognized words.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	2.3 Major Dialogue Act Types.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	DA modeling has mostly been geared toward automatic DA classification, and much less work has been done on applying DA models to automatic speech recognition.	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	0
Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.	5.2 Dialogue Act Classification Using Prosody.	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	DA modeling has mostly been geared toward automatic DA classification, and much less work has been done on applying DA models to automatic speech recognition.	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	Woszczyna and Waibel (1994), for example, trained an ergodic HMM using expectation-maximization to model speech act sequencing.	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	Dialogue Act Example Utterance YEs-No-QUESTION Do you have to have any special training?	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	We chose to follow a recent standard for shallow discourse structure annotation, the Dialog Act Markup in Several Layers (DAMSL) tag set, which was designed by the natural language processing community under the auspices of the Discourse Resource Initiative (Core and Allen 1997).	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus.	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition.	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	For example, unlike in most previous work on DA labeling, the corpus is not task-oriented in nature, and the amount of data used (198,000 utterances) exceeds that in previous studies by at least an order of magnitude (see Table 14).	0
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).	Thus, a DA is approximately the equivalent of the speech act of Searle (1969), the conversational game move of Power (1979), or the adjacency pair part of Schegloff (1968) and Saks, Schegloff, and Jefferson (1974).	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	The choice of tasks was motivated by an analysis of confusions committed by a purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, and BACKCHANNELS for AGREEMENTS (and vice versa).	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states.	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	Woszczyna and Waibel (1994), for example, trained an ergodic HMM using expectation-maximization to model speech act sequencing.	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	@ 2000 Association for Computational Linguistics Table 1 Fragment of a labeled conversation (from the Switchboard corpus).	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	The model predicted upcoming DAs by using bigrams and trigrams conditioned on preceding DAs, trained on a corpus of 2,722 DAs.	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram.	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech.	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	COMMANDS are followed by AGREEMENTS in 23% of the cases, and STATEMENTS elicit BACKCHANNELS in 26% of all cases.	0
dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec­tor trained on Switchboard similar to Stolcke et al.(2000	For example, by definition DECLARATIVE-QUESTIONS are not marked by syntax (e.g., by subject-auxiliary inversion) and are thus confusable with STATEMENTS and OPINIONS.	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	Table 13 Dialogue act tag sets used in three other extensively studied corpora.	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	Approximately 220 of the many possible unique combinations of these codes were used by the coders (Jurafsky, Shriberg, and Biasca 1997).	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	We chose to follow a recent standard for shallow discourse structure annotation, the Dialog Act Markup in Several Layers (DAMSL) tag set, which was designed by the natural language processing community under the auspices of the Discourse Resource Initiative (Core and Allen 1997).	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	Some researchers explicitly used HMM induction techniques to infer dialogue grammars.	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	In our experiments we used the following sources of evidence, either alone or in combination: Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where W refers to the true (hand-transcribed) words spoken in a conversation.	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	DA modeling has mostly been geared toward automatic DA classification, and much less work has been done on applying DA models to automatic speech recognition.	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	5.2 Dialogue Act Classification Using Prosody.	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition.	0
The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.	4 5.1 Dialogue Act Classification Using Words.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.	1
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	Table 2 The 42 dialogue act labels.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	4 5.1 Dialogue Act Classification Using Words.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	Table 13 Dialogue act tag sets used in three other extensively studied corpora.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	Neural networks are therefore a good candidate for a jointly optimized classifier of prosodic and word-level information since one can show that they are a generalization of the integration approach used here.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	In our experiments we used the following sources of evidence, either alone or in combination: Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where W refers to the true (hand-transcribed) words spoken in a conversation.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	Another important role of DA information could be feedback to lower-level processing.	0
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).	3.1 Dialogue Act Likelihoods.	0
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.	1
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	1
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.	0
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.	0
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	The most accurate characterization of Chinese writing is that it is morphosyllabic (DeFrancis 1984): each hanzi represents one morpheme lexically and semantically, and one syllable phonologiÂ­ cally.	0
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	1
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	Full Chinese personal names are in one respect simple: they are always of the form family+given.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	Given that part-of-speech labels are properties of words rather than morphemes, it follows that one cannot do part-of-speech assignment without having access to word-boundary information.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	Figure 4 Input lattice (top) and two segmentations (bottom) of the sentence 'How do you say octopus in Japanese?'.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.	0
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	A morpheme, on the other hand, usually corresponds to a unique hanzi, though there are a few cases where variant forms are found.	0
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	1
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	An initial step of any textÂ­ analysis task is the tokenization of the input into words.	0
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.	0
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	Others depend upon various lexical heurisÂ­ tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.	0
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS).	0
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.	0
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	So, 1: f, xue2shengl+men0 (student+PL) 'students' occurs and we estimate its cost at 11.43; similarly we estimate the cost of f, jiang4+men0 (general+PL) 'generals' (as in 'J' f, xiao3jiang4+men0 'little generals'), at 15.02.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	na me =>1 ha nzi fa mi ly 1 ha nzi gi ve n 4.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	com Â§Cambridge, UK Email: nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	na me =>1 ha nzi fa mi ly 2 ha nzi gi ve n 3.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	For example, in Northern dialects (such as Beijing), a full tone (1, 2, 3, or 4) is changed to a neutral tone (0) in the final syllable of many words: Jll donglgual 'winter melon' is often pronounced donglguaO.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	(student+plural) 'students,' which is derived by the affixation of the plural affix f, menD to the nounxue2shengl.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	Obviously, the presence of a title after a potential name N increases the probability that N is in fact a name.	0
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval.	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Turning now to (1), we have the similar problem that splitting.into.ma3 'horse' andlu4 'way' is more costly than retaining this as one word .ma3lu4 'road.'	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.	0
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:Â¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1.	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	from the subset of the United Informatics corpus not used in the training of the models.	0
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	First, the model assumes independence between the first and second hanzi of a double given name.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t:-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	Clearly, for judges h and h taking h as standard and computing the precision and recall for Jz yields the same results as taking h as the standard, and computing for h, 14 All evaluation materials, with the exception of those used for evaluating personal names were drawn.	0
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper-name identification; the third measures the performance on morphological analysis.	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	1
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	There are thus some very good reasons why segmentation into words is an important task.	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	(See also Wu and Fung [1994].)	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	Wu and Fung introduce an evaluation method they call nk-blind.	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	The relevance of the distinction between, say, phonological words and, say, dictionary words is shown by an example like rpftl_A :;!:Hfllil zhong1hua2 ren2min2 gong4he2-guo2 (China people republic) 'People's Republic of China.'	0
According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved.	This suggests that the backoff model is as reasonable a model as we can use in the absence of further information about the expected cost of a plural form.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	1
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	constitute names, since we have only their segmentation, not the actual classification of the segmented words.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identiÂ­ fication of such names is tricky.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	4.4 Chinese Personal Names.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	In Table 5 we present results from small test corÂ­ pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	Morphologically derived words such as, xue2shengl+men0.	0
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN).	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	1
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Various segmentation approaches were then compared with human performance: 1.	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ­ senting differences in the capabilities of the model per se.	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	What both of these approaches presume is that there is a sinÂ­ gle correct segmentation for a sentence, against which an automatic algorithm can be compared.	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	0
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	Methods that allow multiple segmentations must provide criteria for choosing the best segmentation.	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	We of course also fail to identify, by the methods just described, given names used without their associated family name.	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	Several papers report the use of part-of-speech information to rank segmentations (Lin, Chiang, and Su 1993; Peng and Chang 1993; Chang and Chen 1993); typically, the probability of a segmentation is multiplied by the probability of the tagging(s) for that segmentation to yield an estimate of the total probability for the analysis.	0
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	The second concerns the methods used (if any) to exÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	each word in the lexicon whether or not each string is actually an instance of the word in question.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	Second, comparisons of different methods are not meaningful unless one can evalÂ­ uate them on the same corpus.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	However, it is almost universally the case that no clear definition of what constitutes a "correct" segmentation is given, so these performance measures are hard to evaluate.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	Clearly, for judges h and h taking h as standard and computing the precision and recall for Jz yields the same results as taking h as the standard, and computing for h, 14 All evaluation materials, with the exception of those used for evaluating personal names were drawn.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.	0
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	1
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper-name identification; the third measures the performance on morphological analysis.	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable.	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Without using the same test corpus, direct comparison is obviously difficult; fortunately, Chang et al. include a list of about 60 sentence fragments that exemplify various categories of performance for their system.	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Under this scheme, n human judges are asked independently to segment a text.	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	Since the transducers are built from human-readable descriptions using a lexical toolkit (Sproat 1995), the system is easily maintained and extended.	0
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	There are thus some very good reasons why segmentation into words is an important task.	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	1
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Previous Work.	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features, which can be referred to by finite-state grammatical rules.	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	Yet, some hanzi are far more probable in women's names than they are in men's names, and there is a similar list of male-oriented hanzi: mixing hanzi from these two lists is generally less likely than would be predicted by the independence model.	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	Under this scheme, n human judges are asked independently to segment a text.	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words.	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	We have shown that, at least given independent human judgments, this is not the case, and that therefore such simplistic measures should be mistrusted.	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ­ senting differences in the capabilities of the model per se.	0
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	Under this scheme, n human judges are asked independently to segment a text.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	Methods that allow multiple segmentations must provide criteria for choosing the best segmentation.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0
Similarly, Sproat et al.(1996) also uses multiple human judges.	For each pair of judges consider one judge as the standard,.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	The model described here thus demonstrates great potential for use in widespread applications.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	We can model this probability straightforwardly enough with a probabilistic version of the grammar just given, which would assign probabilities to the individual rules.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	We of course also fail to identify, by the methods just described, given names used without their associated family name.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	pronunciation depends upon word affiliation: tfJ is pronounced deO when it is a prenominal modification marker, but di4 in the word Â§tfJ mu4di4 'goal'; fl; is normally ganl 'dry,' but qian2 in a person's given name.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	set was based on an earlier version of the Chang et a!.	0
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	First, the model assumes independence between the first and second hanzi of a double given name.	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	constitute names, since we have only their segmentation, not the actual classification of the segmented words.	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	The segmentation chosen is the best path through the WFST, shown in (d).	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	Many hanzi have more than one pronunciation, where the correct.	0
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	Both of these analyses are shown in Figure 4; fortunately, the correct analysis is also the one with the lowest cost, so it is this analysis that is chosen.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	Other good classes include JADE and GOLD; other bad classes are DEATH and RAT.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	In (1) the sequencema3lu4 cannot be resolved locally, but depends instead upon broader context; similarly in (2), the sequence :::tcai2neng2 cannot be resolved locally: 1.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	computing the precision of the other's judgments relative to this standard.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	computing the recall of the other's judgments relative to this standard.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	For each pair of judges consider one judge as the standard,.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	For each pair of judges, consider one judge as the standard,.	0
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	21 In Chinese, numerals and demonstratives cannot modify nouns directly, and must be accompanied by.	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	1
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	(1992).	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	(1992).	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas.	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	In a few cases, the criteria for correctness are made more explicit.	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	1
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.	0
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	The only way to handle such phenomena within the framework described here is simply to expand out the reduplicated forms beforehand, and incorporate the expanded forms into the lexical transducer.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	f, nan2gual+men0 'pumpkins' is by no means impossible.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	The performance of our system on those sentences apÂ­ peared rather better than theirs.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	Nonstochastic lexical-knowledge-based approaches have been much more numerÂ­ ous.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	Evaluation of the Segmentation as a Whole.	0
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	Evaluation of Morphological Analysis.	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	1
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features, which can be referred to by finite-state grammatical rules.	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	For example, as Gan (1994) has noted, one can construct examples where the segmenÂ­ tation is locally ambiguous but can be determined on the basis of sentential or even discourse context.	0
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	In Section 6 we disÂ­ cuss other issues relating to how higher-order language models could be incorporated into the model.	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	1
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features, which can be referred to by finite-state grammatical rules.	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Nonstochastic lexical-knowledge-based approaches have been much more numerÂ­ ous.	0
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1.	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	The first point we need to address is what type of linguistic object a hanzi repreÂ­ sents.	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	10 Here we use the Good-Turing estimate (Baayen 1989; Church and Gale 1991), whereby the aggregate probability of previously unseen instances of a construction is estimated as ni/N, where N is the total number of observed tokens and n1 is the number of types observed only once.	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	Of course, since the number of attested (phonemic) Mandarin syllables (roughly 1400, including tonal distinctions) is far smaller than the number of morphemes, it follows that a given syllable could in principle be written with any of several different hanzi, depending upon which morpheme is intended: the syllable zhongl could be lfl 'middle,''clock,''end,' or ,'loyal.'	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	The breakdown of the different types of words found by ST in the test corpus is given in Table 3.	0
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on.	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	1
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identiÂ­ fication of such names is tricky.	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	4.5 Transliterations of Foreign Words.	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	In the case of, the most common usage is as an adverb with the pronunciation jiangl, so that variant is assigned the estimated cost of 5.98, and a high cost is assigned to nominal usage with the pronunciation jiang4.	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN).	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Many hanzi have more than one pronunciation, where the correct.	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	1
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	4.5 Transliterations of Foreign Words.	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	from the subset of the United Informatics corpus not used in the training of the models.	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper.	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	The family name set is restricted: there are a few hundred single-hanzi family names, and about ten double-hanzi ones.	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identiÂ­ fication of such names is tricky.	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.	0
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	In the pinyin transliterations a dash(-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (+) is used, where relevant, to indicate morpheme boundaries of interest.	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	1
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	1
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	Yet, some hanzi are far more probable in women's names than they are in men's names, and there is a similar list of male-oriented hanzi: mixing hanzi from these two lists is generally less likely than would be predicted by the independence model.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	For a sequence of hanzi that is a possible name, we wish to assign a probability to that sequence qua name.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	In this example there are four "input characters," A, B, C and D, and these map respectively to four "pronunciations" a, b, c and d. Furthermore, there are four "words" represented in the dictionary.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper.	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	In this example there are four "input characters," A, B, C and D, and these map respectively to four "pronunciations" a, b, c and d. Furthermore, there are four "words" represented in the dictionary.	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	from the subset of the United Informatics corpus not used in the training of the models.	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	4.4 Chinese Personal Names.	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	The family name set is restricted: there are a few hundred single-hanzi family names, and about ten double-hanzi ones.	0
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	Full Chinese personal names are in one respect simple: they are always of the form family+given.	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	1
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	We thank United Informatics for providing us with our corpus of Chinese text, and BDC for the 'Behavior ChineseEnglish Electronic Dictionary.'	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	This suggests that the backoff model is as reasonable a model as we can use in the absence of further information about the expected cost of a plural form.	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	0
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	A totally nonÂ­ stochastic rule-based system such as Wang, Li, and Chang's will generally succeed in such cases, but of course runs the risk of overgeneration wherever the single-hanzi word is really intended.	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	set was based on an earlier version of the Chang et a!.	0
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	1
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	Previous Work.	1
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)	0
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval.	0
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	Since the segmentation corresponds to the sequence of words that has the lowest summed unigram cost, the segmenter under discussion here is a zeroth-order model.	0
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	Our system does not currently make use of titles, but it would be straightforward to do so within the finite-state framework that we propose.	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features, which can be referred to by finite-state grammatical rules.	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	For example, as Gan (1994) has noted, one can construct examples where the segmenÂ­ tation is locally ambiguous but can be determined on the basis of sentential or even discourse context.	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	With regard to purely morphological phenomena, certain processes are not hanÂ­ dled elegantly within the current framework Any process involving reduplication, for instance, does not lend itself to modeling by finite-state techniques, since there is no way that finite-state networks can directly implement the copying operations required.	0
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	So, 1: f, xue2shengl+men0 (student+PL) 'students' occurs and we estimate its cost at 11.43; similarly we estimate the cost of f, jiang4+men0 (general+PL) 'generals' (as in 'J' f, xiao3jiang4+men0 'little generals'), at 15.02.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	Figure 5 shows how this model is implemented as part of the dictionary WFST.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	For example, as Gan (1994) has noted, one can construct examples where the segmenÂ­ tation is locally ambiguous but can be determined on the basis of sentential or even discourse context.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	(student+plural) 'students,' which is derived by the affixation of the plural affix f, menD to the nounxue2shengl.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	TIS systems in general need to do more than simply compute the.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	Since the segmentation corresponds to the sequence of words that has the lowest summed unigram cost, the segmenter under discussion here is a zeroth-order model.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	The first point we need to address is what type of linguistic object a hanzi repreÂ­ sents.	0
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	Thus in a two-hanzi word like lflli?J zhong1guo2 (middle country) 'China' there are two syllables, and at the same time two morphemes.	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).	1
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	Note that hanzi that are not grouped into dictionary words (and are not identified as singleÂ­ hanzi words), or into one of the other categories of words discussed in this paper, are left unattached and tagged as unknown words.	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	There are two weaknesses in Chang et al.'s model, which we improve upon.	0
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	Figure 5 shows how this model is implemented as part of the dictionary WFST.	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	1
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	In the pinyin transliterations a dash(-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (+) is used, where relevant, to indicate morpheme boundaries of interest.	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	Note also that the costs currently used in the system are actually string costs, rather than word costs.	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	from the subset of the United Informatics corpus not used in the training of the models.	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	It is. based on the traditional character set rather than the simplified character set used in Singapore and Mainland China.	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN).	0
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	There are two weaknesses in Chang et al.'s model, which we improve upon.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.	1
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	First of all, most previous articles report perforÂ­ mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	The performance was 80.99% recall and 61.83% precision.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	Email: gale@research.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	To date we have not done a separate evaluation of foreign-name recognition.	0
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	4.3 Morphological Analysis.	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	Evaluation of Morphological Analysis.	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	Both of these analyses are shown in Figure 4; fortunately, the correct analysis is also the one with the lowest cost, so it is this analysis that is chosen.	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	Not surprisingly some semantic classes are better for names than others: in our corpora, many names are picked from the GRASS class but very few from the SICKNESS class.	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	This is an issue that we have not addressed at the current stage of our research.	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	What both of these approaches presume is that there is a sinÂ­ gle correct segmentation for a sentence, against which an automatic algorithm can be compared.	0
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	An analysis of nouns that occur in both the singular and the plural in our database reveals that there is indeed a slight but significant positive correlation-R2 = 0.20, p < 0.005; see Figure 6.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	We of course also fail to identify, by the methods just described, given names used without their associated family name.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	pronunciations of individual words; they also need to compute intonational phrase boundaries in long utterances and assign relative prominence to words in those utterances.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	The performance of our system on those sentences apÂ­ peared rather better than theirs.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.	1
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	set was based on an earlier version of the Chang et a!.	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	In various dialects of Mandarin certain phonetic rules apply at the word.	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Others depend upon various lexical heurisÂ­ tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Not surprisingly some semantic classes are better for names than others: in our corpora, many names are picked from the GRASS class but very few from the SICKNESS class.	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	Since the segmentation corresponds to the sequence of words that has the lowest summed unigram cost, the segmenter under discussion here is a zeroth-order model.	0
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	The high 1 tone of J1l would not normally neutralize in this fashion if it were functioning as a word on its own.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	Note also that the costs currently used in the system are actually string costs, rather than word costs.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	each word in the lexicon whether or not each string is actually an instance of the word in question.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	On the other hand, in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	Now, for this application one might be tempted to simply bypass the segmentation problem and pronounce the text character-by-character.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	For that application, at a minimum, one would want to know the phonological word boundaries.	0
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	(1992).	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	(1992).	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	Not surprisingly some semantic classes are better for names than others: in our corpora, many names are picked from the GRASS class but very few from the SICKNESS class.	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	In the denomi 11 We have two such lists, one containing about 17,000 full names, and another containing frequencies of.	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical.	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	It can also be seen clearly in this plot that two of the Taiwan speakers cluster very closely together, and the third TaiÂ­ wan speaker is also close in the most significant dimension (the x axis).	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	Methods that allow multiple segmentations must provide criteria for choosing the best segmentation.	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	It may seem surprising to some readers that the interhuman agreement scores reported here are so low.	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	This is because our corpus is not annotated, and hence does not distinguish between the various words represented by homographs, such as, which could be /adv jiangl 'be about to' orInc jiang4 '(military) general'-as in 1j\xiao3jiang4 'little general.'	0
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ­ senting differences in the capabilities of the model per se.	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	1
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	It may seem surprising to some readers that the interhuman agreement scores reported here are so low.	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	Again, famous place names will most likely be found in the dictionary, but less well-known names, such as 1PMÂ± R; bu4lang3-shi4wei2-ke4 'Brunswick' (as in the New Jersey town name 'New Brunswick') will not generally be found.	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas.	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	1
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	For that application, at a minimum, one would want to know the phonological word boundaries.	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.	0
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	Full Chinese personal names are in one respect simple: they are always of the form family+given.	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper.	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	The second concerns the methods used (if any) to exÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	It is. based on the traditional character set rather than the simplified character set used in Singapore and Mainland China.	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	There are two weaknesses in Chang et al.'s model, which we improve upon.	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	The particular classifier used depends upon the noun.	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	Particular relations are also consistent with particular hypotheses about the segmentation of a given sentence, and the scores for particular relations can be incremented or decremented depending upon whether the segmentations with which they are consistent are "popular" or not.	0
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	1
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple: subjects were to mark all places they might plausibly pause if they were reading the text aloud.	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	One class comprises words derived by productive morphologiÂ­ cal processes, such as plural noun formation using the suffix ir, menD.	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:Â¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1.	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	Without using the same test corpus, direct comparison is obviously difficult; fortunately, Chang et al. include a list of about 60 sentence fragments that exemplify various categories of performance for their system.	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	1
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Nonstochastic lexical-knowledge-based approaches have been much more numerÂ­ ous.	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	The second concerns the methods used (if any) to exÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	What both of these approaches presume is that there is a sinÂ­ gle correct segmentation for a sentence, against which an automatic algorithm can be compared.	0
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	Various segmentation approaches were then compared with human performance: 1.	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	constitute names, since we have only their segmentation, not the actual classification of the segmented words.	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	The segmentation chosen is the best path through the WFST, shown in (d).	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	Many hanzi have more than one pronunciation, where the correct.	0
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	Both of these analyses are shown in Figure 4; fortunately, the correct analysis is also the one with the lowest cost, so it is this analysis that is chosen.	0
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	1
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.	1
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	As a partial solution, for pairs of hanzi that co-occur sufficiently often in our namelists, we use the estimated bigram cost, rather than the independence-based cost.	0
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.	0
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.	0
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	0
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	set was based on an earlier version of the Chang et a!.	0
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	A related point is that mutual information is helpful in augmenting existing electronic dictionaries, (cf.	0
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	The second concerns the methods used (if any) to exÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.	0
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	76 16.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	Thus in a two-hanzi word like lflli?J zhong1guo2 (middle country) 'China' there are two syllables, and at the same time two morphemes.	0
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	For that application, at a minimum, one would want to know the phonological word boundaries.	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.	1
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas.	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Yet, some hanzi are far more probable in women's names than they are in men's names, and there is a similar list of male-oriented hanzi: mixing hanzi from these two lists is generally less likely than would be predicted by the independence model.	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Each word is terminated by an arc that represents the transduction between f and the part of speech of that word, weighted with an estimated cost for that word.	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	each word in the lexicon whether or not each string is actually an instance of the word in question.	0
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	1
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	logical rules, and personal names; the transitive closure of the resulting machine is then computed.	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	4.4 Chinese Personal Names.	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	With regard to purely morphological phenomena, certain processes are not hanÂ­ dled elegantly within the current framework Any process involving reduplication, for instance, does not lend itself to modeling by finite-state techniques, since there is no way that finite-state networks can directly implement the copying operations required.	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits.	0
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	For example, suppose one is building a ITS system for Mandarin Chinese.	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen conÂ­ structions, including morphological derivatives and personal names.	1
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	One class comprises words derived by productive morphologiÂ­ cal processes, such as plural noun formation using the suffix ir, menD.	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	4.5 Transliterations of Foreign Words.	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	Others depend upon various lexical heurisÂ­ tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	Full Chinese personal names are in one respect simple: they are always of the form family+given.	0
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	logical rules, and personal names; the transitive closure of the resulting machine is then computed.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	This method, one instance of which we term the "greedy algorithm" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginÂ­ ning) of the sentence is reached.	1
We used a simple greedy algorithm described in [Sproat et al., 1996].	An anti-greedy algorithm, AG: instead of the longest match, take the.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	We of course also fail to identify, by the methods just described, given names used without their associated family name.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	Finally, asÂ­ suming a simple bigram backoff model, we can derive the probability estimate for the particular unseen word iÂ¥1J1l.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	A moment's reflection will reveal that things are not quite that simple.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	In this way, the method reported on here will necessarily be similar to a greedy method, though of course not identical.	0
We used a simple greedy algorithm described in [Sproat et al., 1996].	While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.	1
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	We have argued that the proposed method performs well.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	In the denomi 11 We have two such lists, one containing about 17,000 full names, and another containing frequencies of.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	Word frequencies are estimated by a re-estimation procedure that involves applyÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	However, for our purposes it is not sufficient to repreÂ­ sent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges.	0
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates.	0
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	1
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	This method, one instance of which we term the "greedy algorithm" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginÂ­ ning) of the sentence is reached.	1
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	0
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	An anti-greedy algorithm, AG: instead of the longest match, take the.	0
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..	0
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	0
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.	0
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	On the other hand, in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English.	0
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	Wu and Fung introduce an evaluation method they call nk-blind.	0
The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	The performance was 80.99% recall and 61.83% precision.	1
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.	1
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.	0
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.	0
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	First of all, most previous articles report perforÂ­ mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.	0
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.	0
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.	0
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits.	0
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	To date we have not done a separate evaluation of foreign-name recognition.	0
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	We therefore used the arithmetic mean of each interjudge precision-recall pair as a single measure of interjudge similarity.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	Chinese word segmentation can be viewed as a stochastic transduction problem.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	What both of these approaches presume is that there is a sinÂ­ gle correct segmentation for a sentence, against which an automatic algorithm can be compared.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	An initial step of any textÂ­ analysis task is the tokenization of the input into words.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular seÂ­ quence of hanzi might be a name, but that it is likely to be a name with some probabilÂ­ ity.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).	0
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	As can be seen, GR and this "pared-down" statistical method perform quite similarly, though the statistical method is still slightly better.16 AG clearly performs much less like humans than these methods, whereas the full statistical algorithm, including morphological derivatives and names, performs most closely to humans among the automatic methods.	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	We will show that in both cases the estimated probability is tight.	1
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, "relative frequency," estimator.	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	a What if p is estimated from data?	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	What if the production probabilities are estimated from data?	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	3 Consider any sequence of productions that leads from S to B. If the parent (antecedent) of B arose in.	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper.	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the "relative frequency" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production "." in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions.	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	(Wetherell and others use the designation "consistent" instead of "tight," but in statistics, consistency refers to the asymptotic correctness of an estimator.)	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG.	0
Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.	Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight.	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, "relative frequency," estimator.	1
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	What if the production probabilities are estimated from data?	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper.	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	a What if p is estimated from data?	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG.	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	Therefore, when the corpus consists of yields only, we shall assume a priori a model free of null and unit productions, and study tightness for probabilities estimated under such a model.	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight.	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	We show here that estimated production probabilities always yield proper distributions.	0
Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.	The condition for proper assignment is rather subtle.	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	We will show that in both cases the estimated probability is tight.	1
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	Thus, ~ is again less than ½ and the distribution is again tight.	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's--or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse.	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	a What if p is estimated from data?	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one.	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	What if the production probabilities are estimated from data?	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	More generally, ~ will refer to the probability distribution on (possibly infinite) parse trees induced by the maximum-likelihood estimator.	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	Unfortunately, this simple procedure runs into an unexpected complication: the language generated by the grammar may have probability less than one.	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	The maximum-likelihood probability is tight.	0
When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.	The likelihood is substantially more complex, since p(Y(w)) is now a marginal probability; we need to sum over the set of w E f~ that yield Y(w): p(Y(w)) = E p(Y(w')).	0
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	We will show that in both cases the estimated probability is tight.	1
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?	1
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: A ~ AA a ~ a where a is the only terminal symbol.	0
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	The maximum-likelihood probability is tight.	0
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the "relative frequency" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production "." in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions.	0
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	More generally, ~ will refer to the probability distribution on (possibly infinite) parse trees induced by the maximum-likelihood estimator.	0
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	wlEU~ Y(w¢)=Y(oa) In the case where only yields are observed, the treatment is complicated considerably by the possibility of null productions (A --, 0) and unit productions (A ~ B E V).	0
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight.	0
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	Therefore, when the corpus consists of yields only, we shall assume a priori a model free of null and unit productions, and study tightness for probabilities estimated under such a model.	0
Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).	We will show that if f~ is the set of all (finite) parse trees generated by G, and if f~(ca) is the probability of ca ff fl under the maximum-likelihood production probabilities, then fi(f~) = 1.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice CoeÆcient.	1
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	We believe that the approach in this paper is the ?rst time that decision trees based strictly on bigram features have been employed.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	In these experiments, there were no decision trees that used all of the bigram features identi?ed by the ?ltering step, and for many words the decision tree learner went on to eliminate most of the candidate features.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	Learning continues until all the training examples are accounted for by the decision tree.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	Column 10 shows the accuracy of the decision tree when the Dice CoeÆcient selects the features.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	A decision stump is a one node decision tree(Holte, 1993) that is created by stopping the decision tree learner after the single most informative feature is added to the tree.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	The experimental data is discussed, and then the empirical results are presented.	0
The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear­lier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log–likelihood ratio.This earlier approach was evaluated on the SENSEVAL­1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.	If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	This software is written in Perl and is freely available from www.d.umn.edu/~tpederse.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	However, we believe that fragmentation also re ects on the feature set used for learning.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	We also include three benchmark learning algorithms in this study: the majority classi?er, the decision stump, and the Naive Bayesian classi?er.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice CoeÆcient.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case.	0
We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).	Rather, we will search for bigrams where the component words may be separated by other words in the text.	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.	1
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)).	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and in (Kilgarri?	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	A Decision Tree of Bigrams is an Accurate Predictor of Word Sense	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs.	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)).	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	In other words, an informative feature set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features.	0
in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.	There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word.	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.	1
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)).	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line.	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)).	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	We believe that the approach in this paper is the ?rst time that decision trees based strictly on bigram features have been employed.	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	A Decision Tree of Bigrams is an Accurate Predictor of Word Sense	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.	0
Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001).	Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.	1
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice CoeÆcient.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	The Dice CoeÆcient is a descriptive statistic that provides a measure of association among two words in a corpus.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	The second is to eliminate the ?ltering step by which candidate bigrams are selected by a power divergence statistic.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	The power divergence tests prove to be more reliable since they account for all possible events surrounding two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	The most accurate method is the decision tree based on a feature set determined by the power divergence statistic.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	Columns 9 and 11 show the accuracy of the decision stump based on the power divergence statistic and the Dice CoeÆcient respectively.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	In light of this, (Pedersen, 1996) presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio.	0
In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.	Column 8 shows the accuracy of the decision tree using the J48 learning algorithm and the features identi?ed by a power divergence statistic.	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic.	1
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison.	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	The evaluation at SENSEVAL was based on precision and recall, so we converted those scores to accuracy by taking their product.	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	A Decision Tree of Bigrams is an Accurate Predictor of Word Sense	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	Instead, the decision tree learner would consider all possible bigrams.	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice CoeÆcient.	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family.	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	The second is to eliminate the ?ltering step by which candidate bigrams are selected by a power divergence statistic.	0
• Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).	This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	The focus of our work is on the use of contextual role knowledge for coreference resolution.	1
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics.	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions.	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	In contrast, even though context can be helpful for resolving definite NPs, context can be trumped by the semantics of the nouns themselves.	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	For each resolution in the training data, BABAR also associates the co-referring expression of an NP with the NPâs caseframe.	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	The role that each noun phrase plays in the kidnapping event is key to distinguishing these cases.	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	The ability to redistribute belief values across sets rather than individual hypotheses is key.	0
Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)	The goal of our research was to explore the use of contextual role knowledge for coreference resolution.	0
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	We evaluated BABAR on two domains: terrorism and natural disasters.	1
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	BABAR employs information extraction techniques to represent and learn role relationships.	1
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	Section 4 presents experimen tal results on two corpora: the MUC4 terrorism corpus, and Reuters texts about natural disasters.	0
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	0
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	Our coreference resolver performed well in two domains, and experiments showed that each contextual role knowledge source contributed valuable information.	0
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	The learned information was recycled back into the resolver to improve its performance.	0
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuterâs text collection8 that had a subject code corresponding to natural disasters.	0
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	For the disasters domain, 8245 texts were used for training and the 40 test documents contained 447 anaphoric links.	0
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.	0
Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).	This result suggests that all of contextual role KSs can provide useful information for resolving anaphora.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	an event.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	A contextual role represents the role that a noun phrase plays in an event or relationship.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	The role that each noun phrase plays in the kidnapping event is key to distinguishing these cases.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	For each caseframe, BABAR collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	For IE, the system must be able to distinguish between semantically similar noun phrases that play different roles in an event.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	For each case- frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	First, we parsed the training corpus, collected all the noun phrases, and looked up each head noun in WordNet (Miller, 1990).	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	Figure 1 reveals that an event that âdamagedâ objects may also cause injuries; a disaster that âoccurredâ may be investigated to find its âcauseâ; a disaster may âwreakâ havoc as it âcrossesâ geographic regions; and vehicles that have a âdriverâ may also âcarryâ items.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	For each co-occurrence relation (noun/caseframe for CFLex, and caseframe/caseframe for CFNet), BABAR computes its log-likelihood value and looks it up in the Ï2 table to obtain a confidence level.	0
the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).	We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	The focus of our work is on the use of contextual role knowledge for coreference resolution.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	The goal of our research was to explore the use of contextual role knowledge for coreference resolution.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	Our coreference resolver performed well in two domains, and experiments showed that each contextual role knowledge source contributed valuable information.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	A contextual role represents the role that a noun phrase plays in an event or relationship.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.	0
Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.	Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	For example, co-occurring caseframes may reflect synonymy (e.g., â<patient> kidnappedâ and â<patient> abductedâ) or related events (e.g., â<patient> kidnappedâ and â<patient> releasedâ).	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	For example, a personâs full name will match with just their last name (e.g., âGeorge Bushâ and âBushâ), and a company name will match with and without a corporate suffix (e.g., âIBM Corp.â and âIBMâ).	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	As with lexical expections, the semantic classes of co-referring expressions are 4 They may not be perfectly substitutable, for example one NP may be more specific (e.g., âheâ vs. âJohn F. Kennedyâ).	0
(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	Table 5: Individual Performance of KSs for Disasters (e.g., âthe mayorâ vs. âthe journalistâ).	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	The learned patterns are then normalized and applied to the corpus.	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	These systems rely on a training corpus that has been manually annotated with coreference links.	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	Many researchers have developed coreference resolvers, so we will only discuss the methods that are most closely related to BABAR.	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	Figure 4 shows the seven general knowledge sources (KSs) that represent features commonly used for coreference resolution.	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings 9 We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC4 or Reuters collections.	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).	0
Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)	(Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	Ex: Mr. Bush disclosed the policy by reading it...	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	BABAR uses two methods to identify anaphors that can be easily and reliably resolved with their antecedent: lexical seeding and syntactic seeding.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics.	0
Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example â€œMr.Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.	In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.	0
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found.	1
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.	1
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.	1
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves.	1
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	Contextual role knowledge provides evidence as to whether a candidate is a plausible antecedent for an anaphor.	0
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.	0
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions.	0
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor.	0
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	For example, co-occurring caseframes may reflect synonymy (e.g., â<patient> kidnappedâ and â<patient> abductedâ) or related events (e.g., â<patient> kidnappedâ and â<patient> releasedâ).	0
Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of <NP>â€, â€œkilled <patient>â€.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.	We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	A contextual role represents the role that a noun phrase plays in an event or relationship.	1
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	2.2 Contextual Role Knowledge.	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	The focus of our work is on the use of contextual role knowledge for coreference resolution.	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	In this section, we describe how contextual role knowledge is represented and learned.	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphorâs caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples.	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	Contextual role knowledge provides evidence as to whether a candidate is a plausible antecedent for an anaphor.	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	The goal of our research was to explore the use of contextual role knowledge for coreference resolution.	0
(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	This result suggests that all of contextual role KSs can provide useful information for resolving anaphora.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	The focus of our work is on the use of contextual role knowledge for coreference resolution.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	The goal of our research was to explore the use of contextual role knowledge for coreference resolution.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	Table 1: Syntactic Seeding Heuristics BABARâs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.	0
Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	For example, co-occurring caseframes may reflect synonymy (e.g., â<patient> kidnappedâ and â<patient> abductedâ) or related events (e.g., â<patient> kidnappedâ and â<patient> releasedâ).	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	First, we describe how the caseframes are represented and learned.	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	In our situation, the competing hypotheses are the possible antecedents for an anaphor.	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	In this paper, Section 2 begins by explaining how contextual role knowledge is represented and learned.	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	In this situation, BABAR takes the conservative approach and declines to make a resolution.	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	For each domain, we created a blind test set by manually annotating 40 doc uments with anaphoric chains, which represent sets of m3 (S) = ) X â©Y =S 1 â ) m1 (X ) â m2 (Y ) m1 (X ) â m2 (Y ) (1) noun phrases that are coreferent (as done for MUC6 (MUC6 Proceedings, 1995)).	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	We considered using the MUC6 and MUC7 data sets, but their training sets were far too small to learn reliable co-occurrence statistics for a large set of contextual role relationships.	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	For example, the passive voice pattern â<subject> were kidnappedâ and the active voice pattern âkidnapped <direct object>â are merged into a single normalized pattern âkidnapped <patient>â.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, â<agent> kidnappedâ or âkidnapped <patient>â), and (2) predicate-argument relations associated with both verbs and nouns (e.g., âkidnapped for <np>â or âvehicle with <np>â).	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.	0
In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)	Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	Given a document to process, BABAR uses four modules to perform coreference resolution.	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	These systems rely on a training corpus that has been manually annotated with coreference links.	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	For example, management succession systems must distinguish between a person who is fired and a person who is hired.	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	The probabilities are incorporated into the DempsterShafer model using Equation 1.	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998).	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	Many researchers have developed coreference resolvers, so we will only discuss the methods that are most closely related to BABAR.	0
Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).	For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings 9 We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC4 or Reuters collections.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	But in most cases they can be used interchangably.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	The CFLex and CFNet knowledge sources provide positive evidence that a candidate NP and anaphor might be coreferent.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	Our coreference resolver also incorporates an existential noun phrase recognizer and a DempsterShafer probabilistic model to make resolution decisions.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	Its correct antecedent is âa revolverâ, which is extracted by the caseframe âkilled with <NP>â.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	The DempsterShafer rule for combining pdfs is: to {C}, meaning that it is 70% sure the correct hypothesis is C. The intersection of these sets is the null set because these beliefs are contradictory.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	(Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.	0
The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.	This step ensures that the most frequent terms for each domain are labeled (in case some of them are not in WordNet) and labeled with the sense most appropriate for the domain.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	1
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	The focus of our work is on the use of contextual role knowledge for coreference resolution.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	The goal of our research was to explore the use of contextual role knowledge for coreference resolution.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	The two knowledge sources that use semantic expectations, WordSemCFSem and CFSemCFSem, always return values of -1 or 0.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	So we decided to use semantic class information only to rule out candidates.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	Section 2.1 describes how BABAR generates training examples to use in the learning process.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	Therefore we opted to use the much 7 The DempsterShafer theory assumes that one of the hypotheses in Î¸ is correct, so eliminating all of the hypotheses violates this assumption.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.	0
However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).	Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	The focus of our work is on the use of contextual role knowledge for coreference resolution.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	The goal of our research was to explore the use of contextual role knowledge for coreference resolution.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	BABAR merely identifies caseframes that frequently co-occur in coreference resolutions.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	First, we describe how the caseframes are represented and learned.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	Our coreference resolver performed well in two domains, and experiments showed that each contextual role knowledge source contributed valuable information.	0
Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)	The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.	0
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	We evaluated BABAR on two domains: terrorism and natural disasters.	1
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	1
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.	0
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.	0
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate.	0
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	BABAR achieved recall in the 4250% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters.	0
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	Our coreference resolver performed well in two domains, and experiments showed that each contextual role knowledge source contributed valuable information.	0
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	For each resolution in the training data, BABAR also associates the co-referring expression of an NP with the NPâs caseframe.	0
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.	0
In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	1
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	This result suggests that all of contextual role KSs can provide useful information for resolving anaphora.	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	The gender, number, and scoping KSs eliminate candidates from consideration.	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	The focus of our work is on the use of contextual role knowledge for coreference resolution.	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	We also performed experiments to evaluate the impact of each type of contextual role knowledge separately.	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	The goal of our research was to explore the use of contextual role knowledge for coreference resolution.	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources.	0
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.	0
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	1
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.	1
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.	0
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too.	0
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	BABAR employs information extraction techniques to represent and learn role relationships.	0
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.	0
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	As a (crude) approximation, we normalize the extraction patterns with respect to active and passive voice and label those extractions as agents or patients.	0
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions.	0
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	This result suggests that all of contextual role KSs can provide useful information for resolving anaphora.	0
Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.	For example, the passive voice pattern â<subject> were kidnappedâ and the active voice pattern âkidnapped <direct object>â are merged into a single normalized pattern âkidnapped <patient>â.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, â<agent> kidnappedâ or âkidnapped <patient>â), and (2) predicate-argument relations associated with both verbs and nouns (e.g., âkidnapped for <np>â or âvehicle with <np>â).	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	So we decided to use semantic class information only to rule out candidates.	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	Many researchers have developed coreference resolvers, so we will only discuss the methods that are most closely related to BABAR.	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	For example, a personâs full name will match with just their last name (e.g., âGeorge Bushâ and âBushâ), and a company name will match with and without a corporate suffix (e.g., âIBM Corp.â and âIBMâ).	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	However, using the top-level semantic classes of WordNet proved to be problematic because the class distinctions are too coarse.	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	These systems rely on a training corpus that has been manually annotated with coreference links.	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	3 The Coreference Resolution Model.	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.	0
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	This result suggests that all of contextual role KSs can provide useful information for resolving anaphora.	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	The focus of our work is on the use of contextual role knowledge for coreference resolution.	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources.	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	For example, suppose the current model assigns a belief value of .60 to {A, B}, meaning that it is 60% sure that the correct hypothesis is either A or B. Then new evidence arrives with a belief value of .70 assigned 5 Initially there are no competing hypotheses because all hypotheses are included in Î¸ by definition.	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents.	0
Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.	This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	Table 1: Syntactic Seeding Heuristics BABARâs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	We evaluated BABAR on two domains: terrorism and natural disasters.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.	0
Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.	We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuterâs text collection8 that had a subject code corresponding to natural disasters.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.	1
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	Section 4 presents experimen tal results on two corpora: the MUC4 terrorism corpus, and Reuters texts about natural disasters.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	We evaluated BABAR on two domains: terrorism and natural disasters.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	Figure 4 shows the seven general knowledge sources (KSs) that represent features commonly used for coreference resolution.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	BABAR achieved recall in the 4250% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphorâs caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuterâs text collection8 that had a subject code corresponding to natural disasters.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	The semantic caseframe expectations are used in two ways.	0
Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.	Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	1
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	1
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	71 2 0.9 67 0.9 76 Table 3: Effects of combination using the confidence measure.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	The upper numbers are of the character- based and the lower ones, the subword-based.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.	1
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov).	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	While OOV recognition is very important in word segmentation, a higher IV rate is also desired.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	However, the R-iv rates were getting worse in return for higher R-oov rates.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	This approach is effective for performing desired segmentation based on usersâ requirements to R-oov and R-iv.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Table 1 shows the performance of the dictionary-based segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	A new OOV was thus created.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	In fact, there were no OOV recognition.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	It proves the proposed word-based IOB tagging was very effective.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.	1
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	In this section we introduce a confidence measure approach to combine the two results.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	The effect of the confidence measure is shown in Table 3, where we used Î± = 0.7 and confidence threshold t = 0.8.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	In Section 3.2 we will present the experimental segmentation results of the confidence measure approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	3.2 Effect of the confidence measure.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	1
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	We will use this advantage in the confidence measure approach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.	1
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	A confidence measure threshold, t, was defined for making a decision based on the value.	1
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Its calculation is defined as: C M(tiob |w) = Î±C Miob (tiob |w) + (1 â Î±)Î´(tw , tiob )ng (2) where tiob is the word wâs IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	The upper numbers are of the character- based and the lower ones, the subword-based.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Each subword is given a prior IOB tag, tw . C Miob (t|w), a ï£« M ï£«ï£¶ï£¶ confidence probability derived in the process of IOB tag exp ï£¬)' ï£¬)' Î»k fk (tiâ1 , ti , W ) + )' Âµk gk (ti , W )ï£·ï£· /Z, ï£¬ï£­ i=1 ï£¬ï£­ k k ï£·ï£¸ ï£·ï£¸ (1) ging, is defined as Z = )' T =t0 t1 Â·Â·Â·tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 Â·Â·Â·tM ,ti =t P(T |W, wi ) T =t 0 t1 Â·Â·Â· tM P ( T | W ) where we call fk (tiâ1 , ti , W ) bigram feature functions because the features trigger the previous observation tiâ1 where the numerator is a sum of all the observation sequences with word wi labeled as t. Î´(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	There are several steps to train a subword-based IOB tag- ger.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	However, the R-iv rates were getting worse in return for higher R-oov rates.	1
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	For AS corpus, âAdam Smithâ are two words in the training but become a one- word in the test, âAdamSmithâ.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	While OOV recognition is very important in word segmentation, a higher IV rate is also desired.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	This approach is effective for performing desired segmentation based on usersâ requirements to R-oov and R-iv.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Since the dictionary-based approach is a well-known method, we skip its technical descriptions.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	1
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	71 6 0.9 64 0.9 72 Table 2: Segmentation results by a pure subword-based IOB tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Taking the same example mentioned above, â (whole) (Beijing city)â is labeled as â (whole)/O (Beijing)/B (city)/Iâ in the subword-based tagging, where â (Beijing)/Bâ is labeled as one unit.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Under the scheme, each character of a word is labeled as âBâ if it is the first character of a multiple-character word, or âOâ if the character functions as an independent word, or âIâ otherwise.â For example, â (whole) (Beijing city)â is labeled as â (whole)/O (north)/B (capital)/I (city)/Iâ.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	The upper numbers and the lower numbers are of the character-based and the subword-based, respectively A S CI T Y U M SR P K U Ba ke off be st 0.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	We found that so far all the existing implementations were using character-based IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	For a character-based IOB tagger, there is only one possibility of re-segmentation.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	Its calculation is defined as: C M(tiob |w) = Î±C Miob (tiob |w) + (1 â Î±)Î´(tw , tiob )ng (2) where tiob is the word wâs IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.	0
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	Since the dictionary-based approach is a well-known method, we skip its technical descriptions.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	1
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	0 means the current word; â1, â2, the first or second word to the left; 1, 2, the first or second word to the right.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	2.1 Subword-based IOB tagging using CRFs.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	We found that so far all the existing implementations were using character-based IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.	1
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	It proves the proposed word-based IOB tagging was very effective.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.	1
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	1
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	0
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	This approach will be described in Section 2.2.	0
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	0 means the current word; â1, â2, the first or second word to the left; 1, 2, the first or second word to the right.	0
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.	0
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.	0
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	using the FMM, and then labeled with âIOBâ tags by the CRFs.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	1
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	For the bigram features, we only used the previous and the current observations, tâ1 t0 . As to feature selection, we simply used absolute counts for each feature in the training data.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	For AS corpus, âAdam Smithâ are two words in the training but become a one- word in the test, âAdamSmithâ.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	We found a speed up both in training and test.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	1
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	We regard the words in the subset as the subwords for the IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	The IOB tagging approach adopted in this work is not a new idea.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	For the character-based tagging, we used all the Chinese characters.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	2.1 Subword-based IOB tagging using CRFs.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Since the dictionary-based approach is a well-known method, we skip its technical descriptions.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	We think our proposed subword- based tagging played an important role for the good results.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	The segmentation results of the dictionary-based were re-segmented Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193â196, New York, June 2006.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	71 6 0.9 64 0.9 72 Table 2: Segmentation results by a pure subword-based IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.	1
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	There were no F-score changes for AS and PKU corpora, but the recall rates were improved.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	of the corpora and these scores, refer to (Emerson, 2005).	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	We achieved the highest F-scores in CITYU, PKU and MSR corpora.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	A new OOV was thus created.	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	It includes 298 words with 703 objective and 358 subjective WordNet senses.	1
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	Using MicroWNOp as training set and Wiebe as test set, we achieve an accuracy of 83.2%, which is similar to the results on the MicroWNOp dataset.	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008).	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006).	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous.	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	Method 2 (SL) reflects different degrees of preserving subjectivity.	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	We measure its accuracy for each part-of-speech in the MicroWNOp dataset.	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.	0
The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.	In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u, v) expresses the weight of an edge between two vertices.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework.	1
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	The SVM is also used as a baseline and its features are described in Section 4.3.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	WordNet relations are used to construct the.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Semi-supervised Mincuts always significantly outperform the corresponding SVM classifiers, regardless of whether the subjectivity list is used for setting edge weights.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	U . Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Our algorithm can be used on all WordNet senses whereas theirs is restricted to senses that have distributionally similar words in the MPQA corpus (see Section 2).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	The SVM is also used as a baseline and its features are described in Section 4.3.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	WordNet relations are used to construct the.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Semi-supervised Mincuts always significantly outperform the corresponding SVM classifiers, regardless of whether the subjectivity list is used for setting edge weights.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	U . Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%).	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework.	0
Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.	Our algorithm can be used on all WordNet senses whereas theirs is restricted to senses that have distributionally similar words in the MPQA corpus (see Section 2).	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut.	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	classification vertices in the Mincut approach.	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	To achieve the results of standard supervised approaches with our model, we need less than 20% of their training data.	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	W (S, T ) = ) uâS,vâT w(u, v)The formulation of our semi supervised Mincut for sense subjectivity classification involves the follow Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002).	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	We can also see that we achieve good results without using any other knowledge sources (setting LRNoSL).	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	Using MicroWNOp as training set and Wiebe as test set, we achieve an accuracy of 83.2%, which is similar to the results on the MicroWNOp dataset.	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	U . Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%).	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	subjective or both objective.3 An example here is the antonym relation, where two antonyms such as goodâmorally admirable and evil, wickedâmorally bad or wrong are both subjective.	0
Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.	The mincut algorithm will put vertices âreligiousâ and âscrupulousâ in the same cut (subjective category) as this results in the least cost 0.93 (ignoring the cost of assigning the unrelated sense of âflickerâ).	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.	1
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	In the following we will analyze the best minimum cut algorithm LRMSL in more detail.	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	(2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign.	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	We run our best Mincut LRMSL algorithm with two different settings on Wiebe.	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1).	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u, v) expresses the weight of an edge between two vertices.	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.	0
Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.	In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	Second, different word senses of a single word can actually be of different subjectivity or polarity.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	We compare to a baseline that assigns the most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&Mihalceaâs dataset respectively.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005).	0
The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).	Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	We supplement WordNet entries with information on the subjectivity of its word senses.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	Section 3 describes our proposed semi-supervised minimum cut framework in detail.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006).	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	In addition, we compare to a supervised classifier (see Lesk in Table 2) that just assigns each sense the subjectivity label of its most similar sense in the training data, using Leskâs similarity measure from Pedersenâs WordNet similarity package9.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	3.2 Why might Semi-supervised Minimum.	0
Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.	Second, Mincuts can be easily expanded into a semi-supervised framework (Blum and Chawla, 2001).	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	(2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations.	0
In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).	First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004).	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.	1
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	We use Lesk as it is one of the few measures applicable across all parts-of-speech.	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective.	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge.	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	WordNet relations are used to construct the.	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework.	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	The significantly better performance of semi-supervised mincuts holds across all parts-of- speech but the small set of adverbs, where there is no significant difference between the baseline, SVM and the Mincut algorithm.	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	We therefore simply scan each sense in A, and collect all senses related to it via one of the WordNet relations in Table 1.	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	We measure its accuracy for each part-of-speech in the MicroWNOp dataset.	0
For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.	Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	3.3 Formulation of Semi-supervised Mincuts.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	4.4 Semi-supervised Graph Mincuts.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	3.2 Why might Semi-supervised Minimum.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	The example in Figure 1 explains why semi- supervised Mincuts outperforms the supervised approach.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	We will also explore other semi- supervised algorithms.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	Second, Mincuts can be easily expanded into a semi-supervised framework (Blum and Chawla, 2001).	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	As can be seen, the semi-supervised Mincuts is consistently better than SVM.	0
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005).	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet.	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do.	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative).	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	We then count how often two senses related via a given relation have the same or a different subjectivity label.	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level.	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	Relati on Feature s (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense has to senses in the subjectiv e or objective part of the training set, respectiv ely.	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	Second, different word senses of a single word can actually be of different subjectivity or polarity.	0
The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.	Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective.	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.	1
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	Second, different word senses of a single word can actually be of different subjectivity or polarity.	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	We supplement WordNet entries with information on the subjectivity of its word senses.	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	Method 2 (SL) reflects different degrees of preserving subjectivity.	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006).	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	Moreover, the semi- supervised Mincut with only 200 labeled data items performs even better than SVM with 954 training items (78.9% vs 75.3%), showing that our semi- supervised framework allows for a training data reduction of more than 80%.	0
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	(2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	We supplement WordNet entries with information on the subjectivity of its word senses.	1
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	(2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositiveâhaving a positive electric charge;âprotons are positiveâ (objective) (2) plus, positiveâinvolving advantage or good; âa plus (or positive) factorâ (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	Second, different word senses of a single word can actually be of different subjectivity or polarity.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	We therefore simply scan each sense in A, and collect all senses related to it via one of the WordNet relations in Table 1.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.	0
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	We then count how often two senses related via a given relation have the same or a different subjectivity label.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	The error occurs with many speaking verbs, and each time, we trace it to a different rule.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	In Table 6 are shown feature weights learned for the word-context features.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	This makes sense: if a rule has a variable that can be filled by any English preposition, there is a risk that an incorrect preposition will fill it.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	6.2 Word context features.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	Word context During rule extraction, we retain word alignments from the training data in the extracted rules.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	(2008), we calculate the sentence Bï¬ï¥ïµ scores in (1), (2), and (3) in the context of some previous 1-best translations.	0
We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.	Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218â226, Boulder, Colorado, June 2009.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	First, we have shown that these new features can improve the performance even of top-scoring MT systems.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007).	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	When training over 10,000 features on a modest amount of data, we, like Watanabe et al.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	Therefore, the new features work to discourage these hypotheses.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	In Table 6 are shown feature weights learned for the word-context features.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	In this paper, we address these questions by experimenting with a large number of new features.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	Thus these features work together to attack a frequent problem that our target- syntax features also addressed.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.	0
Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).	Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.	1
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	(2007), did observe overfitting, yet saw improvements on new data.	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	(2007) that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data.	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	When training over 10,000 features on a modest amount of data, we, like Watanabe et al.	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	This seems to be because the is often part of a fixed phrase, such as the White House, and therefore comes naturally as part of larger phrasal rules.	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	Discount features Both of our systems calculate several features based on observed counts of rules in the training data.	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218â226, Boulder, Colorado, June 2009.	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	To remedy this problem, Chiang et al.	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	(2007).	0
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	Syntax-based rule extraction was performed on a 65 million word subset of the training data.	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position.	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	, and another thing is . . .	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	, ein, which are the 10-best translations according to each of: h(e) Â· w Bï¬ï¥ïµ(e) + h(e) Â· w âBï¬ï¥ïµ(e) + h(e) Â· w â¢ For each i, select an oracle translation: (1) 4.1 Target-side.	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	4 MERT: another thing is . . .	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	3 MERT: . . .	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	The IN node here is an overlap point between rules.	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest.	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	Adding the source-side and discount features to Hiero yields a +1.5 Bï¬ï¥ïµ improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bï¬ï¥ïµ improvement.	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	MIRA punishes the case where rules overlap with an IN (preposition) node.	0
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.	Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example.	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, fâ1) with fâ1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>.	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	In Table 6 are shown feature weights learned for the word-context features.	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features.	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	We now turn to features that make use of source-side context.	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	6.2 Word context features.	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	Word context During rule extraction, we retain word alignments from the training data in the extracted rules.	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, âs, said, parentheses, and quotes).	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.	0
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	Table 3 shows word-insertion feature weights.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	Thus they widen the advantage that syntax- based models have over other types of models.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	Thus these features work together to attack a frequent problem that our target- syntax features also addressed.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	1 It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: â¢ Select a batch of input sentences f1, . . .	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	These features are somewhat similar to features used by Watanabe et al.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	For our experiments, we used a 260 million word Chinese/English bitext.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	We add more than 250 features to improve a syntax- based MT systemâalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackâby +1.1 Bï¬ï¥ïµ. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 Bï¬ï¥ïµ improvement.	0
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	A third difficulty with Och et al.âs study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed.	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007).	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 Bï¬ï¥ïµ and+1.1 Bï¬ï¥ïµ, respectively.	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	In Table 6 are shown feature weights learned for the word-context features.	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	In this paper, we address these questions by experimenting with a large number of new features.	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights.	0
The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).	First, we have shown that these new features can improve the performance even of top-scoring MT systems.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	A third difficulty with Och et al.âs study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	We follow this approach here.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	(2007).	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	We chose a stopping iteration based on the Bï¬ï¥ïµ score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3â5 â0.73 3 +0.54 6â10 â0.64 4 +0.29 5+ â0.02 Table 2: Weights learned for discount features.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	Figure 1: Improved syntax-based translations due to MIRA-trained weights.	0
The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).	(2007), did observe overfitting, yet saw improvements on new data.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	1 It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	Table 3 shows word-insertion feature weights.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	Following Chiang et al.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	For our experiments, we used a 260 million word Chinese/English bitext.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	A third difficulty with Och et al.âs study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).	0
We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.	(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	6.2 Word context features.	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	Table 4 shows weights for rule-overlap features.	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	In Table 6 are shown feature weights learned for the word-context features.	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	The IN node here is an overlap point between rules.	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	Word context During rule extraction, we retain word alignments from the training data in the extracted rules.	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	, ein, which are the 10-best translations according to each of: h(e) Â· w Bï¬ï¥ïµ(e) + h(e) Â· w âBï¬ï¥ïµ(e) + h(e) Â· w â¢ For each i, select an oracle translation: (1) 4.1 Target-side.	0
Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.	Adding the source-side and discount features to Hiero yields a +1.5 Bï¬ï¥ïµ improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bï¬ï¥ïµ improvement.	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	(2008), we calculate the sentence Bï¬ï¥ïµ scores in (1), (2), and (3) in the context of some previous 1-best translations.	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	, ein, which are the 10-best translations according to each of: h(e) Â· w Bï¬ï¥ïµ(e) + h(e) Â· w âBï¬ï¥ïµ(e) + h(e) Â· w â¢ For each i, select an oracle translation: (1) 4.1 Target-side.	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	This is a fundamental question for the discipline, particularly as it pertains to improving the best systems we have.	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008).	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights.	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	We now turn to features that make use of source-side context.	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	We use separately- tunable features for each syntactic category.	0
5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.(2008b; 2009).	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	Our syntax-based system transforms source Chinese strings into target English syntax trees.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	Adding the source-side and discount features to Hiero yields a +1.5 Bï¬ï¥ïµ improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bï¬ï¥ïµ improvement.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	We now turn to features that make use of source-side context.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	These features are somewhat similar to features used by Watanabe et al.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	4.2 Source-side features.	0
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009).	Example 6 shows additionally that commas next to speaking verbs are now correctly deleted.	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights.	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	To get more general translation rules, we restructure our English training trees using expectation- maximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al.	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	This grammar can be parsed efficiently using cube pruning (Chiang, 2007).	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	â¢ Can large numbers of feature weights be learned efficiently and stably on modest amounts of data?	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	Since the interface between the trainer and the decoder is fairly simpleâfor each sentence, the decoder sends the trainer a forest, and the trainer returns a weight updateâit is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero.	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)	0
Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.	We see in both cases that one-count rules are strongly penalized, as expected.	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	Thus they widen the advantage that syntax- based models have over other types of models.	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	â¢ Can large numbers of feature weights be learned efficiently and stably on modest amounts of data?	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	A third difficulty with Och et al.âs study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes.	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218â226, Boulder, Colorado, June 2009.	0
Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).	We also notice that the-insertion rules sometimes have a good effect, as in the translation âin the bloom of youth,â but other times have a bad effect, as in âpeople seek areas of the conspiracy.â Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk.	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training.	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	The results (Table 1) show significant improvements in both systems ( p < 0.01) over already very strong MERT baselines.	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	3.3 MIRA training.	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	MIRA: the united states is waiting for israeli clarification on golan settlement plan 2 MERT: . . .	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	Syntax-based rule extraction was performed on a 65 million word subset of the training data.	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	Figure 1: Improved syntax-based translations due to MIRA-trained weights.	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level).	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	Discount features Both of our systems calculate several features based on observed counts of rules in the training data.	0
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences.	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	(2008), we calculate the sentence Bï¬ï¥ïµ scores in (1), (2), and (3) in the context of some previous 1-best translations.	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights.	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	â¢ For each i, select from the forest a set of hypothesis translations ei1, . . .	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level).	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	, ein, which are the 10-best translations according to each of: h(e) Â· w Bï¬ï¥ïµ(e) + h(e) Â· w âBï¬ï¥ïµ(e) + h(e) Â· w â¢ For each i, select an oracle translation: (1) 4.1 Target-side.	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data.	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	A rule like: IN(at) â zai will have feature rule-root-IN set to 1 and all other rule-root features set to 0.	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	We chose a stopping iteration based on the Bï¬ï¥ïµ score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3â5 â0.73 3 +0.54 6â10 â0.64 4 +0.29 5+ â0.02 Table 2: Weights learned for discount features.	0
The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.	Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	The problematic rules can even be non-lexical, e.g.: S(x0:NP-C x1:VP x2:, x3:NP-C x4:VP x5:.)	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	Our rule root features range over the original (non-split) nonterminal set; we have 105 in total.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	We add more than 250 features to improve a syntax- based MT systemâalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackâby +1.1 Bï¬ï¥ïµ. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 Bï¬ï¥ïµ improvement.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	In this section, we describe the new features introduced on top of our baseline systems.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	For our experiments, we used a 260 million word Chinese/English bitext.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	We chose a stopping iteration based on the Bï¬ï¥ïµ score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3â5 â0.73 3 +0.54 6â10 â0.64 4 +0.29 5+ â0.02 Table 2: Weights learned for discount features.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	A rule like: IN(at) â zai will have feature rule-root-IN set to 1 and all other rule-root features set to 0.	0
For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.	The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	When training over 10,000 features on a modest amount of data, we, like Watanabe et al.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	Syntax-based rule extraction was performed on a 65 million word subset of the training data.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	Discount features Both of our systems calculate several features based on observed counts of rules in the training data.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	Word context During rule extraction, we retain word alignments from the training data in the extracted rules.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	3.3 MIRA training.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, fâ1) with fâ1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data.	0
Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.	For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	In Table 6 are shown feature weights learned for the word-context features.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	Table 3 shows word-insertion feature weights.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	We chose a stopping iteration based on the Bï¬ï¥ïµ score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3â5 â0.73 3 +0.54 6â10 â0.64 4 +0.29 5+ â0.02 Table 2: Weights learned for discount features.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	6.2 Word context features.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	Table 4 shows weights for rule-overlap features.	0
First, we used features proposed by Chiang et al.(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).	(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	It turns out that in translation hypotheses that move âX saidâ or âX askedâ away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear.	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	All features are linearly combined and their weights are optimized using MERT.	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	A third difficulty with Och et al.âs study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	This minimization is performed by a variant of sequential minimal optimization (Platt, 1998).	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	(2008) introduce a structural distortion model, which we include in our experiment.	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	1 It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper.	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X â X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997).	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	So we can add a feature that penalizes any rule in which a PP dominates a VBN and NP-C.	0
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218â226, Boulder, Colorado, June 2009.	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	First, we have shown that these new features can improve the performance even of top-scoring MT systems.	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	All features are linearly combined and their weights are optimized using MERT.	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	What linguistic features can improve statistical machine translation (MT)?	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	The baseline model includes 12 features whose weights are optimized using MERT.	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	How did the various new features improve the translation quality of our two systems?	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	In Table 6 are shown feature weights learned for the word-context features.	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007).	0
In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.	Adding the source-side and discount features to Hiero yields a +1.5 Bï¬ï¥ïµ improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bï¬ï¥ïµ improvement.	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	In this paper, we address these questions by experimenting with a large number of new features.	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights.	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	â¢ Can large numbers of feature weights be learned efficiently and stably on modest amounts of data?	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	All features are linearly combined and their weights are optimized using MERT.	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	The baseline model includes 12 features whose weights are optimized using MERT.	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	A third difficulty with Och et al.âs study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.	0
This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.	the average life expectancy of canadaâs previous minority government is only 18 months . . .	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	â¢ Can large numbers of feature weights be learned efficiently and stably on modest amounts of data?	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	All features are linearly combined and their weights are optimized using MERT.	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	The baseline model includes 12 features whose weights are optimized using MERT.	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	A third difficulty with Och et al.âs study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights.	0
The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).	In this paper, we address these questions by experimenting with a large number of new features.	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).	1
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality.	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	How did the various new features improve the translation quality of our two systems?	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	We include two other techniques in our baseline.	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments.	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	We add more than 250 features to improve a syntax- based MT systemâalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackâby +1.1 Bï¬ï¥ïµ. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 Bï¬ï¥ïµ improvement.	0
We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).	The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data.	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.	1
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	(2007); here, we are incorporating some of its features directly into the translation model.	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	, ein, which are the 10-best translations according to each of: h(e) Â· w Bï¬ï¥ïµ(e) + h(e) Â· w âBï¬ï¥ïµ(e) + h(e) Â· w â¢ For each i, select an oracle translation: (1) 4.1 Target-side.	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest.	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	We can directly attack this problem by adding features counti that reward or punish rules seen i times, or features count[i, j] for rules seen between i and j times.	0
Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.	4 37.6ââ Sy nt ax MERT M I R A b a s el in e 2 5 b a s el in e 2 5 o v e rl a p 1 3 2 n o d e c o u n t 1 3 6 al l ta rg et si d e, di s c o u n t 2 8 3 38 .6 39.5 38 .5 39.8â 38 .7 39.9â 38.	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	features String-to-tree MT offers some unique levers to pull, in terms of target-side features.	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	Adding the source-side and discount features to Hiero yields a +1.5 Bï¬ï¥ïµ improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bï¬ï¥ïµ improvement.	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	We represent the translation model as a tree transducer (Knight and Graehl, 2005).	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	What linguistic features can improve statistical machine translation (MT)?	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	For example, there may be an tendency to generate too many determiners or past-tense verbs.	0
For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.	How did the various new features improve the translation quality of our two systems?	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.	1
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	We include two other techniques in our baseline.	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	Our syntax-based baseline includes the generative version of this model already.	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	(2008) introduce a structural distortion model, which we include in our experiment.	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	In this section, we describe the new features introduced on top of our baseline systems.	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	We represent the translation model as a tree transducer (Knight and Graehl, 2005).	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	The baseline model includes 12 features whose weights are optimized using MERT.	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	features String-to-tree MT offers some unique levers to pull, in terms of target-side features.	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25.	0
Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.	â or ââ = significantly better than MERT baseline ( p < 0.05 or 0.01, respectively).	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	In this paper, we address these questions by experimenting with a large number of new features.	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	For example, there may be an tendency to generate too many determiners or past-tense verbs.	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008).	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	A surprising number of the highest-weighted features have to do with translations of dates and bylines.	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	First, we have shown that these new features can improve the performance even of top-scoring MT systems.	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	these fixed phrases is a risk that the generative model is too inclined to take.	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	When training over 10,000 features on a modest amount of data, we, like Watanabe et al.	0
When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.	The feature class bad-rewrite comprises penalties for the following configurations based on our analysis of the tuning set: PP â VBN NP-C PP-BAR â NP-C IN VP â NP-C PP CONJP â RB IN Node count features It is possible that the decoder creates English trees with too many or too few nodes of a particular syntactic category.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	Therefore, the new features work to discourage these hypotheses.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	By contrast, we incorporate features directly into hierarchical and syntax- based decoders.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	We have described a variety of features for statistical machine translation and applied them to syntax- based and hierarchical systems.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	First, we have shown that these new features can improve the performance even of top-scoring MT systems.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	Thus these features work together to attack a frequent problem that our target- syntax features also addressed.	0
Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.	Further: â¢ Do syntax-based translation systems have unique and effective levers to pull when designing new features?	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	We have described a variety of features for statistical machine translation and applied them to syntax- based and hierarchical systems.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	By contrast, we incorporate features directly into hierarchical and syntax- based decoders.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	Adding the source-side and discount features to Hiero yields a +1.5 Bï¬ï¥ïµ improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bï¬ï¥ïµ improvement.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	6.1 Syntax features.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	Our syntax-based system transforms source Chinese strings into target English syntax trees.	0
Chiang et al.(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.	Further: â¢ Do syntax-based translation systems have unique and effective levers to pull when designing new features?	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X â X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997).	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: â¢ Select a batch of input sentences f1, . . .	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	We can directly attack this problem by adding features counti that reward or punish rules seen i times, or features count[i, j] for rules seen between i and j times.	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	The problematic rules can even be non-lexical, e.g.: S(x0:NP-C x1:VP x2:, x3:NP-C x4:VP x5:.)	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25.	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	For Sy st e m Training Fe at ur es # Tu ne Test Hi er o MERT M I R A b a s el in e 1 1 s y nt a x, di st o rt io n 5 6 s y nt a x, di st or ti o n, di s c o u n t 6 1 al l s o ur ce si d e, di s c o u n t 1 0 9 9 0 35 .4 36.1 35 .9 36.9â 36.	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	4 37.6ââ Sy nt ax MERT M I R A b a s el in e 2 5 b a s el in e 2 5 o v e rl a p 1 3 2 n o d e c o u n t 1 3 6 al l ta rg et si d e, di s c o u n t 2 8 3 38 .6 39.5 38 .5 39.8â 38 .7 39.9â 38.	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	â x3 x4 x2 x0 x1 x5 It is therefore difficult to come up with a straightforward feature to address the problem.	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	Sample syntax- based insertion rules are: NPB(DT(the) x0:NN) â x0 S(x0:NP-C VP(VBZ(is) x1:VP-C)) â x0 x1 We notice that our decoder, however, frequently fails to insert words like is and are, which often have no equivalent in the Chinese source.	0
A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.	Bad single-level rewrites Sometimes the decoder uses questionable rules, for example: PP(x0:VBN x1:NP-C) â x0 x1 This rule is learned from 62 cases in our training data, where the VBN is almost always the word given.	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	Adding the source-side and discount features to Hiero yields a +1.5 Bï¬ï¥ïµ improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bï¬ï¥ïµ improvement.	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents.	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	Our rule root features range over the original (non-split) nonterminal set; we have 105 in total.	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk.	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither systemâs basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings.	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	Word context During rule extraction, we retain word alignments from the training data in the extracted rules.	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008).	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols.	0
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	6 40.6ââ Table 1: Adding new features with MIRA significantly improves translation accuracy.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	But this task can be accomplished, in fact quite efficiently, by using the compile-replace technique.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix).	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	The extracted string is compiled into a. second network with the standard regular- expression compiler.	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.	1
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	In Arabic, for example, prefixes and suffixes attach to stems in the usual concatenative way, but stems themselves are formed by a process known informally as interdigitation; while in Malay, noun plurals are formed by a process known as full-stem reduplication.	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems.	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	4.2 Arabic Morphological.	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	Figure 15 presents the final result in which the second filler network in Figure 13 is merged with the intermediate result shown in Figure 14.	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	Most languages build words by simply stringing morphemes (prefixes, roots and suffixes) b i g b i g 0 +Adj g 0 0 +Comp e r together in strict orders.	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	ANALYSES ANALYZER/ GENERATOR WORDS Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words The basic claim or hope of the finite-state approach to natural-language morphology is that relations like that represented in Figure 3 are in fact regular relations, i.e. relations between two regular languages.	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	An example of interdigitation occurs with the Arabic stem katab, which means âwroteâ.	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	Finite-State Non-Concatenative Morphotactics	0
Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.	modeled in finite state terms as concatenation.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	The main difference between merge and classical intersection is in Conditions 1 and 2 below: 1.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	To take a simple non-linguistic example, Figure 8 represents a network that maps the regular expression a* into ^[a*^]; that is, the same expression enclosed between two special delimiters, ^[ and ^], that mark it as a regular- expression substring.Figure 9: After the Application of Compile Replace lower) of the network.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	In the Xerox calculus, expressions of the form A^n, where n is an integer, denote n concatenations of A. {abc} denotes the concatenation of symbols a, b, and c. We also employ ^[ and ^] as delimiter symbols around regular-expression substrings.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	Thus the overt plural of pelabuhan (âportâ), itself a derived form, is phonologically pelabuhanpelabuhan.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	That is, we redefine L as "^[" "[" L XX "]" "^" 2 "^]", and apply the compile-replace operation.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	When a ^[ is encountered, the algorithm looks for a closing ^] and extracts the path between the delimiters to be handled in a special way: 1.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	At this point the lower-side of L contains strings such as dogXXdogXX and madamXXmadamXX where XX is a specially introduced symbol to mark the middle (and the end) of each string.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired.	0
Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000).This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets &apos"&apos[ " and "A 1 " to mark the domain of re duplication.	We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.	1
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	But this task can be accomplished, in fact quite efficiently, by using the compile-replace technique.	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix).	0
Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.	The extracted string is compiled into a. second network with the standard regular- expression compiler.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	The same root ktb can combine with the template CVCVC and a different vocalization ui, signifying perfect aspect and passive voice, producing the stem kutib, which means âwas writtenâ.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	Finite-state morphology in the tradition of the Two-Level (Koskenniemi, 1983) and Xerox implementations (Karttunen, 1991; Karttunen, 1994; Beesley and Karttunen, 2000) has been very successful in implementing large-scale, robust and efficient morphological analyzergenerators for concatenative languages, includ ing the commercially important European languages and non-Indo-European examples like 1 I wish to thank Stuart Newton for this example.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	Finite-State Non-Concatenative Morphotactics	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	Similarly, the root ktb can combine with template CVVCVC and ui to produce kuutib, the root drs can combine with CVCVC and ui to form duris, and so forth.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	But some natural languages also exhibit non-concatenative morphotactics.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	ANALYSES ANALYZER/ GENERATOR WORDS Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words The basic claim or hope of the finite-state approach to natural-language morphology is that relations like that represented in Figure 3 are in fact regular relations, i.e. relations between two regular languages.	0
Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).	However, Koskenniemi himself understood that his initial implementation had significant limitations in handling non-concatenative morphotactic processes: âOnly restricted infixation and reduplication can be handled adequately with the present system.	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	Productive reduplication cannot be described by finite-state or even context-free formalisms.	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	Analyzer/Generator Malay and Indonesian are closely-related languages characterized by rich derivation and little or nothing that could be called inflection.	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	Some extensions or revisions will be necessary for an adequate description of languages possessing extensive infixation or reduplicationâ (Koskenniemi, 1983, 27).	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	Such âconcatenative morphotacticsâ can be impressively productive, especially in agglutinative languages like Aymara (Figure 11) or Turkish, and in agglutinative/polysynthetic languages like Inuktitut (Figure 2)(Mallon, 1999, 2).	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	In work more directly related to the current solution, it was Kataja and Koskenniemi (1988) who first demonstrated that Semitic (Akkadian) roots and patterns5 could be formalized as regular languages, and that the non-concatenative interdigitation of stems could be elegantly formalized as the intersection of those regular languages.	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	That is, they are copied into the result in their current position without consuming a filler symbol.	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	This provides the desired solution, still finite-state, for analyzing and generating full- stem reduplication in Malay.4 4 It is well-known (McCarthy and Prince, 1995) that reduplication can be a more complex phenomenon than it is in Malay.	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.	0
Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).	However, if the âbaseâ language L is finite, we can construct a finite-state network that encodes L and the reduplications of all the strings in L. On the assumption that there are only a finite number of words subject to reduplication (no free compounding), it is possible to construct a lexical transducer for languages 3 In the standard orthography, such reduplicated words are written with a hyphen, e.g. bagibagi, that we will ignore for this example.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	In the regular expression calculus there are several operators that involve concatenation.	1
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	To introduce the merge operation into the Xerox regular expression calculus we need to choose an operator symbol.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	To demonstrate the power of the compile- replace method, let us show how it can be applied to solve another âhardâ problem: identifying and extracting all the palindromes from a lexicon.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	In the Xerox calculus, expressions of the form A^n, where n is an integer, denote n concatenations of A. {abc} denotes the concatenation of symbols a, b, and c. We also employ ^[ and ^] as delimiter symbols around regular-expression substrings.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	We will show a simple and elegant way to do this with strictly finite-state operations.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	modeled in finite state terms as concatenation.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	There are two challenges in modeling natural language morphology: â¢ Morphotactics â¢ Phonological/Orthographical Alternations Finite-state morphology models both using regular expressions.	0
In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].	We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.	1
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	Figure 8: A Network with a Regular-Expression Substring on the Lower Side The application of the compile-replace algorithm to the lower side of the network eliminates the markers, compiles the regular expression a* and maps the upper side of the path to the language resulting from the compilation.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	The compile-replace algorithm, applied to the lower-side of this network, recognizes each individual delimited regular-expression substring like ^[{bagi}^2^], compiles it, and replaces it with the result of the compilation, here bagibagi.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	However, the general-purpose intersection algorithm would be expensive in any nontrivial application, and the interdigitation of stems represents a special case of intersection that we achieve in practice by a much more efficient finite-state algorithm called merge.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	We use the networks in Figure 13 to illustrate the effect of the merge algorithm.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	Like intersection, the merge algorithm operates by following two paths, one in the template network, the other in the filler network, and it constructs the corresponding single path in the result network.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	The upper side contains morphological information; the regular-expression operators appear only on the lower side and are not present in the final result.	0
The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.	The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.	1
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network.	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	Productive reduplication cannot be described by finite-state or even context-free formalisms.	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	The extracted string is compiled into a. second network with the standard regular- expression compiler.	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	We describe a technique, within the Xerox implementation of finite-state morphology, that corrects the limitations at the source, going beyond concatenation to allow the full range of finite-state operations to be used in morphotac- tic description.	0
Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.	We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.	1
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	Analyzer/Generator The current Arabic system has been described in some detail in previous publications (Beesley, 1996; Beesley, 1998a; Beesley, 1998b) and is available for testing on the Internet.7 The modification of the system to use the compile-replace algorithm has not changed the size or the behavior of the system in any way, but it has reduced the compilation time from hours to minutes.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	4.2 Arabic Morphological.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	Finite-state morphology in the tradition of the Two-Level (Koskenniemi, 1983) and Xerox implementations (Karttunen, 1991; Karttunen, 1994; Beesley and Karttunen, 2000) has been very successful in implementing large-scale, robust and efficient morphological analyzergenerators for concatenative languages, includ ing the commercially important European languages and non-Indo-European examples like 1 I wish to thank Stuart Newton for this example.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	Analyzer/Generator Malay and Indonesian are closely-related languages characterized by rich derivation and little or nothing that could be called inflection.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	ANALYSES ANALYZER/ GENERATOR WORDS Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words The basic claim or hope of the finite-state approach to natural-language morphology is that relations like that represented in Figure 3 are in fact regular relations, i.e. relations between two regular languages.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	However, Koskenniemi himself understood that his initial implementation had significant limitations in handling non-concatenative morphotactic processes: âOnly restricted infixation and reduplication can be handled adequately with the present system.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	An example of interdigitation occurs with the Arabic stem katab, which means âwroteâ.	0
A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).	In the most theory- and implementation-neutral form, morphological analysis and generation of written words can be modeled as a relation between the words themselves and analyses of those words.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	The source descriptions may also be written in higher-level notations (e.g. lexc (Karttunen, 1993), twolc (Karttunen and Beesley, 1992) and Replace Rules (Karttunen, 1995; Karttunen, 1996; Kempe and Karttunen, 1996)) that are simply helpful short- hands for regular expressions and that compile, using their dedicated compilers, into finite-state networks.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	In our opinion, there is no substantive difference from a computational point of view.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	Finite-State Non-Concatenative Morphotactics	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	modeled in finite state terms as concatenation.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	The lexicon description defines the morphotactics of the language, and the rules define the alternations.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network.	0
In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.	We describe a technique, within the Xerox implementation of finite-state morphology, that corrects the limitations at the source, going beyond concatenation to allow the full range of finite-state operations to be used in morphotac- tic description.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific	1
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	But this task can be accomplished, in fact quite efficiently, by using the compile-replace technique.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	To understand the general solution to full- stem reduplication using the compile-replace algorithm requires a bit of background.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	The Malay morphological analyzer prototype, written using lexc, Replace Rules, and compile-replace, implements approximately 50 different derivational processes, including pre- fixation, suffixation, prefix-suffix pairs (circum- fixation), reduplication, some infixation, and combinations of these processes.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	The source descriptions may also be written in higher-level notations (e.g. lexc (Karttunen, 1993), twolc (Karttunen and Beesley, 1992) and Replace Rules (Karttunen, 1995; Karttunen, 1996; Kempe and Karttunen, 1996)) that are simply helpful short- hands for regular expressions and that compile, using their dedicated compilers, into finite-state networks.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	gle one using the crossproduct operation.	0
The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).	The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine.	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	1
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Another alternative is to only consider unambiguous synonyms with a single supersense in WORDNET.	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Supersense Tagging of Unknown Nouns using Semantic Similarity	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	This same technique as is used in our approach to supersense tagging.	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Another approach is to use the scores returned by the similarity system.	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy.	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.	0
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.	1
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Finally, we plan to implement a supervised machine learner to replace the fall- back method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set.	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses.	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script.	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.	0
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD- NET 1.7.1 test set.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Bur- gun and Bodenreider (2001) compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available.	0
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.	1
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection.	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense.	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	Supersense Tagging of Unknown Nouns using Semantic Similarity	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	This is determined by looking at the frequency and number of attributes for the unknown word.	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	Lex-files form a set of coarse-grained sense distinctions within WORDNET.	0
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	The co-occurrence window was 500 words which was designed to approximate average document length.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	consistency when classifying similar words into categories.	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	This technique is similar to Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.	1
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Not surprisingly, the similarity system works better than the guessing rules if it has any information at all.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.	0
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	(2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the WORDNET synsets appear ing directly underneath.	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis.	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	3 M 4 9 1 M Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997).	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	C O R P U S D O C S . S E N T S . WO R D S B N C 4 1 2 4 6 . 2 M 1 1 4 M R C V1 8 0 6 7 9 1 8 . 1 M 2 0 7 M C S R -I I I 4 9 1 3 4 9 9 . 3 M 2 2 6 M NA N T C 9 3 0 3 6 7 2 3.	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	We have developed a 2 billion word corpus, shallow- parsed with a statistical NLP pipeline, which is by far the Table 2: Example nouns and their supersenses largest NLP processed corpus described in published re search.	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources.	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	Supersense Tagging of Unknown Nouns using Semantic Similarity	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise.	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above.	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	1
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	This technique is similar to Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Hearst and SchuÂ¨ tze (1993) flatten WORDNET into 726 categories using an algorithm which attempts to minimise the variance in category size.	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Lex-files form a set of coarse-grained sense distinctions within WORDNET.	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself.	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.	0
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	1
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003).	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Some examples from the test sets are given in Table 2 with their supersenses.	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1.	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Again these options have been considered below.	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	Once all five passes have been completed this association list contains all of the noun- modifier/verb pairs which have been extracted from the sentence.	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORD- NET and the construction of new wordnets using the concept structure from English.	0
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	In fact, these experiments are so quick to run we have been able to exhaustively test many combinations of these parameters.	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	How are words with multiple supersenses handled?	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	One solution would be to take the intersection between vectors across words for each supersense (i.e. to find the common contexts that these words appear in).	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	consistency when classifying similar words into categories.	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy.	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise.	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	The co-occurrence window was 500 words which was designed to approximate average document length.	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	The other measure they found to be successful was the entropy of the conditional distribution of surrounding words given the noun.	0
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	The most obvious solution is to sum the context vectors across the words which have each supersense.	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy.	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Supersense Tagging of Unknown Nouns using Semantic Similarity	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	This same technique as is used in our approach to supersense tagging.	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.	0
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in.	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	1
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1.	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Our approach to vector-space similarity is based on the SEXTANT system described in Grefenstette (1994).	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstetteâs table-based results, i.e. the SEXTANT always prefers noun attachment.	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	This same technique as is used in our approach to supersense tagging.	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.	0
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Supersense Tagging of Unknown Nouns using Semantic Similarity	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	These categories are used to label paragraphs with topics, effectively repeating Yarowskyâs (1992) experiments using the their categories rather than Rogetâs thesaurus.	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	SEXTANT extracts relation tuples (w, r, wt ) for each noun, where w is the headword, r is the relation type and wt is the other word.	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.	0
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	Not surprisingly, the similarity system works better than the guessing rules if it has any information at all.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	However, when a preposition exists in the mention, its head word is set as the last word before the preposition.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes âBased-Inâ, âLocatedâ and âResidenceâ under the type âATâ, which are difficult even for human experts to differentiate.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	Normally, the above overlap features are too general to be effective alone.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall.	0
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	1
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	1
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	It also shows that feature-based methods dramatically outperform kernel methods.	0
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.	0
Most of the features used in our system are based on the work in (Zhou et al., 2005).	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	Section 3 and Section 4 describe our approach and various features employed respectively.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	This category of features concerns about the information inherent only in the full parse tree.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	The rest of this paper is organized as follows.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	It only improves the F-measure by 0.8 due to the recall increase.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	Dependency tree th parse trees.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	This suggests that relation detection is critical for relation extraction.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	1
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	It also shows that feature-based methods dramatically outperform kernel methods.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	1
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	Dependency tree th parse trees.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	This category of features concerns about the information inherent only in the full parse tree.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0
 dependency kernel Zhou et al.(2005)	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	1
 dependency kernel Zhou et al.(2005)	Dependency tree th parse trees.	0
 dependency kernel Zhou et al.(2005)	It also shows that feature-based methods dramatically outperform kernel methods.	0
 dependency kernel Zhou et al.(2005)	Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.	0
 dependency kernel Zhou et al.(2005)	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	0
 dependency kernel Zhou et al.(2005)	â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.	0
 dependency kernel Zhou et al.(2005)	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0
 dependency kernel Zhou et al.(2005)	The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head.	0
 dependency kernel Zhou et al.(2005)	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
 dependency kernel Zhou et al.(2005)	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	1
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	Section 3 and Section 4 describe our approach and various features employed respectively.	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	Exploring Various Knowledge in Relation Extraction	0
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype âFounderâ under the type âROLEâ.	1
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set.	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first.	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998).	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.	0
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	â¢ Chunking features are very useful.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.	0
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC).	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: â¢ WM1: bag-of-words in M1 â¢ HM1: head word of M1 3 In ACE, each mention has a head annotation and an.	0
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	1
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	We use the official ACE corpus from LDC.	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	Table 5 separates the performance of relation detection from overall performance on the testing set.	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	1
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	We use the official ACE corpus from LDC.	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	We use the official ACE corpus from LDC.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser (Collins 1999) is employed for full parsing.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Table 5 separates the performance of relation detection from overall performance on the testing set.	0
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	It also shows that feature-based methods dramatically outperform kernel methods.	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	1
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1
Zhou et al.(2005) explore various features in relation extraction using SVM.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	Exploring Various Knowledge in Relation Extraction	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	Section 3 and Section 4 describe our approach and various features employed respectively.	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	This suggests that relation detection is critical for relation extraction.	0
Zhou et al.(2005) explore various features in relation extraction using SVM.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	However, The remaining words that do not have above four classes are manually classified.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	Mentions have three levels: names, nomial expressions or pronouns.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	Two features are defined to include this information: â¢ ET1Country: the entity type of M1 when M2 is a country name â¢ CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	This category of features concerns about the information inherent only in the full parse tree.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	â¢ Chunking features are very useful.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	Most of the chunking features concern about the head words of the phrases between the two mentions.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	During development, 155 of 674 documents in the training set are set aside for fine-tuning the system.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	set as the last word of the mention.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.	1
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	explicit relations occur in text with explicit evidence suggesting the relationships.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	This may be due to the fact that most of relations in the ACE corpus are quite local.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation.	0
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	The testing set is held out only for final evaluation.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser (Collins 1999) is employed for full parsing.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	In this way, we can separately evaluate the contributions of base phrase chunking and full parsing.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	â¢ Chunking features are very useful.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query âWho is the president of the United States?â.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	Last, effective ways need to be explored to incorporate information embedded in the full Collins M.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	This suggests that relation detection is critical for relation extraction.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	Table 5 separates the performance of relation detection from overall performance on the testing set.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: â¢ WM1: bag-of-words in M1 â¢ HM1: head word of M1 3 In ACE, each mention has a head annotation and an.	1
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	However, The remaining words that do not have above four classes are manually classified.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	Most of the chunking features concern about the head words of the phrases between the two mentions.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	â¢ WM2: bag-of-words in M2 â¢ HM2: head word of M2 â¢ HM12: combination of HM1 and HM2 â¢ WBNULL: when no word in between â¢ WBFL: the only word in between when only one word in between â¢ WBF: first word in between when at least two words in between â¢ WBL: last word in between when at least two words in between â¢ WBO: other words in between except first and last words when at least three words in between â¢ BM1F: first word before M1 â¢ BM1L: second word before M1 â¢ AM2F: first word after M2 â¢ AM2L: second word after M2 4.2 Entity Type.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees.	0
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	Mentions have three levels: names, nomial expressions or pronouns.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	1
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Dependency tree th parse trees.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head.	0
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	Last, effective ways need to be explored to incorporate information embedded in the full Collins M.	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	1
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	1
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	It also shows that feature-based methods dramatically outperform kernel methods.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	6 4 5 . 9 5 8 . 2 Locate d 2 4 1 1 3 2 1 2 0 52.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	# of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3: Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	4 5 4 . 8 5 3 . 5 Reside nce 6 6 1 9 9 67.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	Moreover, we also consider the phrase path in between.	0
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Table 4 separately measures the performance of different relation types and major subtypes.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	This suggests that relation detection is critical for relation extraction.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	It is not surprising that the performance on the relation type âNEARâ is low because it occurs rarely in both the training and testing data.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Exploring Various Knowledge in Relation Extraction	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	Table 4 also indicates the low performance on the relation type âATâ although it frequently occurs in both the training and testing data.	0
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	1
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	It achieves 52.8 F- measure on the 24 ACE relation subtypes.	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	0
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.	1
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.	1
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.	0
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	It also includes flags indicating whether the two mentions are in the same NP/PP/VP.	0
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	explicit relations occur in text with explicit evidence suggesting the relationships.	0
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.	0
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.	0
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	We use the official ACE corpus from LDC.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	Others like âPART.Subsidaryâ and âSOCIAL.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0
In the future, we would like to use more effective feature sets Zhou et al.(2005)	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	1
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	It also shows that feature-based methods dramatically outperform kernel methods.	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.	1
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	â¢ Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype âROLE.Citizen-Ofâ from âROLE.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	Two features are defined to include this information: â¢ ET1Country: the entity type of M1 when M2 is a country name â¢ CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	This trigger word list is first gathered from WordNet by checking whether a word has the semantic class âperson|â¦|relativeâ.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.	0
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Section 2 presents related work.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	1
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	It also shows that feature-based methods dramatically outperform kernel methods.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	1
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	1
Zhou et al.(2005) tested their system on the ACE 2003 data;.	It is not surprising that the performance on the relation type âNEARâ is low because it occurs rarely in both the training and testing data.	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype âFounderâ under the type âROLEâ.	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	Table 4 also indicates the low performance on the relation type âATâ although it frequently occurs in both the training and testing data.	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships.	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	It also shows that our system performs best on the subtype âSOCIAL.Parentâ and âROLE.	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	During development, 155 of 674 documents in the training set are set aside for fine-tuning the system.	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection.	0
Zhou et al.(2005) tested their system on the ACE 2003 data;.	It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	However, The remaining words that do not have above four classes are manually classified.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	However, this paper only uses the binary-class version.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	This may be due to the fact that most of relations in the ACE corpus are quite local.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Therefore, they are HM12+M1>M2; 4) HM12+M1<M2.	0
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	1
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype âFounderâ under the type âROLEâ.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Most of the chunking features concern about the head words of the phrases between the two mentions.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	This may be due to the fact that most of relations in the ACE corpus are quite local.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships.	0
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	Section 3 and Section 4 describe our approach and various features employed respectively.	0
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	1
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	0
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	Table 2 also measures the contributions of different features by gradually increasing the feature set.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	We use the official ACE corpus from LDC.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.	0
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	However, The remaining words that do not have above four classes are manually classified.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related.	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	1
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Mentions have three levels: names, nomial expressions or pronouns.	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	However, The remaining words that do not have above four classes are manually classified.	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Head-driven statistical models for natural language parsing.	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	1
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	This category of features concerns about the information inherent only in the full parse tree.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	Moreover, we only apply the simple linear kernel, although other kernels can peform better.	0
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	The rest of this paper is organized as follows.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	It also shows that feature-based methods dramatically outperform kernel methods.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	For details about SVMLight, please see http://svmlight.joachims.org/	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	Normally, the above overlap features are too general to be effective alone.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	For example, the head word of the name mention âUniversity of Michiganâ is âUniversityâ.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	For example, we want to determine whether a person is at a location, based on the evidence in the context.	0
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	In this paper, we separate the features of base phrase chunking from those of full parsing.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	We use the official ACE corpus from LDC.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	For each pair of mentions3, we compute various lexical, syntactic and semantic features.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	0
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Last, effective ways need to be explored to incorporate information embedded in the full Collins M.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Mentions have three levels: names, nomial expressions or pronouns.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	However, The remaining words that do not have above four classes are manually classified.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.	0
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.).	1
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	Therefore, we must extend SVMs to multi-class (e.g. K) such as the ACE RDC task.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	For example, the head word of the name mention âUniversity of Michiganâ is âUniversityâ.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	For example, we want to determine whether a person is at a location, based on the evidence in the context.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.	0
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998).	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: Eâ = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success.	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	3.3 BiTAM3: Word-level Admixture.	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	(6) contains only one word: âNullâ, and the alignment link a is no longer a hidden variable.	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!ï¿½:South Korean).	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	In the first BiTAM model, we assume that topics are sampled at the sentence-level.	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	Overall, BiTAM models achieve performances close to or higher than HMM, using only a very simple IBM1 style alignment model.	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model.	0
There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).	The co-occurrence count, however, only favors âHanGuoâ, and this can easily dominate the decisions of IBM and HMM models due to their ignorance of the topical context.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.	1
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	Given a specific topic-weight vector Î¸d for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	In statistical machine translation, one typically uses parallel data to identify entities such as âword-pairâ, âsentence-pairâ, and âdocument- pairâ.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	Each document- pair is represented as a random mixture of latent topics.	0
Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.	In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair.	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.	1
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	(a) Sample sentence-length Jn from Poisson(Î´); (b) Sample a topic zdn from a Multinomial(Î¸d ); (c) Sample ej from a monolingual model p(ej );(d) Sample each word alignment link aj from a uni form model p(aj ) (or an HMM); (e) Sample each fj according to a topic-specific graphical model representation for the BiTAM generative scheme discussed so far.	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I Î² e I a Î± Î¸ z f J B N M Î± Î¸ z a a f J B Î± Î¸ z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs.	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: Eâ = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	This is realized by using the same topic-weight vector Î¸d and the same topic indicator zdn sampled according to Î¸d, as described in Â§3.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)).	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	(a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).	0
Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.	Given the parameters Î±, B, and the English part E, the joint conditional distribution of the topic-weight vector Î¸, the topic indicators z, the alignment vectors A, and the document F can be written as: Î( K Î±k ) p(Î¸|Î±) = k=1 Î¸Î±1 â1 Â· Â· Â· Î¸Î±K â1 , (3) p(F,A, Î¸, z|E, Î±, B) = k=1 Î(Î±k ) N (4) where the hyperparameter Î± is a K -dimension vector with each component Î±k >0, and Î(x) is the Gamma function.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	For each document-pair, a K -dimensional Dirichlet random variable Î¸d, referred to as the topic-weight vector of the document, can take values in the (K â1)-simplex following a probability density: to the proposed distributions.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	Following a variational EM scheme (Beal and Ghahramani, 2002), we estimate the model parameters Î± and B in an unsupervised fashion.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	Given the parameters Î±, B, and the English part E, the joint conditional distribution of the topic-weight vector Î¸, the topic indicators z, the alignment vectors A, and the document F can be written as: Î( K Î±k ) p(Î¸|Î±) = k=1 Î¸Î±1 â1 Â· Â· Â· Î¸Î±K â1 , (3) p(F,A, Î¸, z|E, Î±, B) = k=1 Î(Î±k ) N (4) where the hyperparameter Î± is a K -dimension vector with each component Î±k >0, and Î(x) is the Gamma function.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	We performed a tenfold cross- validation, and a setting of three-topic is chosen for both the small and the large training data sets.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	We train, for comparison, IBM1&4 and HMM models with 8 iterations of IBM1, 7 for HMM and 3 for IBM4 (18h743) with Null word and a maximum fertility of 3 for ChineseEnglish.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	In the first BiTAM model, we assume that topics are sampled at the sentence-level.	0
We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).	For the large training data setting, we collected additional document- pairs from FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), and Xinhua News (LDC2002E18, document boundaries are kept in our sentence-aligner (Zhao and Vogel, 2002)).	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	This is because BDA partially utilizes similar heuristics on the approximated posterior matrix {Ïdnji} instead of di rect operations on alignments of two directions in the heuristics of Refined.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality, Bleu (Papineni et al., 2002) and its variation of NIST scores are reported.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	Shown in Table 4 are results for the small- data track; the large-data track results are in Table 5.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	Both use the posterior mean of the alignment indicators adnji, captured by what we call the poste rior alignment matrix Ï â¡ {Ïdnji}.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	A close look at the three BiTAMs does not yield significant difference.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	The translation lexicons of Bf,e,k are initialized uniformly in our previous experiments.	0
The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).	33 functional words were removed to highlight the main content of each topic.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	To reduce the data sparsity problem, we introduce two remedies in our models.	1
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	Efficient variational approximation algorithms are designed for inference and parameter estimation.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	We use the lexicons from IBM Model-4 to initialize Bf,e,k to boost the BiTAM models.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	We use TIDES Evalâ02 CE test set as development data to tune the decoder parameters; the Evalâ03 data (919 sentences) is the unseen data.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	To estimate B, Î² (for BiTAM2) and Î±, at most eight variational EM iterations are run on the training data.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	where the Dirichlet parameter Î³, the multinomial parameters (Ï1, Â· Â· Â· , Ïn), and the parameters (Ïn1, Â· Â· Â· , ÏnJn ) are known as variational param eters, and can be optimized with respect to the KullbackLeibler divergence from q(Â·) to the original p(Â·) via an iterative fixed-point algorithm.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	We use the end-user ter minology for source and target languages.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	Under a non-symmetric Dirichlet prior, hyperparameter Î± is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1.	0
Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.	In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	Similar to IBM models, âNullâ word is used for the source words which have no translation counterparts in the target language.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	The co-occurrence count, however, only favors âHanGuoâ, and this can easily dominate the decisions of IBM and HMM models due to their ignorance of the topical context.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	5.3 Topic-Specific Translation.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	The translation lexicons of Bf,e,k are initialized uniformly in our previous experiments.	0
Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.	These models enable word- alignment process to leverage topical contents of document-pairs.	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments.	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels.	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969â976, Sydney, July 2006.	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.	0
Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.	Given a specific topic-weight vector Î¸d for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics.	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent.	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	This is realized by using the same topic-weight vector Î¸d and the same topic indicator zdn sampled according to Î¸d, as described in Â§3.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)).	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	(a) Sample sentence-length Jn from Poisson(Î´); (b) Sample a topic zdn from a Multinomial(Î¸d ); (c) Sample ej from a monolingual model p(ej );(d) Sample each word alignment link aj from a uni form model p(aj ) (or an HMM); (e) Sample each fj according to a topic-specific graphical model representation for the BiTAM generative scheme discussed so far.	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	A unique normalized and real-valued vector Î¸, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions.	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model.	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {Ïdnk }.	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	In general, the monolingual model for English can also be a rich topic-mixture.	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	Lexicons The topic-specific lexicons Bk are smaller in size than IBM1, and, typically, they contain topic trends.	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).	0
â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.	Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: Eâ = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	5.3 Topic-Specific Translation.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	The proposed models significantly improve the alignment accuracy and lead to better translation qualities.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	The word level translation lexicon probabil- r ( (5) ities are topic-specific, and they are parameterized by the matrix B = {Bk }.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.	0
To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.	Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models.	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	For example, the word shot in âIt was a nice shot.â should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	$); South Korean occurs more often with economics and is translated as âHanGuoâ(li!	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	n=1 j=1 Unlike BiTAM1, where the information observed in ei is indirectly passed to z via the node of fj and the hidden variable aj , in BiTAM2, the topics of corresponding English and French sentences are also strictly aligned so that the information observed in ei can be directly passed to z, in the hope of finding more accurate topics.	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	For example, in our training data, North Korean is usually related to politics and translated into âChaoXianâ (Ji!	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances.	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {Ïdnk }.	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	We use TIDES Evalâ02 CE test set as development data to tune the decoder parameters; the Evalâ03 data (919 sentences) is the unseen data.	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations.	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	Assuming sentences are one-to-one correspondent, a document-pair has a sequence of N parallel sentence-pairs {(fn, en)}, where (fn, en) is the ntth parallel sentence-pair.	0
Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).	Monolingual topics learned by BiTAMs are, roughly speaking, fuzzy especially when the number of topics is small.	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.	1
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels.	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair.	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	We use the lexicons from IBM Model-4 to initialize Bf,e,k to boost the BiTAM models.	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments.	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.	0
Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.	Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I Î² e I a Î± Î¸ z f J B N M Î± Î¸ z a a f J B Î± Î¸ z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs.	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.	1
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003).	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	Topic A is about Us-China economic relationships; Topic B relates to Chinese companiesâ merging; Topic C shows the sports of handicapped people.The interpolation smoothing in Â§4.2 is effec tive, and it gives slightly better performance than Laplace smoothing over different number of topics for BiTAM1.	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled.	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	In this paper we investigate three instances of the BiTAM model, They are data-driven and do not need handcrafted knowledge engineering.	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	Similar to IBM models, âNullâ word is used for the source words which have no translation counterparts in the target language.	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object.	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	This is one way of applying the proposed BiTAM models into current state-of-the-art SMT systems for further improvement.	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels.	0
To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).	To further evaluate our BiTAM models, word alignments are used in a phrase-based decoder for evaluating translation qualities.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.	1
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	The proposed models significantly improve the alignment accuracy and lead to better translation qualities.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	2.1 Baseline: IBM Model-1.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	We evaluate BiTAM models on the word alignment accuracy and the translation quality.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1.	0
In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.	In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.	0
A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).	Second: interpolation smoothing.	1
A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).	Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = Î»Bf,e,k +(1âÎ»)p(f |e).	1
A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).	In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.	0
A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).	1, p(f |e) is learned via IBM1; Î» is estimated via EM on held out data.	0
A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).	However, the interpolation leverages the competing baseline lexicon, and this can blur the evaluations of BiTAMâs contributions.	0
A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).	The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {Ïdnk }.	0
A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).	For each document-pair, a K -dimensional Dirichlet random variable Î¸d, referred to as the topic-weight vector of the document, can take values in the (K â1)-simplex following a probability density: to the proposed distributions.	0
A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).	Topic A is about Us-China economic relationships; Topic B relates to Chinese companiesâ merging; Topic C shows the sports of handicapped people.The interpolation smoothing in Â§4.2 is effec tive, and it gives slightly better performance than Laplace smoothing over different number of topics for BiTAM1.	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.	1
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	Confusion networks are generated by choosing one hypothesis as the âskeletonâ, and other hypotheses are aligned against it.	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	The average TER score was computed between each systemâs -best hypothesis and all other hypotheses.	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	System combination has been shown to improve classification performance in various tasks.	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	Improved Word-Level System Combination for Machine Translation	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems.	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen.	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	This guarantees that the best path will not be found from a network generated for a system with zero weight.	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.	0
In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	Recently, confusion network decoding has been applied in machine translation system combination.	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	Section 3 describes confusion network decoding for MT system combination.	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	Recently, confusion network decoding has been applied in machine translation system combination.	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994).	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	The following three rows correspond to the improved confusion network decoding with different optimization metrics.	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	This paper describes an improved confusion network based method to combine outputs from multiple MT systems.	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	The improved confusion network decoding approach allows arbitrary features to be used in the combination.	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.	0
Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).	The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.	1
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	The average TER score was computed between each systemâs -best hypothesis and all other hypotheses.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	The prior probabilities are estimated by viewing the negative average TER scores between the skeleton and other hypotheses as log-probabilities.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	Confusion networks are generated by choosing one hypothesis as the âskeletonâ, and other hypotheses are aligned against it.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	The other hypotheses are aligned against the skeleton.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.	0
Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.	Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	The minimum translation edit alignment is usually found through a beam search.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312â319, Prague, Czech Republic, June 2007.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	The only difference to word error rate is that the TER allows shifts.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	Hypothesis alignment is also very important in confusion network generation.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	ric since it is based on the rate of edits required to transform the hypothesis into the reference.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	METEOR is based on the weighted harmonic mean of the precision and recall measured on uni- gram matches as follows (2) where is the total number of unigram matches, is the hypothesis length, is the reference length and is the minimum number of -gram matches that covers the alignment.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score.	0
Bangalore et al.(2001) used a WER based alignment and Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.	Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	Different alignment methods yield different confusion networks.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	System combination has been shown to improve classification performance in various tasks.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	There are several problems with the previous confusion network decoding approaches.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	Better alignment methods which take synonymy into account should be investigated.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.	0
In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.	However, the optimization experiments showed that the best performance was obtained by having a smoothing factor of 1 which is equivalent to the original priors.	0
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	The final arcs have a probability of one.	1
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.	1
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	All confusion network are connected to a common end node with NULL arcs.	1
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one.	0
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty.	0
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	Language model expansion and re-scoring may help by increasing the probability of more grammatical hypotheses in decoding.	0
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.	0
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.	0
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.	0
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	The number of paths through a confusion network grows exponentially with the number of nodes.	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	Each arc represents an alternative word at that.	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005).	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	System weights may be used to assign a system specific confidence on each word in the network.	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994).	0
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).	Different alignment methods yield different confusion networks.	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007).	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	Section 3 describes confusion network decoding for MT system combination.	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	Recently, confusion network decoding has been applied in machine translation system combination.	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	System weights may be used to assign a system specific confidence on each word in the network.	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	Improved Word-Level System Combination for Machine Translation	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.	0
Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.	System combination has been shown to improve classification performance in various tasks.	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.	1
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	For example using âcat sat the matâ as the skeleton, aligning âcat sitting on the matâ and âhat on a matâ against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word.	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	The size of the test set may influence the quality of these alignments.	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	The alignments from TER are consistent as they do not depend on the test set size.	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments.	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	Also, the METEOR score using the METEOR optimized weights is very high.	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991).	0
We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.	First, each dimension is optimized using a grid-based line minimization algorithm.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	Recently, confusion network decoding has been applied in machine translation system combination.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	The only difference to word error rate is that the TER allows shifts.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312â319, Prague, Czech Republic, June 2007.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	The minimum translation edit alignment is usually found through a beam search.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	System combination has been shown to improve classification performance in various tasks.	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	The same Powellâs method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in (Och, 2003).	0
The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.	For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	Confusion network decoding usually requires finding the path with the highest confidence in the network.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	The only difference to word error rate is that the TER allows shifts.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	It would be interesting to know which tuning metric results in the best translations in terms of human judgment.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	A shift of a sequence of words is counted as a single edit.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	Another approach is to combine outputs from a few highly specialized classifiers.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	The TER score may also be higher than 1 due to insertions.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	The second term is a fragmentation penalty which penalizes the harmonic mean by a factor of up to when ; i.e., there are no matching -grams higher than . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	The number of paths through a confusion network grows exponentially with the number of nodes.	0
In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.	For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	First, each dimension is optimized using a grid-based line minimization algorithm.	1
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006).	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	The size of the test set may influence the quality of these alignments.	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	The alignments from TER are consistent as they do not depend on the test set size.	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments.	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	This paper describes an improved confusion network based method to combine outputs from multiple MT systems.	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	A more efficient algorithm for log-linear models was also proposed.	0
ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).	All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.	1
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991).	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	This allows the addition of language model scores by expanding the lattices or re-scoring -best lists.	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR.	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used.	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	The second set of weights is used to find the final -best from the re-scored -best list.	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	As expected, the scores on the metric used in tuning are the best on that metric.	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	Again, the best scores on each metric are obtained by the combination tuned for that metric.	0
Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.	The best results on a given metric are again obtained by the combination optimized for the corresponding metric.	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.	1
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output.	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.	0
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	This guarantees that the best path will not be found from a network generated for a system with zero weight.	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.	1
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	The number of paths through a confusion network grows exponentially with the number of nodes.	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	Recently, confusion network decoding has been applied in machine translation system combination.	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.	0
Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).	Section 3 describes confusion network decoding for MT system combination.	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007).	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	Section 3 describes confusion network decoding for MT system combination.	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	System weights may be used to assign a system specific confidence on each word in the network.	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	Recently, confusion network decoding has been applied in machine translation system combination.	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.	0
@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.	Either votes or some form of confidences are assigned to each word in the network.	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision.	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994).	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems.	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	Section 3 describes confusion network decoding for MT system combination.	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	The authors would like to thank ISI and University of Edinburgh for sharing their MT system outputs.	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	Powellâs method (Brent, 1973) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set.	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese.	0
If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations.For example, Rosti et al.(2007) report such an effect.	The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	Combination of speech recognition outputs is an example of this approach (Fiscus, 1997).	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	This paper describes an improved confusion network based method to combine outputs from multiple MT systems.	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	The improved confusion network decoding approach allows arbitrary features to be used in the combination.	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	Section 3 describes confusion network decoding for MT system combination.	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	Recently, confusion network decoding has been applied in machine translation system combination.	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005).	0
In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).	Then, a new direction based on the changes in the objective function is estimated to speed up the search.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	The average TER score was computed between each systemâs -best hypothesis and all other hypotheses.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	It is computed as the geometric mean of - gram precisions up to -grams between the hypothesis and reference as follows (1) where is the brevity penalty and are the -gram precisions.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	Better alignment methods which take synonymy into account should be investigated.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	The algorithm explores better weights iteratively starting from a set of initial weights.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.	0
This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.	Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed.	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	When multiple references are provided, the edits from the closest reference are divided by the average reference length.	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	This paper describes an improved confusion network based method to combine outputs from multiple MT systems.	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	Recently, confusion network decoding has been applied in machine translation system combination.	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	All four reference translations available for the tuning and test sets were used.	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision.	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994).	0
The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).	The same Powellâs method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in (Och, 2003).	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005).	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	System weights may be used to assign a system specific confidence on each word in the network.	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	Other evaluation metrics may also be used as the MBR loss function.	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994).	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	System combination has been shown to improve classification performance in various tasks.	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs.	0
In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.	Recently, confusion network decoding has been applied in machine translation system combination.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.	1
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312â319, Prague, Czech Republic, June 2007.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	The alignments from TER are consistent as they do not depend on the test set size.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	The minimum translation edit alignment is usually found through a beam search.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	The size of the test set may influence the quality of these alignments.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.	0
The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).	In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.	1
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	This -best list is then re-scored with the higher order -gram.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	The second set of weights is used to find the final -best from the re-scored -best list.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	The bi-gram and 5-gram English language models were trained on about 7 billion words.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	This allows the addition of language model scores by expanding the lattices or re-scoring -best lists.	0
As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.	A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.	1
Other scores for the word arc are set as in (Rosti et al., 2007).	Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	Each arc represents an alternative word at that.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	However, the other scores are worse in common with the tuning set results.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	Full test set scores are obtained by accumulating the edits and the average reference lengths.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	Full test set scores are obtained by accumulating statistics over all test sentences.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	The prior probabilities are estimated by viewing the negative average TER scores between the skeleton and other hypotheses as log-probabilities.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	Similarly, full test set scores are obtained by accumulating counts over all hypothesis and reference pairs.	0
Other scores for the word arc are set as in (Rosti et al., 2007).	Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	The TER and BLEU optimized combination results beat all single system scores on all metrics.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	The tuning set results on the Arabic to English NIST MT03+MT04 task are shown in Table 1.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	However, the METEOR tuning yields extremely high TER and low BLEU scores.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	Often BLEU scores are reported as percentages and âone BLEU point gainâ usually means a BLEU increase of . Other evaluation metrics have been proposed to replace BLEU.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	Compared to the baseline system which is also optimized for TER, the BLEU score is improved by 0.97 points.	0
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	The BLEU scores are between and , higher being better.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	When multiple references are provided, the edits from the closest reference are divided by the average reference length.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	This paper describes an improved confusion network based method to combine outputs from multiple MT systems.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	Recently, confusion network decoding has been applied in machine translation system combination.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	All four reference translations available for the tuning and test sets were used.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision.	0
The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).	The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994).	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996).	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	TAGs have also been applied to PorÂ­ tuguese in previous work, resulting in a small Portuguese grammar (Kipper, 1994).	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	One of the most controversial areas has to do with polysemy.	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	Verbs in the slide/roll/run intersecÂ­ tion are also allowed to appear in the dative alternation (Carla slid the book to Dale, Carla slid Dale the book), in which the sense of change of location is extended to change of possession.When used intransitively with a path prepo sitional phrase, some of the manner of motion verbs can take on a sense of pseudo-motional existence, in which the subject does not actuÂ­ ally move, but has a shape that could describe a path for the verb (e.g., The stream twists through the valley).	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as "separate by V-ing," where "V" is the basic meaning of that verb (Levin, 1993).	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	The critical point is that, while the verb's meaning can be extended to either "attempted" action or directed motion, these two extensions cannot co-occur - they are mutually exclusive.	0
Levin&aposs classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).	completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an inÂ­ strument as an immediate hypernym.	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	1
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class.	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Dorr, 1997).	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	Simultaneously, the verb was removed from the membership lists of those existing classes.	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap beÂ­ tween Levin and WordNet.	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	"Meander Verbs" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	When examining in detail the intersecÂ­ tive classes just described, which emphasize not only the individual classes, but also their relaÂ­ tion to other classes, we see a rich semantic latÂ­ tice much like WordNet.	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	For each such S = {ct, ...	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	However, apart can also be used with other classes of verbs, including many verbs of motion.	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	The adÂ­ junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	WordNet is an onÂ­ line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each repÂ­ resenting a lexicalized concept.	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	1. Enumerate all sets S = {c1, ...	0
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	There are still many questions that require further investigation.	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	1
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Investigating regular sense extensions based on intersective Levin classes	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	However, there are some interesting differences in which sense extensions are allowed.	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Even though the Levin verb classes are defined by their syntactic behavior, many reflect semanÂ­ tic distinctions made by WordNet, a classificaÂ­ tion hierarchy defined in terms of purely seÂ­ mantic word relations (synonyms, hypernyms, etc.).	0
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed?	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	1
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	3.1 Using intersective Levin classes to.	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	Investigating regular sense extensions based on intersective Levin classes	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	Since intersective classes were built using membership lists rather than the set of defining alternaÂ­ tions, they were similarly incomplete.	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	0
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	2.1 Ambiguities in Levin classes.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	If only one or two verbs were shared between two classes, we assumed this might be due to hoÂ­ mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship inÂ­ volving coherent sets of verbs.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	"Meander Verbs" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compoÂ­ nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	Investigating regular sense extensions based on intersective Levin classes	0
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	The fundamental assumption is that the syntactic frames are a direct reflection of the unÂ­ derlying semantics.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	WordNet is an onÂ­ line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each repÂ­ resenting a lexicalized concept.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	However, apart can also be used with other classes of verbs, including many verbs of motion.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	We also investigated the Portuguese translation of some intersective classes of motion verbs.	0
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	Words and synsets are interrelated by means of lexical and semantic-conceptual links, respecÂ­ tively.	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semanÂ­ tic components.	1
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	The distribution of syntactic frames in which a verb can appear determines its class memberÂ­ ship.	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	The fundamental assumption is that the syntactic frames are a direct reflection of the unÂ­ derlying semantics.	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993).	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect unÂ­ derlying semantic components that constrain alÂ­ lowable arguments.	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	This might be approxiÂ­ mated by automatically extracting the syntacÂ­ tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.	0
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	"Meander Verbs" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	We will be using reÂ­ sources such as dictionaries and online corpora to investigate potential additional members of our classes.	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Even though the Levin verb classes are defined by their syntactic behavior, many reflect semanÂ­ tic distinctions made by WordNet, a classificaÂ­ tion hierarchy defined in terms of purely seÂ­ mantic word relations (synonyms, hypernyms, etc.).	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as "separate by V-ing," where "V" is the basic meaning of that verb (Levin, 1993).	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows.	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class.	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.	0
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	Whereas each WordNet synset is hierarchicalized accordÂ­ ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	The adÂ­ junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.	1
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	Where break is concerned, the only thing speciÂ­ fied is the resulting change of state where the object becomes separated into pieces.	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	In addition, only one verb (pull) has a WordNet sense corÂ­ responding to the change of state - separation semantic component associated with the split class.	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be generÂ­ ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb.	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	The only escape from this limÂ­ itation will be through the use of automated or semi-automated methods of lexical acquisiÂ­ tion.	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	The fundamental assumption is that the syntactic frames are a direct reflection of the unÂ­ derlying semantics.	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	TAGs have also been applied to PorÂ­ tuguese in previous work, resulting in a small Portuguese grammar (Kipper, 1994).	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	Two current approaches to English verb classiÂ­ fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	These subclasses correspond very well to the English subclasses created by the intersective class.	0
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	WordNet does not currently provide a consistent treatment of regular sense extenÂ­ sion (some are listed as separate senses, others are not mentioned at all).	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	The fundamental assumption is that the syntactic frames are a direct reflection of the unÂ­ derlying semantics.	1
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English.	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	Some verbs can be used to describe motion of both animate and inanimate objects, and thus appear in both roll and run verb classes.	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	This filter alÂ­ lowed us to reject the potential intersective class that would have resulted from combining the reÂ­ move verbs with the scribble verbs, for example.	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of "separating" belong to this class because of the component of force in their meaning.	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets.	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	On the other hand, the scribble verbs do form an intersective class with the perforÂ­ mance verbs, since paint and write are also in both classes, in addition to draw.	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative.	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	"Meander Verbs" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.	0
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	1
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Levin verb classes are based on an underlying latÂ­ tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Investigating regular sense extensions based on intersective Levin classes	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes.	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.	0
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993).	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Levin verb classes are based on an underlying latÂ­ tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	The only escape from this limÂ­ itation will be through the use of automated or semi-automated methods of lexical acquisiÂ­ tion.	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Even though the Levin verb classes are defined by their syntactic behavior, many reflect semanÂ­ tic distinctions made by WordNet, a classificaÂ­ tion hierarchy defined in terms of purely seÂ­ mantic word relations (synonyms, hypernyms, etc.).	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions.	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed?	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	{verb of exerting force; no separation imÂ­ plied, but causation of accompanied motion possible) 5.	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	In addition, only one verb (pull) has a WordNet sense corÂ­ responding to the change of state - separation semantic component associated with the split class.	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	TAGs have also been applied to PorÂ­ tuguese in previous work, resulting in a small Portuguese grammar (Kipper, 1994).	0
Levin&aposs study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).	Investigating regular sense extensions based on intersective Levin classes	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	The most specific hypernym common to all the verbs in this intersective class is move, displace, which is also a hypernym for other carry verbs not in the intersection.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	However, apart can also be used with other classes of verbs, including many verbs of motion.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simiÂ­ lar properties with the English verbs, including the causative/inchoative.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the midÂ­ dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	For exÂ­ ample, Eu puxei o ramo e o galho separandoos As in English, derivar and planar are not exterÂ­ nally controllable actions and thus don't take the causativejinchoative alternation common to other verbs in the roll class.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	First, since our experiÂ­ ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compoÂ­ nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.	0
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	motion Figure 3 shows intersective classes involving two classes of verbs of manner of motion (run and roll verbs) and a class of verbs of existence (meÂ­ ander verbs).	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative.	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed?	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Whereas each WordNet synset is hierarchicalized accordÂ­ ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes.	0
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	Simultaneously, the verb was removed from the membership lists of those existing classes.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	These fringe split verbs appear in several other interÂ­ sective classes that highlight the force aspect of their meaning.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	However, there are some interesting differences in which sense extensions are allowed.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	Levin verb classes are based on an underlying latÂ­ tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect unÂ­ derlying semantic components that constrain alÂ­ lowable arguments.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	The distribution of syntactic frames in which a verb can appear determines its class memberÂ­ ship.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	If only one or two verbs were shared between two classes, we assumed this might be due to hoÂ­ mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship inÂ­ volving coherent sets of verbs.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	Some verbs can be used to describe motion of both animate and inanimate objects, and thus appear in both roll and run verb classes.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of "separating" belong to this class because of the component of force in their meaning.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	Words and synsets are interrelated by means of lexical and semantic-conceptual links, respecÂ­ tively.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	The difficulty of determining a suitable lexical representation becomes multiÂ­ plied when more than one language is involved and attempts are made to map between them.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	This filter alÂ­ lowed us to reject the potential intersective class that would have resulted from combining the reÂ­ move verbs with the scribble verbs, for example.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.	0
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	On the other hand, the scribble verbs do form an intersective class with the perforÂ­ mance verbs, since paint and write are also in both classes, in addition to draw.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	These fringe split verbs appear in several other interÂ­ sective classes that highlight the force aspect of their meaning.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Levin verb classes are based on an underlying latÂ­ tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Investigating regular sense extensions based on intersective Levin classes	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	0
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993).	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	We see verb classes as the key to making genÂ­ eralizations about regular extensions of meanÂ­ ing.	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as "separate by V-ing," where "V" is the basic meaning of that verb (Levin, 1993).	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	We have made a preliminary study of the PorÂ­ tuguese translation of the carry verb class.	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	Second, since the translation mapÂ­ pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve â¢ c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	The difficulty of determining a suitable lexical representation becomes multiÂ­ plied when more than one language is involved and attempts are made to map between them.	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	This might be approxiÂ­ mated by automatically extracting the syntacÂ­ tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components.	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.	0
In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&aposS 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)	For these verbs, we have the inÂ­ duced action alternation by using the light verb fazer (make) before the verb, as in Maria fez o barco flutuar (Mary floated the boat).	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	The distribution of syntactic frames in which a verb can appear determines its class memberÂ­ ship.	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	What constitutes a clear sepaÂ­ ration into senses for any one verb, and how can these senses be computationally characterized and distinguished?	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as "separate by V-ing," where "V" is the basic meaning of that verb (Levin, 1993).	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993).	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	SimÂ­ ilarly, draw and yank can be viewed as carry verbs alÂ­ though they are not listed as such.	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	It would be straightÂ­ forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and inÂ­ clude corresponding syntactic information.	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	The fundamental assumption is that the syntactic frames are a direct reflection of the unÂ­ derlying semantics.	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	This might be approxiÂ­ mated by automatically extracting the syntacÂ­ tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.	0
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	Two current approaches to English verb classiÂ­ fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).	1
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	In this experiment, we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	We also investigated the Portuguese translation of some intersective classes of motion verbs.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	When examining in detail the intersecÂ­ tive classes just described, which emphasize not only the individual classes, but also their relaÂ­ tion to other classes, we see a rich semantic latÂ­ tice much like WordNet.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members.	0
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	3.1 Using intersective Levin classes to.	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies.	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory).	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	It uses a number of word and context features rather similar to system M, and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P(tag[features).	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	van Halteren 1996).	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	van Halteren (ed.)	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	However, there has also been some retokenization: genitive markers have been split off and the negative marker "n't" has been reattached.	0
And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).	The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3.	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	1998).	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	van Halteren 1996).	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	van Halteren (ed.)	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	It uses a number of word and context features rather similar to system M, and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P(tag[features).	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3.	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997).	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model.	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.	0
Like Van Halteren et al.(1998), we evaluated two features combinations.	The system used here has access to information about the focus word and the two positions before and after, at least for known words.	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.	1
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	the probability of each tag Tx given that the tagger suggested tag Ti.	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	j hu.	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	It uses a number of word and context features rather similar to system M, and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P(tag[features).	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies.	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	A beam search is then used to find the highest probability tag sequence.	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	1998).	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	As it turns out~ all voting systems outperform the best single tagger, E. 7 Also, the best voting system is the one in which the most specific information is used, Precision-Recall.	0
Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.	This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).	0
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.	1
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).	1
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	The most democratic option is to give each tagger one vote (Majority).	1
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).	0
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	This information can be used by forcing each tagger also to add to the vote for tags suggested by the opposition, by an amount equal to 1 minus the recall on the opposing tag (Precision-Recall).	0
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision).	0
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	6 The question is how large a vote we allow each tagger.	0
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	After comparison, their outputs are combined using several voting strategies and second stage classifiers.	0
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier&aposs vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&aposwise voting.	When abstracting away from individual tags, precision and recall are equal and measure how many tokens are tagged correctly; in this case we also use the more generic term accuracy.	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	The most democratic option is to give each tagger one vote (Majority).	1
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text.	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	Table 2: Accuracy of individual taggers and combination methods.	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	Data driven methods appear to be the more popular.	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	1998).	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.	0
Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.	But the investigation need not be limited to wordclass tagging, for we expect that there are many other NLP tasks where combination could lead to worthwhile improvements.	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems.	1
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers.	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	Pairwise Voting So far, we have only used information on the performance of individual taggers.	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words.	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision).	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	van Halteren 1996).	0
Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).	In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance.	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	van Halteren 1996).	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	van Halteren (ed.)	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	During tagging these rules are applied in sequence to new text.	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	But the investigation need not be limited to wordclass tagging, for we expect that there are many other NLP tasks where combination could lead to worthwhile improvements.	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	Improving Data Driven Wordclass Tagging by System Combination	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	However, there has also been some retokenization: genitive markers have been split off and the negative marker "n't" has been reattached.	0
Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.	However, it is unlikely that we will be able to identify this 4This implies that it is impossible to note if errors counted against a tagger are in fact errors in the benchmark tagging.	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers.	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected.	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	However, specific information is not always superior, for TotPrecision scores higher than TagPrecision.	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	1998).	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	After comparison, their outputs are combined using several voting strategies and second stage classifiers.	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	Pairwise Voting So far, we have only used information on the performance of individual taggers.	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.	0
Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.	9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test.	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	1° Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	The first choice for this is to use a Memory- Based second level learner.	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory).	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers.	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	9Tags (Memory-Based) scores significantly worse than TagPair (p=0.0274) and not significantly better than Precision-Recall (p=0.2766).	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test.	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	Traditionally, these models were categorized as either rule-based/symbolic or corpus-based/probabilistic.	0
Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	However, there has also been some retokenization: genitive markers have been split off and the negative marker "n't" has been reattached.	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Tumer and Gosh 1996).	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	As it turns out~ all voting systems outperform the best single tagger, E. 7 Also, the best voting system is the one in which the most specific information is used, Precision-Recall.	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination.	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	van Halteren 1996).	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	van Halteren (ed.)	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	Pairwise Voting So far, we have only used information on the performance of individual taggers.	0
In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.	However, specific information is not always superior, for TotPrecision scores higher than TagPrecision.	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	A next step is to examine them in pairs.	1
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Data driven methods appear to be the more popular.	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	We not only know whether we should believe what they propose (precision) but also know how often they fail to recognize the correct tag (recall).	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	After comparison, their outputs are combined using several voting strategies and second stage classifiers.	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	5 The most straightforward selection method is an n-way vote.	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context).	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	van Halteren 1996).	0
The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).	van Halteren (ed.)	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	The Viterbi algorithm is used to determine the most probable tag sequence.	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	Pairwise Voting So far, we have only used information on the performance of individual taggers.	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986).	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems.	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	As it turns out~ all voting systems outperform the best single tagger, E. 7 Also, the best voting system is the one in which the most specific information is used, Precision-Recall.	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	Table 2: Accuracy of individual taggers and combination methods.	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors.	0
We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.	Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern.	1
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	This part is used to train the individual tag- gers.	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	Pairwise Voting So far, we have only used information on the performance of individual taggers.	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	van Halteren 1996).	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	van Halteren (ed.)	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	For this experiment we have selected four systems, primarily on the basis of availability.	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	But we have even more information on how well the taggers perform.	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if "as well as" is taken to be a coordination conjunction, it is tagged "as_CC1 well_CC2 as_CC3", using three related but different ditto tags.	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	The changes are mainly cosmetic, e.g. non-alphabetic characters such as "$" in tag names have been replaced.	0
For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).	The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers.	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.	1
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	van Halteren 1996).	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	van Halteren (ed.)	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances.	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	As it turns out~ all voting systems outperform the best single tagger, E. 7 Also, the best voting system is the one in which the most specific information is used, Precision-Recall.	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	But we have even more information on how well the taggers perform.	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.	In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune.	1
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	1° Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	However, it appears more useful to give more weight to taggers which have proved their quality.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	Obviously there is still room for a closer examination of the differences between the combination methods, e.g. the question whether Memory-Based combination would have performed better if we had provided more training data than just Tune, and of the remaining errors, e.g. the effects of inconsistency in the data (cf.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers.	0
Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.	Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	Table 2: Accuracy of individual taggers and combination methods.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if "as well as" is taken to be a coordination conjunction, it is tagged "as_CC1 well_CC2 as_CC3", using three related but different ditto tags.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.	0
For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.	Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model.	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.	1
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	As it turns out~ all voting systems outperform the best single tagger, E. 7 Also, the best voting system is the one in which the most specific information is used, Precision-Recall.	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	It shows that for 99.22% of Tune, at least one tagger selects the correct tag.	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	the probability of each tag Tx given that the tagger suggested tag Ti.	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.	0
One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.	In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag.	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.	1
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3.	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	9Tags (Memory-Based) scores significantly worse than TagPair (p=0.0274) and not significantly better than Precision-Recall (p=0.2766).	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	Now there are more varied systems available, a variety which we hope will lead to better combination effects.	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Tumer and Gosh 1996).	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0.	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.	0
The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.	After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone.	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986).	1
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	However, it appears more useful to give more weight to taggers which have proved their quality.	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	This is much easier and can quickly lead to a model which produces results with a 'reasonably' good quality.	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	We accept that we are measuring quality in relation to a specific tagging rather than the linguistic truth (if such exists) and can only hope the tagged LOB corpus lives up to its reputation.	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations.	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	Now there are more varied systems available, a variety which we hope will lead to better combination effects.	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	First of all, tagging is a widely researched and well-understood task (cf.	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	van Halteren 1996).	0
The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.	Its tagging, which was manually checked and corrected, is generally accepted to be quite accurate.	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	Table 2: Accuracy of individual taggers and combination methods.	1
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	For this experiment we have selected four systems, primarily on the basis of availability.	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	Pairwise Voting So far, we have only used information on the performance of individual taggers.	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	As it turns out~ all voting systems outperform the best single tagger, E. 7 Also, the best voting system is the one in which the most specific information is used, Precision-Recall.	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997).	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	van Halteren 1996).	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	However, it appears more useful to give more weight to taggers which have proved their quality.	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	van Halteren (ed.)	0
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&aposs, which turned out to have the worst accuracy of the four competing methods.	But we have even more information on how well the taggers perform.	0
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.	1
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	1° Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.	1
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers.	0
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.	0
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	During the training phase, cases containing information about the word, the context and the correct tag are stored in memory.	0
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	We conjecture that pruning is not beneficial when the interesting cases are very rare.	0
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test.	0
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags.	0
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	It turns out that the increase in the individual taggers is quite limited when compared to combination.	0
With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word	van Halteren 1996).	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	1
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	This result is comparable with the results described in (Baldwin 1997).	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	In addition, preliminary experiments show that the approach can be successÂ­ fully adapted for other languages with minimum modifications.	0
Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).	In fact, our evaluation shows that the reÂ­ sults are comparable to syntax-based methods (Lappin & Leass I994).	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	1
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	Robust pronoun resolution with limited knowledge	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	1
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	Robust pronoun resolution with limited knowledge	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	Similarly to the first evaluation, we found that the robust approach was not very successful on senÂ­ tences with too complicated syntax - a price we have to pay for the "convenience" of developing a knowlÂ­ edge-poor system.	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).	We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish.	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the reÂ­ maining candidates (see next section).	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	Lexical reiteration Lexically reiterated items are likely candidates for antecedent (a NP scores 2 if is repeated within the same paragraph twice or more, 1 if repeated once and 0 if not).	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as "antecedent indicators").	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	Examine the current sentence and the two preÂ­.	0
LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	1
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	This measure (Mitkov 1998b) applies only to anaphors "ambiguous" from the point of view of number and gender (i.e. to those "tough" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	These scores have been determined experimentally on an empirical basis and are constantly being upÂ­ dated.	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	This preference can be viewed as a modification of the collocation preference.	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	There might be cases where one or more of the antecedent indicators do not "point" to the correct antecedent.	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	If still no choice is possible, the most recent from the remaining candiÂ­ dates is selected as the antecedent.	0
Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).	The results from experiment 1 and experiment 2 can be summarised in the following (statistically) slightly more representative figures.	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	This measure (Mitkov 1998b) applies only to anaphors "ambiguous" from the point of view of number and gender (i.e. to those "tough" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	The evaluation carried out was manual to ensure that no added error was genÂ­ erated (e.g. due to possible wrong sentence/clause detection or POS tagging).	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	These scores have been determined experimentally on an empirical basis and are constantly being upÂ­ dated.	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	To avoid any terminological confusion, we shall therefore use the more neutral term "success rate" while discussing the evaluation.	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.	0
Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	Robust pronoun resolution with limited knowledge	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaphÂ­ ora resolution.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	Similarly to the first evaluation, we found that the robust approach was not very successful on senÂ­ tences with too complicated syntax - a price we have to pay for the "convenience" of developing a knowlÂ­ edge-poor system.	0
However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).	1
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	Examine the current sentence and the two preÂ­.	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	A case where the system failed was when the anaphor and the antecedent were in the same senÂ­ tence and where preference was given to a candidate in the preceding sentence.	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we conÂ­ sider it as the preferred candidate (1, 0).	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	Givenness Noun phrases in previous sentences representing the "given information" (theme) 1 are deemed good candidates for antecedents and score I (candidates not representing the theme score 0).	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1).	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	For anaphors in simple sentences, noun phrases in the previous senÂ­ tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1).	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaphÂ­ ora resolution.	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.	0
The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)	In a coherent text (Firbas 1992), the given or known information, or theme, usually appears first, and thus forms a coÂ­ referential link with the preceding text.	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1).	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	For the time being, we are using the same scores for Polish.	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	We should point out that the antecedent indicators are preferences and not absolute factors.	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	If still no choice is possible, the most recent from the remaining candiÂ­ dates is selected as the antecedent.	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	As expected, the most frequent indicaÂ­ tors were not the most discriminative ones.	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	Top symptoms like "lexical reiteration" asÂ­ sign score "2" whereas "non-prepositional" noun phrases are given a negative score of "-1".	0
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	The collocation preference here is restricted to the patterns "noun phrase (pronoun), verb" and "verb, noun phrase (pronoun)".	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	We should point out that the antecedent indicators are preferences and not absolute factors.	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	2.2 Informal description of the algorithm.	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	3.3 Comparison to similar approaches: comparaÂ­.	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994).	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	Our preference-based approach showed clear suÂ­ periority over both baseline models.	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	For this purpose we have drawn up a compreÂ­ hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resoluÂ­ tion has addressed the problem of "agreement excepÂ­ tions".	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	Given that our approach is robust and returns anÂ­ tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's "resolve all" version by simulating it manually on the same training data used in evaluation B above.	0
We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).	Empirical evidence sugÂ­ gests that because of the salience of the noun phrases which follow them, the verbs listed above are particularly good indicators.	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	1
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	Robust pronoun resolution with limited knowledge	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	2.2 Informal description of the algorithm.	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	Examine the current sentence and the two preÂ­.	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	Given that our approach is robust and returns anÂ­ tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's "resolve all" version by simulating it manually on the same training data used in evaluation B above.	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	The collocation preference here is restricted to the patterns "noun phrase (pronoun), verb" and "verb, noun phrase (pronoun)".	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.	0
The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)	Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we conÂ­ sider it as the preferred candidate (1, 0).	0
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as "antecedent indicators").	1
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	1
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	Robust pronoun resolution with limited knowledge	0
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the reÂ­ maining candidates (see next section).	0
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish.	0
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.	0
3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).	Input is checked against agreement and for a number of antecedent indicators.	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the reÂ­ maining candidates (see next section).	1
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	Similarly to the evaluation for English, we comÂ­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	This measure (Mitkov 1998b) applies only to anaphors "ambiguous" from the point of view of number and gender (i.e. to those "tough" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	Input is checked against agreement and for a number of antecedent indicators.	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	For anaphors in simple sentences, noun phrases in the previous senÂ­ tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1).	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	Empirical evidence sugÂ­ gests that because of the salience of the noun phrases which follow them, the verbs listed above are particularly good indicators.	0
The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).	Whilst some of the indicators are more genre-specific (term preferÂ­ ence) and others are less genre-specific ("immediate reference"), the majority appear to be genreÂ­ independent.	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	Syntactic parallelism, useful in discrimiÂ­ nating between identical pronouns on the basis of their syntactic function, also has to be forgone.	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	Robust pronoun resolution with limited knowledge	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaphÂ­ ora resolution.	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	In fact, our evaluation shows that the reÂ­ sults are comparable to syntax-based methods (Lappin & Leass I994).	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.	Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism".	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	For this purpose we have drawn up a compreÂ­ hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resoluÂ­ tion has addressed the problem of "agreement excepÂ­ tions".	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	These scores have been determined experimentally on an empirical basis and are constantly being upÂ­ dated.	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaphÂ­ ora resolution.	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	0
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	The resolution of anaphors was carried out with a sucÂ­ cess rate of 95.8%.	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	We should point out that the antecedent indicators are preferences and not absolute factors.	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	Robust pronoun resolution with limited knowledge	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).	0
How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	The evaluation carried out was manual to ensure that no added error was genÂ­ erated (e.g. due to possible wrong sentence/clause detection or POS tagging).	1
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).	1
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	These scores have been determined experimentally on an empirical basis and are constantly being upÂ­ dated.	0
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	The resolution of anaphors was carried out with a sucÂ­ cess rate of 95.8%.	0
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.	0
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.	0
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	0
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994).	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism preferÂ­ ences (see below).	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990).	0
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the "non-prepositional noun phrases" heuristics (penalty) would be overturned by the "collocational preference" heuristics.	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.	1
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	R ob ust aQ pr oa ch B a s el i n e s u b je ct B as eli ne m os t re ce nt Su cc es s rat e (= Pr ec isi on / Re ca ll) 8 9.	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	Out of 223 proÂ­ nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric "it").	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	The sample texts conÂ­ tained 180 pronouns among which were 120 inÂ­ stances of exophoric reference (most being zero proÂ­ nouns).	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	21dentification of clauses in complex sentences is do e heuristically.	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the "non-prepositional noun phrases" heuristics (penalty) would be overturned by the "collocational preference" heuristics.	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	55 % I 48 .5 5 % 6 5 . 9 5 % The lower figure in "Baseline subject" corresponds to "recall" and the higher figure- to "precision".	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	A case where the system failed was when the anaphor and the antecedent were in the same senÂ­ tence and where preference was given to a candidate in the preceding sentence.	0
Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&aposclo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, page 872).	1We use the simple heuristics that the given information is the first noun phrase in a non-imperative sentence.	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	1
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	We should point out that the antecedent indicators are preferences and not absolute factors.	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	The resolution of anaphors was carried out with a sucÂ­ cess rate of 95.8%.	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	languages An attractive feature of any NLP approach would be its language "universality".	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.	0
Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	For this purpose we have drawn up a compreÂ­ hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resoluÂ­ tion has addressed the problem of "agreement excepÂ­ tions".	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaphÂ­ ora resolution.	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	Syntactic parallelism, useful in discrimiÂ­ nating between identical pronouns on the basis of their syntactic function, also has to be forgone.	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	The sample texts conÂ­ tained 180 pronouns among which were 120 inÂ­ stances of exophoric reference (most being zero proÂ­ nouns).	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.	0
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.	Immediate reference In technical manuals the "immediate reference" clue can often be useful in identifying the antecedent.	0
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.	1
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	1
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	1
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.	0
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.	0
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources".	0
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	1
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	languages An attractive feature of any NLP approach would be its language "universality".	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	In the second experiment we evaluated the apÂ­ proach from the point of view also of its "critical success rate".	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	Lack of semantic knowledge rules out the use of verb seÂ­ mantics and semantic parallelism.	0
Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.	Robust pronoun resolution with limited knowledge	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	1
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	Our preference-based approach showed clear suÂ­ periority over both baseline models.	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism preferÂ­ ences (see below).	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism".	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj.	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the synÂ­ tactic function/semantic role of each individual word.	0
Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).	3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism preferÂ­ ences (see below).	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990).	0
Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).	When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the "non-prepositional noun phrases" heuristics (penalty) would be overturned by the "collocational preference" heuristics.	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates of the approaches selected for comparison and tested on the same data.	1
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998).	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	The success rate of the "Baseline Subject" was 29.2%, whereas the success rate of "Baseline Most Recent NP" was 62.5%.	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	Immediate reference In technical manuals the "immediate reference" clue can often be useful in identifying the antecedent.	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	The evaluation indicated 83.6% success rate.	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolvÂ­ ing anaphors (with critical success rate of 86.2%).	0
Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.	For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	1
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	Robust pronoun resolution with limited knowledge	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	For this purpose we have drawn up a compreÂ­ hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resoluÂ­ tion has addressed the problem of "agreement excepÂ­ tions".	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish.	0
Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.	tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources".	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	1
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	Robust pronoun resolution with limited knowledge	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	Given that our approach is robust and returns anÂ­ tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's "resolve all" version by simulating it manually on the same training data used in evaluation B above.	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	In fact, our evaluation shows that the reÂ­ sults are comparable to syntax-based methods (Lappin & Leass I994).	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).	We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish.	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	1
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994).	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish.	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	Given that our approach is robust and returns anÂ­ tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's "resolve all" version by simulating it manually on the same training data used in evaluation B above.	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	The approach being robust (an attempt is made to resolve each anaphor and a proÂ­ posed antecedent is returned), this figure represents both "precision" and "recall" if we use the MUC terminology.	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	This measure (Mitkov 1998b) applies only to anaphors "ambiguous" from the point of view of number and gender (i.e. to those "tough" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.	0
Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.	Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the synÂ­ tactic function/semantic role of each individual word.	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	1
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources".	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree.	0
Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.	1
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	Robust pronoun resolution with limited knowledge	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).	0
Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.	Lexical reiteration Lexically reiterated items are likely candidates for antecedent (a NP scores 2 if is repeated within the same paragraph twice or more, 1 if repeated once and 0 if not).	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	1
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish.	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	In terms of frequency of use ("number of nonzero applications"/"number of anaphors"), the most freÂ­ quently used indicator proved to be referential disÂ­ tance used in 98.9% of the cases, followed by term preference (97.8%), givenness (83.3%), lexical reitÂ­ eration (64.4%), definiteness (40%), section heading (37.8%), immediate reference (31.1%) and collocaÂ­ tion (11.1%).	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	In the following we shall outline some the indicators used and shall illustrate them by exÂ­ amples.	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	If immediate reference does not hold, propose the candidate with higher score for collocational pattern.	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	Example: Insert the cassettei into the VCR making sure iti is suitable for the length of recording.	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	Whilst some of the indicators are more genre-specific (term preferÂ­ ence) and others are less genre-specific ("immediate reference"), the majority appear to be genreÂ­ independent.	0
The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).	If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	1
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaphÂ­ ora resolution.	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	CogNIAC successfully resolved the pronouns in 75% of the cases.	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	This case and other cases suggest that it might be worthwhile reconsiderÂ­ ing/refining the weights for the indicator "referential distance".	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree.	0
Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)	The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolvÂ­ ing anaphors (with critical success rate of 86.2%).	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	Robust pronoun resolution with limited knowledge	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	We have recently adapted the approach for AraÂ­ bic as well (Mitkov & Belguith 1998).	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism".	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	For the time being, we are using the same scores for Polish.	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.	0
Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.	tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources".	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	Similarly to the evaluation for English, we comÂ­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	A case where the system failed was when the anaphor and the antecedent were in the same senÂ­ tence and where preference was given to a candidate in the preceding sentence.	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	This measure (Mitkov 1998b) applies only to anaphors "ambiguous" from the point of view of number and gender (i.e. to those "tough" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	Empirical evidence sugÂ­ gests that because of the salience of the noun phrases which follow them, the verbs listed above are particularly good indicators.	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we conÂ­ sider it as the preferred candidate (1, 0).	0
ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).	We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	For the time being, we are using the same scores for Polish.	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	Given that our approach is robust and returns anÂ­ tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's "resolve all" version by simulating it manually on the same training data used in evaluation B above.	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish.	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	Lack of semantic knowledge rules out the use of verb seÂ­ mantics and semantic parallelism.	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997).	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree.	0
These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).	In fact, our evaluation shows that the reÂ­ sults are comparable to syntax-based methods (Lappin & Leass I994).	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	Robust pronoun resolution with limited knowledge	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism".	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources".	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree.	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).	For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	Robust pronoun resolution with limited knowledge	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0).	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	The collocation preference here is restricted to the patterns "noun phrase (pronoun), verb" and "verb, noun phrase (pronoun)".	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism".	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998).	0
While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.	While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	In fact, our evaluation shows that the reÂ­ sults are comparable to syntax-based methods (Lappin & Leass I994).	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources".	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	The sample texts conÂ­ tained 180 pronouns among which were 120 inÂ­ stances of exophoric reference (most being zero proÂ­ nouns).	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.	0
Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.	The approach being robust (an attempt is made to resolve each anaphor and a proÂ­ posed antecedent is returned), this figure represents both "precision" and "recall" if we use the MUC terminology.	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	1
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates of the approaches selected for comparison and tested on the same data.	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	Robust pronoun resolution with limited knowledge	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources".	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism".	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolvÂ­ ing anaphors (with critical success rate of 86.2%).	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	Lack of semantic knowledge rules out the use of verb seÂ­ mantics and semantic parallelism.	0
They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)	To avoid any terminological confusion, we shall therefore use the more neutral term "success rate" while discussing the evaluation.	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	1
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	2.2 Informal description of the algorithm.	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	R ob ust aQ pr oa ch B a s el i n e s u b je ct B as eli ne m os t re ce nt Su cc es s rat e (= Pr ec isi on / Re ca ll) 8 9.	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	Robust pronoun resolution with limited knowledge	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	Out of 223 proÂ­ nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric "it").	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or possesÂ­ sive pronouns.	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	The collocation preference here is restricted to the patterns "noun phrase (pronoun), verb" and "verb, noun phrase (pronoun)".	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1).	0
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	The relaÂ­ tively low figures for the majority of indicators should not be regarded as a surprise: firstly, we should bear in mind that in most cases a candidate was picked (or rejected) as an antecedent on the baÂ­ sis of applying a number of different indicators and secondly, that most anaphors had a relatively high number of candidates for antecedent.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	Similarly to the evaluation for English, we comÂ­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	This case and other cases suggest that it might be worthwhile reconsiderÂ­ ing/refining the weights for the indicator "referential distance".	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	Typically, our preference-based model proved superior to both baseline models when the anteceÂ­ dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	A case where the system failed was when the anaphor and the antecedent were in the same senÂ­ tence and where preference was given to a candidate in the preceding sentence.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the anteÂ­ cedent.	0
In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.	If still no choice is possible, the most recent from the remaining candiÂ­ dates is selected as the antecedent.	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish.	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	Robust pronoun resolution with limited knowledge	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	We have recently adapted the approach for AraÂ­ bic as well (Mitkov & Belguith 1998).	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	Whilst some of the indicators are more genre-specific (term preferÂ­ ence) and others are less genre-specific ("immediate reference"), the majority appear to be genreÂ­ independent.	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the "non-prepositional noun phrases" heuristics (penalty) would be overturned by the "collocational preference" heuristics.	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	1We use the simple heuristics that the given information is the first noun phrase in a non-imperative sentence.	0
Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).	Immediate reference In technical manuals the "immediate reference" clue can often be useful in identifying the antecedent.	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	1
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adaptaÂ­ tion.	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	This rule is ignored if there are no definite articles, possessive or demonstrative proÂ­ nouns in the paragraph (this exception is taken into account because some English user's guides tend to omit articles).	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism".	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates of the approaches selected for comparison and tested on the same data.	0
Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.	Our evaluaÂ­ tion, based on 63 examples (anaphors) from a techÂ­ nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %).	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.	1
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	For the time being, we are using the same scores for Polish.	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	For this purpose we have drawn up a compreÂ­ hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resoluÂ­ tion has addressed the problem of "agreement excepÂ­ tions".	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	0
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	Robust pronoun resolution with limited knowledge	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	1
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	2.2 Informal description of the algorithm.	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1.	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources".	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	Immediate reference In technical manuals the "immediate reference" clue can often be useful in identifying the antecedent.	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	Top symptoms like "lexical reiteration" asÂ­ sign score "2" whereas "non-prepositional" noun phrases are given a negative score of "-1".	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	If immediate reference does not hold, propose the candidate with higher score for collocational pattern.	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.	0
The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).	Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. "toner bottle", "bottle of toner", "the bottle").	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	The success rate of the "Baseline Subject" was 29.2%, whereas the success rate of "Baseline Most Recent NP" was 62.5%.	1
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	The first BaseÂ­ line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	Typically, our preference-based model proved superior to both baseline models when the anteceÂ­ dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number.	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	The "Baseline subject" model tested on the same data scored 33.9% recall and 67.9% precision, whereas "Baseline most recent" scored 66.7%.	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	Similarly to the evaluation for English, we comÂ­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	The results from experiment 1 and experiment 2 can be summarised in the following (statistically) slightly more representative figures.	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	If this indicator does not hold again, go for the most recent candidate.	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	Our evaluation estabÂ­ lished the critical success rate as 82%.	0
However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.	3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.	1
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	Robust pronoun resolution with limited knowledge	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	Top symptoms like "lexical reiteration" asÂ­ sign score "2" whereas "non-prepositional" noun phrases are given a negative score of "-1".	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure.	0
Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.	Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism".	0
Stevenson and Joanis, 2003 for English semantic verb classes	The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.	0
Stevenson and Joanis, 2003 for English semantic verb classes	These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.	0
Stevenson and Joanis, 2003 for English semantic verb classes	We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	These classes also assign the same semantic arguments, but differ in their prepositional alternants.	0
Stevenson and Joanis, 2003 for English semantic verb classes	3.1 The Verb Classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	These three classes also assign the same semantic roles but differ in prepositional alternants.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).	0
Stevenson and Joanis, 2003 for English semantic verb classes	These classes generally allow the same syntactic frames, but differ in the possible semantic role assignment.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Semi-supervised Verb Class Discovery Using Noisy Features	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).	1
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	In the next two sections, we present unsupervised and minimally supervised approaches to this problem.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Semi-supervised Verb Class Discovery Using Noisy Features	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	In the remainder of the paper, we first briefly review our feature space and present our experimental classes and verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.	1
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	This method did not work as well as running C5.0, which presumably captures important feature interactions that are ignored in the individual MI calculations.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	We find that the unsupervised method we tried cannot be consistently applied to our data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	We also find that our semi-supervised method (Seed) is linguistically plausible, and performs as well as or better than features manually determined based on linguistic knowledge (Ling).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Unfortunately, this promising method did not prove practical for our data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	We used the chunker (partial parser) of Abney (1991) to preprocess the corpus, which (noisily) determines the NP subject and direct object of a verb, as well as the PPs potentially associated with it.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	The unsupervised feature selection method, on the other hand, was not usable for our data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).	1
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	The 13-way task includes all of our classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	We extracted from the resulting decision trees the union of all features used, which formed the reduced feature set for that task.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	The second column of Table 2 includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	In the remainder of the paper, we first briefly review our feature space and present our experimental classes and verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).	1
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	To model this kind of approach, we selected a sample of five seed verbs from each class.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	We began with this same set of 20 verbs per class for our current work.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	To answer this, we ran experiments using 50 different randomly selected seed verb sets for each class.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Our experimental verbs were selected as follows.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).	1
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, a general feature space means that most features will be irrelevant to any given verb discrimination task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	indicated by the class description given in Levin.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	However, it gives important information about the quality of a clustering: The other measures being equal, a clustering with a higher value indicates tighter and more separated clusters, suggesting stronger inherent patterns in the data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	(1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	However, in many aspects of computational linguistics, it has been found that a small amount of labelled data contains sufficient information to allow us to go beyond the limits of completely unsupervised approaches.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.	We also find that our semi-supervised method (Seed) is linguistically plausible, and performs as well as or better than features manually determined based on linguistic knowledge (Ling).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Table 1 above shows the number of verbs in each class at the end of this process.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	(See Table 3 for the number of features in the Ling and Seed sets.)	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	This performance comparison tentatively suggests that good feature selection can be helpful in our task.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1. T hr ee m ul ti w ay ta sk s ex pl or e pe rf or m an ce o ve r a la rg er n u m be r of cl as se s: T he 6 w ay ta sk in v ol v es th e C he at , St ea lâ R e m ov e, W ip e, S pr ay /L o a d, Fi ll, an d â O th er V er bs of P ut ti n g â cl as se s, al l of w hi ch u n de rg o si m il ar lo ca ti v e Figure 1: The dendrograms and values for the 2-way Wipe/StealâRemove task, using the Ling and Seed sets.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Semi-supervised Verb Class Discovery Using Noisy Features	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Another striking result is the difference in values, which are very much higher than those for Ling (which are in turn much higher than for Full).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Wipe... the dust/the dust from the table/the table.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	These figures are reported with our results in Table 2 below.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Table 1 above shows the number of verbs in each class at the end of this process.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	We used the simple Euclidean distance for the former, and Ward linkage for the latter.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Many feature sets performed very well, and some far outperformed our best results using other feature selection methods.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	The results for these feature sets in clustering are given in the second subcolumn (Ling) under each of the , , and measures in Table 2.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Ward linkage essentially minimizes the distances of all cluster points to the centroid, and thus is less sensitive to outliers than some other methods.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	For example, Figure 1 shows two dendrograms using different feature sets (Ling and Seed, described in Section 5) for the same set of verbs from two classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).	1
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Another striking result is the difference in values, which are very much higher than those for Ling (which are in turn much higher than for Full).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	In our clustering experiments, we find that smaller subsets of features generally perform better than the full set of features.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	For example, Figure 1 shows two dendrograms using different feature sets (Ling and Seed, described in Section 5) for the same set of verbs from two classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	We extracted from the resulting decision trees the union of all features used, which formed the reduced feature set for that task.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	On the 2-way tasks, the performance on average is very close to that of the full feature set for the and measures.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, their study used a small set of five features manually devised for a set of three particular classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	We began with this same set of 20 verbs per class for our current work.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.	1
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.2.1 Accuracy We can assign each cluster the class label of the majority of its members.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	The formula is as follows (Hubert and Arabie, 1985): where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between themâ there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	In performing hierarchical clustering, both a vector distance measure and a cluster distance (âlinkageâ) measure are specified.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Silhouette values vary from +1 to -1, with +1 indicating that the point is near the centroid of its own cluster, and -1 indicating that the point is very close to another cluster (and therefore likely in the wrong cluster).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	A value of 0 suggests that a point is not clearly in a particular cluster.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	(1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	To model this kind of approach, we selected a sample of five seed verbs from each class.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.	1
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.	1
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	We calculate the mean silhouette of all points in a clustering to obtain an overall measure of how well the clusters are separated.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	In performing hierarchical clustering, both a vector distance measure and a cluster distance (âlinkageâ) measure are specified.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, since we fix our number of clusters to the number of classes, the measure remains informative.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	It is therefore considered âcorrected for chance,â given a fixed number of clusters.3 In tests of the measure on some contrived cluster- ings, we found it quite conservative, and on our experimental clusterings it did not often attain values higher than .25.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	1
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	For example, some classes differ in both their semantic roles and frames, while others have the same roles in different frames, or different roles in the same frames.1 Here we summarize the argument structure distinctions between the classes; Table 1 below lists the classes with their Levin class numbers.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Learning the argument structure properties of verbsâthe semantic roles they assign and their mapping to syntactic positionsâis both particularly important and difficult.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	To these 6, the 8-way task adds the Run and Sound Emission verbs, which also undergo locative alternations.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/StealâRemove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/StealâRemove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	For example, the location argument, the truck, is direct object in I loaded the truck with hay, and object of a preposition in I loaded hay onto the truck.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.	1
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	On the other hand, in contrast to Schulte im Walde and Brew (2002), we demonstrated that accurate subcategorization statistics are unnecessary (see also Sarkar and Tripasai, 2002).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to âthe curse of dimensionalityâ?	1
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, a general feature space means that most features will be irrelevant to any given verb discrimination task.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	More research is needed on the definition of the general feature space, as well as on the methods for selecting a more useful set of features for clustering.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	5.3 Unsupervised Feature Selection.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	(1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.	1
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	We find that the unsupervised method we tried cannot be consistently applied to our data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Levinâs classes form a hierarchy of verb groupings with shared meaning and syntax.	1
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, Schulte im Waldeâs features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, a general feature space means that most features will be irrelevant to any given verb discrimination task.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.	1
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	In performing hierarchical clustering, both a vector distance measure and a cluster distance (âlinkageâ) measure are specified.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	We calculate the mean silhouette of all points in a clustering to obtain an overall measure of how well the clusters are separated.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.2.1 Accuracy We can assign each cluster the class label of the majority of its members.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	C5.0 is supervised accuracy; Base is on random clusters.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	The formula is as follows (Hubert and Arabie, 1985): where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between themâ there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	Because the achieved depends on the precise size of clusters, we calculated mean over the best scenario (with equal-sized clusters), yielding a conservative estimate (i.e., an upper bound) of the baseline.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)	We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).	1
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Since it is a general corpus, we do not expect any strong overall domain bias in verb usage.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	We purposely did not carry out any linguistic analysis, although we did check that each verb was reasonably frequent (with log frequencies ranging from 2.6 to 5.1).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	We began with this same set of 20 verbs per class for our current work.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	We used the chunker (partial parser) of Abney (1991) to preprocess the corpus, which (noisily) determines the NP subject and direct object of a verb, as well as the PPs potentially associated with it.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.	1
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	The Potsdam Commentary Corpus	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English.	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	The wounds are still healing.), entity-attribute (e.g., She 2001), who determined that in their corpus of German computer tests, 38% of relations were lexically signalled.	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines.	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization).	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	Like in the co-reference annotation, GÂ¨otzeâs proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet.	0
Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.	The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	1
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	The Potsdam Commentary Corpus	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	There is a âcore corpusâ of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	One conclusion drawn from this annotation effort was that for humans and machines alike, 2 www.sfs.nphil.unituebingen.de/Elwis/stts/ stts.html 3 www.coli.unisb.de/sfb378/negra-corpus/annotate.	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes.	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	Hence we decided to select ten commentaries to form a âcore corpusâ, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence.	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	This paper, however, provides a comprehensive overview of the data collection effort and its current state.	0
However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.	Links can be of two different kinds: anaphoric or bridging (definite noun phrases picking up an antecedent via world-knowledge).	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	annotation guidelines that tell annotators what to do in case of doubt.	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	Our annotators pointed out that very often they made almost random decisions as to what relation to choose, and where to locate the boundary of a span.	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	All annotations are done with specific tools and in XML; each layer has its own DTD.	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) â¢ the full text, â¢ the annotation values for the activated annotation set (co-reference), â¢ the actual annotation tiers, and â¢ the portion of text currently âin focusâ (which also appears underlined in the full text).	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose.	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	The motivation for our more informal approach was the intuition that there are so many open problems in rhetorical analysis (and more so for German than for English; see below) that the main task is qualitative investigation, whereas rigorous quantitative analyses should be performed at a later stage.	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	â¢ Some tools would allow for the desired annotation mode, but are so complicated (they can be used for many other purposes as well) that annotators take a long time getting used to them.	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	There is a âcore corpusâ of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	The paper is organized as follows: Section 2 explains the different layers of annotation that have been produced or are being produced.	0
Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).	A different but supplementary perspective on discourse-based information structure is taken 11ventionalized patterns (e.g., order of informa by one of our partner projects, which is inter tion in news reports).	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	And then there are decisions that systems typically hard-wire, because the linguistic motivation for making them is not well understood yet.	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	Unexpectedly, because the ministries of treasury and education both had prepared the teacher plan together.	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	That is, we can use the discourse parser on PCC texts, emulating for instance a âco-reference oracleâ that adds the information from our co-reference annotations.	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	Nonetheless, the prospect of a network of annotated discourse resources seems particularly promising if not only a single annotation layer is used but a whole variety of them, so that a systematic search for correlations between them becomes possible, which in turn can lead to more explanatory models of discourse structure.	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	The annotator can then âclick awayâ those words that are here not used as connectives (such as the conjunction und (âandâ) used in lists, or many adverbials that are ambiguous between connective and discourse particle).	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	3.5 Improved models of discourse.	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	Either save money at any cost - or give priority to education.	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	On the other hand, we are interested in the application of rhetorical analysis or âdiscourse parsingâ (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5).	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	This means that the PCC cannot grow particularly quickly.	0
Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.	One key issue here is to seek a discourse-based model of information structure.	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	1
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	The Potsdam Commentary Corpus	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	The motivation for our more informal approach was the intuition that there are so many open problems in rhetorical analysis (and more so for German than for English; see below) that the main task is qualitative investigation, whereas rigorous quantitative analyses should be performed at a later stage.	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	The wounds are still healing.), entity-attribute (e.g., She 2001), who determined that in their corpus of German computer tests, 38% of relations were lexically signalled.	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose.	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.	0
For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).	rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries.	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	That is, we can use the discourse parser on PCC texts, emulating for instance a âco-reference oracleâ that adds the information from our co-reference annotations.	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	structure Besides the applications just sketched, the over- arching goal of developing the PCC is to build up an empirical basis for investigating phenomena of discourse structure.	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information.	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	One key issue here is to seek a discourse-based model of information structure.	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	3.5 Improved models of discourse.	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	A different but supplementary perspective on discourse-based information structure is taken 11ventionalized patterns (e.g., order of informa by one of our partner projects, which is inter tion in news reports).	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	Since DaneËsâ proposals of âthematic development patternsâ, a few suggestions have been made as to the existence of a level of discourse structure that would predict the information structure of sentences within texts.	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	Within the RST âuser communityâ there has also been discussion whether two levels of discourse structure should not be systematically distinguished (intentional versus informational).	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future.	0
Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.	On the other hand, we are interested in the application of rhetorical analysis or âdiscourse parsingâ (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5).	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	One key issue here is to seek a discourse-based model of information structure.	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures.	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	Like in the co-reference annotation, GÂ¨otzeâs proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet.	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	We use MMAX for this annotation as well.	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	The corpus has been annotated with six different types of information, which are characterized in the following subsections.	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	On the other hand, we are interested in the application of rhetorical analysis or âdiscourse parsingâ (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5).	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	This concerns on the one hand the basic question of retrieval, i.e. searching for information across the annotation layers (see 3.1).	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	We respond to this on the one hand with a format for its underspecification (see 2.4) and on the other hand with an additional level of annotation that attends only to connectives and their scopes (see 2.5), which is intended as an intermediate step on the long road towards a systematic and objective treatment of rhetorical structure.	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	A different but supplementary perspective on discourse-based information structure is taken 11ventionalized patterns (e.g., order of informa by one of our partner projects, which is inter tion in news reports).	0
Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.	The web-based Annis imports data in a variety of XML formats and tagsets and displays it in a tier-orientedway (optionally, trees can be drawn more ele gantly in a separate window).	0
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.	1
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.	1
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).	0
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.	0
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	0
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	The Potsdam Commentary Corpus	0
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	On the other hand, we are interested in the application of rhetorical analysis or âdiscourse parsingâ (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5).	0
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	One key issue here is to seek a discourse-based model of information structure.	0
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	structure Besides the applications just sketched, the over- arching goal of developing the PCC is to build up an empirical basis for investigating phenomena of discourse structure.	0
Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.	Since DaneËsâ proposals of âthematic development patternsâ, a few suggestions have been made as to the existence of a level of discourse structure that would predict the information structure of sentences within texts.	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	1
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	The Potsdam Commentary Corpus	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures.	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	We developed a first version of annotation guidelines for co-reference in PCC (Gross 2003), which served as basis for annotating the core corpus but have not been empirically evaluated for inter-annotator agreement yet.	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	The wounds are still healing.), entity-attribute (e.g., She 2001), who determined that in their corpus of German computer tests, 38% of relations were lexically signalled.	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	annotation guidelines that tell annotators what to do in case of doubt.	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	And indeed, converging on annotation guidelines is even more difficult than it is with co-reference.	0
The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)	Figure 1: Translation of PCC sample commentary (STTS)2.	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	1
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences.	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	And time is short.	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	Upon identifying an anaphoric expression (currently restricted to: pronouns, prepositional adverbs, definite noun phrases), the an- notator first marks the antecedent expression (currently restricted to: various kinds of noun phrases, prepositional phrases, verb phrases, sentences) and then establishes the link between the two.	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	(Carlson, Marcu 2001) responded to this situation with relatively precise (and therefore long!)	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	Preferences for constituent order (especially in languages with relatively free word order) often belong to this group.	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	Clearly this poses a number of research challenges, though, such as the applicability of tag sets across different languages.	0
The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.	11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here.	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.	1
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	The Potsdam Commentary Corpus	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	Figure 1: Translation of PCC sample commentary (STTS)2.	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	The wounds are still healing.), entity-attribute (e.g., She 2001), who determined that in their corpus of German computer tests, 38% of relations were lexically signalled.	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	The significant drop in number of pupils will begin in the fall of 2003.	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose.	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	There are still some open issues to be resolved with the format, but it represents a first step.	0
(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.	basically complete, yet some improvements and extensions are still under way.	0
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.	1
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	The corpus has been annotated with six different types of information, which are characterized in the following subsections.	1
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	0
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	This offers the well-known advantages for inter- changability, but it raises the question of how to query the corpus across levels of annotation.	0
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.	0
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.	0
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	There is a âcore corpusâ of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.	0
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	The Potsdam Commentary Corpus	0
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators.	0
Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.	In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines.	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).	1
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	While RST (Mann, Thompson 1988) proposed that a single relation hold between adjacent text segments, SDRT (Asher, Lascarides 2003) maintains that multiple relations may hold simultaneously.	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	Within the RST âuser communityâ there has also been discussion whether two levels of discourse structure should not be systematically distinguished (intentional versus informational).	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English.	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	Nonetheless, the prospect of a network of annotated discourse resources seems particularly promising if not only a single annotation layer is used but a whole variety of them, so that a systematic search for correlations between them becomes possible, which in turn can lead to more explanatory models of discourse structure.	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	On the other hand, we are interested in the application of rhetorical analysis or âdiscourse parsingâ (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5).	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	There is a âcore corpusâ of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.	0
For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).	A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers SuÂ¨ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence.	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised.	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences.	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	That is, we can use the discourse parser on PCC texts, emulating for instance a âco-reference oracleâ that adds the information from our co-reference annotations.	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	(Hartmann 1984), for example, used the term Reliefgebung to characterize the distibution of main and minor information in texts (similar to the notion of nuclearity in RST).	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	We developed a first version of annotation guidelines for co-reference in PCC (Gross 2003), which served as basis for annotating the core corpus but have not been empirically evaluated for inter-annotator agreement yet.	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	2.1 Part-of-speech tags.	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information.	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	In the rhetorical tree, nuclearity information is then used to extract a âkernel treeâ that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in a summary).	0
We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.	Trying to integrate constituent ordering and choice of referring expressions, (Chiarcos 2003) developed a numerical model of salience propagation that captures various factors of authorâs intentions and of information structure for ordering sentences as well as smaller constituents, and picking appropriate referring expressions.10 Chiarcos used the PCC annotations of co-reference and information structure to compute his numerical models for salience projection across the generated texts.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	Nonetheless, the prospect of a network of annotated discourse resources seems particularly promising if not only a single annotation layer is used but a whole variety of them, so that a systematic search for correlations between them becomes possible, which in turn can lead to more explanatory models of discourse structure.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	The motivation for our more informal approach was the intuition that there are so many open problems in rhetorical analysis (and more so for German than for English; see below) that the main task is qualitative investigation, whereas rigorous quantitative analyses should be performed at a later stage.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	Preferences for constituent order (especially in languages with relatively free word order) often belong to this group.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	Thus we opted not to take the step of creating more precise written annotation guidelines (as (Carlson, Marcu 2001) did for English), which would then allow for measuring inter-annotator agreement.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	Indeed there are several open issues.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	Again, the idea is that having a picture of syntax, co-reference, and sentence-internal information structure at oneâs disposal should aid in finding models of discourse structure that are more explanatory and can be empirically supported.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	And indeed, converging on annotation guidelines is even more difficult than it is with co-reference.	0
For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)	Asking the annotator to also formulate the question is a way of arriving at more reproducible decisions.	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	Intuitively, a reliable instance is one that is highly associated with as many reliable patterns as possible (i.e., we have more confidence in an instance when multiple reliable patterns instantiate it.)	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves.	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	The recall of a pattern p can be approximated by the fraction of input instances in I' that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision).	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	Indeed, the filtering of unreliable patterns and instances during the bootstrapping algorithm not only discards the patterns that are unrelated to the actual relation, but also patterns that are too generic and ambiguous â hence resulting in a loss of recall.	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	At a given iteration in Espresso, the intuition behind our solution is that the Web is large enough that correct instances will be instantiated by many of the currently accepted patterns P. Hence, we can distinguish between generic patterns and incorrect patterns by inspecting the relative frequency distribution of their instances using the patterns in P. More formally, given an instance i produced by a generic or incorrect pattern, we count how many times i instantiates on the Web with every pattern in P, using Google.	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	We propose the following solution that helps both in distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances produced by generic patterns.	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	Our reliability measure ensures that overly generic patterns, which may potentially have very low precision, are discarded.	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction.	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision.	0
We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.	This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances.	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.	1
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	For each semantic relation, we manually extracted a set of seed examples.	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	The reliability of the manually supplied seed instances are rÎ¹(i) = 1.	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes.	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances.	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	Given a small set of seed instances for a particular relation, the system learns reliable lexical patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances.	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	For a specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the relation at hand (e.g., Pablo Picasso is-a artist).	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks.	0
Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).	To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	Specifically, for each instance iâ I, the system creates a set of queries, using each pattern in P' with its y term instantiated with iâs y term.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its membersâ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	The number in the parentheses for each relation denotes the total number of seeds.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	E CHEM corpus) and evaluating their quality manually using one human judge2.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	Sample seeds used for each semantic relation and sample outputs from Espresso.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	For each output set, per relation, we evaluate the precision of the system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 1 PR04 does not require any seeds..	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	The pattern discovery phase takes as input a set of instances I' and produces as output a set of lexical patterns P. For the first iteration I' = Is, the set of initial seeds.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	For each semantic relation, we manually extracted a set of seed examples.	0
Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).	In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	Given a small set of seed instances for a particular relation, the system learns reliable lexical patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	Unlike Girju et al. [11] that propose a highly supervised machine learning approach based on selectional restriction, ours is an unsupervised method based on statistical evidence obtained from the Web.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	In small corpora, the number of extracted instances can be too low to guarantee sufficient statistical evidence for the pattern discovery phase of the next iteration.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	Although our results in Section 4.2 do not include this algorithm, we performed a small experiment by adding an a-posteriori generic pattern recovery phase to Espresso.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.)	0
A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).	Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point).	1
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	The sample consists of 5,951,432 words extracted from the following data files: AP890101 â AP890131, AP890201 â AP890228, and AP890310 â AP890319.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	In this section, we present a preliminary comparison of Espresso with two state of the art systems on the task of extracting various semantic relations.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	Term definition.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: Â succession: This relation indicates that one proper noun succeeds another in a position or title.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	We propose the following solution that helps both in distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances produced by generic patterns.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	Datasets We perform our experiments using the following two datasets: Â TREC9: This dataset consists of a sample of articles from the Aquaint (TREC9) newswire text collection.	0
To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).	Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	Datasets We perform our experiments using the following two datasets: Â TREC9: This dataset consists of a sample of articles from the Aquaint (TREC9) newswire text collection.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	In future work, this parameter can be learned using a development corpus.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its membersâ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	Â PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	There are many avenues of future work.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks.	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations	0
In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.	Following [17], we define the relative recall of system A given system B, RA|B, as: RA|B = RA = C A P Ã A = A RB CB PB Ã B Using the precision estimates, PA, from our precision experiments, we can estimate CA â PA Ã |A|, where A is the total number of instances of a particular relation discovered by system A. 2 In future work, we will perform this evaluation using multiple judges in order to obtain confidence bounds and.	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.	1
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	For example, expanding the relation ânew record of a criminal convictionâ part-of âFBI reportâ, the following new instances are obtained: ânew recordâ part-of âFBI reportâ, and ârecordâ part-of âFBI reportâ.	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	Hence, we can capture relations between complex terms, such as ârecord of a criminal convictionâ part-of âFBI reportâ.	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	Early results appear very promising.	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)âs part-NP(movie#1)].	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	Preliminary results show that Espresso generates highly precise relations, but at the expense of lower recall.	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	Experimental results, for all relations and the two different corpus sizes, show that Espresso greatly outperforms the other two methods on precision.	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	Adding these instances to the results from Table 5 decreases the system precision from 60% to 51%, but dramatically increases Espressoâs recall by a factor of 8.16.	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	Although our results in Section 4.2 do not include this algorithm, we performed a small experiment by adding an a-posteriori generic pattern recovery phase to Espresso.	0
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	New instances are then created from the retrieved Web results (e.g. âCanada ; countryâ) and added to I. We are currently exploring filtering mechanisms to avoid retrieving too much noise.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.	1
Pennacchiotti and Pantel [32] describes a system called Espresso.	The relative recall is always given in relation to the Espresso system.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	For example, in Table 2, RH02 has a relative recall of 5.31 with Espresso, which means that the RH02 system output 5.31 times more correct relations than Espresso (at a cost of much Table 8.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	Â PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	System performance on the is-a relation on the TREC9 dataset.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	System performance on the is-a relation on the CHEM dataset.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts.	0
Pennacchiotti and Pantel [32] describes a system called Espresso.	System performance on the part-of relation on the TREC9 dataset.	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	The recall of a pattern p can be approximated by the fraction of input instances in I' that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision).	1
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	However, generic patterns, while having low precision, yield a high recall, as also reported by [11].	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances.	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	The challenge, then, is to harness the expressive power of the generic patterns whilst maintaining the precision of Espresso.	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	Furthermore, it is important to note that there are several other generic patterns, like [Xâs Y], from which we expect a similar precision of 50% with a continual increase of recall.	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	In this phase, Espresso retrieves from the corpus the set of instances I that match any of the lexical patterns in P'.	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	â  Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances.	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	â  Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances.	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	â  Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances.	0
Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.	Precision and Recall.	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	To date, most research on lexical relation harvesting has focused on is-a and part-of relations.	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction.	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	Systems We compare the results of Espresso with the following two state of the art extraction systems: Â RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.)	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12].	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	Â PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label.	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision.	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	Moreover, to cope with data sparsity, a syntactic expansion phase is also carried out.	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun.	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	Most common are pattern-based approaches.	0
As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.	Then, the Web counts can also be used to filter out incorrect instances from the generic patternsâ instantiations.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.	1
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	Most common are pattern-based approaches.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	As mentioned above in Section 4.3, we are working on improving system recall with a web-based method to identify generic patterns and filter their instances.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	To date, most research on lexical relation harvesting has focused on is-a and part-of relations.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	Unlike Girju et al. [11] that propose a highly supervised machine learning approach based on selectional restriction, ours is an unsupervised method based on statistical evidence obtained from the Web.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	For example, in Table 2, RH02 has a relative recall of 5.31 with Espresso, which means that the RH02 system output 5.31 times more correct relations than Espresso (at a cost of much Table 8.	0
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	In this phase, Espresso selects among the patterns P those that are most reliable.	0
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	We evaluate this relation on the TREC9 corpus.	1
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: Â succession: This relation indicates that one proper noun succeeds another in a position or title.	1
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts.	0
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	Â PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label.	0
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	System performance on the part-of relation on the TREC9 dataset.	0
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	System performance on the succession relation on the TREC9 dataset.	0
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	We tested the 7,634 instances extracted by the generic pattern [X of Y] on the CHEM corpus for the part-of relation.	0
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	System performance on the part-of relation on the CHEM dataset.	0
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	In this phase, Espresso retrieves from the corpus the set of instances I that match any of the lexical patterns in P'.	0
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	To date, most research on lexical relation harvesting has focused on is-a and part-of relations.	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	Furthermore, the approach clearly distinguishes the participants of an event by the semantic roles they bear.	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	appropriate semantic role), the relative clauses, and the control construction (vaccine is the agent of the prevent event).	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	UDRSs are either ordinarly DRSs, DRSs conjoined by the merge (for which we use the semicolon), or NP/N: A N/N: record N: date Î»q.Î»p.( x ;q@x;p@x) Î»p.Î»x.( y record(y) nn(y,x) ;p@x) Î»x. date(x) [fa] N: record date y Î»x.( record(y) nn(y,x) ; ) date(x) . . .	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	Wide-Coverage Semantic Analysis with Boxer	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	Boxer is a wide-coverage system for semantic interpretation.	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	All sources of Boxer are available for download and free of noncommercial use.	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	Boxer is an open-domain tool for computing and reasoning with semantic representations.	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	In the context of Boxer, these semantic representations are defined in the shape of lambda-DRSs.	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	Boxer outputs either resolved semantic representations (in other words, completely disambiguated DRSs), or underspecified representations, where some ambiguities are left unresolved in the semantic representation.	0
We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.	The semantic lexicon defines a suitable mapping from categories to semantic representations.	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.	1
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	Boxer is a wide-coverage system for semantic interpretation.	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	The output of Boxer for this text is shown in Figure 3.	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet (Kipper et al., 2008), and has some attractive formal properties (Dowty, 1989).	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	Figure 3: Boxer output for Shared Task Text 2	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008).	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	Van der Sandtâs proposal is cast in DRT, and therefore relatively easy to integrate in Boxerâs semantic formalism.	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	Only the box format is shown here â Boxer is also able to output the DRSs in Prolog or XML encodings.	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	The preposition âInâ starting the second sentence was incorrectly analysed by the parser.	0
This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).	Because Boxer has the option to output unresolved DRSs too, it is possible to include external anaphora or coreference resolution components.	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007).	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008).	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	Boxer is distributed with the C&C tools and freely available for research purposes.	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	Comments: Because there were two questions in this text we parsed it using the C&C parser with the model trained on questions.	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	Comments: The quotes were removed in the tokenisation phase, because the C&C parser, being trained on a corpus without quotes, performs badly on texts containing quotes.	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called âboxesâ because of the way they are graphically displayed) for English sentences and texts.	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	Figure 3: Boxer output for Shared Task Text 2	0
In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).	It was able to produce DRSs for all texts.	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called âboxesâ because of the way they are graphically displayed) for English sentences and texts.	1
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Wide-Coverage Semantic Analysis with Boxer	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Boxer is a wide-coverage system for semantic interpretation.	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001).	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Discourse Representation Structures (DRSs).	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Boxer was able to produce semantic representation for all text without any further modifications to the software.	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.	0
Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).	Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001).	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer	1
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	Boxer is distributed with the C&C tools and freely available for research purposes.	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007).	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	Comments: The quotes were removed in the tokenisation phase, because the C&C parser, being trained on a corpus without quotes, performs badly on texts containing quotes.	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency.	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	Comments: Because there were two questions in this text we parsed it using the C&C parser with the model trained on questions.	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	a virus --[lex] --[lex] by np:nb/n n ---------------------[lex] -----------[fa] Cervical cancer caused ((s:pss\np)\(s:pss\np))/np np:nb ---[lex] --[lex] ---[lex] --------------------------------------[fa] n/n n is s:pss\np (s:pss\np)\(s:pss\np) ------------[fa] ----------------[lex] -----------------------------------------------[ba] n (s:dcl\np)/(s:pss\np) s:pss\np ------------[tc] ---------------------------------------------------------------------[fa] np s:dcl\np --------------------------------------------------------------------------------------[ba] s:dcl Figure 2: CCG derivation as generated by the C&C tools 3.2 Lexicon.	0
For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).	Wide-Coverage Semantic Analysis with Boxer	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001).	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	Boxer deals fine with the passive construction (assigned the 1 This text was taken from the Economist Volume 387 Number 8582, page 92.	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	We wonât show the standard translation from DRS to FOL here (Blackburn et al., 2001; Bos, 2004; Kamp and Reyle, 1993).	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	The semantic lexicon defines a suitable mapping from categories to semantic representations.	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	CCGbank hosts more than a thousand different categories.	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	As we can see from the example and Boxerâs analysis various things go right and various things go wrong.	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	The complex NP âJoao Pedro Fonseca and Marta Gomesâ was distributively interpreted, rather than collective.	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005).	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.	0
Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.	Merge- reduction is the process of eliminating the merge operation by forming a new DRS resulting from the union of the domains and conditions of the argument DRSso of a merge, respectively (obeying certain constraints).	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	Boxer is an open-domain tool for computing and reasoning with semantic representations.	1
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis.	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic.	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	The semantic lexicon defines a suitable mapping from categories to semantic representations.	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	Boxer outputs either resolved semantic representations (in other words, completely disambiguated DRSs), or underspecified representations, where some ambiguities are left unresolved in the semantic representation.	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	Boxer was able to produce semantic representation for all text without any further modifications to the software.	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	In the context of Boxer, these semantic representations are defined in the shape of lambda-DRSs.	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.	0
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	Boxer is distributed with the C&C tools and freely available for research purposes.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	The semantic lexicon defines a suitable mapping from categories to semantic representations.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	The input text needs to be tokenised with one sentence per line.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	In CCG, the syntactic lexicon comprises the set of lexical categories.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	Boxer is an open-domain tool for computing and reasoning with semantic representations.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	In the context of Boxer, these semantic representations are defined in the shape of lambda-DRSs.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	Boxer outputs either resolved semantic representations (in other words, completely disambiguated DRSs), or underspecified representations, where some ambiguities are left unresolved in the semantic representation.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	Boxer deals fine with the passive construction (assigned the 1 This text was taken from the Economist Volume 387 Number 8582, page 92.	0
This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)	In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007).	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.	1
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	This task has in fact been performed by human annotators in the DAESO-project.	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	2.2 Pairwise similarity.	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity.	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision.	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	The headlines are stemmed using the porter stemmer for Dutch (Kraaij and Pohlmann, 1994).	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76.	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	Thanks also to Wauter Bosma for originally mining the headlines from Google News.	0
GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.(2009).	Our second approach is to calculate the similarity between pairs of headlines directly.	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News.	1
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles.	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	The original Google News headline clusters are reclustered into finer grained sub-clusters.	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	Thanks also to Wauter Bosma for originally mining the headlines from Google News.	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	Using headlines of news articles clustered by Google News, and finding good paraphrases within these clusters is an effective route for obtaining pairs of paraphrased sentences with reasonable precision.	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period AprilâAugust 2006.	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006).	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	For each news article, the headline and the first 150 characters of the article were stored.	0
Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.	An optional final step is to add alignments that are implied by previous alignments.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.	1
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	Thanks also to Wauter Bosma for originally mining the headlines from Google News.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	Of course the coverage of our method is still somewhat limited: only paraphrases that have some words in common will be extracted.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period AprilâAugust 2006.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	2.2 Pairwise similarity.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	This is not a bad thing: we are particularly interested in extracting paraphrase patterns at the constituent level.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	Using headlines of news articles clustered by Google News, and finding good paraphrases within these clusters is an effective route for obtaining pairs of paraphrased sentences with reasonable precision.	0
Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].	Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006).	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation.	1
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	For the development of our system we use data which was obtained in the DAESO-project.	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	This task has in fact been performed by human annotators in the DAESO-project.	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76.	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	In each newly obtained cluster all headlines can be aligned to each other.	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006).	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	However, as Barzilay and Elhadad (2003) have pointed out, sentence mapping in this way is only effective to a certain extent.	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.	0
The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].	Of course the coverage of our method is still somewhat limited: only paraphrases that have some words in common will be extracted.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	Our first approach is to use a clustering algorithm to cluster similar headlines.	1
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	This task has in fact been performed by human annotators in the DAESO-project.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...âK ]) three headlines shown in the example.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	In contrast to traditional concept-to-text systems, text-to-text generation systems convert source text to target text, where typically the source and target text share the same meaning to some extent.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	2.1 Clustering.	0
Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.	Our second approach is to calculate the similarity between pairs of headlines directly.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.	1
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	2.1 Clustering.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	Sub- clustering is no trivial task, however.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	We use the k-means implementation in the CLUTO1 software package.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	Our first approach is to use a clustering algorithm to cluster similar headlines.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.	0
Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.	For the development of our system we use data which was obtained in the DAESO-project.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	Sub- clustering is no trivial task, however.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	Table 2 displays the paraphrase detection precision and recall of our two approaches.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	If it is lower than the lower theshold, it is rejected.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	If the similarity is higher than the upper threshold, it is accepted.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	For the development of our system we use data which was obtained in the DAESO-project.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	We measure the performance of our approaches by comparing to human annotation of sub- clusterings.	0
However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.	However, as Barzilay and Elhadad (2003) have pointed out, sentence mapping in this way is only effective to a certain extent.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	Our first approach is to use a clustering algorithm to cluster similar headlines.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	These alignments can be made with existing alignment tools such as the GIZA++ toolkit.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	In contrast to traditional concept-to-text systems, text-to-text generation systems convert source text to target text, where typically the source and target text share the same meaning to some extent.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	Roughly 13,000 clusters were retrieved.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	For the development of our system we use data which was obtained in the DAESO-project.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	2.1 Clustering.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	Our second approach is to calculate the similarity between pairs of headlines directly.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	For each news article, the headline and the first 150 characters of the article were stored.	0
The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.	Sub- clustering is no trivial task, however.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	As soon as P K 1(k) ex ceeds a threshold, k â 1 is selected as the optimum number of clusters.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	Our first approach is to use a clustering algorithm to cluster similar headlines.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	If this similarity exceeds the upper threshold, it is accepted.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	We use an FÎ² -score with a Î² of 0.25 as we favour precision over recall.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	If the similarity is higher than the upper threshold, it is accepted.	0
Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.	We measure the performance of our approaches by comparing to human annotation of sub- clusterings.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	We use an FÎ² -score with a Î² of 0.25 as we favour precision over recall.	1
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	Although we could aim for a higher precision by assigning higher values to the thresholds, we still want some recall and variation in our paraphrases.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	We use this method to collect a large amount of aligned paraphrases in an automatic fashion.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	Table 2 displays the paraphrase detection precision and recall of our two approaches.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period AprilâAugust 2006.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	Of course the coverage of our method is still somewhat limited: only paraphrases that have some words in common will be extracted.	0
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.	1
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period AprilâAugust 2006.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	We aim to build a high-quality paraphrase corpus.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	Using headlines of news articles clustered by Google News, and finding good paraphrases within these clusters is an effective route for obtaining pairs of paraphrased sentences with reasonable precision.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	To that end 865 clusters were manually subdivided into sub-clusters of headlines that show clear semantic overlap.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	These services collect multiple articles covering the same event.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	To obtain only paraphrase pairs, the clusters need to be more coherent.	0
So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.	Thanks also to Wauter Bosma for originally mining the headlines from Google News.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.	1
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period AprilâAugust 2006.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	We use this method to collect a large amount of aligned paraphrases in an automatic fashion.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	We divide the annotated headline clusters in a development set of 40 clusters, while the remainder is used as test data.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	In each newly obtained cluster all headlines can be aligned to each other.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	To obtain only paraphrase pairs, the clusters need to be more coherent.	0
To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.	For the development of our system we use data which was obtained in the DAESO-project.	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.	1
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	Identification and Treatment of Multiword Expressions applied to Information Retrieval	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	In general, the MWEs insertion improves the results of retrieval for relevant documents, because the indexing of specific terms makes it easier to retrieve specific documents related to these terms.	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	Since this list contains all types of MWEs, it was necessary to further filter these to obtain compound nouns only, using morphosyn- tactic information obtained by the TreeTagger (Schmid, 1994), which for English is reported to have an accuracy of 96.36%â (Schmid, 1994).	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	38 04 71 Table 8: Ranking for Topic #141CN In sum, the MWEs insertion seems to improve retrieval bringing more relevant documents, due to a more precise indexing of specific terms.	0
The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).	The structure of a document contains information about the identifier of a term in a document (TERM ID), the lemma of a term (LEMA) and also its morphosyntactic tag (POS).	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	before.	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	In order to incorporate MWEs as units for the IR system to index, we adopted a very simple heuristics that concatenated together all terms composing an MWE using â â (e.g. letter bomb as letter bomb).	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	For this purpose, we used the same indices used before and we performed an individual evaluation of the topics, to obtain a better understanding on where the identification of MWEs improves or degrades the results.	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	In this work, we focused on MWEs composed of exactly two words (i.e. bigrams).	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	(2002) defines MWEs as a sequence of words that acts as a single unit at some level of linguistic analysis.	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997).	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	The rationale was that documents containing specific MWEs can be indexed more adequately than those containing the words of the expression separately.	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die).	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	In order to make the experiment feasible, we used only bigrams formed by compound nouns, in other words, when the POS of both words was NN (Noun).	0
Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.(2011).	This task aimed to explore the contribution of the disambiguation of words to bilingual or monolingual IR.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions.	1
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	Identification and Treatment of Multiword Expressions applied to Information Retrieval	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	One of the great challenges of NLP is the identification of such expressions, âhiddenâ in texts of various genres.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus).	0
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	As a results, our documents were formed only by lemmas and the next step is the indexing of documents using an IR system.	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die).	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	Identification and Treatment of Multiword Expressions applied to Information Retrieval	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	Since this list contains all types of MWEs, it was necessary to further filter these to obtain compound nouns only, using morphosyn- tactic information obtained by the TreeTagger (Schmid, 1994), which for English is reported to have an accuracy of 96.36%â (Schmid, 1994).	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	In general, the MWEs insertion improves the results of retrieval for relevant documents, because the indexing of specific terms makes it easier to retrieve specific documents related to these terms.	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	As a result, retrieval quality should increase.	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.	0
As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.Acosta et al.(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.	The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	Thus, the documents in the corpus have been annotated by a disambiguation system.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	First, it is interesting to observe the values of MAP for all topics that have been altered by the identification of MWEs.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	Identification and Treatment of Multiword Expressions applied to Information Retrieval	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	These values are shown in Table 6.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	The loss in retrieval quality as a result from MWE identification for BCN was not expected.	0
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	The term âmultiword expressionâ has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on.	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101â109, Portland, Oregon, USA, 23 June 2011.	1
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	Identification and Treatment of Multiword Expressions applied to Information Retrieval	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents.	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	In general, the MWEs insertion improves the results of retrieval for relevant documents, because the indexing of specific terms makes it easier to retrieve specific documents related to these terms.	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus).	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.	0
 Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).	Since this list contains all types of MWEs, it was necessary to further filter these to obtain compound nouns only, using morphosyn- tactic information obtained by the TreeTagger (Schmid, 1994), which for English is reported to have an accuracy of 96.36%â (Schmid, 1994).	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.	1
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	Identification and Treatment of Multiword Expressions applied to Information Retrieval	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents.	0
Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Retrieval (IR).	One of the great challenges of NLP is the identification of such expressions, âhiddenâ in texts of various genres.	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system.	1
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	Each bigram present in a predefined dictionary and occurring in a document is treated as a single term, for indexing and retrieval purposes.	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	Identification and Treatment of Multiword Expressions applied to Information Retrieval	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	One of the great challenges of NLP is the identification of such expressions, âhiddenâ in texts of various genres.	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.	0
Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).	Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.	1
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	These values are shown in Table 6.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	The test collection has a total of 310 query topics.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	The term âmultiword expressionâ has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	We see also that the document that was in first position in the Baseline ranking, has its score decreased and was ranked in fourth position in the ranking given by the CN.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	A variation of corpora, different from newspaper articles, because each domain has a specific terminology, can also be an interesting subject for further evaluation.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	Analyzing the WCN, we can identify that this index has the lowest gain compared to all other indices: 32.14%, although having also the lowest loss.	0
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	This document contained information on a âsmall bomb located outside the of the Russian embassyâ and has is not relevant to topic 141, being properly relegated to a lower position.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die).	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	The experiments performed evaluate the insertion of MWEs in results obtained in the IR system.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	To evaluate the results of the IR system, we need to use metrics that estimate how well a userâs query was satisfied by the system.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	Our largest gain was with the index GS, where 55.56% of the topics have improved, but the same index showed the highest percentage of loss, 22.22%.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	(2002) defines MWEs as a sequence of words that acts as a single unit at some level of linguistic analysis.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	In order to investigate the effects of indexing of MWEs for IR, the results of queries are analyzed using IR quality metrics.	0
For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.	The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit.	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.	1
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among).	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words.	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	This collection of confusion sets will be used for evaluating the methods throughout the paper.	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words.	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	Ya.rowsky proposed decision lists as a way to get the best of both methods.	0
Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&aposre.	Yarowsky has exploited this complementarity by combining the two methods using decision lists.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.	1
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	A method is presented for doing this, based on Bayesian classifiers.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	A method for doing this, based on Bayesian classifiers, was presented.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	This section presents a method of doing this based on Bayesian classifiers.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	Two classes of methods have been shown to be useful for resolving lexical ambiguity.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	Two classes of methods have been shown useful for resolving lexical ambiguity.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	\Vhen order matters, other more syntax-based methods, such as collocations and trigrams, are appropriate.	0
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	Table 1 shows the performance of the baseline method for 18 confusion sets.	1
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	Table 3 shows the results of varyingÂ£ for the usual confusion sets.	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7.	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	This section presents a method of doing this based on Bayesian classifiers.	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	Each method will be described in terms of its operation on a single confusion set C = {Wt, ...	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	The ambiguity among words is modelled by confusion sets.	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	This treatment requires a collection of confusion sets to start with.	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	The w; that produces the highest-probability sentence is selected.	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	In the end, the Wi with the highest probability, given the evidence, is selected.	0
The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).	Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	Having said that we resolve conflicts between two collocations by eliminating one of them, we still need to specify which one.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	One is based on finding words in the dictionary that are one typo away from each other [Mays et al., 1991).1 Another finds words that have the same or similar pronunciations.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	It was also used for all experiments involving the method of collocations.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	This provides a lower bound on the performance we would expect from the other methods, which use more than just the priors.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	We believe the improvement is due to considering all of the evidence, rather than just the single strongest piece, which makes the method more robust to inaccurate judgements about which piece of evidence is "strongest".	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	On the other hand, consider the context word how, which allegedly also implies peace.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	If two pieces of evidence conflict, we simply eliminate one of them, and base our decision on the rest of the evidence.	0
We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).	, cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.	0
Golding (1995) builds a classifier based on a rich set of context features.	A method is presented for doing this, based on Bayesian classifiers.	1
Golding (1995) builds a classifier based on a rich set of context features.	The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.	0
Golding (1995) builds a classifier based on a rich set of context features.	If both features are context words, we say the features never conflict (as in the method of context words).	0
Golding (1995) builds a classifier based on a rich set of context features.	Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax.	0
Golding (1995) builds a classifier based on a rich set of context features.	The method is described in terms of "features" rather than "collocations" to reflect its full generality; the features could be context words a.s well as collocations.	0
Golding (1995) builds a classifier based on a rich set of context features.	The idea is to make one big list of all features - in this case, context words and collocations.	0
Golding (1995) builds a classifier based on a rich set of context features.	This method is essentially the same as the one for collocations (see Figure 2), except that it uses context words as well as collocations for the features.	0
Golding (1995) builds a classifier based on a rich set of context features.	A method for doing this, based on Bayesian classifiers, was presented.	0
Golding (1995) builds a classifier based on a rich set of context features.	This allows us to handle context words in the same Bayesian framework as will be used later for other binary features (see Section 3.3).	0
Golding (1995) builds a classifier based on a rich set of context features.	In fact, the method subsumes the method of context words -it does everything that method does, and resolves conflicts among its features as well.	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.	1
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	A Bayesian hybrid method for context-sensitive spelling correction	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	Ya.rowsky proposed decision lists as a way to get the best of both methods.	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	A method is presented for doing this, based on Bayesian classifiers.	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	A method for doing this, based on Bayesian classifiers, was presented.	0
A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)	A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error).	0
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.	1
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	Table 1 shows the performance of the baseline method for 18 confusion sets.	1
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	This section presents a method of doing this based on Bayesian classifiers.	0
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.	0
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	(In subsequent tables, confusion sets will be referred to by their most frequent word.)	0
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	The "Most frequent word" column gives the word in the confusion set that occurred most frequently in the training corpus.	0
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	1Constructing confusion sets in this way requires assigning each word in the lexicon its own confusion set.	0
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	= max: p( w;if) ' As an example of using the metric, suppose f is the context word arid, and suppose that arid coÂ­ occurs 10 times with desert and 1 time with dessert in the training corpus.	0
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.	0
The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.	Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of "\Vords Commonly Confused" in the back of the Random House unabridged dictionary [Fiexner, 1983].	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction.	1
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	Two classes of methods have been shown to be useful for resolving lexical ambiguity.	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	Two classes of methods have been shown useful for resolving lexical ambiguity.	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	A Bayesian hybrid method for context-sensitive spelling correction	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	\Ve treat context-sensitive spelling correction as a task of word disambiguation.	0
Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).	, cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.	1
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	(3) Choose the word in the confusion set with the highest probability.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	(3) Choose the word in the confusion set with the highest probability.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	For instance, walk has the ta.g set {Ns, v}, corresponding to its use as a singular noun and as a verb.4 For a tag to match a word, the ta.g must be a member of the word's tag set.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	45 for each word in the confusion set.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	= abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}Â­ trigrams are often better than the Bayesian method.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	For example, the members of the confusion set {I, me} occurred 840 times in the test corpus, the breakdown being 744 I and 96 me. The baseline method predicted I every time, and thus was right 744 times, for a score of 744/840 = 0.886.	0
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k.	1
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	, cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.	1
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.	1
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity.	0
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	This evidence is combined using Bayes' rule.	0
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	A more conventional interpretation is to take into account the number of occurrences of each Cj within the Â±k-word window, and to estimate p(cilw;) accordingly.	0
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	C on fu si on se t No.	0
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence.	0
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	It can be thought of as the feature's reliability a.t picking out that w; from the others in the confusion set.	0
For each si, the probability is computed with Bayes&apos rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then	50 C on fu si on se t Ba sel in e Cwords Collocs Â± 3 2 Dl ist Dlist R el y U(xiy) w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0.902 0.931 0 . 8 8 6 0.914.	0
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.	1
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	1
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction.	1
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	A Bayesian hybrid method for context-sensitive spelling correction	0
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.	0
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem.	0
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.	0
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	3.4 Hybrid method 1: Decision lists.	0
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	\Ve hypothesize that even better performance can be obtained by taking into account all available evidence.	0
Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.	The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	3.5 Hybrid method 2: Bayesian classifiers.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	In such cases, the Bayesian hybrid method is clearly better.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	A Bayesian hybrid method for context-sensitive spelling correction	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	3.4 Hybrid method 1: Decision lists.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	It can be seen that trigrams and the Bayesian hybrid method each have their better moments.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.	0
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of "minimal competency" for comparison with the other methods Context words Tests for particular words within Â±k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	To compare the two strength metrics, we tried both on some practice confusion sets.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	We allow two types of syntactic elements: words, and part-of-speech tags.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	, cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	3.5 Hybrid method 2: Bayesian classifiers.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ).	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	We smooth the data by adding 1 to the count.	0
In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.	matches is used to classify the target word.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take "features" in that figure to include both context words and collocations.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	We allow two types of syntactic elements: words, and part-of-speech tags.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	Differences from the method of context words are highlighted in boldface.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.	0
These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.	Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right.	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	Table 1 shows the performance of the baseline method for 18 confusion sets.	1
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	The methods handle multiple confusion sets by applying the same technique to each confusion set independently.	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	This collection of confusion sets will be used for evaluating the methods throughout the paper.	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the "confusable" relation is no longer transitive.	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	The bottom line of the table shows the number of collocations learned, averaged over all confusion sets, also as a function of e. One peculiar property of the reliability metric is that it ignores the prior probabilities of the words in the confusion set.	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	1Constructing confusion sets in this way requires assigning each word in the lexicon its own confusion set.	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	is appropriate for use on the right-hand side of Bayes' rule, where the choice of word in the confusion set is t.he "given".	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	We start with the observation that there is no need to use every word in the Â±k-word window to discriminate among the words in the confusion set.	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	The table shows that the Bayesian hybrid method does at least as well as the previous four methods for almost every confusion set.	0
All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).	To deal with this problem, we invoke our earlier observation that there is no need to use all the evidence.	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	P r u n e f e a t u r e s t h a t h a v e i n s u f f i c i e n t d a t a . o r a r e u n i n f o r m a t i v e d i s c r i m i n a t o r s . S o r t t h e r e m a i n i n g f e a t u r e s i n o r d e r o f d e c r e a s i n g s t r e n g t h . Store the list of features (and their associat ed statistics ) for use at run time.	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	of t r a i n i n g t e s t c a s e s c a s e s M os t Baseline f r e q u e n t w o r d w h et h e r, w e at h er 3 3 1 2 4 5 I, m e 61 2.	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	= abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2.	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	50 C on fu si on se t Ba sel in e Cwords Collocs Â± 3 2 Dl ist Dlist R el y U(xiy) w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0.902 0.931 0 . 8 8 6 0.914.	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	51 C on fu si on se t B as eli 11 e C w or ds Collocs Dlist Bayes Â± 3 : : ; 2 R e l y R e l y Trigra ms w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 5 0 ..	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	C on fu si on se t No.	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	48 C on te xt w or d pe ac e pzece co rp s p e a c e u n i t e d n a t i o n s o u r h e a r t j u s t i c e s t a t e a m e r i c a n a i d i n t e r n a t i o n a l w o m e n w a r w o r l d p i e c e o v e r m u s t g r e a t u n d e r h o w 4 9 1 4 1 1 2 0 0 1 5 0 2 7 1 1 2 0 1 2 0 1 2 0 1 1 0 1 1 0 1 1 0 1 0 0 2 0 1 4 0 3 1 1 . 5 1 1 4 1 1 1 1 1 1 1 0 1 1 0 1 t w o f o r a b o u t e v e r y l i t t l e l o n g o n e t h e so 5 1 2 8 3 3 8 4 9 4 9 5 1 0 6 1 1 1 4 2 3 1 7 9 113 9 1 4 1 6 2 2 To tal oc cu rr en ce s 1 8 4 126 Table 4: Excerpts from the list of 43 context words learned for {peace, piece} with k = 24.	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u'r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y' re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets.	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	Going back to the {desert, dessert} example, a collocation that would imply desert might be: PREP the C on fu si on se t B a s e l i n e C w or ds Cwords Cwords Cwords Â± 3 Â± 6 Â± 1 2 Â± 2 4 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e pr in ci pl e si gh t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . . 5 7 5 0 . 5 3 8 0 . 5 3 0 0 . 4 4 2 0 . 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.922 0.927 0.922 0 . 9 1 0 . 8 6 2 0.795 0.743 0.702 0 . 8 6 1 0.849 0.801 0.743 0 . 9 3 1 0.901 0.896 0.855 0 . 7 9 1 0.795 0.793 0.755 0 . 7 4 7 0.741 0.759 0.716 0 . 8 1 6 0.783 0.774 0.736 0 . 6 4 6 0.622 0.636 0.639 0 . 6 3 9 0.614 0.602 0.614 0 ..	0
Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.	47 C o nf us io n se t B a s e l i n e C o H oe s CoHoes CoHoes : : : ; 1 : S 2 : S 3 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 . 5 0 . 5 3 8 0 . . 5 3 0 0 . 4 4 2 0 . : 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 3 9 0.931 0.931 0 . 9 7 9 0.981 0.980 0 . 9 4 3 0.945 0.950 0 . 9 1 9 0.909 0.909 0 . 9 6 6 0.965 0.966 0 . 8 5 3 0.853 0.842 0 . 8 2 1 0.821 0.821 0 . 8 7 7 0.887 0.887 0 . 6 4 6 0.646 0.681 0 . 6 6 3 0.639 0.639 0 . 8 0 7 0.807 0.807 0 . 6 9 9 0.730 0.733 0 . 8 4 9 0.840 0.863 0 . 8 0 0 0.789 0.789 0 . 8 6 9 0.869 0.852 0 . 9 1 1 0.932 0.932 0 . 8 4 1 0.812 0.812 0 . 3 4 1 0.318 0.318 A vg no.	0
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].	1
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.	1
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	Table 3 shows the results of varyingÂ£ for the usual confusion sets.	0
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.	0
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	5 3 0 0 . 4 4 2 0 . 3 9 3 0 . : 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.931 0.93.5 0.935 0 . 9 1 4 0.981 0.980 0.985 0 . 8 6 2 0.945 0.931 0.942 0 . 8 6 1 0.909 0.932 0.924 0 . 9 3 1 0.96.5 0.967 0.973 0 . 7 9 1 0.853 0.842 0.869 0 . 7 4 7 0.821 0.821 0.827 0 . 8 1 6 0.887 0.868 0.901 0 . 6 4 6 0.646 0.629 0.662 0 . 6 : 3 9 0.639 0.627 0.639 0 ..	0
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	The last line of the table gives the total number of occurrences of peace and piece in the training corpus.	0
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	0.981 0 . 8 6 3 0.862 0.945 0 . 8 6 1 0.861 0.909 0 . 8 0 7 0.931 0.965 0 . 7 8 0 0.791 0.853 0 . 7 4 1 0.747 0.821 0 . 7 2 6 0.816 0.887 0 . 6 2 7 0.646 0.646 0 . 6 1 4 0.639 0.639 0 . 5 7 . 5 0..575 0.807 0 . 5 3 8 0.759 0.730 0 . 5 3 0 0.530 0.840 0 . 4 4 2 0.695 0.789 0 . 3 9 3 0.754 0.869 0 . 3 0 6 0.726 0.932 0 . 2 9 0 0.290 0.812 0 . 1 1 4 0.455 0.318 0.	0
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	The last line of the table gives the total numÂ­ ber of occurrences of peace and piece in the training corpus.	0
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	Table 6 shows the performance of decision lists with each metric for the usual confusion sets.	0
Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.	47 C o nf us io n se t B a s e l i n e C o H oe s CoHoes CoHoes : : : ; 1 : S 2 : S 3 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 . 5 0 . 5 3 8 0 . . 5 3 0 0 . 4 4 2 0 . : 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 3 9 0.931 0.931 0 . 9 7 9 0.981 0.980 0 . 9 4 3 0.945 0.950 0 . 9 1 9 0.909 0.909 0 . 9 6 6 0.965 0.966 0 . 8 5 3 0.853 0.842 0 . 8 2 1 0.821 0.821 0 . 8 7 7 0.887 0.887 0 . 6 4 6 0.646 0.681 0 . 6 6 3 0.639 0.639 0 . 8 0 7 0.807 0.807 0 . 6 9 9 0.730 0.733 0 . 8 4 9 0.840 0.863 0 . 8 0 0 0.789 0.789 0 . 8 6 9 0.869 0.852 0 . 9 1 1 0.932 0.932 0 . 8 4 1 0.812 0.812 0 . 3 4 1 0.318 0.318 A vg no.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	Sentence probabilities are calculated using a part-of-speech trigram model.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	For each context word that appears in the context of the ambiguous target word, update the probabilities.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	The w; that produces the highest-probability sentence is selected.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	(3) Choose the word in the confusion set with the highest probability.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	In the end, the Wi with the highest probability, given the evidence, is selected.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	(3) Choose the word in the confusion set with the highest probability.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	The "Baseline" column gives the prediction accuracy of the baseline system on the test corpus.	0
A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).	For each feature that matches the context of the ambiguous target word, and does not conflict with a feature accepted previously, update the probabilities.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	To determine whether a context word cis a useful discriminator, we run a chi-square test [Fleiss, 1981] to check for an association between the presence of c and the choice of word in the confusion set.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	\Ve treat context-sensitive spelling correction as a task of word disambiguation.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	A Bayesian hybrid method for context-sensitive spelling correction	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of "minimal competency" for comparison with the other methods Context words Tests for particular words within Â±k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction.	0
The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).	The methods handle multiple confusion sets by applying the same technique to each confusion set independently.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	(3) Choose the word in the confusion set with the highest probability.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	(3) Choose the word in the confusion set with the highest probability.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	For instance, walk has the ta.g set {Ns, v}, corresponding to its use as a singular noun and as a verb.4 For a tag to match a word, the ta.g must be a member of the word's tag set.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	45 for each word in the confusion set.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}Â­ trigrams are often better than the Bayesian method.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	For example, the members of the confusion set {I, me} occurred 840 times in the test corpus, the breakdown being 744 I and 96 me. The baseline method predicted I every time, and thus was right 744 times, for a score of 744/840 = 0.886.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.	is appropriate for use on the right-hand side of Bayes' rule, where the choice of word in the confusion set is t.he "given".	0
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	1
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	A method for doing this, based on Bayesian classifiers, was presented.	1
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	A Bayesian hybrid method for context-sensitive spelling correction	0
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.	0
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.	0
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	\Ve treat context-sensitive spelling correction as a task of word disambiguation.	0
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	A method is presented for doing this, based on Bayesian classifiers.	0
Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.	The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction.	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	Trigrams are at their worst when the words in the confusion set have the same part of speech.	1
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	It can be seen that trigrams and the Bayesian hybrid method each have their better moments.	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995].	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take "features" in that figure to include both context words and collocations.	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}Â­ trigrams are often better than the Bayesian method.	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	Thus, for {between, among}, for example, where both words are prepositions, trigrams score the same as the baseline method.	0
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	3.5 Hybrid method 2: Bayesian classifiers.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.	1
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	A method is presented for doing this, based on Bayesian classifiers.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	A method for doing this, based on Bayesian classifiers, was presented.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	This section presents a method of doing this based on Bayesian classifiers.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	Ya.rowsky proposed decision lists as a way to get the best of both methods.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	3.5 Hybrid method 2: Bayesian classifiers.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	\Vhen order matters, other more syntax-based methods, such as collocations and trigrams, are appropriate.	0
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	3.5 Hybrid method 2: Bayesian classifiers.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	In such cases, the Bayesian hybrid method is clearly better.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	A Bayesian hybrid method for context-sensitive spelling correction	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	It can be seen that trigrams and the Bayesian hybrid method each have their better moments.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	The table shows that the Bayesian hybrid method does at least as well as the previous four methods for almost every confusion set.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework.	0
We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)	The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take "features" in that figure to include both context words and collocations.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	Two classes of methods have been shown to be useful for resolving lexical ambiguity.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	Two classes of methods have been shown useful for resolving lexical ambiguity.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error).	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	\Ve treat context-sensitive spelling correction as a task of word disambiguation.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	Ya.rowsky proposed decision lists as a way to get the best of both methods.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.	0
For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).	A Bayesian hybrid method for context-sensitive spelling correction	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations.	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	We allow two types of syntactic elements: words, and part-of-speech tags.	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework.	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	If both features are context words, we say the features never conflict (as in the method of context words).	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	There is also some redundancy between the collocations and the context words of the previous section (e.g., for corps).	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	To compare the two strength metrics, we tried both on some practice confusion sets.	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	Consider, for example, the following collocations for desert: PREP the in the the These collocations are highly interdependent- we will say they conflict.	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take "features" in that figure to include both context words and collocations.	0
Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.	The method is described in terms of "features" rather than "collocations" to reflect its full generality; the features could be context words a.s well as collocations.	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction.	1
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	\Ve treat context-sensitive spelling correction as a task of word disambiguation.	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	A Bayesian hybrid method for context-sensitive spelling correction	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among).	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.	0
This general scheme has been used to deÂ­ rive classifiers for a variety of natural lanÂ­ guage applications including speech applicaÂ­ tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and contextÂ­ sensitive spelling correction (Gol95).	In general, we will say that a collocation and a context word conflict iff the collocation contains an explicit test for the context word.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	We believe the improvement is due to considering all of the evidence, rather than just the single strongest piece, which makes the method more robust to inaccurate judgements about which piece of evidence is "strongest".	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	Yarowsky has exploited this complementarity by combining the two methods using decision lists.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	The method is described in terms of "features" rather than "collocations" to reflect its full generality.	0
MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).	Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	A method is presented for doing this, based on Bayesian classifiers.	1
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	3.5 Hybrid method 2: Bayesian classifiers.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	A method for doing this, based on Bayesian classifiers, was presented.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	This section presents a method of doing this based on Bayesian classifiers.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	In such cases, the Bayesian hybrid method is clearly better.	0
A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)	A Bayesian hybrid method for context-sensitive spelling correction	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework.	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	This problem is addressed in the next section.	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of "\Vords Commonly Confused" in the back of the Random House unabridged dictionary [Fiexner, 1983].	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	Yarowsky has exploited this complementarity by combining the two methods using decision lists.	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	Previous work [Yarowsky, 1994] shows that sma.ller values of k (3 or 4) work well for resolving local syntactic ambiguities, while larger values (20 to 50) are suitable for resolving semantic ambiguities.	0
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.	1
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus.	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	The "Baseline" column gives the prediction accuracy of the baseline system on the test corpus.	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	This metric is therefore the one that will be used from here on.	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	, cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	If we look back at the training corpus for the supporting data for this word, we find excerpts such a.s: But oh, how I do sometimes need just a moment of rest, and peace ..	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence.	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	We smooth the data by adding 1 to the count.	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of "\Vords Commonly Confused" in the back of the Random House unabridged dictionary [Fiexner, 1983].	0
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)	The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within Â±k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way.	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction.	1
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	A Bayesian hybrid method for context-sensitive spelling correction	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	\Ve treat context-sensitive spelling correction as a task of word disambiguation.	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework.	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.	0
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction.	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	Consider, for example, the context word walk, and the following collocations: (1) (2) (3) CONJ walk v PREP 7 1Â£ we use the U (xiy) metric instead, then d cision lists fall down on different examples; e.g., {its, it's}.	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	Besides the reliability metric, therefore, we also considered an alternative metric: the uncertainty coefficient of x, denoted U(xiy) [Press et al., 1988, p..501].	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	To deal with this problem, we invoke our earlier observation that there is no need to use all the evidence.	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take "features" in that figure to include both context words and collocations.	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	We start with the observation that there is no need to use every word in the Â±k-word window to discriminate among the words in the confusion set.	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the "confusable" relation is no longer transitive.	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	This provides a lower bound on the performance we would expect from the other methods, which use more than just the priors.	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	Each method will be described in terms of its operation on a single confusion set C = {Wt, ...	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	The method is described in terms of "features" rather than "collocations" to reflect its full generality.	0
We use the metric described in (Yarowsky, 1994; Golding, 1995).	We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3).	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	\Ve treat context-sensitive spelling correction as a task of word disambiguation.	1
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	\Ve then apply each of the two component methods mentioned aboveÂ­ context words and collocations.	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction.	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	As mentioned above, most of the legitimate context words show up for small k; thus as k gets large, the limited number of legitimate context words gets overwhelmed by the 5% of the spurious correlations that make it through our filter.	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the "confusable" relation is no longer transitive.	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	It can be seen that performance generally degrades as k increases.	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	Two classes of methods have been shown to be useful for resolving lexical ambiguity.	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	Two classes of methods have been shown useful for resolving lexical ambiguity.	0
More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;	This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.	1
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	A new complication arises for collocations, however, in that collocations, unlike context words, cannot be assumed independent.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	A method is presented for doing this, based on Bayesian classifiers.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	3.5 Hybrid method 2: Bayesian classifiers.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	A method for doing this, based on Bayesian classifiers, was presented.	0
There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.	This section presents a method of doing this based on Bayesian classifiers.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	A Bayesian hybrid method for context-sensitive spelling correction	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	It can be seen that trigrams and the Bayesian hybrid method each have their better moments.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	3.5 Hybrid method 2: Bayesian classifiers.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	In such cases, the Bayesian hybrid method is clearly better.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	The table shows that the Bayesian hybrid method does at least as well as the previous four methods for almost every confusion set.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.	0
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995].	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	This preserves the strongest non-conflicting evidence as the basis for our answer.	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	A Bayesian hybrid method for context-sensitive spelling correction	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	\Ve treat context-sensitive spelling correction as a task of word disambiguation.	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.	0
Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).	\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a "community" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	EVALUATION TASKS A basic characterization of the challenge presented by each evaluation task is as follows:  Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	This evaluation is called the MET (Multilingual Named Entity) and, like MUC6, was carried out under the auspices of the Tipster Text program.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	For most events, however, the fill is one of a large handful of possibilities, including "chairman," "president," "chief executive [officer]," "CEO," "chief operating officer," "chief financial officer," etc. 438 DISCUSSION: CRITIQUE OF TASKS Named Entity The primary subject for review in the NE evaluation is its limited scope.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	The NE evaluation results serve mainly to document in the MUC context what was already strongly suspected: 1.	0
Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).	NAMED ENTITY The Named Entity (NE) task requires insertion of SGML tags into the text stream.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	About half the systems focused only on individual coreference, which has direct relevance to the other MUC6 evaluation tasks.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	On the basis of the results of the dry run, in which two of the nine systems scored over 90%, we were not surprised to find official scores that were similarly high, but it was not expected that so many systems would enter the formal evaluation and perform so well.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a "community" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	For percentages, about half the systems had 0% error, which reflects the simplicity of that particular subtask.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	These scores indicate that pronoun resolution techniques as well as proper noun matching techniques are good, compared to the techniques required to determine references involving common noun phrases.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	Six of the seven sites that participated in the coreference evaluation also participated in the MUC6 information extraction evaluation, and five of the six made use of the results of the processing that produced their coreference output in the processing that produced their information extraction output.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	The decision to minimize the annotation effort makes it difficult to do detailed quantitative analysis of the results.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	In the case of ORG_DESCRIPTOR, the results of the CO evaluation seem to provide further evidence for the relative inadequacy of current techniques for relating entity descriptions with entity names.	0
As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.	This evaluation is called the MET (Multilingual Named Entity) and, like MUC6, was carried out under the auspices of the Tipster Text program.	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	The amount of agreement between the two annotators was found to be 80% recall and 82% precision.	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the "key" and the other annotator's templates were treated as the "response".	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	However, there were at least three factors that might lead one to expect higher levels of performance than seen in previous MUC evaluations: 1.	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	Leaving aside the fact that descriptors are common noun phrases, which makes them less obvious candidates for extraction than proper noun phrases would be, what reasons can we find to account for the relatively low performance on the ORG_DESCRIPTOR slot?	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation.	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	 It was very small (only 30 articles).	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	Since the development time for the MUC6 task was extremely short, it could be expected that the test would result in only modest performance levels.	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	However, the organization portion of the TE task is not limited to recognizing the referential identity between full and shortened names; it requires the use of text analysis techniques at all levels of text structure to associate the descriptive and locative information with the appropriate entity.	0
The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.	Restriction of the corpus to Wall Street Journal articles resulted in a limited variety of markables and in reliance on capitalization to identify candidates for annotation.	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	 Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.	1
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe.	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	An algorithm developed by the MITRE Corporation for MUC6 was implemented by SAIC and used for scoring the task.	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	The algorithm compares the equivalence classes defined by the coreference links in the manually-generated answer key and the system-generated response.	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	About half the systems focused only on individual coreference, which has direct relevance to the other MUC6 evaluation tasks.	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1].	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	For MUC6, text filtering scores were as high as 98% recall (with precision in the 80th percentile) or 96% precision (with recall in the 80th percentile).	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	Using a simple counting scheme, the algorithm obtains recall and precision scores by determining the minimal perturbations required to align the equivalence classes in the key and response.	0
Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos ability to recog nize coreference among noun phrases (Sund heim, 1995).	These scores indicate that pronoun resolution techniques as well as proper noun matching techniques are good, compared to the techniques required to determine references involving common noun phrases.	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..	1
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	The slot that most systems performed best on is NEWSTATUS; the lowest error score posted on that slot is 47% (median of 55%).	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	Best and average error per response fill Organization object slot scores for TE task With respect to performance on ORG_DESCRIPTOR, note that there may be multiple descriptors (or none) in the text.	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one.	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	But the problems are certainly tractable; none of the fifteen TE entities in the key (ten ORGANIZATION entities and five PERSON entities) was miscategofized by all of the systems.	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	The identification of a name as that of an organization (hence, instantiation of an ORGANIZATION object) or as a person (PERSON object) is a named entity identification task.	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation.	0
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	As indicated in table 2, all systems performed better on identifying person names than on identifying organization or location names, and all but a few systems performed better on location names than on organization names.	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.	1
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the "key" and the other annotator's templates were treated as the "response".	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	The amount of agreement between the two annotators was found to be 80% recall and 82% precision.	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	The annotators' problems with ONZI'HE_JOB were probably more substantive, since the heuristics documented in the appendix were complex and sometimes hard to map onto the expressions found in the news articles.	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	The annotators' problems with VACANCY_REASON may have had more to do with understanding what the scenario definition was saying than with understanding what the news articles were saying.	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	Paraphrased summary of ST outputs for walkthrough article The article was relatively straightforward for the annotators who prepared the answer key, and there were no substantive differences in the output produced by each of the two annotators.	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	It should be noted that human performance on this task was also relatively low, but it is unclear whether the degree of disagreement can be accounted for primarily by the reasons given above or whether the disagreement is attributable to the fact that the guidelines for that slot had not been finalized at the time when the annotators created their version of the keys.	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	Human performance was measured in terms of variability between the outputs produced by the two NRaD and SAIC evaluators for 30 of the articles in the test set (the same 30 articles that were used for NE and CO testing).	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	These two slots caused problems for the annotators as well as for the systems.	0
The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.	One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure.	0
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.	1
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	In this article, the management succession scenario will be used as the basis for discussion.	1
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.	0
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.	0
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	Restriction of the corpus to Wall Street Journal articles resulted in a limited variety of markables and in reliance on capitalization to identify candidates for annotation.	0
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.	0
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants.	0
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	Management Succession Template Structure intentional and is comparable to the richness of the MUC3 "TST2" test set and the MUC4 "TST4" test set.	0
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.	0
The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).	From the 100 test articles, a subset of 30 articles (some relevant to the Scenario Template task, others not) was selected for use as the test set for the Named Entity and Coreference tasks.	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	What would performance be on data where case provided no (reliable) clues and for languages where case doesn't distinguish names?	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	It was also unexpected that one of the systems would match human performance on the task.	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	Systems scored approximately 1525 points lower (F-measure) on ST than on TE.	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	Performance on TE overall is as high as 80% on the F-measure, with performance on ORGANIZATION objects significantly lower (70th percentile) than on PERSON objects (90th percentile).	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	Top performance on PERSON objects came close to human performance, while performance on ORGANIZATION objects fell significantly short of human performance, with the caveat that human performance was measured on only a portion of the test set.	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	TE Results on "Walkthrough Article" TE performance of all systems on the walkthrough article was not as good as performance on the test set as a whole, but the difference is small for about half the systems.	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates.	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	TE Results on Some Aspects of Task Given the more varied extraction requirements for the ORGANIZATION object, it is not surprising that performance on that portion of the TE task was not as good as on the PERSON object 5, as is clear in figure 5.	0
It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).	The interannotator variability test provides reference points indicating human performance on the different aspects of the NE task.	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..	1
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	As indicated in table 2, all systems performed better on identifying person names than on identifying organization or location names, and all but a few systems performed better on location names than on organization names.	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations.	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem.	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	Non-markables include names of products and other miscellaneous names ("Macintosh," "Wall Street Journal" (in reference to the periodical as a physical object), "Dow Jones Industrial Average"); names of groups of people and miscellaneous usages of person names ("Republicans," "GrammRudman," "Alzheimer['s]"); addresses and adjectival forms of location names ("53140 Gatchell Rd.," "American"); indirect and vague mentions of dates and times ("a few minutes after the hour," "thirty days before the end of the year"); and miscellaneous uses of numbers, including some that are similar to currency or percentage expressions ("[Fees] 1 3/4," "12 points," "1.5 times").	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	It involves a three-way distinction for ENAMEX and only a two-way distinction for NUMEX and TIMEX, and it offers the possibility of confusing names of one type with names of another, especially the possibility of confusing organization names with person names.	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	Recognition of alternative ways of identifying an entity constitutes a large portion of the Coreference task and another critical portion of the Template Element task and has been shown to represent only a modest challenge when the referents are names or pronouns.	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	Sometimes it is difficult to distinguish them from names of other types, especially from person names.	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	Organization names are varied in their form, consisting of proper nouns, general vocabulary, or a mixture of the two.	0
In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that "common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks" (Sundheim 1995: 16).	In the case of ORG_DESCRIPTOR, the results of the CO evaluation seem to provide further evidence for the relative inadequacy of current techniques for relating entity descriptions with entity names.	0
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.	1
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.	1
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION	0
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years.	0
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	For past MUC evaluations, the formal run had been conducted using the same scenario as the dry run, and the task definition was released well before the dry run.	0
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	However, there were at least three factors that might lead one to expect higher levels of performance than seen in previous MUC evaluations: 1.	0
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations.	0
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates.	0
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well.	0
The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].	Figure 6 indicates the relative amount of error contributed by each of the slots in the ORGANIZATION object.	0
