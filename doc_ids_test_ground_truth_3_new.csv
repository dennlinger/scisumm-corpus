query	doc_id	label	similiarity
 dependency kernel Zhou et al.(2005)	[33, 25, 24, 51, 32, 34, 37, 44, 210, 93]	[0, 0, 0, 0, 1, 0, 1, 0, 0, 0]	[0.19075581431388855, 0.14697283506393433, 0.1448613703250885, 0.10288023203611374, 0.4487808346748352, 0.09357528388500214, 0.5795416235923767, 0.2213517725467682, 0.3495897650718689, 0.10809650272130966]
(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.	[107, 47, 46, 172, 158, 152, 151, 55, 106, 40]	[1, 1, 1, 0, 1, 1, 1, 0, 1, 0]	[0.401379257440567, 0.5151947140693665, 0.6249021291732788, 0.3940044343471527, 0.5251443386077881, 0.6677483320236206, 0.5896075963973999, 0.1661631017923355, 0.5507010221481323, 0.12697087228298187]
(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.	[110, 23, 147, 22, 14, 69, 128, 214, 113, 215]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.740173876285553, 0.3945876955986023, 0.3935936391353607, 0.6123743653297424, 0.17483513057231903, 0.11242307722568512, 0.11057311296463013, 0.12984152138233185, 0.16164369881153107, 0.14820696413516998]
3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.	[54, 418, 135, 468, 284, 134, 114, 170, 93, 33]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.30586227774620056, 0.12276077270507812, 0.15751150250434875, 0.31025460362434387, 0.10265609622001648, 0.3267519176006317, 0.17461195588111877, 0.1839953064918518, 0.40690338611602783, 0.17609727382659912]
5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.	[281, 282, 280, 284, 283, 39, 158, 399, 286, 177]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7705246806144714, 0.41618573665618896, 0.6627115607261658, 0.23057369887828827, 0.2626720666885376, 0.1794068068265915, 0.15694500505924225, 0.27705878019332886, 0.1274452805519104, 0.18270926177501678]
7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).	[34, 193, 132, 93, 195, 19, 88, 96, 4, 31]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6628305912017822, 0.39570286870002747, 0.18214501440525055, 0.4701091945171356, 0.36201924085617065, 0.14115425944328308, 0.1105014756321907, 0.19620957970619202, 0.11932536959648132, 0.3030985891819]
A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.	[65, 24, 6, 111, 61, 165, 198, 13, 63, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2174881398677826, 0.3857539892196655, 0.14760662615299225, 0.14492450654506683, 0.19775046408176422, 0.11837819963693619, 0.16343413293361664, 0.14269685745239258, 0.11979172378778458, 0.16016815602779388]
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	[40, 164, 1, 24, 0, 128, 16, 203, 166, 206]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7639179825782776, 0.4277263283729553, 0.12549515068531036, 0.16507966816425323, 0.36749503016471863, 0.349197119474411, 0.11160772293806076, 0.10051038861274719, 0.10844795405864716, 0.1680489480495453]
A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	[398, -4, 67, 87, 89, 461, 138, 71, 141, 467]	[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]	[0.5442363619804382, 0.521656334400177, 0.21763186156749725, 0.7775134444236755, 0.13024213910102844, 0.2848832905292511, 0.5120747089385986, 0.11978523433208466, 0.10542620718479156, 0.1057271882891655]
A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).	[4, 45, 144, 196, 193, 13, 192, 214, 7, 39]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11834673583507538, 0.16784292459487915, 0.13717611134052277, 0.14179803431034088, 0.14976204931735992, 0.18306468427181244, 0.18811216950416565, 0.17363573610782623, 0.1809096783399582, 0.10549051314592361]
A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).	[40, 216, 163, 13, 38, 7, 193, 37, 192, 198]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.15727537870407104, 0.12973269820213318, 0.1082363948225975, 0.169850155711174, 0.15399377048015594, 0.1777380257844925, 0.14620143175125122, 0.4634299576282501, 0.17650863528251648, 0.12347576767206192]
"According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the ""correct"" segmentation, and the figure reduces as more people become involved."	[325, 410, 70, 303, 133, 415, 136, 348, 300, 54]	[0, 1, 0, 1, 0, 1, 1, 1, 0, 1]	[0.2677568793296814, 0.6925853490829468, 0.20768921077251434, 0.7848443388938904, 0.10695291310548782, 0.7242353558540344, 0.6222832202911377, 0.768293559551239, 0.25406351685523987, 0.61429762840271]
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	[13, 3, 15, 44, 25, 14, 0, 154, 56, 2]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12486156076192856, 0.14497525990009308, 0.15279345214366913, 0.11866577714681625, 0.13603642582893372, 0.22297759354114532, 0.168943390250206, 0.10522417724132538, 0.19029420614242554, 0.18134216964244843]
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	[104, 42, 25, 61, 0, 19, 14, 150, 21, 58]	[0, 0, 1, 1, 1, 0, 0, 1, 0, 0]	[0.30371028184890747, 0.11479001492261887, 0.5183003544807434, 0.4548884928226471, 0.5790674090385437, 0.2455798238515854, 0.19604763388633728, 0.4570160508155823, 0.10578761994838715, 0.15861104428768158]
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	[0, 14, 30, 101, 25, 152, 9, 104, 19, 44]	[1, 0, 1, 1, 1, 1, 1, 0, 0, 0]	[0.4753445088863373, 0.3339385390281677, 0.6908012628555298, 0.41648510098457336, 0.445691853761673, 0.486672580242157, 0.4501505196094513, 0.20330139994621277, 0.17772266268730164, 0.10863810777664185]
Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.	[100, 144, 196, 19, 215, 131, 5, 132, 211, 163]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3723094165325165, 0.11650730669498444, 0.1080077737569809, 0.187578946352005, 0.1109006330370903, 0.14422421157360077, 0.1706894189119339, 0.16837552189826965, 0.15761291980743408, 0.14226311445236206]
Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.	[20, 5, 130, 211, 2, 192, 17, 19, 4, 108]	[1, 0, 0, 0, 1, 1, 0, 0, 0, 0]	[0.5738874673843384, 0.314858615398407, 0.2916402518749237, 0.29074186086654663, 0.47427862882614136, 0.4657745957374573, 0.35171201825141907, 0.10515784472227097, 0.1695682406425476, 0.39505547285079956]
Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.	[147, 1, 217, 19, 48, 49, 75, 13, 18, 73]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.521506130695343, 0.28219887614250183, 0.10770568251609802, 0.18019096553325653, 0.11698489636182785, 0.12382733076810837, 0.11431708931922913, 0.17448464035987854, 0.13375481963157654, 0.5916736721992493]
Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information	[17, 192, 2, 40, 31, 93, 193, 8, 34, 18]	[1, 1, 1, 1, 1, 0, 0, 0, 1, 0]	[0.43725356459617615, 0.5433446168899536, 0.5364988446235657, 0.4856385886669159, 0.6195159554481506, 0.24537484347820282, 0.27445051074028015, 0.24097517132759094, 0.4756034016609192, 0.2423907369375229]
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	[58, 57, 111, 108, 21, 71, 148, 150, 120, 29]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6240143775939941, 0.6222761273384094, 0.14314647018909454, 0.3819185197353363, 0.11664460599422455, 0.3460808992385864, 0.34472474455833435, 0.2071121782064438, 0.18615999817848206, 0.39387714862823486]
An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.	[13, 1, 18, 92, 12, 19, 49, 11, 73, 48]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3132132291793823, 0.463009238243103, 0.3722008764743805, 0.16179384291172028, 0.2902508080005646, 0.2187892198562622, 0.20799151062965393, 0.1646902710199356, 0.09828757494688034, 0.1251484602689743]
An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.	[121, 119, 23, 153, 334, 89, 172, 247, 168, 390]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.376370370388031, 0.18651603162288666, 0.46060654520988464, 0.17173615097999573, 0.10349360853433609, 0.11382784694433212, 0.09122844785451889, 0.10449862480163574, 0.1804608702659607, 0.10006070137023926]
Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).	[2, 168, 0, 64, 21, 44, 47, 162, 45, 103]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.12598757445812225, 0.14223182201385498, 0.5309241414070129, 0.2691868841648102, 0.2046068161725998, 0.13283969461917877, 0.109810471534729, 0.11248259246349335, 0.11687108129262924, 0.1107795387506485]
As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.	[297, 228, 70, 300, 335, 445, 16, 6, 33, 389]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.5239039659500122, 0.49981027841567993, 0.279829740524292, 0.37795403599739075, 0.1092737466096878, 0.10965022444725037, 0.5721185803413391, 0.2632850706577301, 0.11998941749334335, 0.20769909024238586]
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	[167, 110, 3, 23, 205, 159, 202, 0, 67, 95]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 1]	[0.2376355230808258, 0.24218638241291046, 0.27929890155792236, 0.31847453117370605, 0.31847453117370605, 0.2190457582473755, 0.26083025336265564, 0.4886578917503357, 0.17430974543094635, 0.5244653224945068]
As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.	[158, 73, 21, 283, 24, 161, 338, 468, 162, 16]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20548351109027863, 0.4186461567878723, 0.2311442643404007, 0.13746684789657593, 0.2871919870376587, 0.18417023122310638, 0.13435891270637512, 0.22627851366996765, 0.24809500575065613, 0.23221968114376068]
As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.	[297, 133, 130, 70, 293, 33, 433, 370, 368, -6]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.433660089969635, 0.47283291816711426, 0.624238133430481, 0.2836766839027405, 0.2756783664226532, 0.30472704768180847, 0.20403844118118286, 0.09505461901426315, 0.11884757876396179, 0.15423136949539185]
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	[23, 22, 3, 205, 2, 67, 204, 24, 119, 156]	[1, 0, 1, 1, 0, 1, 0, 0, 0, 0]	[0.601766049861908, 0.18847395479679108, 0.5513744950294495, 0.601766049861908, 0.1926422119140625, 0.4943344295024872, 0.2691238522529602, 0.13227331638336182, 0.27418798208236694, 0.3061804175376892]
As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.	[325, 136, 247, 353, 350, 370, -6, 70, 346, 330]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22625517845153809, 0.09945765882730484, 0.1348860263824463, 0.2095409482717514, 0.32832202315330505, 0.12766589224338531, 0.29101237654685974, 0.18546399474143982, 0.22469724714756012, 0.1269730031490326]
At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).	[135, 4, 77, 106, 17, 0, 173, 95, 49, 132]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.10248049348592758, 0.21815940737724304, 0.47846919298171997, 0.1947217583656311, 0.13356049358844757, 0.2553458511829376, 0.11400722712278366, 0.2231793850660324, 0.2094630002975464, 0.1916438341140747]
Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).	[416, 123, 2, 137, 18, 418, 109, 419, 124, 403]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.10660036653280258, 0.10683229565620422, 0.24884645640850067, 0.17040479183197021, 0.12588241696357727, 0.11312397569417953, 0.11921312659978867, 0.12740112841129303, 0.5849704146385193, 0.12319459766149521]
BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).	[12, 48, 15, 14, 10, 76, 39, 1, 122, 38]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1263035535812378, 0.11906468123197556, 0.11583469808101654, 0.12513351440429688, 0.10590746253728867, 0.13730373978614807, 0.09795689582824707, 0.12323988974094391, 0.14017337560653687, 0.16357910633087158]
Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).	[67, 37, 24, 44, 39, 216, 32, 31, 164, 189]	[0, 1, 0, 0, 0, 0, 1, 1, 1, 1]	[0.1346401572227478, 0.65696781873703, 0.21793465316295624, 0.1775464415550232, 0.10942300409078598, 0.1373833417892456, 0.5873785018920898, 0.4662112593650818, 0.5686728954315186, 0.6372137665748596]
Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.	[155, 145, 102, 5, 211, 193, 103, 42, 19, 104]	[1, 1, 1, 0, 0, 0, 1, 0, 0, 0]	[0.6760412454605103, 0.40675997734069824, 0.4228641092777252, 0.3864954113960266, 0.3660827577114105, 0.33004820346832275, 0.42842817306518555, 0.3549875319004059, 0.3897053301334381, 0.17261949181556702]
Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.	[230, 284, 141, 74, 235, 78, 140, 204, 266, 426]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17882537841796875, 0.11091215163469315, 0.12886905670166016, 0.1454654186964035, 0.13135270774364471, 0.25036561489105225, 0.10589717328548431, 0.18133458495140076, 0.34362927079200745, 0.14959314465522766]
Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.	[195, 34, 97, 143, 140, 86, 211, 3, 132, 4]	[1, 0, 0, 1, 1, 0, 0, 0, 0, 0]	[0.47159460186958313, 0.2715238332748413, 0.3292379081249237, 0.40024253726005554, 0.44334548711776733, 0.20985502004623413, 0.16589592397212982, 0.14982645213603973, 0.20388883352279663, 0.17685528099536896]
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).	[25, 0, 33, 14, 104, 1, 37, 44, 19, 4]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.332390159368515, 0.47265005111694336, 0.2283926010131836, 0.16793765127658844, 0.18020139634609222, 0.2476353496313095, 0.13537020981311798, 0.10790333151817322, 0.13189880549907684, 0.1915367990732193]
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).	[104, 25, 14, 0, 44, 45, 152, 33, 30, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16040106117725372, 0.19255758821964264, 0.14534716308116913, 0.2288660705089569, 0.10719937831163406, 0.10150256752967834, 0.15968474745750427, 0.1721118688583374, 0.24749518930912018, 0.10417675971984863]
Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.	[283, 284, 280, 281, 399, 286, 161, 103, 285, 24]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.18409281969070435, 0.12129344046115875, 0.664158046245575, 0.15292543172836304, 0.2428542971611023, 0.14383870363235474, 0.13970188796520233, 0.15527750551700592, 0.10609652101993561, 0.2193859964609146]
Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.	[91, 89, 117, -3, 93, 329, 76, 92, 22, 104]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7656483054161072, 0.5268856287002563, 0.1994665116071701, 0.29964011907577515, 0.17990313470363617, 0.48017019033432007, 0.1012084111571312, 0.27221202850341797, 0.12429120391607285, 0.2380474954843521]
Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).	[51, 33, 23, 134, 415, 137, 3, 418, 440, 459]	[0, 1, 1, 0, 1, 1, 1, 0, 0, 1]	[0.17712396383285522, 0.4205453395843506, 0.5447107553482056, 0.3084976375102997, 0.44844746589660645, 0.5701342225074768, 0.5061196088790894, 0.10170426964759827, 0.27839896082878113, 0.42568275332450867]
Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.	[64, 137, 190, 54, 153, 21, 347, 75, 353, 51]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.2131335586309433, 0.47980502247810364, 0.20709069073200226, 0.6131942868232727, 0.1523391604423523, 0.13551945984363556, 0.26825079321861267, 0.11621671169996262, 0.12619426846504211, 0.09728635102510452]
Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).	[88, -1, 20, 21, 65, -5, 33, 54, 2, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15016789734363556, 0.10743638128042221, 0.12503978610038757, 0.1177675649523735, 0.16834358870983124, 0.15120814740657806, 0.1563788503408432, 0.15281091630458832, 0.10664186626672745, 0.15686526894569397]
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).	[161, 173, 129, 172, 155, 6, 109, 3, 19, 132]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12197908759117126, 0.28434455394744873, 0.1864723265171051, 0.11477846652269363, 0.1388566493988037, 0.0983654037117958, 0.174261674284935, 0.1280236691236496, 0.14339230954647064, 0.13713569939136505]
Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)	[51, 32, 37, 25, 44, 162, 215, 33, 164, 204]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.10117349028587341, 0.3951092064380646, 0.5807905793190002, 0.09985201805830002, 0.198635995388031, 0.1109790951013565, 0.11471283435821533, 0.10409194976091385, 0.24907585978507996, 0.12664295732975006]
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	[3, 13, 154, 15, 75, 105, 17, 42, 70, 25]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21421386301517487, 0.10884542763233185, 0.14847253262996674, 0.12304211407899857, 0.10682400315999985, 0.1278349757194519, 0.14673854410648346, 0.13922074437141418, 0.14099282026290894, 0.10582403093576431]
Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).	[112, 108, -5, 115, 2, 305, -1, 65, 55, 308]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.20175564289093018, 0.19763222336769104, 0.28754305839538574, 0.5985168814659119, 0.3222272992134094, 0.20048238337039948, 0.2642350196838379, 0.14288799464702606, 0.17580869793891907, 0.35022759437561035]
Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.	[63, 41, 109, 62, 110, 39, 100, 3, 70, 73]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.3163962960243225, 0.179475337266922, 0.160143181681633, 0.23274795711040497, 0.5524685382843018, 0.24385596811771393, 0.17493946850299835, 0.18074096739292145, 0.12775155901908875, 0.10090532898902893]
Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.	[5, 3, 44, 45, 40, 146, 93, 4, 47, 156]	[1, 0, 1, 0, 1, 0, 0, 0, 1, 0]	[0.47556838393211365, 0.3835533559322357, 0.5532720685005188, 0.21271486580371857, 0.40594255924224854, 0.2737739086151123, 0.23192298412322998, 0.2026134729385376, 0.41830557584762573, 0.34386545419692993]
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	[22, 203, 1, 0, 12, 34, 167, 151, 2, 24]	[0, 0, 0, 1, 0, 0, 1, 0, 0, 0]	[0.10457947105169296, 0.2047489732503891, 0.242847740650177, 0.45831599831581116, 0.20954276621341705, 0.21268460154533386, 0.5289720296859741, 0.14659631252288818, 0.10042815655469894, 0.0955248475074768]
Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).	[93, 143, 195, 33, 34, 37, 96, 216, 32, 193]	[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]	[0.11146397888660431, 0.2315497100353241, 0.23546546697616577, 0.23176361620426178, 0.09605749696493149, 0.6450108885765076, 0.14774863421916962, 0.12827256321907043, 0.5796158909797668, 0.12697507441043854]
Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).	[87, 53, 61, 19, 62, 10, 167, 217, 60, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12657992541790009, 0.12881872057914734, 0.11052606999874115, 0.13123978674411774, 0.11836592108011246, 0.10497137904167175, 0.11735814064741135, 0.1026151180267334, 0.12757520377635956, 0.09860221296548843]
Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.	[28, 60, 133, 26, 97, 37, 134, 110, 147, 195]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.21496286988258362, 0.25354140996932983, 0.27249521017074585, 0.30081161856651306, 0.18820175528526306, 0.6763502955436707, 0.3817256689071655, 0.12456698715686798, 0.10400984436273575, 0.3084985911846161]
Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.	[193, 197, 4, 93, 195, 97, 143, 34, 31, 85]	[0, 0, 0, 1, 0, 0, 0, 1, 1, 0]	[0.3812723457813263, 0.13022330403327942, 0.16347360610961914, 0.5047459602355957, 0.31866180896759033, 0.39474692940711975, 0.2460387647151947, 0.539745569229126, 0.5903689861297607, 0.12305411696434021]
Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).	[30, 81, 84, 83, 41, 92, 80, 79, 3, 122]	[0, 1, 1, 1, 0, 0, 0, 0, 1, 0]	[0.12100809067487717, 0.6192120313644409, 0.46942752599716187, 0.615368127822876, 0.1216864213347435, 0.1252518594264984, 0.12978078424930573, 0.19715465605258942, 0.40477436780929565, 0.13110405206680298]
Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).	[325, 300, 383, 93, 181, 56, 21, 136, 18, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3608839809894562, 0.25737303495407104, 0.1508939415216446, 0.29691821336746216, 0.12952709197998047, 0.11827065050601959, 0.14670930802822113, 0.0998518168926239, 0.11335910111665726, 0.1422559916973114]
Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).	[325, 300, 383, 93, 181, 56, 21, 136, 18, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3219382166862488, 0.2625672519207001, 0.15916092693805695, 0.28175321221351624, 0.1257411241531372, 0.11785488575696945, 0.14080046117305756, 0.10330462455749512, 0.11361118406057358, 0.13688147068023682]
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	[78, 0, 32, 25, 19, 1, 101, 137, 30, 33]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4472064971923828, 0.3174693286418915, 0.1953366994857788, 0.32724228501319885, 0.1389165222644806, 0.20224902033805847, 0.1690291166305542, 0.11968567222356796, 0.2809433043003082, 0.18462447822093964]
Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.	[40, 17, 192, 2, 60, 130, 36, 162, 163, 28]	[1, 1, 1, 1, 1, 0, 0, 1, 0, 0]	[0.4142196774482727, 0.456947922706604, 0.5058188438415527, 0.47737595438957214, 0.47763293981552124, 0.25199127197265625, 0.31313905119895935, 0.48176899552345276, 0.16609181463718414, 0.18340207636356354]
First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).	[325, 246, 300, 133, 33, 93, 264, 221, 74, 374]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13055269420146942, 0.10038190335035324, 0.28217318654060364, 0.1594604104757309, 0.28022271394729614, 0.3539573550224304, 0.390644371509552, 0.1832534521818161, 0.11359690874814987, 0.21983158588409424]
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	[62, 86, 44, 77, 182, 50, 42, 110, 158, 75]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20030394196510315, 0.14518393576145172, 0.11378484219312668, 0.2691907286643982, 0.11112180352210999, 0.12076292186975479, 0.11212445050477982, 0.397222101688385, 0.16407757997512817, 0.10394217073917389]
For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).	[88, 402, 52, 461, 22, 23, 21, 71, 73, 410]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.27669256925582886, 0.6451173424720764, 0.2452293038368225, 0.2601292133331299, 0.2166234254837036, 0.15161417424678802, 0.18539310991764069, 0.0996328815817833, 0.26608502864837646, 0.09393879026174545]
For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).	[60, 17, 192, 34, 36, 2, 39, 144, 196, 126]	[1, 0, 1, 1, 1, 0, 1, 0, 0, 0]	[0.7640702724456787, 0.21317893266677856, 0.44154661893844604, 0.41759270429611206, 0.4106256067752838, 0.30238059163093567, 0.5057339072227478, 0.21325553953647614, 0.18751533329486847, 0.14283281564712524]
For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].	[108, 112, 305, 174, 32, 191, 172, 190, 140, 308]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10875502973794937, 0.10357628017663956, 0.12433791905641556, 0.13993501663208008, 0.12322543561458588, 0.18222171068191528, 0.10368379205465317, 0.11728478968143463, 0.16174502670764923, 0.2255358099937439]
For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).	[20, 196, 144, 86, 63, 158, 5, 112, 197, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11382006853818893, 0.20083019137382507, 0.17397256195545197, 0.09973455220460892, 0.18262217938899994, 0.1101377084851265, 0.15938788652420044, 0.24723437428474426, 0.11445659399032593, 0.10103308409452438]
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	[1, 19, 151, 27, 152, 135, 0, 25, 139, 21]	[0, 0, 1, 0, 0, 0, 1, 0, 0, 0]	[0.2819327414035797, 0.2605845034122467, 0.4877777695655823, 0.19856928288936615, 0.1401529312133789, 0.34323596954345703, 0.5258118510246277, 0.36549776792526245, 0.1050604060292244, 0.15357111394405365]
Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.	[51, 162, 37, 213, 215, 6, 197, 144, 196, 124]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.09947711229324341, 0.1486773043870926, 0.6498231887817383, 0.10426141321659088, 0.10265251994132996, 0.10522256791591644, 0.10153120756149292, 0.10691220313310623, 0.10162263363599777, 0.13936378061771393]
Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).	[412, 410, 408, 314, 318, 319, 313, 317, 161, 128]	[1, 1, 1, 1, 1, 0, 1, 1, 0, 1]	[0.48309147357940674, 0.7043790817260742, 0.5245869755744934, 0.484595388174057, 0.4740939438343048, 0.31064602732658386, 0.555860161781311, 0.5669217705726624, 0.3322325050830841, 0.5347509384155273]
How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.	[193, 5, 3, 211, 42, 195, 41, 4, 213, 25]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6079315543174744, 0.5288026928901672, 0.5050129890441895, 0.5032865405082703, 0.33784255385398865, 0.3347150683403015, 0.3544095754623413, 0.39298295974731445, 0.20950454473495483, 0.3330022692680359]
However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.	[193, 41, 3, 20, 18, 130, 143, 5, 211, 42]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.22807572782039642, 0.7509945631027222, 0.4504922330379486, 0.3175077736377716, 0.3466441333293915, 0.15977922081947327, 0.15737025439739227, 0.11874198168516159, 0.12041598558425903, 0.15139225125312805]
However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.	[38, 5, 3, 211, 195, 42, 75, 213, 163, 216]	[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]	[0.5083448886871338, 0.510533332824707, 0.21719063818454742, 0.505454957485199, 0.310273140668869, 0.4289776384830475, 0.1315929889678955, 0.13935586810112, 0.15052345395088196, 0.41043874621391296]
However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.	[56, 194, 120, 23, 214, 192, 18, 25, 141, 218]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13280926644802094, 0.1254342943429947, 0.13670261204242706, 0.11390423774719238, 0.10904420912265778, 0.14140982925891876, 0.11363043636083603, 0.14807265996932983, 0.14228196442127228, 0.1274237483739853]
Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).	[141, 169, 11, 47, 34, 75, 23, 36, 39, 72]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10070692747831345, 0.11026859283447266, 0.13510221242904663, 0.12704221904277802, 0.15687823295593262, 0.16669531166553497, 0.1564597636461258, 0.14407016336917877, 0.13740487396717072, 0.16684086620807648]
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	[67, 2, 70, 56, 101, 8, 60, 62, 130, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4188912808895111, 0.21461494266986847, 0.19468247890472412, 0.12576881051063538, 0.2393687218427658, 0.11728829145431519, 0.3655015528202057, 0.17050987482070923, 0.26610416173934937, 0.1994556039571762]
In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.	[89, 91, 117, 121, 455, 370, 329, 108, 249, 54]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5515868067741394, 0.5924102067947388, 0.28922170400619507, 0.1120380312204361, 0.11376628279685974, 0.09595063328742981, 0.17015551030635834, 0.0943232923746109, 0.14829318225383759, 0.2559431195259094]
In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).	[419, 24, 20, 135, 123, 117, 128, 398, 170, 54]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15458039939403534, 0.36721962690353394, 0.12204741686582565, 0.166861891746521, 0.15424412488937378, 0.20577865839004517, 0.36310863494873047, 0.11564736068248749, 0.12561675906181335, 0.1212688684463501]
In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).	[107, 114, 162, 245, 328, 428, 158, 398, -5, 170]	[0, 0, 1, 1, 1, 0, 0, 0, 0, 1]	[0.36972367763519287, 0.36638176441192627, 0.6445411443710327, 0.6107954978942871, 0.40820419788360596, 0.11618731915950775, 0.1802884340286255, 0.08700360357761383, 0.30444228649139404, 0.43901100754737854]
In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).	[92, 433, 129, 121, 328, 412, 191, 19, 187, 293]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.31157436966896057, 0.1098509430885315, 0.276233047246933, 0.20198892056941986, 0.23698517680168152, 0.3630378544330597, 0.6494106650352478, 0.14756909012794495, 0.15666905045509338, 0.22819919884204865]
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).	[65, 6, 229, 24, 234, 41, 69, 33, 76, 31]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.3918643891811371, 0.10662736743688583, 0.5208602547645569, 0.16379275918006897, 0.16799579560756683, 0.15641942620277405, 0.15886028110980988, 0.1225067600607872, 0.22967660427093506, 0.14567209780216217]
In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).	[108, 109, 89, 112, 117, 402, 393, 305, 240, 335]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13796687126159668, 0.12373045086860657, 0.13130727410316467, 0.10515870898962021, 0.15633375942707062, 0.2073371261358261, 0.1111232191324234, 0.24833707511425018, 0.2440490424633026, 0.10622982680797577]
In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).	[165, 157, 97, 110, 100, 109, 40, 112, 52, 29]	[1, 0, 0, 1, 0, 0, 0, 0, 1, 0]	[0.41226524114608765, 0.13834479451179504, 0.19267620146274567, 0.5927801132202148, 0.2259255200624466, 0.3803756833076477, 0.22560156881809235, 0.18951569497585297, 0.4407927989959717, 0.391083687543869]
In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.	[89, 104, 117, 114, 135, 107, 120, 329, 108, 119]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49263229966163635, 0.41615599393844604, 0.30444034934043884, 0.3003285527229309, 0.1526350975036621, 0.1972532570362091, 0.2948373854160309, 0.14999277889728546, 0.12185374647378922, 0.11382846534252167]
In the future, we would like to use more effective feature sets Zhou et al.(2005)	[25, 216, 24, 42, 63, 53, 195, 112, 19, 5]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10017625242471695, 0.2034383863210678, 0.14894890785217285, 0.14230431616306305, 0.16587230563163757, 0.11735579371452332, 0.10091489553451538, 0.3268224895000458, 0.1007797122001648, 0.09674675017595291]
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	[13, 8, 21, 18, 201, 12, 11, 202, 67, 40]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.46647655963897705, 0.12474965304136276, 0.28471270203590393, 0.1502765417098999, 0.1020602434873581, 0.24260950088500977, 0.14267076551914215, 0.09968633204698563, 0.1187620609998703, 0.7631706595420837]
Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).	[130, 300, 18, -2, 70, 297, 20, 283, 51, -3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09582071006298065, 0.2935091555118561, 0.1347176432609558, 0.1651924103498459, 0.17643485963344574, 0.27081555128097534, 0.19649580121040344, 0.117709681391716, 0.09968076646327972, 0.14047016203403473]
Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).	[17, 192, 2, 5, 25, 216, 163, 211, 11, 34]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11821158230304718, 0.13973984122276306, 0.1345464438199997, 0.1250826120376587, 0.1517544686794281, 0.11363829672336578, 0.11921286582946777, 0.12130267918109894, 0.12136989086866379, 0.11635981500148773]
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	[40, 195, 52, 53, 201, 111, 84, 166, 146, 168]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5911169648170471, 0.1258564293384552, 0.14500002562999725, 0.17338697612285614, 0.1445743441581726, 0.13097526133060455, 0.17364542186260223, 0.12199322134256363, 0.161934494972229, 0.13681481778621674]
It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).	[1, 51, 0, 192, 48, 13, 43, 116, 39, 38]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4227559268474579, 0.15364325046539307, 0.29956236481666565, 0.1454782634973526, 0.12068053334951401, 0.10273440182209015, 0.1582183986902237, 0.11083202064037323, 0.300510048866272, 0.12175590544939041]
It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.	[121, 122, 149, 117, 190, 84, 6, 43, 160, 214]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.258791446685791, 0.13934607803821564, 0.5907342433929443, 0.22167997062206268, 0.21456271409988403, 0.3462298512458801, 0.22251416742801666, 0.24809113144874573, 0.12934638559818268, 0.10789211094379425]
Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).	[49, 161, 68, 4, 23, 6, 173, 129, 93, 172]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40457072854042053, 0.12118799984455109, 0.12613148987293243, 0.16114766895771027, 0.2211264818906784, 0.15804845094680786, 0.27914953231811523, 0.2386443316936493, 0.10890358686447144, 0.10817421227693558]
Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.	[20, 9, 51, 1, 0, 8, 25, 52, 11, 124]	[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.168068990111351, 0.7663133144378662, 0.11263762414455414, 0.1028020903468132, 0.3301169276237488, 0.37856021523475647, 0.628493070602417, 0.11322309076786041, 0.22278033196926117, 0.3547442853450775]
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	[164, 89, 12, 151, 167, 24, 206, 202, 13, 0]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.14728084206581116, 0.16804586350917816, 0.14147846400737762, 0.12939400970935822, 0.4339921474456787, 0.10249724984169006, 0.10340234637260437, 0.10437515377998352, 0.1385658234357834, 0.32824796438217163]
Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.	[138, 67, 398, 457, -4, 434, 399, 456, 420, 286]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3546801209449768, 0.11383986473083496, 0.1625319868326187, 0.1114458292722702, 0.12669186294078827, 0.09938482195138931, 0.11189910024404526, 0.12073821574449539, 0.12101691961288452, 0.1753016710281372]
Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.	[4, 15, 29, 26, 44, 30, 117, 74, 0, 45]	[1, 1, 1, 1, 1, 0, 0, 0, 1, 0]	[0.670304000377655, 0.6361470818519592, 0.6089624166488647, 0.5112727284431458, 0.44276508688926697, 0.3666892647743225, 0.25983932614326477, 0.20824842154979706, 0.5557908415794373, 0.39620333909988403]
More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.	[85, 77, 84, 223, 169, 231, 48, 201, 220, 183]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13715766370296478, 0.10882895439863205, 0.15829983353614807, 0.10516051203012466, 0.11787066608667374, 0.10829056054353714, 0.1153547540307045, 0.11358664184808731, 0.12985141575336456, 0.0996536836028099]
More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).	[25, 193, 93, 18, 3, 24, 192, 85, 40, 34]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10736381262540817, 0.12289918959140778, 0.1108858659863472, 0.10475457459688187, 0.10207457840442657, 0.16688492894172668, 0.142498180270195, 0.12582708895206451, 0.10194724053144455, 0.15047885477542877]
Most of the features used in our system are based on the work in (Zhou et al., 2005).	[108, 6, 20, 44, 212, 144, 19, 18, 3, 2]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11212456226348877, 0.0924922302365303, 0.09065894037485123, 0.28213831782341003, 0.0912017822265625, 0.0988505482673645, 0.2273004800081253, 0.10350313037633896, 0.10350390523672104, 0.14941304922103882]
Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).	[128, 214, 121, 191, 194, 43, 6, 18, 67, 141]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1542922407388687, 0.17432817816734314, 0.17103227972984314, 0.12001784890890121, 0.14420849084854126, 0.11623285710811615, 0.10183137655258179, 0.10322059690952301, 0.13606519997119904, 0.1247650608420372]
Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).	[17, 8, 12, 0, 24, 4, 2, 49, 42, 13]	[0, 0, 0, 1, 1, 0, 0, 0, 0, 1]	[0.16064919531345367, 0.12240124493837357, 0.3308373689651489, 0.4319017231464386, 0.7749766707420349, 0.1572568714618683, 0.13730821013450623, 0.25492537021636963, 0.2051699012517929, 0.7736973762512207]
Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)	[202, 16, 233, 44, 45, 193, 277, 235, 232, 77]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12390992790460587, 0.23454974591732025, 0.18868468701839447, 0.14062903821468353, 0.1400851160287857, 0.15375684201717377, 0.10302425175905228, 0.1397840976715088, 0.1958644986152649, 0.22614668309688568]
No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).	[325, 21, 22, 54, 51, 132, 412, 304, -6, 115]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5954806804656982, 0.2636810839176178, 0.12763279676437378, 0.10252828896045685, 0.10329645127058029, 0.10880997776985168, 0.12416242063045502, 0.22187182307243347, 0.13036289811134338, 0.10218290239572525]
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	[78, 79, 50, 143, 43, 72, 81, 74, 37, 45]	[1, 0, 0, 1, 1, 0, 0, 0, 0, 0]	[0.4725520610809326, 0.1667954921722412, 0.22815781831741333, 0.5440483093261719, 0.4759121239185333, 0.3488583266735077, 0.3653779923915863, 0.1898820847272873, 0.15251238644123077, 0.13710731267929077]
Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).	[1, 39, 2, 50, 184, 0, 192, 43, 174, 154]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.3464580178260803, 0.2607201933860779, 0.0970982164144516, 0.5472095012664795, 0.1394796073436737, 0.3347678780555725, 0.09754735976457596, 0.150053933262825, 0.25159892439842224, 0.10911664366722107]
One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).	[138, -4, 398, 114, 67, 153, 117, 141, 455, 420]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.5152847766876221, 0.48094892501831055, 0.5207602977752686, 0.3375099301338196, 0.23548182845115662, 0.4720836877822876, 0.2696795165538788, 0.107472263276577, 0.13171394169330597, 0.12411533296108246]
One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.	[33, 2, 45, 19, 7, 151, 60, 9, 38, 75]	[0, 0, 0, 0, 1, 1, 0, 0, 0, 0]	[0.26439517736434937, 0.38075196743011475, 0.1253431737422943, 0.2500908374786377, 0.5293265581130981, 0.47084280848503113, 0.13642865419387817, 0.3720824718475342, 0.2569998800754547, 0.10719858109951019]
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).	[39, 38, 126, 148, 25, 24, 213, 1, 214, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5760994553565979, 0.11671462655067444, 0.2945813536643982, 0.2269326001405716, 0.2587171196937561, 0.13924060761928558, 0.0954909697175026, 0.35207599401474, 0.11911018192768097, 0.3286115229129791]
One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.	[134, 188, -5, 65, 115, 280, 415, 399, 56, 137]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.4201817810535431, 0.3219149708747864, 0.1755681335926056, 0.23383314907550812, 0.13773153722286224, 0.7452763319015503, 0.21381907165050507, 0.24432608485221863, 0.34060052037239075, 0.36891645193099976]
Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.	[174, 27, 67, 26, 29, 80, 117, 12, 74, 42]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4839038550853729, 0.34416478872299194, 0.3209053575992584, 0.35027506947517395, 0.41985079646110535, 0.11870458722114563, 0.14513446390628815, 0.30855315923690796, 0.14557218551635742, 0.17509767413139343]
Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.	[40, 174, 27, 42, 29, 28, 30, 26, 6, 74]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.31517133116722107, 0.5504923462867737, 0.29651498794555664, 0.2568005621433258, 0.4157455265522003, 0.16036628186702728, 0.1466430425643921, 0.2771802246570587, 0.24201905727386475, 0.10804415494203568]
Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	[72, 4, 37, 78, 129, 14, 21, 25, 138, 43]	[1, 0, 0, 1, 0, 1, 0, 0, 0, 1]	[0.5308807492256165, 0.16036562621593475, 0.13514414429664612, 0.4083857238292694, 0.14643509685993195, 0.4871877431869507, 0.10110636055469513, 0.23308251798152924, 0.29791346192359924, 0.503022313117981]
Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.	[12, 57, 41, 31, 9, 106, 42, 198, 30, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2639855146408081, 0.21695458889007568, 0.2714667022228241, 0.1097443476319313, 0.10732929408550262, 0.2404138296842575, 0.14257366955280304, 0.21932336688041687, 0.12464620918035507, 0.18551279604434967]
Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.	[57, 13, 0, 225, 59, 44, 19, 43, 2, 187]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.11983843147754669, 0.11152368038892746, 0.5364570617675781, 0.3423767387866974, 0.1179579645395279, 0.14125020802021027, 0.11751122027635574, 0.11624372005462646, 0.17463701963424683, 0.1075272187590599]
Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.	[124, 125, 48, 115, 353, 123, 390, 168, 263, 137]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7703629732131958, 0.5389464497566223, 0.11128488928079605, 0.11892624199390411, 0.12488432228565216, 0.10254919528961182, 0.14309903979301453, 0.11647448688745499, 0.16843187808990479, 0.16504721343517303]
Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).	[17, 36, 50, 9, 216, 2, 33, 38, 45, 30]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.11174007505178452, 0.1081196665763855, 0.1286204755306244, 0.18183252215385437, 0.2300475686788559, 0.18191666901111603, 0.4243195652961731, 0.16402365267276764, 0.11403849720954895, 0.10238402336835861]
Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.	[17, 163, 162, 215, 2, 216, 108, 5, 192, 25]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.12336014211177826, 0.23121561110019684, 0.4678841531276703, 0.2840850353240967, 0.16379021108150482, 0.2007942944765091, 0.18891924619674683, 0.19300052523612976, 0.1614748239517212, 0.3600124418735504]
Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).	[45, 52, 30, 37, 216, 67, 38, 144, 196, 108]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.23993021249771118, 0.16488346457481384, 0.11385254561901093, 0.7107105255126953, 0.125143364071846, 0.134572371840477, 0.10096284002065659, 0.13532105088233948, 0.11226489394903183, 0.10984846949577332]
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	[72, 8, 1, 151, 5, 21, 131, 135, 0, 89]	[0, 0, 1, 1, 1, 0, 1, 1, 1, 0]	[0.17010630667209625, 0.10907280445098877, 0.6547819375991821, 0.701337993144989, 0.5872367024421692, 0.09840021282434464, 0.45065608620643616, 0.41712701320648193, 0.6433970332145691, 0.14974890649318695]
Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).	[17, 42, 29, 8, 159, 168, 170, 166, 18, 35]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.1278889775276184, 0.12261359393596649, 0.17412161827087402, 0.19100764393806458, 0.1472247987985611, 0.7415128946304321, 0.16424456238746643, 0.154469296336174, 0.171950101852417, 0.17893068492412567]
Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.	[110, 59, 123, 126, 19, 95, 39, 34, 22, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.35720857977867126, 0.1694202870130539, 0.35534289479255676, 0.10322236269712448, 0.12304960936307907, 0.16007199883460999, 0.1398921012878418, 0.10533365607261658, 0.5647496581077576, 0.35803288221359253]
Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.	[1, 139, 123, 192, 184, 2, 42, 48, 81, 46]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10731835663318634, 0.18830513954162598, 0.11770062148571014, 0.10867772996425629, 0.13935740292072296, 0.09301362931728363, 0.25564056634902954, 0.11376646906137466, 0.2623104155063629, 0.155829519033432]
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	[8, 9, 0, 1, 151, 25, 5, 21, 19, 61]	[0, 1, 1, 1, 1, 1, 1, 0, 0, 0]	[0.13303346931934357, 0.43455561995506287, 0.6559692025184631, 0.4309055805206299, 0.5200150012969971, 0.43935221433639526, 0.4938863217830658, 0.10265900194644928, 0.18913571536540985, 0.1847170740365982]
Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).	[23, 20, 24, 185, 52, 65, 154, 340, 371, 133]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.19148187339305878, 0.11767175793647766, 0.2123836874961853, 0.16739198565483093, 0.14421947300434113, 0.11863622814416885, 0.2290961593389511, 0.4999394416809082, 0.13499489426612854, 0.10258065909147263]
Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.	[17, 3, 173, 4, 81, 11, 85, 83, 0, 98]	[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]	[0.4819721579551697, 0.6537597179412842, 0.6719875335693359, 0.41169869899749756, 0.615591824054718, 0.44221118092536926, 0.4639931917190552, 0.5762460827827454, 0.43816429376602173, 0.30551570653915405]
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	[13, 0, 202, 21, 201, 126, 18, 44, 209, 15]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1368141621351242, 0.5505343675613403, 0.10478884726762772, 0.15656356513500214, 0.11109684407711029, 0.32914069294929504, 0.18610253930091858, 0.11679010093212128, 0.13745667040348053, 0.3255271911621094]
Similarly, Sproat et al.(1996) also uses multiple human judges.	[412, 350, 130, 70, 346, 297, 353, 113, 83, 330]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5838902592658997, 0.37620091438293457, 0.13779889047145844, 0.4500807225704193, 0.28890570998191833, 0.298031747341156, 0.2081964910030365, 0.28504616022109985, 0.12213040888309479, 0.10450258105993271]
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;	[21, 33, 20, 49, 54, 23, 3, 75, 32, 427]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16156458854675293, 0.1749485433101654, 0.15910284221172333, 0.14407043159008026, 0.18099474906921387, 0.13452477753162384, 0.13085578382015228, 0.1986895352602005, 0.17049628496170044, 0.14752769470214844]
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	[40, 96, 170, 6, 55, 39, 175, 195, 144, 111]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7352138757705688, 0.12060520052909851, 0.11016441881656647, 0.1594589352607727, 0.16574937105178833, 0.11625286936759949, 0.20732808113098145, 0.1219102069735527, 0.1262834072113037, 0.13773183524608612]
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	[195, 17, 52, 53, 35, 55, 181, 111, 18, 40]	[0, 0, 1, 0, 0, 0, 0, 1, 0, 1]	[0.22604359686374664, 0.11336340010166168, 0.44019705057144165, 0.13045091927051544, 0.1642252802848816, 0.26269978284835815, 0.1616402268409729, 0.422254741191864, 0.14919161796569824, 0.7074425220489502]
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.	[53, 52, 18, 28, 17, 63, 14, 35, 84, 40]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.13096411526203156, 0.4634585976600647, 0.22278310358524323, 0.10362619906663895, 0.13888464868068695, 0.1767454594373703, 0.10079946368932724, 0.13020969927310944, 0.12512552738189697, 0.6471744775772095]
Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).	[137, 195, 132, 58, 128, 46, 85, 35, 27, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.12229740619659424, 0.11005429923534393, 0.1377149224281311, 0.15085521340370178, 0.10913258790969849, 0.1319877803325653, 0.12144088745117188, 0.10670540481805801, 0.13888591527938843, 0.7688450813293457]
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	[8, 151, 101, 9, 10, 19, 36, 88, 130, 25]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10122482478618622, 0.10927017033100128, 0.14427132904529572, 0.1297953873872757, 0.19042958319187164, 0.16977910697460175, 0.1746089905500412, 0.1177849993109703, 0.12498955428600311, 0.1284124255180359]
Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).	[90, 168, 12, 0, 13, 23, 173, 49, 4, 17]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.12353791296482086, 0.7292852997779846, 0.15901996195316315, 0.15606281161308289, 0.7169833779335022, 0.12496091425418854, 0.11557348072528839, 0.12194740772247314, 0.16230033338069916, 0.15300488471984863]
Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).	[17, 0, 4, 173, 12, 168, 49, 13, 3, 6]	[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]	[0.12597998976707458, 0.3518770933151245, 0.11738044768571854, 0.19195018708705902, 0.33517318964004517, 0.7494756579399109, 0.17503687739372253, 0.7572897672653198, 0.1487748622894287, 0.17500638961791992]
Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.	[138, 0, -4, 65, 71, 455, 398, 137, 67, 35]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4946631193161011, 0.5877085328102112, 0.3810715079307556, 0.16040246188640594, 0.10213356465101242, 0.14302502572536469, 0.17090678215026855, 0.3688022494316101, 0.1576286256313324, 0.25098246335983276]
Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.	[399, 281, 284, 170, 383, 286, 283, 280, 116, 123]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.350406676530838, 0.3241519629955292, 0.20308995246887207, 0.23774206638336182, 0.3075993061065674, 0.11437342315912247, 0.14358237385749817, 0.6971172094345093, 0.11458181589841843, 0.10713671892881393]
Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)	[168, 114, 119, 117, 124, 19, 444, 455, 172, 125]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.15199777483940125, 0.11411774158477783, 0.184250608086586, 0.1089165061712265, 0.7559399604797363, 0.15869808197021484, 0.1155715212225914, 0.14501401782035828, 0.15870946645736694, 0.17576831579208374]
Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.	[47, 0, 44, 68, 2, 87, 225, 69, 109, 64]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.22929948568344116, 0.5347611904144287, 0.20523029565811157, 0.7179649472236633, 0.1218458041548729, 0.21088264882564545, 0.18635299801826477, 0.10876365005970001, 0.10911376774311066, 0.35626545548439026]
Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.	[8, 45, 52, 30, 120, 7, 38, 213, 9, 79]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10451783239841461, 0.2009987235069275, 0.10004016757011414, 0.10594279319047928, 0.2943449914455414, 0.12417439371347427, 0.10100329667329788, 0.09676254540681839, 0.1116686761379242, 0.1385098397731781]
The Chinese person-name model is a modified version of that described in Sproat et al.(1996).	[399, 458, 242, 237, 264, 466, 227, 462, 228, 80]	[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.1058913841843605, 0.5146370530128479, 0.37687504291534424, 0.21307878196239471, 0.15076570212841034, 0.2832586467266083, 0.6344279646873474, 0.15873001515865326, 0.12452683597803116, 0.1280028074979782]
The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).	[33, 53, -1, 20, 3, 4, 130, 28, 293, 49]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.2105415314435959, 0.5351003408432007, 0.1194000095129013, 0.13362883031368256, 0.4331315755844116, 0.244850292801857, 0.14840346574783325, 0.230340838432312, 0.14438804984092712, 0.310055673122406]
The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.	[-4, 398, 457, 138, 444, 153, 141, 67, 461, 190]	[1, 0, 0, 1, 0, 1, 0, 0, 0, 0]	[0.5445500016212463, 0.287100613117218, 0.1385573446750641, 0.5925083756446838, 0.19773989915847778, 0.40211671590805054, 0.1512049436569214, 0.12484204024076462, 0.10138271749019623, 0.16388943791389465]
The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.	[94, 139, 82, 91, 42, 119, 100, 8, 130, 32]	[0, 0, 1, 1, 1, 0, 0, 0, 0, 0]	[0.3448665142059326, 0.27698421478271484, 0.5580627918243408, 0.501822292804718, 0.595449686050415, 0.3308875560760498, 0.12214266508817673, 0.14754533767700195, 0.2083912044763565, 0.12182541936635971]
The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.	[52, 118, 47, 116, 80, 54, 6, 138, 117, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15447810292243958, 0.10475426912307739, 0.305875688791275, 0.12489878386259079, 0.11817406862974167, 0.19770458340644836, 0.11865250766277313, 0.16406705975532532, 0.15324899554252625, 0.14363427460193634]
The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.	[192, 3, 170, 15, 105, 53, 54, 33, 1, 16]	[1, 0, 1, 1, 1, 0, 0, 1, 0, 0]	[0.5636348724365234, 0.35823190212249756, 0.5208151340484619, 0.7356656193733215, 0.5752463340759277, 0.17610350251197815, 0.2951820194721222, 0.7405409812927246, 0.13266156613826752, 0.10167529433965683]
The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.	[120, 24, 25, 46, 13, 19, 192, 5, 2, 138]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5074737668037415, 0.14538799226284027, 0.10281139612197876, 0.16060194373130798, 0.115059994161129, 0.12093060463666916, 0.1168806329369545, 0.12973558902740479, 0.0996500551700592, 0.2118527889251709]
The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.	[34, 93, 39, 86, 88, 3, 18, 193, 144, 97]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34890177845954895, 0.11149009317159653, 0.1630256026983261, 0.19917264580726624, 0.10693071782588959, 0.09514601528644562, 0.09315792471170425, 0.10506466031074524, 0.14007139205932617, 0.16520793735980988]
The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).	[58, 6, 28, 35, 36, 21, 197, 59, 196, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10300055146217346, 0.1351083517074585, 0.12224698811769485, 0.15656854212284088, 0.1158222109079361, 0.2823220193386078, 0.18526402115821838, 0.10105268657207489, 0.09838744252920151, 0.12568432092666626]
The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).	[17, 12, 11, 23, 9, 4, 8, 0, 66, 173]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11527145653963089, 0.17988014221191406, 0.13821302354335785, 0.1331854909658432, 0.1273789405822754, 0.12064344435930252, 0.17638462781906128, 0.17950339615345, 0.1917785406112671, 0.10862591862678528]
The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).	[20, 357, 367, 110, 124, 358, 245, -3, 244, 364]	[0, 0, 1, 1, 1, 1, 1, 0, 0, 0]	[0.13355988264083862, 0.3131420314311981, 0.5165261030197144, 0.6010525226593018, 0.5363097190856934, 0.6411429047584534, 0.638941764831543, 0.1526186168193817, 0.3058428168296814, 0.34445974230766296]
"The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it ""maximum matching"", we call this method ""longest match"" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:."	[108, 305, 112, 307, 23, 187, 16, 426, 172, 123]	[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]	[0.21756073832511902, 0.3788697123527527, 0.2550453245639801, 0.4452473521232605, 0.48094412684440613, 0.13249284029006958, 0.20311115682125092, 0.12167736887931824, 0.10564250499010086, 0.1592949479818344]
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	[164, 20, 195, 10, 52, 53, 40, 203, 202, 166]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.19721175730228424, 0.11840032041072845, 0.1611734926700592, 0.3221296966075897, 0.2979780435562134, 0.11738560348749161, 0.6733331680297852, 0.09274638444185257, 0.15292415022850037, 0.13606058061122894]
The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.	[7, 2, 21, 49, 3, 0, 11, 42, 12, 8]	[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.15829645097255707, 0.20501285791397095, 0.6380977034568787, 0.5234606862068176, 0.15561853349208832, 0.23000343143939972, 0.2267998456954956, 0.20253849029541016, 0.39948782324790955, 0.15495803952217102]
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.	[138, -4, 398, 67, 434, 153, 141, 444, 71, 443]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.614953339099884, 0.624573826789856, 0.3979775309562683, 0.18334616720676422, 0.20337389409542084, 0.47301462292671204, 0.15476886928081512, 0.20715929567813873, 0.08884213864803314, 0.14472992718219757]
The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).	[170, 238, 145, 183, 16, 342, 336, 426, 193, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31098806858062744, 0.37611010670661926, 0.3176502287387848, 0.13926012814044952, 0.218230739235878, 0.14107762277126312, 0.1524035930633545, 0.14453251659870148, 0.12535905838012695, 0.3444090783596039]
Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).	[40, 2, 17, 192, 60, 163, 130, 36, 162, 5]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10605145245790482, 0.1153220385313034, 0.10640762001276016, 0.10858063399791718, 0.17823319137096405, 0.10735485702753067, 0.11554571986198425, 0.11598941683769226, 0.15175268054008484, 0.11949513107538223]
There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.	[93, 103, 248, 117, 89, 329, 94, 443, 91, 428]	[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.3292357325553894, 0.40518850088119507, 0.18208293616771698, 0.2425721287727356, 0.22267724573612213, 0.24883295595645905, 0.19662240147590637, 0.10718794912099838, 0.421787291765213, 0.11208046972751617]
There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.	[398, 67, 455, 190, 444, 377, 404, 374, 20, 443]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2243497520685196, 0.4270722568035126, 0.32720518112182617, 0.39409199357032776, 0.18415988981723785, 0.15176410973072052, 0.15798242390155792, 0.13324201107025146, 0.12729445099830627, 0.09830041974782944]
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower	[325, 300, 332, 331, 257, 113, 384, 347, 370, 181]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12261433899402618, 0.2561973035335541, 0.2262643575668335, 0.11987306922674179, 0.16491158306598663, 0.16407768428325653, 0.16248299181461334, 0.1496204137802124, 0.13084326684474945, 0.13706067204475403]
There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).	[108, 112, 305, 284, 124, 122, 308, 307, 187, 135]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.10244395583868027, 0.11917246878147125, 0.1422549933195114, 0.12048264592885971, 0.48086053133010864, 0.11966937780380249, 0.2378556728363037, 0.12189621478319168, 0.17066504061222076, 0.10658612102270126]
There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).	[415, 416, 134, 135, 125, 159, 123, 54, 51, 23]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7569102644920349, 0.11856257915496826, 0.34825047850608826, 0.12986265122890472, 0.26493901014328003, 0.2838432192802429, 0.11048164963722229, 0.654252290725708, 0.14587850868701935, 0.20676784217357635]
There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.	[147, 1, 186, 149, 154, 9, 63, 143, 41, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20266951620578766, 0.22856557369232178, 0.1677146852016449, 0.09387592226266861, 0.24172347784042358, 0.11897458881139755, 0.13680294156074524, 0.14324994385242462, 0.11256708204746246, 0.12771381437778473]
There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.	[1, 51, 2, 43, 48, 192, 126, 46, 184, 32]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19512763619422913, 0.11077284812927246, 0.10608641803264618, 0.11029083281755447, 0.11233080178499222, 0.1004180833697319, 0.11171720176935196, 0.11847906559705734, 0.10209505259990692, 0.2110031694173813]
These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.	[51, 25, 32, 37, 215, 44, 162, 33, 204, 164]	[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.1313057839870453, 0.11456045508384705, 0.5511636137962341, 0.520239531993866, 0.09643913060426712, 0.2517836391925812, 0.13564075529575348, 0.14369970560073853, 0.1220109760761261, 0.18602962791919708]
This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).	[1, 192, 2, 39, 0, 50, 14, 3, 16, 170]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 1]	[0.19301800429821014, 0.5539405941963196, 0.26762762665748596, 0.09154444187879562, 0.5611451268196106, 0.15435656905174255, 0.13390113413333893, 0.2869775593280792, 0.2634349763393402, 0.45576173067092896]
This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).	[47, 4, 108, 0, 3, 5, 146, 158, 156, 15]	[0, 0, 1, 1, 0, 1, 0, 0, 1, 1]	[0.3999899923801422, 0.31630954146385193, 0.45263683795928955, 0.5584424138069153, 0.3198024034500122, 0.4581046402454376, 0.3132902979850769, 0.2700725793838501, 0.424740731716156, 0.5436009764671326]
This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).	[15, 4, 3, 110, 146, 71, 108, 156, 40, 5]	[1, 0, 0, 1, 0, 0, 1, 1, 0, 1]	[0.5301207304000854, 0.30369848012924194, 0.3223720192909241, 0.5359875559806824, 0.33789193630218506, 0.33267340064048767, 0.4481128454208374, 0.4279581904411316, 0.34425660967826843, 0.4730139374732971]
This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).	[30, 24, 163, 26, 162, 57, 218, 25, 15, 63]	[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]	[0.11034652590751648, 0.15043476223945618, 0.1354304552078247, 0.5357763767242432, 0.5865517854690552, 0.141728013753891, 0.1120757982134819, 0.11062031984329224, 0.23529569804668427, 0.12876489758491516]
This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.	[31, 69, 18, 19, 41, 93, 49, 34, 39, 94]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.575849711894989, 0.31226328015327454, 0.23220191895961761, 0.175446555018425, 0.20251302421092987, 0.24915693700313568, 0.10901384800672531, 0.1415701061487198, 0.14432206749916077, 0.1799749732017517]
This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).	[163, 100, 132, 155, 193, 131, 184, 128, 94, 2]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.2063046097755432, 0.3979281187057495, 0.26090648770332336, 0.2207014113664627, 0.12565800547599792, 0.2552869915962219, 0.17554119229316711, 0.10277300328016281, 0.4201204180717468, 0.12202804535627365]
This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].	[49, 6, 109, 15, 73, 161, 29, 30, 9, 129]	[0, 1, 0, 1, 0, 0, 0, 0, 1, 0]	[0.2704923152923584, 0.42453765869140625, 0.22404338419437408, 0.42436113953590393, 0.34670591354370117, 0.27617645263671875, 0.3692428469657898, 0.17758654057979584, 0.4746401607990265, 0.25977492332458496]
This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).	[65, 23, 19, 21, 137, 73, 20, -3, 51, -5]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11222004145383835, 0.11785066872835159, 0.15216273069381714, 0.11313237249851227, 0.18873940408229828, 0.11578495800495148, 0.1252140998840332, 0.10709293931722641, 0.1465165615081787, 0.10408774763345718]
This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class	[4, 15, 71, 74, 47, 40, 29, 30, 156, 27]	[0, 1, 0, 0, 1, 0, 1, 0, 1, 0]	[0.32271116971969604, 0.5518050193786621, 0.3177779018878937, 0.26170098781585693, 0.43693482875823975, 0.3493841588497162, 0.5024809241294861, 0.2890661060810089, 0.4596380591392517, 0.26045870780944824]
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	[204, 200, 32, 22, 2, 10, 18, 152, 11, 40]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.18899457156658173, 0.10449639707803726, 0.13751168549060822, 0.12077093124389648, 0.10266651213169098, 0.3076404333114624, 0.1057070940732956, 0.1679580956697464, 0.13113082945346832, 0.7092444896697998]
Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).	[65, 148, 58, 225, 30, 13, 41, 45, 31, 33]	[0, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.29094162583351135, 0.7492865324020386, 0.5794844031333923, 0.656719446182251, 0.13552354276180267, 0.16464200615882874, 0.1342019885778427, 0.13269570469856262, 0.15325608849525452, 0.13367456197738647]
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.	[21, 44, 25, 74, 14, 62, 81, 8, 102, 130]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10832709819078445, 0.2037937492132187, 0.1616889387369156, 0.20054812729358673, 0.272036612033844, 0.14282210171222687, 0.1096310093998909, 0.09974605590105057, 0.1977558434009552, 0.13723167777061462]
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).	[39, 43, 42, 139, 82, 119, 48, 22, 2, 51]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.22904734313488007, 0.40159547328948975, 0.41238754987716675, 0.23769763112068176, 0.1856584995985031, 0.2266985923051834, 0.1329500377178192, 0.13066576421260834, 0.09090673923492432, 0.10568583756685257]
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	[23, 205, 3, 83, 39, 18, 164, 197, 66, 106]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.276367723941803, 0.276367723941803, 0.3242753744125366, 0.16130641102790833, 0.23593536019325256, 0.10334018617868423, 0.10183156281709671, 0.09334607422351837, 0.2238820344209671, 0.11035073548555374]
To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.	[399, 398, 170, 159, 28, 416, 280, 284, 433, 200]	[0, 0, 1, 1, 0, 0, 1, 0, 0, 0]	[0.24486881494522095, 0.09186850488185883, 0.45225000381469727, 0.43912258744239807, 0.3091293275356293, 0.1811862736940384, 0.6407580971717834, 0.12547264993190765, 0.11580682545900345, 0.10819875448942184]
To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).	[143, 8, 146, 121, 42, 12, 2, 55, 170, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14276713132858276, 0.13654623925685883, 0.15978682041168213, 0.14632651209831238, 0.10459250956773758, 0.11764001101255417, 0.10856905579566956, 0.10005759447813034, 0.0962250828742981, 0.1978696584701538]
To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];	[4, 171, 170, 39, 122, 17, 14, 194, 13, 8]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3535396456718445, 0.6135038137435913, 0.23211561143398285, 0.09333287179470062, 0.2635143995285034, 0.3170399069786072, 0.14712034165859222, 0.3601577877998352, 0.19963280856609344, 0.3782484531402588]
Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.	[15, 23, 125, 42, 48, 49, 46, 122, 61, 51]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7444647550582886, 0.18957409262657166, 0.3501379191875458, 0.1972198486328125, 0.20017366111278534, 0.13635018467903137, 0.1782601773738861, 0.5864163041114807, 0.17631295323371887, 0.19009633362293243]
Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.	[11, 10, 12, 3, 8, 9, 0, 4, 173, 49]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 1]	[0.329342782497406, 0.31092190742492676, 0.37890857458114624, 0.3366699814796448, 0.23685941100120544, 0.164617657661438, 0.46397271752357483, 0.21267929673194885, 0.14954698085784912, 0.41359269618988037]
Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.	[284, 281, 133, 103, 161, 162, 399, 24, 135, 33]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17773737013339996, 0.2578374147415161, 0.10262847691774368, 0.16204838454723358, 0.14013615250587463, 0.2178811877965927, 0.20646978914737701, 0.21174924075603485, 0.1092948168516159, 0.10373736172914505]
VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.	[4, 156, 110, 5, 158, 47, 6, 74, 40, 0]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 1]	[0.5557560324668884, 0.5892319083213806, 0.563135027885437, 0.4752242863178253, 0.21899114549160004, 0.48362961411476135, 0.387975811958313, 0.21646881103515625, 0.33016958832740784, 0.4925088584423065]
Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).	[17, 4, 173, 55, 14, 158, 36, 170, 162, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19939345121383667, 0.35303330421447754, 0.34531643986701965, 0.15187154710292816, 0.14824078977108002, 0.25013089179992676, 0.10245265066623688, 0.11925540119409561, 0.3428291082382202, 0.3766058087348938]
Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.	[157, 135, 4, 18, 16, 170, 35, 30, 42, 14]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.13133302330970764, 0.22574061155319214, 0.2659895122051239, 0.271396279335022, 0.7805560827255249, 0.12318811565637589, 0.10593729466199875, 0.19557298719882965, 0.09866835176944733, 0.16056202352046967]
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	[197, 2, 22, 204, 193, 136, 72, 14, 196, 198]	[1, 0, 1, 1, 1, 1, 1, 0, 0, 0]	[0.757358729839325, 0.39842328429222107, 0.49425503611564636, 0.5476163625717163, 0.5974220633506775, 0.47691410779953003, 0.5141251087188721, 0.19062566757202148, 0.14721690118312836, 0.3728961646556854]
We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)	[60, 24, 25, 149, 130, 215, 42, 160, 5, 49]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13365601003170013, 0.1679748296737671, 0.09885211288928986, 0.16727259755134583, 0.09483572840690613, 0.14023810625076294, 0.13403257727622986, 0.12179545313119888, 0.11980840563774109, 0.1322861760854721]
We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).	[20, 3, 5, 156, 172, 98, 110, 47, 157, 109]	[1, 0, 0, 0, 1, 0, 1, 0, 0, 0]	[0.780653178691864, 0.3889378309249878, 0.3656840920448303, 0.3148806095123291, 0.41838371753692627, 0.24435493350028992, 0.4399532973766327, 0.24282558262348175, 0.1634802371263504, 0.22449837625026703]
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	[3, 23, 204, 205, 22, 24, 38, 2, 206, 27]	[0, 0, 1, 0, 1, 1, 1, 1, 1, 1]	[0.17574913799762726, 0.22784371674060822, 0.7471544742584229, 0.22784371674060822, 0.5727629065513611, 0.4130708575248718, 0.4327274560928345, 0.5082975029945374, 0.4152950644493103, 0.5287310481071472]
We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).	[179, 22, 25, 20, 31, 27, 59, 61, 112, 169]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.10268136113882065, 0.10065355896949768, 0.6697465777397156, 0.11892802268266678, 0.09898996353149414, 0.11078731715679169, 0.12324453890323639, 0.12109776586294174, 0.10041751712560654, 0.154105544090271]
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.	[129, 4, 32, 34, 25, 104, 137, 74, 44, 143]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.44398772716522217, 0.6612562537193298, 0.1303531974554062, 0.28915515542030334, 0.20374687016010284, 0.10446494817733765, 0.5393407940864563, 0.1298457384109497, 0.11463242024183273, 0.13345058262348175]
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	[129, 4, 34, 137, 104, 74, 44, 143, 45, 150]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.49397584795951843, 0.6696403622627258, 0.39038193225860596, 0.5411630272865295, 0.12902195751667023, 0.168452188372612, 0.10623141378164291, 0.16255681216716766, 0.13207097351551056, 0.14137756824493408]
We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).	[111, 153, 31, 114, 139, 100, 164, 73, 6, 161]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 1]	[0.42053288221359253, 0.5443589091300964, 0.23010534048080444, 0.5773669481277466, 0.22486227750778198, 0.3002714514732361, 0.13108988106250763, 0.38428330421447754, 0.24108143150806427, 0.4178110361099243]
We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.	[144, 196, 86, 108, 5, 163, 192, 211, 2, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32497650384902954, 0.2128562331199646, 0.10931845009326935, 0.28833889961242676, 0.3039201498031616, 0.243325874209404, 0.21033906936645508, 0.2816237807273865, 0.21368369460105896, 0.10127194225788116]
We think that many cases of ambiguÂ­ ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).	[47, 38, 174, 21, 22, 5, 114, 1, 14, 157]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09028267860412598, 0.3653152287006378, 0.39008814096450806, 0.34119659662246704, 0.11671074479818344, 0.1458277702331543, 0.12141717970371246, 0.09843165427446365, 0.09843165427446365, 0.0983019694685936]
We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).	[17, 52, 45, 86, 2, 46, 3, 36, 112, 53]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.095513254404068, 0.13204814493656158, 0.3106754422187805, 0.09736005961894989, 0.1540614366531372, 0.1580389142036438, 0.09351067245006561, 0.09849338978528976, 0.46289652585983276, 0.14764980971813202]
We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.	[196, 46, 5, 211, 19, 144, 52, 163, 15, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14847305417060852, 0.12349457293748856, 0.17936253547668457, 0.164484903216362, 0.10367806255817413, 0.1532367765903473, 0.19310198724269867, 0.12440933287143707, 0.11305023729801178, 0.11940785497426987]
We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).	[86, 24, 112, 93, 53, 28, 19, 5, 39, 211]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11352569609880447, 0.16643941402435303, 0.19960346817970276, 0.12459611892700195, 0.1302974820137024, 0.1197180300951004, 0.17692884802818298, 0.17211787402629852, 0.10277687013149261, 0.17350292205810547]
We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.	[169, 195, 106, 183, 59, 179, 167, 47, 122, 112]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15438376367092133, 0.15040223300457, 0.21646562218666077, 0.2097088247537613, 0.18677330017089844, 0.1130310446023941, 0.18010307848453522, 0.27827706933021545, 0.12188942730426788, 0.10355482995510101]
We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).	[17, 130, 60, 192, 20, 212, 6, 108, 2, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1265898197889328, 0.11360820382833481, 0.1370687633752823, 0.14850249886512756, 0.13506406545639038, 0.12803611159324646, 0.14213283360004425, 0.16959600150585175, 0.12117990106344223, 0.14969667792320251]
We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation	[108, 305, 112, 181, 135, 168, 328, 307, 284, 134]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17125019431114197, 0.4873555898666382, 0.3022266626358032, 0.12643669545650482, 0.22355833649635315, 0.10870543867349625, 0.09612680226564407, 0.2200416624546051, 0.0940934494137764, 0.3710998296737671]
We used a simple greedy algorithm described in [Sproat et al., 1996].	[109, 307, 305, 263, 118, 456, 230, 328, 466, 168]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.10915850847959518, 0.441707581281662, 0.4400908648967743, 0.3307512104511261, 0.1770915985107422, 0.14547854661941528, 0.35037845373153687, 0.08901702612638474, 0.22338628768920898, 0.0967797189950943]
We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).	[122, 39, 125, 170, 35, 195, 15, 1, 8, 25]	[1, 0, 1, 0, 0, 0, 1, 0, 0, 1]	[0.5695116519927979, 0.11489493399858475, 0.46657854318618774, 0.30220890045166016, 0.17019177973270416, 0.10246624052524567, 0.7117927074432373, 0.0984242707490921, 0.16092343628406525, 0.681434690952301]
While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)	[231, 165, 149, 94, 1, 49, 47, 65, 2, 63]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20564806461334229, 0.11686839908361435, 0.21752755343914032, 0.2503928244113922, 0.36101385951042175, 0.23405839502811432, 0.14685817062854767, 0.09778493642807007, 0.2175651639699936, 0.11923357099294662]
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).	[108, 67, 456, 461, 224, 433, 117, 393, 133, 154]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.14018715918064117, 0.1531117856502533, 0.3435264527797699, 0.12356771528720856, 0.4846707582473755, 0.09731853753328323, 0.12851479649543762, 0.11718368530273438, 0.1130245253443718, 0.11295381188392639]
While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.	[18, 5, 193, 211, 42, 8, 41, 3, 128, 17]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.11945157498121262, 0.3903537392616272, 0.14277143776416779, 0.3406465947628021, 0.43001267313957214, 0.11015432327985764, 0.11498157680034637, 0.11043643206357956, 0.14193080365657806, 0.11885851621627808]
While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.	[45, 36, 30, 39, 17, 214, 34, 52, 31, 23]	[1, 0, 0, 1, 0, 0, 1, 0, 1, 0]	[0.5183442831039429, 0.38987261056900024, 0.1680920124053955, 0.521100640296936, 0.2742926776409149, 0.1134319081902504, 0.5358607769012451, 0.12045589089393616, 0.6544224619865417, 0.2868843972682953]
Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).	[142, 128, 296, 405, 132, 361, 363, 357, 125, 302]	[0, 1, 0, 1, 0, 1, 1, 1, 0, 0]	[0.1521626114845276, 0.5422809720039368, 0.3385218381881714, 0.5741938948631287, 0.20822104811668396, 0.4457271993160248, 0.46524301171302795, 0.4532003402709961, 0.11719366163015366, 0.10480432957410812]
Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).	[128, 296, 405, 132, 361, 363, 142, 357, 109, 125]	[1, 1, 1, 0, 1, 1, 0, 1, 0, 0]	[0.608976423740387, 0.5777584314346313, 0.6694172024726868, 0.18608859181404114, 0.4763381779193878, 0.5283693671226501, 0.16517983376979828, 0.4708826243877411, 0.17305812239646912, 0.12552662193775177]
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	[25, 0, 44, 14, 104, 152, 19, 30, 62, 45]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2912994623184204, 0.4354037940502167, 0.12666045129299164, 0.17450816929340363, 0.19495217502117157, 0.19239769876003265, 0.12146049737930298, 0.33493277430534363, 0.12347184121608734, 0.09503035247325897]
Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.	[4, 133, 0, 25, 24, 17, 29, 2, 5, 108]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31623390316963196, 0.2642010450363159, 0.2686237692832947, 0.331552118062973, 0.17937736213207245, 0.11309408396482468, 0.1055852547287941, 0.1202714741230011, 0.16534970700740814, 0.14499925076961517]
Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.	[130, 17, 6, 36, 160, 25, 35, 43, 40, 213]	[0, 1, 1, 1, 1, 1, 1, 1, 0, 0]	[0.39996352791786194, 0.592639684677124, 0.4637655019760132, 0.47060999274253845, 0.5163302421569824, 0.4004325866699219, 0.606070876121521, 0.47822022438049316, 0.28485432267189026, 0.24736829102039337]
Zhou et al.(2005) explore various features in relation extraction using SVM.	[17, 36, 2, 0, 39, 45, 24, 25, 5, 108]	[0, 0, 1, 1, 1, 0, 0, 0, 0, 0]	[0.26512715220451355, 0.3370346128940582, 0.4370273947715759, 0.5432392954826355, 0.5087949633598328, 0.10594360530376434, 0.1378270536661148, 0.2676149010658264, 0.20371799170970917, 0.15151721239089966]
Zhou et al.(2005) tested their system on the ACE 2003 data;.	[24, 25, 150, 156, 184, 20, 44, 129, 6, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1402176469564438, 0.10738462954759598, 0.15265317261219025, 0.13898470997810364, 0.12916024029254913, 0.10458239167928696, 0.26337939500808716, 0.11714359372854233, 0.09189968556165695, 0.12984926998615265]
[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.	[190, 172, 170, 168, 408, 263, 173, 171, 124, 403]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.27059659361839294, 0.3696225881576538, 0.29237571358680725, 0.2945547103881836, 0.29141709208488464, 0.15490730106830597, 0.19602058827877045, 0.13258802890777588, 0.2812165915966034, 0.5843902826309204]
\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).	[22, 114, 3, 58, 174, 44, 166, 157, 146, 21]	[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]	[0.15536540746688843, 0.1018751859664917, 0.17500285804271698, 0.668228805065155, 0.4041348695755005, 0.26530274748802185, 0.13249076902866364, 0.12025148421525955, 0.18272940814495087, 0.38146957755088806]
participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).	[70, 36, 112, 115, 8, 157, 23, 31, 98, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2822195589542389, 0.24907027184963226, 0.25578656792640686, 0.2815404236316681, 0.18998919427394867, 0.1163623034954071, 0.20029114186763763, 0.14274844527244568, 0.105907142162323, 0.09603728353977203]
utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.	[419, 55, 151, 335, 123, 119, 417, 421, 121, 45]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10561234503984451, 0.11714521795511246, 0.11541995406150818, 0.1082315593957901, 0.09219951182603836, 0.11350170522928238, 0.2553766965866089, 0.1544174998998642, 0.11484120786190033, 0.1264318823814392]
we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.	[19, 5, 131, 60, 211, 24, 138, 2, 134, 17]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16136427223682404, 0.15320472419261932, 0.2938855290412903, 0.12370140850543976, 0.15376333892345428, 0.14101941883563995, 0.3073842227458954, 0.09785662591457367, 0.250309020280838, 0.09974584728479385]
â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).	[6, 21, 118, 52, 169, 112, 61, 7, 30, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1271640956401825, 0.2854803502559662, 0.1149473711848259, 0.13443750143051147, 0.16066208481788635, 0.11073412746191025, 0.1287001222372055, 0.13297611474990845, 0.11965789645910263, 0.1231163740158081]
