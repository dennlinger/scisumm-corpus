query	doc_id	label	similiarity
(12.5%) matches what Choi et al (2001) reported (12%), while the error for HCWM (12.1%) is higher than that reported for the version with a paragraph-based 500-dimension LSI space (9%) but appears comparable to their sentence-based 400-dimension LSI space.	[172, 173, 196, 171, 113, 182, 97, 112, 217, 85]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5237107872962952, 0.5541856288909912, 0.372112900018692, 0.24963776767253876, 0.15145109593868256, 0.17010627686977386, 0.06047579273581505, 0.071647047996521, 0.059402041137218475, 0.06207747757434845]
(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.(2011)) and machine translation (“MT”: Weller et al.(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).	[30, 15, 217, 34, 3, 27, 7, 0, 12, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.24609240889549255, 0.13667432963848114, 0.15226423740386963, 0.4717353880405426, 0.21074621379375458, 0.3521248996257782, 0.061791278421878815, 0.18412981927394867, 0.6380922198295593, 0.080852210521698]
(Abney, 1997) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures.	[258, 134, 333, 89, 265, 2, 9, 194, 248, 174]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.6888226866722107, 0.5928308963775635, 0.5903720855712891, 0.4565965235233307, 0.53001868724823, 0.18138791620731354, 0.18138791620731354, 0.42391395568847656, 0.4052617847919464, 0.09852637350559235]
(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality.	[6, 94, 19, 105, 139, 23, 9, 5, 104, 112]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6909371614456177, 0.6741603016853333, 0.5315223336219788, 0.3999378979206085, 0.2993682622909546, 0.21828991174697876, 0.25730010867118835, 0.14156188070774078, 0.21875973045825958, 0.23753264546394348]
(Berland and Charniak, 1999) use hand crafted patterns to discover part-of (meronymy) relation ships, and (Chklovski and Pantel, 2004) discover various interesting relations between verbs.	[8, 10, 15, 28, 16, 14, 29, 21, 13, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.27154794335365295, 0.17892052233219147, 0.2587600350379944, 0.11284062266349792, 0.21018466353416443, 0.14142204821109772, 0.11898718774318695, 0.15213781595230103, 0.14963726699352264, 0.08125235140323639]
(Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing.	[78, 135, 17, 79, 37, 19, 72, 18, 101, 133]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24564166367053986, 0.07613606750965118, 0.07591571658849716, 0.12174996733665466, 0.13371683657169342, 0.06409378349781036, 0.053368039429187775, 0.051514528691768646, 0.12244761735200882, 0.05461721867322922]
(Brent, 1993) relies on local morphosyntactic cues (such as the -ing suffix, except where such a word follows a determiner or a preposition other than to) in the untagged Brown Corpus as probabilistic indicators of six different predefined subcategorisation frames.	[6, 168, 76, 15, 65, 105, 62, 5, 14, 43]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 1]	[0.69234699010849, 0.7011475563049316, 0.7587351202964783, 0.69234699010849, 0.36610743403434753, 0.24136723577976227, 0.478130042552948, 0.4052489697933197, 0.4052489697933197, 0.6530150771141052]
(Brill, 1995) uses lexicon for initial annotation of the training corpus, where each word in the lexicon has a set POS tags seen for the word in the training corpus.	[49, 61, 130, 47, 33, 116, 40, 64, 12, 30]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7248691320419312, 0.6319467425346375, 0.4394517242908478, 0.45637932419776917, 0.4029078781604767, 0.2977961301803589, 0.3068861961364746, 0.2222728133201599, 0.06140551343560219, 0.26122891902923584]
(Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative sub cat egorization frequency for individual predicates.	[95, 0, 26, 93, 97, 27, 24, 92, 94, 22]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2081182301044464, 0.5448474884033203, 0.2752813994884491, 0.09067906439304352, 0.151798814535141, 0.2133846879005432, 0.0714421272277832, 0.1057840958237648, 0.10425475984811783, 0.10162779688835144]
(Briscoe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser.	[72, 66, 198, 131, 20, 59, 0, 3, 151, 191]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6827730536460876, 0.4324673116207123, 0.4324673116207123, 0.3889273405075073, 0.2191643863916397, 0.24879370629787445, 0.43642663955688477, 0.06053033843636513, 0.2191643863916397, 0.24879370629787445]
(Callison-Burch et al, 2007) reported that the inter coder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation.	[146, 145, 157, 179, 148, 136, 123, 73, 153, 162]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43259334564208984, 0.5232678055763245, 0.06880780309438705, 0.17867209017276764, 0.41484707593917847, 0.08221925795078278, 0.21817366778850555, 0.045095257461071014, 0.05771562084555626, 0.057202473282814026]
(Callison-Burch et al, 2011) The workshop's human evaluation component has been gradually refined over several years, and as a consequence it has produced a fantastic collection of publicly available data consisting primarily of pairwise judgements of translation systems made by human assessors across a wide variety of languages and tasks.	[277, 19, 8, 23, 204, 239, 16, 210, 18, 9]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6113026738166809, 0.44227534532546997, 0.3592554032802582, 0.08040473610162735, 0.7203403115272522, 0.06840141117572784, 0.1641267091035843, 0.14153288304805756, 0.0780564397573471, 0.4474973976612091]
(Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous context free grammar (SCFG) deletion rules.	[149, 1, 2, 7, 17, 10, 21, 84, 11, 148]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7080113887786865, 0.5822479724884033, 0.4847186505794525, 0.1595875769853592, 0.408098965883255, 0.23843201994895935, 0.1073262020945549, 0.2691105902194977, 0.11220673471689224, 0.1380155384540558]
(Ge et al 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs.	[2, 0, 23, 10, 113, 4, 6, 204, 150, 137]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7151999473571777, 0.8469943404197693, 0.4199715852737427, 0.40582215785980225, 0.13658224046230316, 0.13090619444847107, 0.49213671684265137, 0.29526835680007935, 0.33746689558029175, 0.23697063326835632]
(Hasegawa et al 2004) used large corpora and an Extended Named Entity tagger to find novel relations and their participants.	[69, 0, 18, 63, 56, 19, 38, 64, 65, 5]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.651585578918457, 0.7808276414871216, 0.5362551808357239, 0.6174420118331909, 0.1007842943072319, 0.08631066977977753, 0.12569910287857056, 0.09140374511480331, 0.2633233070373535, 0.16577529907226562]
(Kanayama and Nasukawa, 2006) reported that it was appropriate in 72.2% of cases.	[7, 123, 4, 73, 196, 139, 8, 19, 118, 69]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6164844632148743, 0.6060399413108826, 0.30282437801361084, 0.31833046674728394, 0.5153806209564209, 0.17040760815143585, 0.04575080797076225, 0.08840704709291458, 0.15912564098834991, 0.08398483693599701]
(Kessler et al, 1997) combine these views by saying that a genre should not be so broad that the texts belonging to it don't share any distinguishing properties.	[24, 170, 15, 13, 21, 183, 22, 30, 5, 89]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6535166501998901, 0.44377002120018005, 0.11158303171396255, 0.11680141091346741, 0.16458635032176971, 0.4073016941547394, 0.12568652629852295, 0.1508471965789795, 0.3857205808162689, 0.4134438931941986]
(Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency.	[49, 168, 85, 8, 17, 18, 137, 31, 35, 88]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6532462239265442, 0.3152313828468323, 0.11101084202528, 0.21081309020519257, 0.06157059222459793, 0.13770419359207153, 0.31873413920402527, 0.20281082391738892, 0.05137988552451134, 0.06577065587043762]
(Li and Roth, 2002) obtain a better performance for English, around a 92.5% in terms of accuracy.	[176, 174, 18, 157, 160, 164, 59, 9, 198, 196]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28816547989845276, 0.5268478393554688, 0.26062750816345215, 0.16625668108463287, 0.15739202499389648, 0.23029789328575134, 0.08737350255250931, 0.04068358242511749, 0.07070710510015488, 0.06487845629453659]
(Li and Roth, 2002) propose a system based on SNoW.	[116, 89, 16, 132, 90, 25, 11, 13, 39, 85]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7532634735107422, 0.3909473121166229, 0.14450456202030182, 0.08047901839017868, 0.48030194640159607, 0.05572156608104706, 0.05149192363023758, 0.2590368390083313, 0.05014873668551445, 0.22043435275554657]
(Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue).	[0, 22, 156, 175, 203, 133, 51, 44, 148, 96]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7422763705253601, 0.6115162968635559, 0.21822284162044525, 0.14865681529045105, 0.08595521748065948, 0.0956316664814949, 0.060154158622026443, 0.053330231457948685, 0.24898508191108704, 0.21152633428573608]
(Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems.	[1, 15, 40, 44, 4, 39, 0, 92, 25, 90]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7976273894309998, 0.3778613805770874, 0.39048659801483154, 0.3614858090877533, 0.21931873261928558, 0.24871517717838287, 0.3176281750202179, 0.3710924983024597, 0.08589077740907669, 0.13470558822155]
(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing.	[76, 14, 0, 105, 1, 15, 19, 65, 102, 22]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.34167513251304626, 0.22446228563785553, 0.5051581859588623, 0.15635797381401062, 0.06496983021497726, 0.2743639349937439, 0.2068967968225479, 0.13626785576343536, 0.1501569151878357, 0.13557127118110657]
(Pado et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic roles.	[74, 20, 16, 3, 129, 39, 44, 80, 12, 100]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4864048957824707, 0.5437830686569214, 0.11654201149940491, 0.16693805158138275, 0.3393218219280243, 0.08308722823858261, 0.12398139387369156, 0.06586432456970215, 0.09365928173065186, 0.04813629016280174]
(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper.	[21, 20, 175, 67, 1, 179, 119, 211, 166, 132]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5828903317451477, 0.6021856069564819, 0.10303962975740433, 0.23008984327316284, 0.24148006737232208, 0.05608111619949341, 0.05271299555897713, 0.32664698362350464, 0.20825621485710144, 0.06001096963882446]
(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus.	[2, 30, 162, 4, 3, 24, 0, 18, 14, 27]	[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.4636707007884979, 0.5412488579750061, 0.3553958535194397, 0.3712564706802368, 0.2726627588272095, 0.1446937471628189, 0.5349081754684448, 0.1857786476612091, 0.3145848214626312, 0.2643851339817047]
(Riezler et al, 2007) adopted an SMT-based method to query expansion in answer retrieval.	[130, 0, 1, 8, 13, 134, 2, 25, 123, 15]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.6973414421081543, 0.4401540756225586, 0.20202262699604034, 0.4713490307331085, 0.20596076548099518, 0.37284865975379944, 0.48147374391555786, 0.5013121366500854, 0.21739746630191803, 0.42523494362831116]
(Sudo et al, 2003a) consists of three phases to learn extraction patterns from the source documents for a scenario specified by the user.	[87, 46, 47, 72, 27, 84, 56, 96, 35, 22]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5213562250137329, 0.44637149572372437, 0.48314905166625977, 0.2105650156736374, 0.1453738659620285, 0.14275169372558594, 0.08721595257520676, 0.1714036762714386, 0.09095431119203568, 0.22481504082679749]
(Surdeanu et al, 2008), (Burchardt et al, 2006) and (Kawahara et al, 2002).	[24, 87, 5, 50, 10, 26, 52, 9, 34, 29]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10102063417434692, 0.07582119852304459, 0.1166611909866333, 0.09558513760566711, 0.06056459993124008, 0.08039339631795883, 0.08274948596954346, 0.09820961207151413, 0.08325202763080597, 0.08380086719989777]
(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability.	[117, 3, 139, 32, 68, 31, 35, 141, 23, 214]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 1]	[0.2833470404148102, 0.18598045408725739, 0.15253674983978271, 0.13527151942253113, 0.053251806646585464, 0.35870835185050964, 0.10374536365270615, 0.5097569227218628, 0.09720464050769806, 0.5461557507514954]
(Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases.	[13, 59, 8, 86, 7, 83, 6, 60, 95, 97]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5269343852996826, 0.2579856216907501, 0.3766686022281647, 0.12117235362529755, 0.1516050398349762, 0.19606097042560577, 0.06437075138092041, 0.08989014476537704, 0.30588963627815247, 0.10603436082601547]
(Wan et al, 2005) employ an approach similar to that of (Mann and Yarowsky,2003), and have developed a system called Web Hawk.	[46, 66, 33, 14, 130, 2, 125, 29, 51, 10]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5139777064323425, 0.09645283222198486, 0.15973074734210968, 0.04793389141559601, 0.06099636107683182, 0.11101843416690826, 0.2131822258234024, 0.18016082048416138, 0.07234378159046173, 0.17225520312786102]
(Wellington et al, 2006) argue that these restrictions reduce our ability to model translation equivalence effectively.	[35, 31, 0, 47, 1, 6, 23, 30, 73, 27]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7340102195739746, 0.15505902469158173, 0.5934499502182007, 0.057073306292295456, 0.08841299265623093, 0.0896267518401146, 0.11701987683773041, 0.11801661550998688, 0.053222231566905975, 0.08664921671152115]
(Xue and Palmer, 2004) found out that different features suited for different sub-tasks of SRL ,i.e. argument identification and classification.	[3, 79, 56, 35, 21, 66, 26, 38, 44, 48]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7411690950393677, 0.7411690950393677, 0.15442748367786407, 0.22015231847763062, 0.20098567008972168, 0.11410491168498993, 0.5848692059516907, 0.11219026893377304, 0.2280638962984085, 0.08004636317491531]
(Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.	[26, 147, 205, 13, 58, 57, 12, 0, 27, 186]	[1, 1, 1, 0, 0, 0, 1, 1, 0, 0]	[0.7672294974327087, 0.6764444708824158, 0.6220107674598694, 0.45103681087493896, 0.34267956018447876, 0.4300994575023651, 0.7073377966880798, 0.7426179647445679, 0.19584868848323822, 0.25198107957839966]
(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level.	[2, 0, 17, 89, 13, 11, 97, 1, 38, 18]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8063978552818298, 0.4224991798400879, 0.21159283816814423, 0.7614189982414246, 0.2728365361690521, 0.1522723287343979, 0.1713976114988327, 0.17766353487968445, 0.05686337500810623, 0.09665863960981369]
"... we would probably not use the term ""genre"" to describe merely the class of texts that have the objective of persuading someone to do something, since that class which would include editorials, sermons, prayers, advertisements, and so forth has no distinguishing formal properties (Kessler et al, 1997, p. 33)."	[24, 25, 30, 22, 40, 12, 133, 4, 23, 96]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8172950744628906, 0.7670117616653442, 0.3188011050224304, 0.6521907448768616, 0.2182302325963974, 0.24039746820926666, 0.07264849543571472, 0.32828161120414734, 0.10103165358304977, 0.08962655812501907]
104 kind of BIBLE evaluation has been estimated at 62% precision and 60% recall (Melamed, 1995).	[45, 62, 48, 202, 50, 179, 58, 203, 65, 205]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.596926748752594, 0.04886667802929878, 0.3604586124420166, 0.16479790210723877, 0.23360750079154968, 0.3596329391002655, 0.2949765622615814, 0.08685329556465149, 0.18165387213230133, 0.08091601729393005]
22-24 was 4.2%, which is comparable to related work in the literature, e.g. Suzuki and Isozaki (2008) (7%) and Spoustova et al (2009) (4-5%).	[72, 121, 143, 154, 4, 30, 160, 145, 97, 133]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32803595066070557, 0.06388388574123383, 0.05670331045985222, 0.0691930502653122, 0.060398463159799576, 0.06063871830701828, 0.28241968154907227, 0.06418225914239883, 0.06475556641817093, 0.05701817572116852]
3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011).	[48, 97, 79, 221, 85, 58, 96, 41, 216, 18]	[1, 1, 1, 0, 0, 0, 1, 0, 0, 0]	[0.5565834045410156, 0.6191704869270325, 0.5703363418579102, 0.2857687473297119, 0.18851865828037262, 0.29281100630760193, 0.5538557767868042, 0.10171189159154892, 0.24437691271305084, 0.11283377557992935]
3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004).	[69, 197, 38, 177, 178, 161, 124, 170, 119, 101]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7367062568664551, 0.6664308905601501, 0.5540608763694763, 0.28886309266090393, 0.1142699345946312, 0.23274193704128265, 0.22888903319835663, 0.12210025638341904, 0.09477189183235168, 0.18653489649295807]
3.3.1 Dependency Structures The first set of these features include typed dependency structures (de Marneffe and Manning, 2008) which describe the grammatical relationships between words.	[140, 5, 45, 66, 1, 91, 118, 29, 16, 119]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.192330464720726, 0.22483932971954346, 0.05946182459592819, 0.30354568362236023, 0.11812931299209595, 0.23687617480754852, 0.06424374878406525, 0.060083113610744476, 0.06339208781719208, 0.08376076817512512]
5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011).	[153, 28, 146, 150, 5, 100, 73, 93, 177, 21]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7475748658180237, 0.1947842836380005, 0.16817472875118256, 0.2633230686187744, 0.17194288969039917, 0.25318899750709534, 0.30966201424598694, 0.0595163069665432, 0.07153906673192978, 0.16828234493732452]
67 search, but a starting point would be the approach by (Briscoe and Carroll, 1997).	[194, 32, 59, 47, 63, 29, 22, 153, 163, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42543819546699524, 0.392740398645401, 0.07134019583463669, 0.08436404168605804, 0.05283820256590843, 0.11510725319385529, 0.08802653104066849, 0.08802653104066849, 0.392740398645401, 0.05932524800300598]
A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories.	[17, 27, 150, 28, 21, 29, 6, 46, 4, 56]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34450963139533997, 0.6867120862007141, 0.158096045255661, 0.10373466461896896, 0.4605949819087982, 0.3613080680370331, 0.11457815766334534, 0.35166049003601074, 0.05863163247704506, 0.2065383940935135]
A Poisson distribution can be justified and has been used in order to model the length distribution of word and morph tokens [e.g., (Creutz and Lagus, 2002)], but for morph types we have chosen the gamma distribution, which has a thicker tail.	[172, 76, 8, 109, 116, 79, 108, 167, 90, 134]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36943501234054565, 0.4676097333431244, 0.06382790207862854, 0.0670766606926918, 0.125186949968338, 0.18306279182434082, 0.06967659294605255, 0.08199790120124817, 0.15020862221717834, 0.44749653339385986]
A benchmark dataset of 27,937 such quadruples was extracted from the Wall Street Journal corpus by Ratnaparkhi et al (1994) and has been the basis of many subsequent studies comparing machine learning algorithms and lexical resources.	[87, 32, 69, 2, 27, 78, 11, 82, 99, 146]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7107886075973511, 0.13049520552158356, 0.3709864616394043, 0.08186015486717224, 0.052686356008052826, 0.30177822709083557, 0.0605115108191967, 0.1446535587310791, 0.08931641280651093, 0.2798417806625366]
A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996).	[101, 111, 102, 109, 110, 105, 103, 57, 30, 107]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7612748742103577, 0.4251680374145508, 0.11480465531349182, 0.2842457592487335, 0.2992834448814392, 0.42721736431121826, 0.18344062566757202, 0.38356709480285645, 0.0672982931137085, 0.11650574952363968]
A comparison of both methods can be found in Zens and Ney (2003).	[5, 122, 64, 48, 18, 225, 171, 159, 106, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4092561602592468, 0.49291321635246277, 0.15828603506088257, 0.08226782083511353, 0.07008709758520126, 0.05895555764436722, 0.07023517042398453, 0.06120951101183891, 0.04831387847661972, 0.11720744520425797]
A comparison of unlexicalised PCFG parsing (Ku?bler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the TuBa D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991).	[18, 7, 24, 17, 35, 0, 68, 6, 64, 77]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2196103185415268, 0.055540140718221664, 0.056446775794029236, 0.08048144727945328, 0.09299250692129135, 0.07104381918907166, 0.1221478208899498, 0.05488813295960426, 0.07174815982580185, 0.058190904557704926]
A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008).	[0, 3, 1, 2, 302, 5, 391, 514, 371, 419]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.723389744758606, 0.516671359539032, 0.6305555701255798, 0.08474243432283401, 0.4545707702636719, 0.26952415704727173, 0.06428240984678268, 0.10747084021568298, 0.4839648902416229, 0.2802121639251709]
A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).	[19, 29, 6, 0, 32, 334, 27, 20, 83, 40]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3109744191169739, 0.27934375405311584, 0.11786188185214996, 0.32873424887657166, 0.2659790813922882, 0.15986669063568115, 0.06049075350165367, 0.08454154431819916, 0.1946144551038742, 0.08345116674900055]
A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model.	[71, 109, 5, 13, 38, 99, 105, 103, 9, 110]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25755634903907776, 0.2901410460472107, 0.24022185802459717, 0.38000503182411194, 0.25124114751815796, 0.45306965708732605, 0.3166752755641937, 0.08600405603647232, 0.09477189183235168, 0.20102272927761078]
A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).	[77, 19, 63, 10, 12, 25, 8, 14, 11, 70]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.31544923782348633, 0.05741008371114731, 0.0741535946726799, 0.21928326785564423, 0.1923476904630661, 0.05816249921917915, 0.5919442772865295, 0.15434297919273376, 0.19260168075561523, 0.1446218639612198]
A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).	[25, 9, 18, 333, 262, 53, 263, 335, 258, 321]	[1, 0, 1, 1, 1, 1, 0, 0, 1, 0]	[0.6342253684997559, 0.3978034257888794, 0.6774483919143677, 0.655928909778595, 0.5266548991203308, 0.5913463830947876, 0.29218658804893494, 0.2518348693847656, 0.5106419324874878, 0.14097018539905548]
A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).	[25, 18, 333, 262, 9, 53, 263, 26, 215, 212]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.6234321594238281, 0.5740286707878113, 0.5400131344795227, 0.3983399271965027, 0.2683981955051422, 0.5232457518577576, 0.24700325727462769, 0.3466973900794983, 0.21205604076385498, 0.12083093076944351]
A number of nonconcatenative grammar formalisms has been put forward, such as head-wrapping grammars (HG) (Pollard, 1984), extra position grammars (XG) (Pereira, 1981).	[7, 85, 54, 49, 5, 6, 91, 203, 86, 213]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4461047947406769, 0.18327227234840393, 0.153269961476326, 0.6236788034439087, 0.39730992913246155, 0.4618409276008606, 0.2107897251844406, 0.14289076626300812, 0.1728251576423645, 0.07415074855089188]
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011).	[15, 164, 23, 26, 1, 160, 22, 191, 147, 187]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8151363134384155, 0.7545074224472046, 0.4365426003932953, 0.10211082547903061, 0.2050899714231491, 0.14556686580181122, 0.15768533945083618, 0.22769485414028168, 0.06314447522163391, 0.16391226649284363]
A perceptron algorithm gives 97.11% (Collins, 2002).	[12, 3, 0, 9, 8, 10, 1, 18, 13, 4]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41167163848876953, 0.19317936897277832, 0.2345665991306305, 0.3802126348018646, 0.05407441034913063, 0.10087042301893234, 0.0499175526201725, 0.24077606201171875, 0.04298050329089165, 0.04395006224513054]
A set of stop-words is also removed, using the same list originally employed by several competitive systems (Utiyama and Isahara, 2001).	[106, 30, 124, 52, 47, 133, 113, 107, 110, 98]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6953826546669006, 0.1757233589887619, 0.30475616455078125, 0.0596226267516613, 0.1038888692855835, 0.04693453386425972, 0.14918598532676697, 0.4765276610851288, 0.06010282412171364, 0.048606470227241516]
A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006).	[123, 124, 5, 19, 48, 20, 13, 148, 15, 130]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5158926844596863, 0.21233786642551422, 0.10304924100637436, 0.20204538106918335, 0.06251966953277588, 0.05813951790332794, 0.05844670534133911, 0.04538713023066521, 0.08628489822149277, 0.12798641622066498]
A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004).	[96, 87, 35, 89, 28, 18, 88, 99, 119, 128]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.5183449387550354, 0.1147521510720253, 0.14080359041690826, 0.13883459568023682, 0.04783523455262184, 0.04495465010404587, 0.06187554448843002, 0.13590583205223083, 0.5550177097320557, 0.05821256712079048]
A spelling-based model that directly maps English letter sequences into Arabic letters was developed by Al-Onaizan and Knight (2002).	[71, 47, 46, 41, 79, 81, 86, 120, 39, 44]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7877402305603027, 0.7741106152534485, 0.23301208019256592, 0.6683769822120667, 0.14788343012332916, 0.4513624906539917, 0.18661093711853027, 0.272350013256073, 0.15137338638305664, 0.09686014801263809]
A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998).	[45, 109, 37, 42, 96, 46, 185, 15, 14, 92]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4508651793003082, 0.33893221616744995, 0.2942696511745453, 0.12085286527872086, 0.1867997646331787, 0.07053615152835846, 0.09922605752944946, 0.07854349911212921, 0.0714864730834961, 0.2678378224372864]
A third approach, exemplified by Moldovan et al (2003) and Raina et al (2005), is to translate dependency parses into neo-Davidsonian-style quasi-logical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al, 1988).	[229, 65, 109, 35, 17, 23, 167, 207, 260, 124]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11125341057777405, 0.07689715921878815, 0.07562371343374252, 0.1312635838985443, 0.06579053401947021, 0.08515246212482452, 0.08030057698488235, 0.0546577125787735, 0.05507441982626915, 0.0616736114025116]
A third, clean-up pass is then performed to partially disambiguate the identified WordNet glosses with Brill's part-of-speech tagger (Brill, 1995), which performs with up to 95% accuracy, and eliminates errors introduced into the list by part-of-speech ambiguity of some words acquired in pass 1 and from the seed list.	[36, 122, 28, 16, 164, 49, 43, 51, 0, 225]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7524542808532715, 0.11969157308340073, 0.1733490228652954, 0.19292142987251282, 0.15799163281917572, 0.09376613795757294, 0.18946760892868042, 0.2220192849636078, 0.243676096200943, 0.34095779061317444]
A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).	[36, 33, 84, 173, 53, 56, 187, 5, 30, 44]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4097713530063629, 0.23968037962913513, 0.4037257432937622, 0.1764139086008072, 0.14361031353473663, 0.07480097562074661, 0.17058303952217102, 0.0853201374411583, 0.052230630069971085, 0.3388075530529022]
A tweet-specific tokenizer (Gimpel et al, 2011) is employed, and the dependency parsing results are computed by Stanford Parser (Klein and Manning, 2003).	[19, 83, 58, 2, 20, 36, 26, 87, 22, 24]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6950036287307739, 0.717185914516449, 0.11444324254989624, 0.1775716096162796, 0.09773518145084381, 0.3437115252017975, 0.0678618922829628, 0.26168206334114075, 0.061471547931432724, 0.05639158934354782]
A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al, 2007) and work by Das et al (2010).	[0, 15, 51, 16, 91, 102, 105, 14, 93, 77]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8319632411003113, 0.1508958637714386, 0.10547292977571487, 0.19855764508247375, 0.18494240939617157, 0.14897048473358154, 0.05864229053258896, 0.043090738356113434, 0.07942400872707367, 0.06241825595498085]
A wide range of annotations from part of speech (Hi and Hwa, 2005) and chunks (Yarowsky et al, 2001) to word senses (Diab and Resnik, 2002), dependencies (Hwa et al, 2002) and semantic roles (Pado and Lapata, 2009) have been successfully transferred between languages.	[32, 28, 98, 49, 100, 44, 45, 6, 12, 99]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05708610266447067, 0.07289016991853714, 0.09203365445137024, 0.04853149875998497, 0.05008799582719803, 0.12991616129875183, 0.06398869305849075, 0.11586327850818634, 0.06040821969509125, 0.053926367312669754]
A wide spectrum of tasks have been studied under review mining, ranging from coarse-grained document-level polarity classification (Pang et al,2002) to fine-grained extraction of opinion expressions and their targets (Wu et al, 2009).	[165, 1, 9, 17, 13, 38, 123, 178, 4, 16]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7817525267601013, 0.10495032370090485, 0.36904242634773254, 0.16338373720645905, 0.4616267681121826, 0.09355664253234863, 0.061243318021297455, 0.0610397569835186, 0.05640764161944389, 0.08146500587463379]
A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al (2009).	[90, 89, 87, 152, 138, 121, 118, 78, 64, 185]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6452193260192871, 0.7768266201019287, 0.5664151906967163, 0.20921805500984192, 0.10309390723705292, 0.23071059584617615, 0.20011042058467865, 0.435177743434906, 0.23064957559108734, 0.07520487159490585]
Abney (1997) pointed out that the non-context free dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of log linear models for defining probability distributions over the parses of a unification grammar.	[53, 84, 18, 146, 81, 372, 1, 8, 113, 15]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4534328877925873, 0.5116629600524902, 0.4679947793483734, 0.4977148771286011, 0.48561176657676697, 0.20223791897296906, 0.17291894555091858, 0.17291894555091858, 0.21502265334129333, 0.15562789142131805]
Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework.	[51, 69, 219, 5, 12, 372, 52, 90, 322, 18]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7127543091773987, 0.22313550114631653, 0.15956982970237732, 0.17355574667453766, 0.17355574667453766, 0.22052502632141113, 0.3570294976234436, 0.20571720600128174, 0.23625899851322174, 0.17464415729045868]
Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends.	[137, 13, 58, 109, 127, 116, 64, 17, 83, 145]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6864095330238342, 0.21108709275722504, 0.10206736624240875, 0.05319233983755112, 0.2922035753726959, 0.09046326577663422, 0.0824681743979454, 0.07969266176223755, 0.062268830835819244, 0.08549968153238297]
Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003).	[9, 1, 64, 20, 12, 117, 34, 41, 125, 188]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.11743287742137909, 0.042143311351537704, 0.04339596629142761, 0.10462474822998047, 0.062236279249191284, 0.1558157056570053, 0.15914174914360046, 0.119747094810009, 0.629963219165802, 0.13916566967964172]
Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997).	[22, 45, 134, 68, 62, 54, 26, 20, 163, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.05271122232079506, 0.12851864099502563, 0.061215486377477646, 0.06621625274419785, 0.04674680531024933, 0.35241982340812683, 0.05322308465838432, 0.1060132086277008, 0.6425365805625916, 0.4602491557598114]
Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4.	[29, 87, 10, 7, 88, 108, 1, 48, 4, 106]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.5971814393997192, 0.6465251445770264, 0.5178453922271729, 0.41853347420692444, 0.5667245984077454, 0.3715560734272003, 0.49666956067085266, 0.28302857279777527, 0.381778359413147, 0.21633179485797882]
Addressing word sense disambiguation, Gale et al (1992) introduced the idea of a word sense located at the discourse-level and observed a 93 strong one-sense-per-discourse tendency, i.e. several occurrences of a polysemous word form have a tendency to belong to the same semantic class within one discourse.	[109, 101, 5, 110, 74, 4, 3, 114, 75, 113]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4220634996891022, 0.6749559640884399, 0.33829787373542786, 0.16091345250606537, 0.11874948441982269, 0.30184608697891235, 0.49298378825187683, 0.05925720930099487, 0.15020914375782013, 0.10428434610366821]
After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson,2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as? Achilles? that consists of four modules mainly: Regular expression extractor, dictionary-based Ngramsegmentation, CRF-based sub word tagging (Zhang et al, 2006), and confidence-based segmentation.	[0, 73, 1, 25, 72, 12, 100, 96, 20, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6839133501052856, 0.3321012258529663, 0.27670997381210327, 0.2453184723854065, 0.25473272800445557, 0.10567713528871536, 0.08354750275611877, 0.07546180486679077, 0.12126216292381287, 0.08637461811304092]
After the initial effort by Hindle and Rooth (1993), it has become clear that this area needs statistical methods in which an easy integration of many information sources is possible.	[23, 181, 33, 228, 66, 34, 37, 54, 226, 220]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5264344215393066, 0.07724419236183167, 0.17950642108917236, 0.48361238837242126, 0.23692482709884644, 0.07252234220504761, 0.14050227403640747, 0.16091641783714294, 0.1308460533618927, 0.2657833695411682]
Afterwards, the creation of CLTE corpus by using Mechanical Turk is described on (Negri et al, 2011) and a corpus freely available for CLTE is published (Castillo, 2011).	[14, 29, 35, 33, 98, 17, 75, 71, 23, 18]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5247207880020142, 0.28050652146339417, 0.10301588475704193, 0.5491110682487488, 0.26261529326438904, 0.1640896201133728, 0.08379662036895752, 0.09394339472055435, 0.06498143821954727, 0.05287324637174606]
Again, according to the work of Carletta et al (1997), a minimum kappa score of 0.67 is required to draw tentative conclusions.	[173, 24, 70, 45, 31, 6, 3, 14, 175, 17]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4954341650009155, 0.17689472436904907, 0.08053624629974365, 0.06403226405382156, 0.077522411942482, 0.1088535264134407, 0.08755704015493393, 0.1727103441953659, 0.08920209854841232, 0.07537189871072769]
Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint.	[123, 33, 32, 28, 7, 202, 127, 3, 124, 88]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5707011818885803, 0.42022112011909485, 0.39138883352279663, 0.22829508781433105, 0.08277665078639984, 0.09234698116779327, 0.059153445065021515, 0.19749672710895538, 0.1024942696094513, 0.047176215797662735]
Al-Onaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters.	[146, 51, 57, 65, 62, 22, 68, 64, 69, 70]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6288716197013855, 0.6478381752967834, 0.3641229569911957, 0.4873347580432892, 0.34792736172676086, 0.34387218952178955, 0.3139168918132782, 0.06613815575838089, 0.17521199584007263, 0.07442961633205414]
Algorithms for the generation of referring expressions commonly use this as a starting point, proposing that properties are organized in some linear order (Dale and Reiter, 1995) or weighted order (Krahmer et al, 2003) as input.	[98, 48, 15, 306, 69, 284, 30, 71, 2, 6]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.2720600664615631, 0.4049842059612274, 0.19619666039943695, 0.16939744353294373, 0.20864343643188477, 0.08761879801750183, 0.5585575103759766, 0.3477213978767395, 0.16800753772258759, 0.16800753772258759]
All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system.	[90, 20, 45, 34, 19, 17, 31, 56, 23, 18]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5410873889923096, 0.19168183207511902, 0.12465788424015045, 0.12087789922952652, 0.16897240281105042, 0.24726945161819458, 0.08965718746185303, 0.05189582332968712, 0.06400531530380249, 0.06410995125770569]
All feature functions in Figure 1, except the NW function, are derived from models presented in (Gao et al, 2003).	[100, 181, 95, 86, 3, 55, 49, 1, 209, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05482209846377373, 0.05366222560405731, 0.11684443801641464, 0.08950604498386383, 0.04709559306502342, 0.07282398641109467, 0.05020151287317276, 0.10726702958345413, 0.08444428443908691, 0.05812938138842583]
All features are automatically extracted from the Robust Minimal Recursion Semantics (RMRS, Copestake, 2004) representation of the sentence in which the noun phrase appears (obtained via a RASP parse, Briscoe et al, 2006).	[1, 45, 2, 64, 7, 13, 28, 54, 57, 4]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06422058492898941, 0.21012191474437714, 0.0869530662894249, 0.10922037810087204, 0.06801245361566544, 0.0486830398440361, 0.05438307672739029, 0.05388451740145683, 0.049243997782468796, 0.09936034679412842]
All improvements on two test sets are statistically significant by the bootstrap resampling (Koehn, 2004).	[165, 195, 2, 119, 118, 11, 180, 10, 194, 13]	[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.6582282781600952, 0.6483943462371826, 0.31997039914131165, 0.5692360997200012, 0.29513999819755554, 0.24233460426330566, 0.16245612502098083, 0.7199415564537048, 0.1911877691745758, 0.072060726583004]
All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).	[224, 1, 133, 140, 223, 131, 18, 0, 4, 211]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47767841815948486, 0.2803790867328644, 0.23809410631656647, 0.12298840284347534, 0.3070017099380493, 0.20181064307689667, 0.18123473227024078, 0.17677997052669525, 0.2390172779560089, 0.2727023661136627]
All the COL03 systems are results obtained using the restriction of the output of Collins (2003) parser.	[31, 272, 35, 144, 73, 252, 302, 316, 53, 99]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7096109986305237, 0.5953547954559326, 0.5636301636695862, 0.38524293899536133, 0.4109775722026825, 0.4375757873058319, 0.22657696902751923, 0.2665356695652008, 0.44951769709587097, 0.06760405004024506]
Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels.	[4, 111, 22, 5, 14, 35, 20, 34, 26, 32]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6188334226608276, 0.5449786186218262, 0.24440306425094604, 0.10034524649381638, 0.307243287563324, 0.10683417320251465, 0.2172023355960846, 0.10472043603658676, 0.0362948440015316, 0.4149986803531647]
Alshawi et al (2000) and Hwa et al (2005) explore transfer of deeper syntactic structure: dependency grammars.	[54, 67, 149, 34, 26, 140, 23, 0, 22, 107]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09405968338251114, 0.07733262330293655, 0.13968366384506226, 0.08290281891822815, 0.06935877352952957, 0.06253048777580261, 0.06181207671761513, 0.22577589750289917, 0.04745950922369957, 0.08147067576646805]
Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see Litkowski, 2004 for more details).	[23, 82, 40, 84, 74, 34, 33, 24, 78, 25]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6967847943305969, 0.5749208927154541, 0.13310377299785614, 0.052355073392391205, 0.08695966005325317, 0.2601234018802643, 0.2929200530052185, 0.0824037715792656, 0.0823889821767807, 0.0893133282661438]
Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al, 2009).	[2, 0, 9, 40, 113, 24, 6, 125, 127, 44]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3673989772796631, 0.6543676853179932, 0.17159907519817352, 0.08832808583974838, 0.3705718517303467, 0.1001894548535347, 0.1568935215473175, 0.11379516124725342, 0.05975289270281792, 0.2095349133014679]
Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts.	[6, 190, 53, 30, 187, 174, 136, 32, 3, 139]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6724691987037659, 0.6230051517486572, 0.3174125850200653, 0.13025426864624023, 0.12740662693977356, 0.1045369878411293, 0.20678207278251648, 0.06476609408855438, 0.0476972833275795, 0.2490071952342987]
Also, some work has incorporated unsupervised word clusters as features, including that of Koo et al (2008) and Suzuki et al (2009), who utilized unsupervised word clusters created using the Brown et al (1992) hierarchical clustering algorithm.	[85, 128, 86, 7, 35, 83, 10, 182, 151, 17]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8156909942626953, 0.06116844341158867, 0.06033634766936302, 0.21103505790233612, 0.08120814710855484, 0.29298830032348633, 0.10726143419742584, 0.3337399363517761, 0.06505002826452255, 0.2649715840816498]
Also, we minimize rather than maximize due to the fact we transform the model probabilities with? log (like Roth and Yih (2004)).	[210, 217, 22, 75, 167, 128, 25, 20, 78, 159]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4570010006427765, 0.23073887825012207, 0.06115071475505829, 0.04349631816148758, 0.44265472888946533, 0.44722670316696167, 0.14007671177387238, 0.054803770035505295, 0.09360472857952118, 0.3654523491859436]
Also, we will try different similarity score functions for both the clustering and the anchor approaches, as those surveyed in Corley and Mihalcea (2005).	[42, 7, 37, 59, 21, 30, 48, 47, 16, 18]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.44181859493255615, 0.20756378769874573, 0.39340195059776306, 0.048833172768354416, 0.08770088106393814, 0.20978422462940216, 0.11863904446363449, 0.36349058151245117, 0.1239730715751648, 0.16493593156337738]
Although (McDonald et al, 2005) used the prefix of each word form instead of word form itself as features, character-level features here for Chinese is essentially different from that.	[88, 95, 92, 87, 44, 15, 105, 91, 89, 127]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6120063662528992, 0.3497430682182312, 0.36260786652565, 0.3163156807422638, 0.07824799418449402, 0.06252579391002655, 0.11870959401130676, 0.3655284643173218, 0.08058690279722214, 0.04907975345849991]
Although Ahn et al (2007) compared their results with those presented by Mani and Wilson (2000), they went on to point out that, for a variety of reasons, the numbers they provided were not really comparable.	[15, 144, 121, 100, 81, 5, 134, 150, 17, 7]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6620283722877502, 0.17035554349422455, 0.12058564275503159, 0.19848835468292236, 0.0568515807390213, 0.25599536299705505, 0.06415903568267822, 0.05845015496015549, 0.06909119337797165, 0.0655793771147728]
Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice.	[104, 19, 41, 123, 127, 110, 46, 122, 148, 126]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5359113812446594, 0.5525324940681458, 0.27951887249946594, 0.23025262355804443, 0.16918598115444183, 0.3092159926891327, 0.08478697389364243, 0.09754227846860886, 0.13830125331878662, 0.06354861706495285]
Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper.	[0, 56, 26, 4, 20, 223, 11, 177, 155, 131]	[1, 1, 0, 0, 0, 1, 0, 1, 0, 0]	[0.7922551035881042, 0.6213356256484985, 0.4866637885570526, 0.3325912356376648, 0.391023188829422, 0.6142420768737793, 0.4406731426715851, 0.5241619944572449, 0.116976298391819, 0.20844212174415588]
Although the method in (Suzuki and Isozaki 2008) is quite general, it is hard to see how it can be applied to the query classification problem.	[133, 123, 162, 165, 95, 171, 39, 10, 19, 58]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0532725527882576, 0.08393450081348419, 0.15755777060985565, 0.20623786747455597, 0.061497289687395096, 0.045976217836141586, 0.07006654888391495, 0.06075712665915489, 0.04981061443686485, 0.16831377148628235]
Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)).	[128, 7, 13, 47, 126, 28, 57, 3, 98, 104]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7463212609291077, 0.6800652146339417, 0.6800652146339417, 0.05593046545982361, 0.16950643062591553, 0.2331613004207611, 0.14841783046722412, 0.1956406980752945, 0.12979665398597717, 0.062435247004032135]
Although we agree with van Deemter (2002) and others that the careful use of negation and disjunction can improve REs, these connectives must not be overused.	[174, 89, 140, 2, 5, 22, 183, 180, 41, 59]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5864328742027283, 0.5883689522743225, 0.21261931955814362, 0.22485388815402985, 0.22485388815402985, 0.056032322347164154, 0.3778817653656006, 0.08300233632326126, 0.06361353397369385, 0.060766033828258514]
Although we are primarily concerned with unsupervised property discovery, it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al, 2006) who approached the problem using supervised machine learning techniques and require labeled data.	[11, 72, 63, 50, 157, 152, 129, 20, 17, 3]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6021375060081482, 0.37687939405441284, 0.3431548476219177, 0.09792701154947281, 0.16840653121471405, 0.2660946547985077, 0.2407364547252655, 0.09033458679914474, 0.2294139713048935, 0.07936947792768478]
Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well.	[180, 40, 2, 158, 173, 18, 55, 45, 96, 99]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07936454564332962, 0.19921350479125977, 0.05794558301568031, 0.059350330382585526, 0.048223692923784256, 0.15715667605400085, 0.2517082989215851, 0.40849635004997253, 0.09961829334497452, 0.10414906591176987]
Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Sima'an, 2002).	[31, 100, 18, 33, 32, 58, 84, 62, 77, 56]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6886488199234009, 0.32211780548095703, 0.2638891041278839, 0.31200551986694336, 0.3071761429309845, 0.34172728657722473, 0.12509261071681976, 0.12853722274303436, 0.061685673892498016, 0.21059811115264893]
Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication.	[38, 160, 36, 153, 6, 170, 152, 33, 49, 151]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34606748819351196, 0.10008573532104492, 0.16003480553627014, 0.25937578082084656, 0.1431354284286499, 0.10299688577651978, 0.16711735725402832, 0.06483066827058792, 0.0461098775267601, 0.04991498589515686]
Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings.	[9, 20, 152, 158, 159, 15, 157, 24, 153, 164]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7670210003852844, 0.5763747692108154, 0.32682734727859497, 0.33740463852882385, 0.30701929330825806, 0.25597506761550903, 0.15578551590442657, 0.15678031742572784, 0.28404101729393005, 0.11404068022966385]
An alternate way to optimize weights over translation features is described in Och and Ney (2002).	[75, 60, 12, 37, 71, 119, 23, 77, 31, 3]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5290985107421875, 0.3902936577796936, 0.16631145775318146, 0.09693282842636108, 0.045951493084430695, 0.19918712973594666, 0.047220710664987564, 0.056991096585989, 0.3690432906150818, 0.07476597279310226]
An analysis of the Pearson correlation of the baseline features (Callison-Burch et al, 2012) with human quality assessments shows that the two strongest individual predictors of post-editing effort are the n-gram language model perplexities estimated on source and target sentences.	[351, 332, 221, 349, 327, 311, 323, 298, 231, 368]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.38430696725845337, 0.5233093500137329, 0.5588423609733582, 0.10240738093852997, 0.1320209801197052, 0.31617432832717896, 0.25979891419410706, 0.07520268112421036, 0.13985618948936462, 0.18625523149967194]
An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training.	[2, 11, 163, 34, 190, 114, 53, 188, 63, 45]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17346099019050598, 0.32925692200660706, 0.4062693119049072, 0.30962520837783813, 0.251300573348999, 0.11954210698604584, 0.17897716164588928, 0.31714916229248047, 0.10536566376686096, 0.07350973784923553]
An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting.	[208, 151, 201, 122, 133, 7, 56, 11, 62, 17]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6701540946960449, 0.7422020435333252, 0.6344176530838013, 0.21187445521354675, 0.45241469144821167, 0.13815288245677948, 0.24038170278072357, 0.16466890275478363, 0.2024237960577011, 0.1225334107875824]
An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004).	[162, 17, 117, 11, 16, 116, 161, 159, 2, 118]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.40705716609954834, 0.27031946182250977, 0.20886506140232086, 0.5353547930717468, 0.17742587625980377, 0.1277271956205368, 0.10168938338756561, 0.25743865966796875, 0.08368629217147827, 0.3313940763473511]
An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet.	[91, 192, 90, 75, 77, 201, 76, 118, 130, 26]	[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]	[0.7120316624641418, 0.7153216600418091, 0.5077080130577087, 0.5052675604820251, 0.5614538788795471, 0.5244295597076416, 0.2652607560157776, 0.09548567980527878, 0.3506312370300293, 0.047135014086961746]
"An hypothesized acoustic phonetic edit signal, ""a markedly abrupt cut-off of the speech signal"" (Hindle, 1983 ,p.123), is assumed to mark the interruption of fluent speech (cf. (Labov, 1966))."	[52, 115, 61, 44, 37, 240, 114, 126, 187, 15]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7832685708999634, 0.3146618902683258, 0.28763315081596375, 0.23729462921619415, 0.15160344541072845, 0.15160344541072845, 0.18480516970157623, 0.2663975954055786, 0.08678169548511505, 0.1270592212677002]
An initial PSM is bootstrapped using limited prior knowledge such as a small amount of transliterations, which may be obtained by exploiting co-occurrence information (Sproat et al, 2006).	[143, 48, 4, 66, 114, 27, 69, 9, 10, 138]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6278011798858643, 0.5150291919708252, 0.15646907687187195, 0.08320450782775879, 0.05992601066827774, 0.07944182306528091, 0.08848308026790619, 0.05977318435907364, 0.09086862951517105, 0.05718368664383888]
An other obvious system improvement would be to incorporate more advanced word-based features in the DTs, such as questions about word classes (Tillmann and Zhang 2005, Tillmann 2004).	[36, 11, 107, 7, 16, 38, 89, 113, 105, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26449984312057495, 0.2014165222644806, 0.06255891919136047, 0.3480202853679657, 0.08253489434719086, 0.04765870422124863, 0.0686851441860199, 0.048000432550907135, 0.0711345300078392, 0.047478821128606796]
Another application of word vectors is compositional vector grammar (Socher et al, 2013).	[242, 22, 2, 50, 144, 0, 82, 73, 132, 102]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.3587070107460022, 0.44331711530685425, 0.22335323691368103, 0.3264806866645813, 0.26057812571525574, 0.5641142725944519, 0.26720598340034485, 0.1588267832994461, 0.12498325109481812, 0.10083925724029541]
Another approach (Hatzivassiloglou et al,1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts).	[0, 214, 201, 44, 35, 287, 80, 154, 216, 62]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4150503873825073, 0.4981072247028351, 0.20255132019519806, 0.11182066053152084, 0.06545587629079819, 0.3579496145248413, 0.06841307878494263, 0.20873431861400604, 0.07007744908332825, 0.28333353996276855]
Another approach focused on sentence extraction (Fung and Cheung, 2004).	[56, 0, 41, 40, 149, 139, 13, 110, 140, 94]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48262080550193787, 0.37968456745147705, 0.09902983158826828, 0.3785044252872467, 0.17166170477867126, 0.046944618225097656, 0.33180752396583557, 0.399148166179657, 0.3363288938999176, 0.2834313213825226]
Another approach is taken by two other commonly used metrics, ME TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006).	[8, 2, 7, 5, 1, 4, 3, 9, 6, 0]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4250822067260742, 0.5872861742973328, 0.3674737215042114, 0.319736510515213, 0.1976507306098938, 0.09586715698242188, 0.09595277160406113, 0.11880417913198471, 0.10002052783966064, 0.2295992225408554]
Another evaluation resource (Esuli and Sebastiani, 2006) is resorted to in order to recover the evaluation values of all the hypernyms for a particular verb.	[44, 43, 53, 114, 52, 125, 46, 32, 54, 49]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6546968221664429, 0.5403879880905151, 0.15249136090278625, 0.13299988210201263, 0.20261770486831665, 0.05988365039229393, 0.05268716812133789, 0.05455230921506882, 0.13325893878936768, 0.04984462633728981]
Another important difference is that in (Fujii and Ishikawa, 2001), they evaluate only the performance of cross-language information retrieval but not that of translation estimation. (Cao and Li, 2002) proposed a method of compositional translation estimation for compounds.	[12, 142, 225, 81, 2, 216, 13, 32, 153, 157]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6670529246330261, 0.46188411116600037, 0.3836919367313385, 0.17300783097743988, 0.13844414055347443, 0.05130043253302574, 0.14318452775478363, 0.10628029704093933, 0.17734521627426147, 0.3334300220012665]
Another method, by (Leacock et al, 1998), normalizes path distance based on the depth of hierarchy.	[233, 88, 203, 225, 181, 216, 131, 13, 326, 19]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5066426396369934, 0.4322984516620636, 0.11309396475553513, 0.07372531294822693, 0.06831834465265274, 0.06888864189386368, 0.08743254840373993, 0.08383896946907043, 0.1338578164577484, 0.049073558300733566]
Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods.	[45, 7, 68, 39, 6, 79, 78, 44, 10, 51]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7100201845169067, 0.43012574315071106, 0.38840025663375854, 0.3629233241081238, 0.3968926966190338, 0.266202449798584, 0.43223387002944946, 0.26666441559791565, 0.16350948810577393, 0.10762917995452881]
Another recent method that has been proposed for training sequence models with constraints is Chang et al (2007).	[21, 168, 6, 27, 33, 116, 10, 115, 157, 101]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26788628101348877, 0.48680543899536133, 0.08900019526481628, 0.05027099698781967, 0.05033768713474274, 0.44239693880081177, 0.059977851808071136, 0.09724701195955276, 0.45300230383872986, 0.06443889439105988]
Answer Extraction: We select the top 5 ranked sentences and return them as Collins, 1997, can be used to capture the binary dependencies between the head of each phrase.	[74, 23, 69, 5, 67, 90, 45, 75, 19, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4430040717124939, 0.364505410194397, 0.28677958250045776, 0.04861355945467949, 0.058401256799697876, 0.08794654160737991, 0.1850692480802536, 0.19909363985061646, 0.046823110431432724, 0.0626504197716713]
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007).	[82, 79, 33, 19, 5, 96, 189, 163, 39, 160]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4097760021686554, 0.44992902874946594, 0.20120221376419067, 0.1540757268667221, 0.3079753518104553, 0.4596855640411377, 0.09508519619703293, 0.17874610424041748, 0.10624348372220993, 0.08414550870656967]
Application of cascades of weighted string transducers (WSTs) has been well-studied (Mohri,1997).	[53, 476, 532, 176, 5, 11, 457, 515, 518, 48]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5819138288497925, 0.0539008304476738, 0.09992887079715729, 0.527821958065033, 0.26209497451782227, 0.26209497451782227, 0.2945340871810913, 0.1531112641096115, 0.43273064494132996, 0.2866075038909912]
Approaches such as harvesting parallel corpora from the web (Resnik and Smith, 2003) address the creation of data.	[117, 388, 346, 118, 96, 328, 347, 13, 94, 27]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6108928918838501, 0.2756955623626709, 0.3261515498161316, 0.3867190480232239, 0.36208152770996094, 0.23241499066352844, 0.3612145185470581, 0.3837529718875885, 0.12818662822246552, 0.12357447296380997]
Approaches to the induction of morphology as presented in e.g. Schone and Jurafsky (2001) or Goldsmith (2001) show that the morphological properties of a small subset of languages can be induced with high accuracy, most of the existing approaches are motivated by applied or engineering concerns, and thus make assumptions that are less cognitively plausible.	[42, 14, 10, 30, 111, 6, 187, 118, 7, 1]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6347140669822693, 0.6632097959518433, 0.6060608625411987, 0.38773205876350403, 0.2675630748271942, 0.2826310694217682, 0.11751092970371246, 0.20396406948566437, 0.1141127347946167, 0.2748225927352905]
As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult.	[179, 46, 169, 60, 17, 102, 48, 57, 74, 96]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5850970149040222, 0.041596945375204086, 0.28136005997657776, 0.08003434538841248, 0.12662693858146667, 0.11797517538070679, 0.09249427914619446, 0.08024843782186508, 0.0776747614145279, 0.3388133943080902]
As Fung and Church (1994) we wish to estimate the bilingual lexicon directly.	[2, 118, 3, 15, 89, 37, 65, 119, 34, 25]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.6854608058929443, 0.6548848748207092, 0.28381162881851196, 0.5892704725265503, 0.7106934785842896, 0.2707251012325287, 0.3838123679161072, 0.38248181343078613, 0.06545339524745941, 0.0518764965236187]
As Grosz and Sidner (Grosz and Sidner, 1986) pointed out, cue phrases such as now and well serve to indicate a topic change.	[568, 107, 603, 235, 608, 37, 561, 105, 598, 628]	[1, 1, 1, 0, 1, 1, 0, 1, 0, 0]	[0.7715224027633667, 0.551416277885437, 0.5831436514854431, 0.20515607297420502, 0.6004904508590698, 0.6417842507362366, 0.49730968475341797, 0.6408968567848206, 0.32502615451812744, 0.16915085911750793]
As a further analysis, we have examined the performance of our base ME model on the same test set as that used in Gildea and Jurafsky (2000).	[68, 45, 47, 104, 20, 66, 102, 71, 16, 37]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4228106141090393, 0.33106887340545654, 0.20108136534690857, 0.07865447551012039, 0.33819183707237244, 0.21320731937885284, 0.05518167093396187, 0.06692227721214294, 0.08742620050907135, 0.10217589884996414]
As a result, if we do not compare the machine learning methods involved in the two approaches, but rather the features used in learning, our features are a natural generalization of (Chen and Rambow, 2003).	[117, 2, 169, 57, 19, 239, 160, 116, 17, 226]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5803364515304565, 0.12695308029651642, 0.2880910336971283, 0.17697837948799133, 0.09095216542482376, 0.12695308029651642, 0.1469573676586151, 0.0696057453751564, 0.1007368266582489, 0.06712234765291214]
As a result, many timex interpretation systems are a mixture of both rule-based and machine learning approaches (Mani and Wilson, 2000).	[92, 2, 96, 78, 23, 45, 4, 72, 150, 144]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2532702684402466, 0.25470033288002014, 0.20737044513225555, 0.049496617168188095, 0.1211998462677002, 0.26869651675224304, 0.04349421709775925, 0.05185090750455856, 0.10075242072343826, 0.05964628607034683]
As a somewhat radical alternative to taxonomical relationships, other ways of measuring semantic similarity based on distributional evidence have been put forward in the literature (see, among others, Brown et al 1991, Gale et al 1992, Pereira and Tishby 1992), which emphasise the role played by context in this game.	[14, 53, 42, 51, 36, 88, 2, 3, 59, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06182290241122246, 0.05995621159672737, 0.08412075787782669, 0.06777843087911606, 0.0764990970492363, 0.10469957441091537, 0.0834432914853096, 0.055005576461553574, 0.06288084387779236, 0.06319908797740936]
As a way of enriching such a template-like knowledge, Pantel et al (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots.	[17, 51, 19, 0, 161, 42, 22, 20, 79, 127]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.513655424118042, 0.31008821725845337, 0.23247221112251282, 0.7497959733009338, 0.05846107751131058, 0.19449366629123688, 0.1456606686115265, 0.13136281073093414, 0.15767191350460052, 0.04561242461204529]
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository.	[171, 177, 165, 49, 166, 172, 162, 59, 178, 169]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7779375314712524, 0.5253166556358337, 0.15071283280849457, 0.1363855004310608, 0.278714120388031, 0.24953113496303558, 0.17026428878307343, 0.11572148650884628, 0.11510910093784332, 0.23343172669410706]
As an additional experiment, we tested the Cross EM aligner (Liang et al, 2006) from the Berkeley Aligner package on the MSR data.	[86, 134, 7, 20, 1, 25, 60, 87, 64, 100]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31450605392456055, 0.2518177926540375, 0.15782704949378967, 0.10992579162120819, 0.0659206286072731, 0.05153805762529373, 0.055829837918281555, 0.05227884277701378, 0.047819700092077255, 0.04915617033839226]
As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)	[11, 15, 278, 47, 168, 71, 225, 285, 293, 61]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3891128599643707, 0.22925807535648346, 0.34182170033454895, 0.30861642956733704, 0.22601474821567535, 0.19167622923851013, 0.48458924889564514, 0.059307780116796494, 0.15598756074905396, 0.2898332476615906]
As hidden Markov models have been used both for name finding (Bikel et al (1997)) and tokenization (Cutting et al.	[3, 1, 5, 6, 30, 135, 11, 82, 27, 137]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4141049385070801, 0.19636794924736023, 0.1131298691034317, 0.05067000910639763, 0.06296879053115845, 0.08767671883106232, 0.08571726083755493, 0.05526348575949669, 0.07262927293777466, 0.07490259408950806]
As in Clarke et al (2010), we obviate the need for annotated logical forms by considering the end-to-end problem of mapping questions to answers.	[1, 127, 138, 159, 170, 177, 66, 181, 23, 167]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5080735683441162, 0.6195387244224548, 0.3389314115047455, 0.2774938941001892, 0.29735302925109863, 0.04657088592648506, 0.2542782127857208, 0.04944393411278725, 0.050050750374794006, 0.1595865786075592]
As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999).	[1, 20, 34, 22, 10, 102, 137, 14, 21, 71]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.525204598903656, 0.7396464347839355, 0.44114887714385986, 0.26501932740211487, 0.24111802875995636, 0.11031968146562576, 0.2563943862915039, 0.08836223930120468, 0.27884265780448914, 0.29405397176742554]
As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder.	[49, 48, 39, 16, 61, 18, 2, 171, 30, 128]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4566044807434082, 0.5765187740325928, 0.10254283249378204, 0.18355225026607513, 0.33848533034324646, 0.09075808525085449, 0.05030462145805359, 0.06989849358797073, 0.0538804866373539, 0.08218317478895187]
As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system.	[66, 4, 198, 220, 223, 218, 190, 75, 222, 148]	[0, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.3781169652938843, 0.25562092661857605, 0.5232183933258057, 0.19812172651290894, 0.15698635578155518, 0.5883687734603882, 0.1424475461244583, 0.055327337235212326, 0.1151953712105751, 0.29644185304641724]
As methodologies deriving well-formedness of a sentence we use super tagging (Bangalore and Joshi, 1999) with lightweight dependency analysis (LDA) (Bangalore, 2000), link grammars (Sleator and Temperley, 1993) and a maximum entropy (ME) based chunk parser (Bender et al, 2003).	[300, 270, 337, 103, 43, 274, 273, 265, 286, 348]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.5537444353103638, 0.32986968755722046, 0.3461379408836365, 0.24435052275657654, 0.18706175684928894, 0.0803770050406456, 0.06512677669525146, 0.5101652145385742, 0.1807398647069931, 0.09942249208688736]
As noted by (Rosenberg and Hirschberg, 2007), the Q measure does not explicitly address the completeness of the suggested clustering.	[124, 122, 18, 59, 11, 126, 65, 60, 39, 248]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7731472253799438, 0.7928210496902466, 0.7276136875152588, 0.12743446230888367, 0.3375292718410492, 0.22782954573631287, 0.2533516585826874, 0.46170103549957275, 0.16770516335964203, 0.47317638993263245]
As our baseline, we use two methods of comparing semantic vectors: sj1 and sj2, both introduced by Schone and Jurafsky (2001).	[136, 85, 184, 152, 46, 137, 20, 23, 173, 1]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5018994808197021, 0.10222063213586807, 0.16497910022735596, 0.532834529876709, 0.15033236145973206, 0.28347906470298767, 0.05197746306657791, 0.06688728928565979, 0.11996417492628098, 0.12698033452033997]
As reported elsewhere the resulting Kappa statistics (Carletta, 1996) over the annotated data yields 0.7.	[48, 1, 59, 66, 81, 0, 2, 5, 23, 65]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17880605161190033, 0.19676770269870758, 0.11693236231803894, 0.05024782940745354, 0.04880848899483681, 0.3418714106082916, 0.046373315155506134, 0.046373315155506134, 0.05265951156616211, 0.13112187385559082]
As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al, 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources.	[6, 18, 2, 7, 23, 4, 5, 38, 34, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7953202128410339, 0.15447494387626648, 0.10665937513113022, 0.18225064873695374, 0.12809333205223083, 0.2675841748714447, 0.12420263141393661, 0.07168463617563248, 0.06866414844989777, 0.054637402296066284]
As stated above, the separation of environmental, agent and task factors was motivated by Walker et al (1997).	[15, 169, 2, 29, 126, 19, 18, 70, 154, 137]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6549044251441956, 0.1662323921918869, 0.16815054416656494, 0.2978154122829437, 0.06308753043413162, 0.278120219707489, 0.2666226327419281, 0.20576944947242737, 0.05709753930568695, 0.059035323560237885]
As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b).	[182, 23, 60, 42, 131, 61, 21, 180, 63, 113]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5043666362762451, 0.5043666362762451, 0.19097448885440826, 0.11971919238567352, 0.13523095846176147, 0.06098895147442818, 0.393937885761261, 0.393937885761261, 0.24791568517684937, 0.05447310581803322]
As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively.	[105, 18, 163, 16, 15, 96, 185, 138, 20, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05425989627838135, 0.11633075773715973, 0.21308408677577972, 0.17938286066055298, 0.11529541015625, 0.05210678651928902, 0.07150810956954956, 0.04962729290127754, 0.07691674679517746, 0.2260044664144516]
As we said at the out 211 set, we don't necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work.	[65, 39, 29, 107, 49, 40, 26, 120, 11, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06734435260295868, 0.05095302313566208, 0.05787965655326843, 0.2076852023601532, 0.10648902505636215, 0.051741763949394226, 0.10363951325416565, 0.06063029542565346, 0.10771846026182175, 0.06230790540575981]
Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm:.	[95, 52, 205, 109, 77, 76, 22, 4, 63, 48]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7200961709022522, 0.11793869733810425, 0.2923601567745209, 0.3553987145423889, 0.43149474263191223, 0.1674346774816513, 0.17668616771697998, 0.27430278062820435, 0.07616288959980011, 0.14994537830352783]
At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003).	[47, 25, 35, 141, 10, 78, 4, 1, 33, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5704469084739685, 0.3422483205795288, 0.1678401529788971, 0.24158541858196259, 0.13115321099758148, 0.06300729513168335, 0.23131364583969116, 0.16689278185367584, 0.10384705662727356, 0.5248128175735474]
At the other end of the spectrum, Resnik and Smith (2003) search the Web to detect web pages that are translations of each other.	[37, 53, 140, 33, 52, 85, 44, 54, 168, 111]	[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6775680184364319, 0.4997105002403259, 0.51466965675354, 0.5131098628044128, 0.2990240454673767, 0.12945520877838135, 0.3540148437023163, 0.11507736891508102, 0.09999334067106247, 0.15552052855491638]
At the same time, representations such as FunQL (Kate et al, 2005), which was used in Clarke et al (2010), are simpler but lack the full expressive power of lambda calculus.	[141, 109, 52, 17, 33, 206, 155, 44, 118, 58]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4984223246574402, 0.12406805157661438, 0.2482990324497223, 0.04491904005408287, 0.0492887943983078, 0.24736575782299042, 0.39154037833213806, 0.054218996316194534, 0.0622270368039608, 0.0532042533159256]
BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation.	[0, 3, 5, 11, 1, 59, 6, 10, 17, 9]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8139903545379639, 0.33116960525512695, 0.19932927191257477, 0.27966636419296265, 0.12485004961490631, 0.07281272858381271, 0.05724222958087921, 0.2079172134399414, 0.04857872426509857, 0.06948503851890564]
Back-transliteration is a difficult problem as exemplified in (Knight and Graehl 1997, Chen, et.al. 1998).	[21, 25, 40, 31, 36, 103, 48, 3, 145, 126]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7139507532119751, 0.5411423444747925, 0.2869953215122223, 0.6340609788894653, 0.25202709436416626, 0.16499647498130798, 0.06733782589435577, 0.07680854946374893, 0.08616533130407333, 0.11282432824373245]
Banea et al (2008) demonstrate that machine translation can perform quite well when extending the subjectivity analysis to multilingual environment, which makes it inspiring to replicate their work on lexicon-based sentiment analysis.	[0, 90, 112, 84, 16, 7, 29, 11, 18, 126]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8107250928878784, 0.2332874983549118, 0.15826629102230072, 0.16419224441051483, 0.15360397100448608, 0.12124303728342056, 0.16184458136558533, 0.21509219706058502, 0.15440377593040466, 0.051579222083091736]
Bannard and Callison-Burch (2005) defined a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases as: Ppara (p1 ,p2)= sum piv Pt (piv|p1) Pt (p2|piv) where Pt denotes translation probabilies.	[102, 43, 52, 48, 47, 49, 35, 32, 4, 40]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5921405553817749, 0.19619806110858917, 0.3401297628879547, 0.39784809947013855, 0.20763741433620453, 0.3322667181491852, 0.1095118299126625, 0.1789538711309433, 0.09331796318292618, 0.052037693560123444]
Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences.	[481, 171, 163, 161, 627, 89, 108, 26, 29, 191]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.357230544090271, 0.3232959508895874, 0.2943764626979828, 0.41993772983551025, 0.27102985978126526, 0.1695895940065384, 0.10333087295293808, 0.21702727675437927, 0.43849024176597595, 0.14671123027801514]
Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data.	[78, 74, 14, 31, 28, 1, 19, 44, 16, 60]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.6404132843017578, 0.34467029571533203, 0.23394857347011566, 0.08650023490190506, 0.08902549743652344, 0.30296072363853455, 0.5834425091743469, 0.0508054718375206, 0.06811847537755966, 0.13693371415138245]
Based on an analysis of the PennBioIE corpus (Kulick et al, 2004), detailed distributional results are provided on alternation patterns for several nominalizations with high frequency of occurrence in biomedical text, such as activation and treatment.	[80, 6, 70, 57, 75, 71, 0, 1, 22, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21148279309272766, 0.17536291480064392, 0.10313036292791367, 0.05758233368396759, 0.07279206067323685, 0.054659537971019745, 0.438170462846756, 0.07607249915599823, 0.07316092401742935, 0.08750606328248978]
Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems.	[8, 27, 41, 103, 119, 14, 118, 112, 3, 117]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6219650506973267, 0.6080735325813293, 0.6157311797142029, 0.42833441495895386, 0.3337193727493286, 0.299834668636322, 0.3382943570613861, 0.23791514337062836, 0.1752709299325943, 0.19492460787296295]
Because equivalence is the most fundamental semantic relationship, techniques for generating and recognizing paraphrases play an important role in a wide range of natural language processing tasks (Madnani and Dorr, 2010).	[2, 102, 7, 100, 567, 599, 198, 38, 28, 11]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6956730484962463, 0.6991877555847168, 0.6956730484962463, 0.22863611578941345, 0.4303344190120697, 0.3937560021877289, 0.1957121193408966, 0.3885454535484314, 0.11982891708612442, 0.3495490252971649]
Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al (2009)).	[124, 0, 5, 1, 7, 180, 130, 170, 6, 23]	[1, 1, 1, 0, 0, 0, 1, 1, 0, 0]	[0.6331853270530701, 0.7254227995872498, 0.5873448252677917, 0.29477980732917786, 0.28696006536483765, 0.1714567244052887, 0.6694883704185486, 0.6601687073707581, 0.2736772298812866, 0.3489140272140503]
Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al (2005).	[31, 125, 30, 3, 13, 122, 32, 106, 55, 105]	[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6566165685653687, 0.24955889582633972, 0.6361848711967468, 0.6401556730270386, 0.4294973909854889, 0.20006303489208221, 0.26367834210395813, 0.05968719348311424, 0.32108473777770996, 0.09251022338867188]
Berland and Charniak (1999) suggest their work may be useful for building a lexicon or ontology, like WordNet.	[11, 4, 19, 7, 13, 20, 21, 29, 103, 125]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6396269798278809, 0.6069971323013306, 0.1943536400794983, 0.04619402065873146, 0.12207727134227753, 0.26617661118507385, 0.09033491462469101, 0.11185130476951599, 0.2499428540468216, 0.09915219247341156]
Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a).	[83, 64, 29, 82, 25, 26, 234, 54, 60, 85]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6863136291503906, 0.7317937016487122, 0.0836358591914177, 0.37667885422706604, 0.2912241518497467, 0.16049590706825256, 0.06594062596559525, 0.09881994873285294, 0.1416839361190796, 0.05484690144658089]
Best-first parsers deal with this by allowing an upward propagation, which updates such edges' scores (Caraballo and Charniak, 1998).	[189, 61, 234, 187, 202, 178, 58, 182, 175, 60]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5524735450744629, 0.3809959590435028, 0.3040296733379364, 0.3090488612651825, 0.24560274183750153, 0.17241859436035156, 0.11818189918994904, 0.29633334279060364, 0.3791869878768921, 0.10360130667686462]
Birch et al (2007) then investigated source-side CCG super tag features, but did not show an improvement for Dutch-English.	[19, 24, 95, 68, 159, 30, 118, 209, 133, 208]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7650258541107178, 0.4066762924194336, 0.2630663514137268, 0.2013942301273346, 0.22901037335395813, 0.12925368547439575, 0.07775136083364487, 0.11665994673967361, 0.050855230540037155, 0.05541113391518593]
Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster).	[147, 146, 148, 151, 36, 84, 202, 13, 7, 11]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8139433860778809, 0.8153011202812195, 0.6717141270637512, 0.303742378950119, 0.22784285247325897, 0.07256792485713959, 0.06710497289896011, 0.07011860609054565, 0.06211666390299797, 0.050797365605831146]
BitPar (Schmid, 2006) is a probabilistic context free parser using bit-vector operations (Schmid, 2004).	[3, 1, 0, 9, 25, 5, 18, 24, 4, 2]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.734344482421875, 0.7422778606414795, 0.6788477897644043, 0.30331286787986755, 0.31800156831741333, 0.15191234648227692, 0.3441871106624603, 0.04915305972099304, 0.10724231600761414, 0.08588248491287231]
Blunsom et al (2009) describe a blocked sampler following John son et al (2007) which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O (|f |3|e|3) complexity.	[43, 84, 133, 155, 16, 39, 108, 116, 152, 26]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5009709000587463, 0.1625562310218811, 0.26972800493240356, 0.08174201101064682, 0.05225634202361107, 0.053215790539979935, 0.08686454594135284, 0.05879112333059311, 0.04981139674782753, 0.10805752128362656]
Bod (1992) demonstrated that DOP can be implemented using conventional context-free parsing techniques.	[3, 32, 55, 96, 107, 109]	[1, 1, 0, 0, 0, 0]	[0.7777029871940613, 0.8005467653274536, 0.07715368270874023, 0.08083376288414001, 0.09142499417066574, 0.07287587970495224]
Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004).	[10, 111, 9, 135, 11, 130, 104, 106, 138, 139]	[1, 1, 1, 1, 1, 0, 1, 0, 0, 0]	[0.7726311087608337, 0.7166848182678223, 0.7300166487693787, 0.5488563179969788, 0.5745195746421814, 0.3891986310482025, 0.6396828889846802, 0.25548064708709717, 0.301289826631546, 0.09696543961763382]
Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing.	[0, 22, 15, 2, 17, 160, 9, 68, 1, 33]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7178337574005127, 0.5697224140167236, 0.2959102988243103, 0.36522939801216125, 0.23061922192573547, 0.1664951890707016, 0.10086619108915329, 0.0824531689286232, 0.076514832675457, 0.06651465594768524]
Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space.	[103, 162, 78, 88, 96, 134, 104, 125, 18, 75]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1302889585494995, 0.16264209151268005, 0.3280085027217865, 0.25504255294799805, 0.07796281576156616, 0.07141560316085815, 0.13462060689926147, 0.13196933269500732, 0.12055014073848724, 0.08004648983478546]
Boyd-Graber et al (2007) describe a related topic model, LDAWN, for word sense disambiguation that adds an intermediate layer of latent variables Z on which the Markov model parameters are conditioned.	[0, 67, 1, 221, 5, 97, 43, 4, 16, 66]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7870649099349976, 0.42886561155319214, 0.3466983735561371, 0.13240094482898712, 0.08698383718729019, 0.0974593237042427, 0.2556074559688568, 0.21580277383327484, 0.08457934856414795, 0.05037267878651619]
Brennan et al [1987] propose an algorithm for pronoun resolution based on centering theory.	[35, 27, 8, 1, 92, 15, 98, 78, 0, 61]	[0, 0, 1, 0, 0, 0, 0, 0, 1, 0]	[0.41202014684677124, 0.49843576550483704, 0.5269651412963867, 0.10717812180519104, 0.15182048082351685, 0.15273204445838928, 0.2778019607067108, 0.366521954536438, 0.7777235507965088, 0.050028517842292786]
Brill and Moore (2000) showed that adding a source language model increases the accuracy significantly.	[128, 81, 12, 2, 7, 8, 23, 17, 123, 129]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5159977078437805, 0.3144000172615051, 0.14847350120544434, 0.4702720642089844, 0.4702720642089844, 0.10034149140119553, 0.1852594017982483, 0.38626834750175476, 0.16269001364707947, 0.3539191782474518]
Brill and Resnik (1994) applied Error-Driven TransformationBased Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model.	[117, 53, 118, 33, 27, 23, 6, 110, 36, 109]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19899170100688934, 0.5495641231536865, 0.14435547590255737, 0.14330485463142395, 0.13355617225170135, 0.08015269041061401, 0.051621999591588974, 0.2832088768482208, 0.06282442063093185, 0.24312494695186615]
Briscoe and Carroll (2006) discuss issues raised by this re annotation.	[114, 144, 4, 35, 34, 40, 38, 128, 149, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6545730829238892, 0.7564840316772461, 0.34224560856819153, 0.35424116253852844, 0.2798514664173126, 0.09843997657299042, 0.09029298275709152, 0.20562197268009186, 0.1151527389883995, 0.06396611034870148]
Brockett et al (2006) employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors.	[1, 0, 33, 129, 11, 127, 35, 125, 133, 135]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6938679814338684, 0.2774287164211273, 0.3948672115802765, 0.1033896654844284, 0.37678056955337524, 0.11088128387928009, 0.07725183665752411, 0.05593389272689819, 0.18966402113437653, 0.09716741740703583]
Brown et al (1991) and Gale and Church (1993) are amongst the most cited works in text alignment work.	[18, 7, 17, 128, 137, 129, 15, 3, 32, 6]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.712627112865448, 0.7579607963562012, 0.07235236465930939, 0.20114077627658844, 0.1428724229335785, 0.18278871476650238, 0.12995809316635132, 0.08629483729600906, 0.11290784180164337, 0.05847357586026192]
Building a dictionary from scratch is not possible this way or at least computationally un-feasible (see Rapp, 1995).	[29, 2, 17, 19, 43, 56, 1, 13, 14, 7]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.653091311454773, 0.3974754512310028, 0.050287097692489624, 0.07082253694534302, 0.35750091075897217, 0.38134336471557617, 0.06428354978561401, 0.052726276218891144, 0.049314457923173904, 0.04957101121544838]
Building on Lin (1998), McCarthy et al (2003) measure the semantic similarity between expressions (verb particles) as a whole and their component words (verb).	[51, 52, 57, 54, 58, 44, 138, 131, 7, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4373141825199127, 0.45702704787254333, 0.4690132141113281, 0.07234451174736023, 0.24869076907634735, 0.08191148936748505, 0.31724339723587036, 0.060768697410821915, 0.09160586446523666, 0.05590745806694031]
But for other tasks, such as machine translation (Brown et al, 1990), the chief merit of unlabeled data is simply that nothing else is available.	[202, 1, 173, 0, 24, 3, 12, 5, 14, 52]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5005251169204712, 0.17639099061489105, 0.08090849965810776, 0.5689775347709656, 0.2653447687625885, 0.09722913801670074, 0.37411177158355713, 0.21545687317848206, 0.1514035165309906, 0.2006891667842865]
But if we use existing techniques for parsing DCGs, then we are also confronted with an undecidability problem: the recognition problem for DCGs is undecidable (Pereira and Warren, 1983).	[146, 39, 67, 145, 59, 116, 169, 138, 165, 21]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.64093416929245, 0.6062961220741272, 0.1800997108221054, 0.4171385169029236, 0.10199589282274246, 0.22163255512714386, 0.21478012204170227, 0.08417209982872009, 0.1259813904762268, 0.11253725737333298]
But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002).	[0, 46, 1, 25, 10, 83, 57, 24, 2, 23]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7703977227210999, 0.6682702302932739, 0.4027532637119293, 0.4192694425582886, 0.2131595015525818, 0.40888914465904236, 0.4576161801815033, 0.2068760097026825, 0.4474518597126007, 0.4050556719303131]
By finding semantic differences between the selectional preferences, it can articulate the higher-order structure of conceptual metaphors ((Mason, 2004), p. 24), finding mappings like LIQUID -> MONEY.	[32, 29, 31, 290, 5, 33, 206, 13, 91, 103]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 1]	[0.7820734977722168, 0.6997305750846863, 0.20899592339992523, 0.34452444314956665, 0.3838212788105011, 0.40757057070732117, 0.37660491466522217, 0.3838212788105011, 0.5009678602218628, 0.5620064735412598]
By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995).	[50, 1, 6, 0, 17, 11, 8, 23, 37, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06199340894818306, 0.0417652353644371, 0.1614430993795395, 0.3109169006347656, 0.06459750235080719, 0.15491171181201935, 0.07694698125123978, 0.11775042116641998, 0.0444784015417099, 0.04492275044322014]
By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008).	[27, 0, 29, 20, 128, 131, 136, 132, 135, 18]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4625384211540222, 0.6453086733818054, 0.12939104437828064, 0.12893177568912506, 0.133067324757576, 0.18640169501304626, 0.06957332789897919, 0.20184879004955292, 0.0916970744729042, 0.06331334263086319]
C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations.	[22, 3, 12, 48, 35, 154, 25, 21, 66, 112]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46628567576408386, 0.4447275698184967, 0.4252064824104309, 0.48220688104629517, 0.14131109416484833, 0.1753871887922287, 0.26394322514533997, 0.324992835521698, 0.07814278453588486, 0.46265608072280884]
C99 KA to denote the C99 algorithm (Choi, 2000) when the aver age number of boundaries in the reference data was provided to the algorithm.	[117, 122, 133, 130, 121, 33, 125, 131, 66, 144]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6076419949531555, 0.6895554661750793, 0.4486360549926758, 0.24937891960144043, 0.0682322084903717, 0.05080453306436539, 0.07810584455728531, 0.10653529316186905, 0.3974447250366211, 0.08865709602832794]
CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar.	[153, 157, 54, 154, 155, 55, 241, 127, 379, 330]	[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.3694992959499359, 0.5279991626739502, 0.32187074422836304, 0.09209847450256348, 0.21935662627220154, 0.08214806020259857, 0.10436874628067017, 0.14520566165447235, 0.5650992393493652, 0.17584940791130066]
Callison-Burch et al (2006) propose the use of paraphrases as a means of dealing with unseen source phrases.	[73, 37, 4, 67, 87, 12, 81, 48, 76, 99]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6767290830612183, 0.7507908344268799, 0.2978269159793854, 0.25630462169647217, 0.11918052285909653, 0.05522163584828377, 0.048207804560661316, 0.245920792222023, 0.05855368450284004, 0.07018604129552841]
Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality.	[16, 52, 85, 132, 2, 6, 12, 84, 34, 117]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.777948260307312, 0.6853628158569336, 0.507637619972229, 0.4621906578540802, 0.14897993206977844, 0.09451662749052048, 0.22006744146347046, 0.39544540643692017, 0.25219467282295227, 0.2218104749917984]
Callison-Burch et al (2012) report for several automatic metrics on the whole WMT12 English-to-Czech dataset, the best of which correlates at?= 0.18.	[6, 127, 104, 299, 5, 1, 243, 251, 13, 3]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7922198176383972, 0.7898939847946167, 0.5863590836524963, 0.08470509946346283, 0.06636886298656464, 0.05249238759279251, 0.09403262287378311, 0.05755878984928131, 0.05911821499466896, 0.11987427622079849]
Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists.	[15, 0, 3, 54, 27, 58, 108, 219, 22, 24]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5542984008789062, 0.7574400901794434, 0.475057989358902, 0.4041121304035187, 0.3060048818588257, 0.39035889506340027, 0.3594449758529663, 0.07503319531679153, 0.25205129384994507, 0.1037139743566513]
Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain: focus on the situational descriptions explicitly pertaining to a single protagonist, a series of references within a document that are automatically labeled as co referent.	[64, 2, 95, 67, 18, 22, 14, 54, 52, 222]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49455952644348145, 0.13066266477108002, 0.25017493963241577, 0.22694215178489685, 0.1555652767419815, 0.2998437285423279, 0.1073964387178421, 0.12768685817718506, 0.0672328919172287, 0.057682644575834274]
Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence.	[4, 26, 3, 0, 41, 1, 184, 186, 36, 187]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06690443307161331, 0.06165751814842224, 0.10742968320846558, 0.2513054311275482, 0.10368310660123825, 0.12084982544183731, 0.08385517448186874, 0.14507226645946503, 0.06006390601396561, 0.07460716366767883]
Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006).	[28, 58, 134, 152, 6, 25, 57, 5, 75, 32]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.344518780708313, 0.5877042412757874, 0.23411373794078827, 0.18835917115211487, 0.11593351513147354, 0.16227510571479797, 0.2320910543203354, 0.046225402504205704, 0.15472553670406342, 0.24789001047611237]
Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data.	[57, 56, 54, 73, 40, 48, 13, 63, 1, 64]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43169528245925903, 0.4655579626560211, 0.2733575999736786, 0.15987534821033478, 0.20314806699752808, 0.3780960440635681, 0.04880886524915695, 0.11908042430877686, 0.4171443581581116, 0.22413209080696106]
Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions.	[66, 65, 77, 24, 31, 32, 30, 78, 4, 6]	[1, 1, 0, 0, 1, 0, 1, 0, 0, 0]	[0.7262330651283264, 0.60036700963974, 0.48248741030693054, 0.08258983492851257, 0.521350085735321, 0.23980532586574554, 0.5780699849128723, 0.1975201666355133, 0.09446835517883301, 0.22140847146511078]
Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework.	[158, 35, 51, 1, 5, 0, 48, 187, 18, 159]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.08465635031461716, 0.2685263752937317, 0.313157856464386, 0.28241124749183655, 0.28241124749183655, 0.5239165425300598, 0.3631691634654999, 0.2016904652118683, 0.18376129865646362, 0.07047916203737259]
Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter.	[50, 119, 3, 14, 43, 128, 36, 34, 79, 44]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.752856433391571, 0.5032197833061218, 0.4233299493789673, 0.4345535635948181, 0.43420442938804626, 0.1183084025979042, 0.2659420073032379, 0.07826744765043259, 0.18982864916324615, 0.1150493398308754]
Choi 2000 Choi's (2000) state-of-the-art approach to finding segment boundaries.	[1, 81, 6, 16, 140, 5, 44, 78, 66, 3]	[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.47005462646484375, 0.5364010334014893, 0.06343638151884079, 0.10242793709039688, 0.06751799583435059, 0.16062891483306885, 0.26578888297080994, 0.5135603547096252, 0.18329018354415894, 0.08908567577600479]
Choudhury et al. (2007) implemented the noisy channel through a Hidden-Markov Model (HMM) able to handle both graphemic variants and phonetic plays as proposed by (Toutanova and Moore, 2002), while Cook and Stevenson (2009) enhanced the model by adapting the channel's noise P (O|W, wf) according to a list of predefined observed word formations {wf}: stylistic variation, word clipping, phonetic abbreviations, etc.	[31, 1, 28, 16, 27, 36, 33, 14, 127, 157]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35238128900527954, 0.3033987283706665, 0.4411662220954895, 0.18332350254058838, 0.24962054193019867, 0.20567649602890015, 0.0904112309217453, 0.12823574244976044, 0.1189379096031189, 0.1756971925497055]
Chunks as a separate level have also been used in Collins (1996) and Ratnaparkhi (1997).	[31, 16, 28, 78, 80, 108, 105, 20, 67, 29]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08678626269102097, 0.04693426936864853, 0.05114993825554848, 0.057238660752773285, 0.4502094089984894, 0.04381011798977852, 0.059986457228660583, 0.06590855121612549, 0.1458837389945984, 0.07413482666015625]
Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002).	[2, 13, 74, 1, 52, 94, 124, 78, 79, 116]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10309550166130066, 0.18518461287021637, 0.08521076291799545, 0.1662898063659668, 0.07430928200483322, 0.17949295043945312, 0.13489870727062225, 0.436625212430954, 0.2044258415699005, 0.0955272987484932]
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures.	[3, 19, 15, 22, 18, 17, 7, 21, 1, 14]	[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.4880822002887726, 0.7072277069091797, 0.2160956859588623, 0.48398151993751526, 0.45974719524383545, 0.2949531078338623, 0.33326661586761475, 0.5638400912284851, 0.2861965000629425, 0.4140060245990753]
Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes.	[37, 41, 11, 7, 5, 38, 12, 68, 81, 4]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5084069967269897, 0.47368335723876953, 0.08276790380477905, 0.15579670667648315, 0.16213567554950714, 0.38781359791755676, 0.10098239779472351, 0.09455018490552902, 0.06068943440914154, 0.14975272119045258]
Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model.	[70, 34, 26, 15, 146, 97, 27, 106, 32, 114]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4941248893737793, 0.10699580609798431, 0.15060336887836456, 0.07274230569601059, 0.25725170969963074, 0.12787650525569916, 0.07371588796377182, 0.10982585698366165, 0.11094702780246735, 0.11868257820606232]
Collins et al (2005) address this problem by reordering German sentences to more closely parallel English word order, prior to translation by a PSMT system.	[21, 23, 35, 58, 20, 78, 96, 82, 124, 6]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6558898091316223, 0.42061924934387207, 0.09994484484195709, 0.28751739859580994, 0.15952910482883453, 0.6708142161369324, 0.2594163417816162, 0.4783119261264801, 0.07994402199983597, 0.11329013854265213]
Computational approaches are mostly concerned with inferring implicitly expressed metonymic relations in English texts (Fass 1991).	[80, 38, 90, 609, 93, 558, 119, 283, 605, 463]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2039264589548111, 0.34906330704689026, 0.35457754135131836, 0.06910921633243561, 0.047936249524354935, 0.21814346313476562, 0.046411264687776566, 0.1101033166050911, 0.06812238693237305, 0.05594843253493309]
Concerning relatedness measure, additional corpus-based measures such as Web-basedmeasures (Cimiano and Wenderoth, 2007) or measures based on syntactic relations (Pustejovsky et al, 1993) could appear to be useful for improving the ranking of the extracted relations.	[5, 12, 294, 292, 267, 24, 336, 136, 266, 295]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2500526010990143, 0.2500526010990143, 0.4412390887737274, 0.11129146814346313, 0.24889840185642242, 0.23169443011283875, 0.25751781463623047, 0.3293459713459015, 0.10485848784446716, 0.431583434343338]
Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing.	[168, 103, 18, 158, 196, 155, 145, 98, 13, 116]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.5153481364250183, 0.309955358505249, 0.3968009948730469, 0.23192301392555237, 0.35010412335395813, 0.3889637887477875, 0.1851407140493393, 0.05704484507441521, 0.5737481713294983, 0.27635011076927185]
Confidence intervals at 95% confidence level following (Koehn, 2004).	[141, 115, 111, 126, 136, 108, 134, 116, 114, 106]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7761291265487671, 0.4290866553783417, 0.27542585134506226, 0.45785319805145264, 0.19176259636878967, 0.40833133459091187, 0.23903578519821167, 0.24835990369319916, 0.20273497700691223, 0.2835029363632202]
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004).	[1, 0, 12, 85, 86, 84, 141, 125, 19, 16]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7927560210227966, 0.7538653612136841, 0.4662832021713257, 0.2849304676055908, 0.27558276057243347, 0.589841365814209, 0.3645334839820862, 0.2980702519416809, 0.06644973158836365, 0.07507728785276413]
Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).	[86, 92, 85, 11, 1, 104, 8, 15, 40, 68]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6749829649925232, 0.4104853570461273, 0.3435264527797699, 0.16055741906166077, 0.43808621168136597, 0.2733611464500427, 0.24849706888198853, 0.3839261531829834, 0.4492145776748657, 0.4149007201194763]
Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)).	[167, 166, 26, 49, 137, 148, 14, 69, 136, 2]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5330484509468079, 0.5198885202407837, 0.23009145259857178, 0.1299978792667389, 0.3170446753501892, 0.06704335659742355, 0.157065749168396, 0.1998222917318344, 0.15917228162288666, 0.0701693594455719]
Constructs such as Initiative and Control (Whittaker and Stenton, 1988), which attempt to operationalize the authority over a discourse's structure, fall under the umbrella of positioning.	[1, 2, 130, 135, 7, 14, 131, 18, 142, 137]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4263511896133423, 0.42958223819732666, 0.11202151328325272, 0.17914193868637085, 0.1199132651090622, 0.4022771418094635, 0.23183602094650269, 0.0941937118768692, 0.06388340145349503, 0.15323933959007263]
Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al, 2003).	[84, 81, 80, 86, 77, 112, 111, 79, 62, 75]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6688287854194641, 0.6431887745857239, 0.08160868287086487, 0.0642879530787468, 0.6673683524131775, 0.09601552784442902, 0.07804914563894272, 0.0778433233499527, 0.1729794293642044, 0.12616583704948425]
Continuing this work Moldovan et al (Moldovan et al, 2003) built a logic prover for Question Answering.	[0, 2, 18, 24, 14, 71, 141, 144, 146, 28]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8049159646034241, 0.4101075828075409, 0.09639503061771393, 0.0706203505396843, 0.15681973099708557, 0.06646909564733505, 0.27513012290000916, 0.4410278797149658, 0.444879949092865, 0.4925360381603241]
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees.	[1, 23, 24, 13, 68, 133, 70, 34, 79, 18]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.771054208278656, 0.5511831641197205, 0.15424112975597382, 0.05573302134871483, 0.25940486788749695, 0.07378437370061874, 0.07351170480251312, 0.09993459284305573, 0.055422764271497726, 0.07255208492279053]
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions.	[20, 0, 2, 94, 18, 130, 19, 52, 60, 95]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3191496729850769, 0.7665714025497437, 0.23816148936748505, 0.30586451292037964, 0.12724822759628296, 0.35842740535736084, 0.0894998237490654, 0.3619537949562073, 0.08991208672523499, 0.25504013895988464]
Curran and Clark (2003) describes the model and explains how Generalised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights.	[1, 7, 15, 3, 42, 21, 108, 34, 33, 40]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.45928043127059937, 0.6102169752120972, 0.3635656237602234, 0.5764797925949097, 0.4625200033187866, 0.17698794603347778, 0.23166422545909882, 0.06281302124261856, 0.10928815603256226, 0.14772194623947144]
Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term.	[114, 119, 117, 120, 137, 111, 115, 108, 135, 118]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6779837608337402, 0.7159912586212158, 0.6783114671707153, 0.5109443068504333, 0.1462574005126953, 0.1096663624048233, 0.26036137342453003, 0.12153469771146774, 0.23051127791404724, 0.19547544419765472]
Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons.	[111, 3, 114, 131, 40, 112, 113, 135, 96, 144]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7311227321624756, 0.2256380170583725, 0.13652607798576355, 0.18042252957820892, 0.3581543564796448, 0.2112988829612732, 0.21680022776126862, 0.32914865016937256, 0.057994723320007324, 0.2608606815338135]
Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.	[57, 0, 3, 20, 80, 21, 11, 12, 9, 4]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.11566461622714996, 0.5305449962615967, 0.384635865688324, 0.5750867128372192, 0.2972644865512848, 0.06145661696791649, 0.07856637984514236, 0.1711234301328659, 0.26842793822288513, 0.0845152959227562]
Current set of semantic relation types m MindNet These relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al (1996).	[113, 81, 89, 64, 166, 102, 93, 111, 114, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4278039336204529, 0.12006125599145889, 0.17180556058883667, 0.21950823068618774, 0.11625923961400986, 0.06108221411705017, 0.06980916857719421, 0.09968505054712296, 0.21563367545604706, 0.1790894865989685]
Currently there are about a dozen input/output conversion filters available, covering various existing data formats including the TigerXML format, the for mats of the Penn Treebank (Marcus et al, 1994), the CoNLL-X shared task format (Buchholz and Marsi, 2006), and the formats of the Latin Dependency (Bamman and Crane, 2006), Sinica (Chu Ren et al, 2000), Slovene Dependency (D? zero ski et al, 2006) (SDT), and Alpino (van der Beek et al., 2002) tree banks.	[86, 0, 2, 69, 19, 66, 20, 91, 47, 111]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7761281728744507, 0.40924888849258423, 0.29568010568618774, 0.33126160502433777, 0.2635105550289154, 0.1751353144645691, 0.3267843425273895, 0.28826943039894104, 0.16837851703166962, 0.3340088427066803]
Currently, FLO supports the LinGOrealiser (Carrollet al, 1999), but we are also looking at FLO modules for RealPro (Lavoie and Rambow, 1997) and FUF/SURGE (Elhadad et al, 1997).	[60, 8, 6, 9, 16, 48, 7, 3, 36, 24]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7383605241775513, 0.704522967338562, 0.5896492004394531, 0.06786210834980011, 0.10959792137145996, 0.07669530063867569, 0.06385798752307892, 0.06811590492725372, 0.32902175188064575, 0.11504272371530533]
"Defining ""entailment"" is quite difficult when dealing with expert annotators and still more with non-experts, as was noted by Negri et al. (2011)."	[4, 25, 46, 18, 24, 27, 2, 126, 118, 171]	[1, 1, 1, 0, 0, 0, 0, 1, 0, 0]	[0.5641978979110718, 0.5790992975234985, 0.612963080406189, 0.401592880487442, 0.19208618998527527, 0.20272387564182281, 0.26253488659858704, 0.5246530175209045, 0.17390018701553345, 0.16631345450878143]
Details of the mention detection and coreference system can be found in (Florian et al, 2004).	[160, 33, 73, 3, 157, 156, 68, 52, 82, 139]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12520062923431396, 0.295661598443985, 0.04721113294363022, 0.10101483762264252, 0.34211501479148865, 0.2548741400241852, 0.40581053495407104, 0.05407625436782837, 0.1463344395160675, 0.27991199493408203]
Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004).	[16, 7, 14, 29, 5, 50, 1, 24, 97, 77]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7524571418762207, 0.04671461507678032, 0.21291860938072205, 0.057079464197158813, 0.08070533722639084, 0.11297362297773361, 0.07331521809101105, 0.06834620237350464, 0.06299399584531784, 0.049216125160455704]
Developing a better TM is a fundamental issue for those applica tions. Researchers at IBM first described such a statistical TM in (Brown et al, 1988).	[4, 108, 187, 0, 130, 35, 48, 8, 201, 202]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24716803431510925, 0.16470147669315338, 0.11460655927658081, 0.15697777271270752, 0.055680081248283386, 0.1507435142993927, 0.041877444833517075, 0.044434092938899994, 0.10976992547512054, 0.09781938046216965]
Dinu and Lapata (2010) used Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model templates' latent senses, determining rule applicability based on the similarity between the two sides of the rule when instantiated by the context, while Ritter et al (2010) used LDA to model argument classes, considering a rule valid for a given argument instantiation if its instantiated templates are drawn from the same hidden topic.	[15, 0, 157, 136, 40, 86, 74, 200, 66, 61]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6461063623428345, 0.7291420698165894, 0.10841482132673264, 0.11499016731977463, 0.09818095713853836, 0.13840782642364502, 0.13438570499420166, 0.05508209764957428, 0.16605731844902039, 0.1037139743566513]
Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)).	[11, 152, 165, 12, 23, 66, 5, 99, 22, 138]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39636385440826416, 0.3430643081665039, 0.07744428515434265, 0.11368178576231003, 0.11839994788169861, 0.23493005335330963, 0.08869549632072449, 0.05539112165570259, 0.12330709397792816, 0.06803518533706665]
Distant supervision is provided by the following constraint: every relation instance r (e 1, e 2) K must be expressed by at least one sentence in S (e 1, e 2), the set of sentences that mention both e 1 and e 2 (Hoffmann et al, 2011).	[89, 57, 30, 41, 76, 13, 11, 14, 60, 50]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4328839182853699, 0.4097267687320709, 0.14448986947536469, 0.10225298255681992, 0.12108197808265686, 0.14630253612995148, 0.05448776111006737, 0.08115917444229126, 0.059651199728250504, 0.2229401022195816]
Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts.	[2, 0, 9, 40, 36, 1, 96, 38, 32, 60]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48853299021720886, 0.8210793137550354, 0.07934997230768204, 0.18904884159564972, 0.06357365846633911, 0.4233736991882324, 0.33481431007385254, 0.11066706478595734, 0.14756274223327637, 0.09511712193489075]
Domain: Following (Wilson et al, 2009), we apply a feature indicating the domain of the document to which a sentence belongs.	[261, 80, 269, 43, 427, 209, 277, 33, 127, 83]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 1]	[0.7910214066505432, 0.4922596216201782, 0.11679703742265701, 0.09044487029314041, 0.10321246832609177, 0.255136102437973, 0.6122578978538513, 0.28411799669265747, 0.04881443455815315, 0.5130115151405334]
Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview.	[118, 40, 67, 53, 74, 69, 39, 87, 27, 1]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.639366865158081, 0.2575697600841522, 0.1749548614025116, 0.06855540722608566, 0.1637210100889206, 0.07538069039583206, 0.4901106357574463, 0.07161717861890793, 0.352464884519577, 0.25304922461509705]
Dunning (1993) gives the formula for the statistic we are calling G2 in a form that is very compact, but not necessarily the most illuminating.	[117, 116, 149, 104, 136, 27, 33, 137, 122, 57]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6013663411140442, 0.284462034702301, 0.052954234182834625, 0.20108987390995026, 0.055211830884218216, 0.05821297690272331, 0.05530523136258125, 0.14906471967697144, 0.08815009891986847, 0.27475211024284363]
Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation.	[85, 120, 109, 17, 102, 4, 32, 24, 119, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4878235459327698, 0.32743266224861145, 0.2046741396188736, 0.2541671097278595, 0.2958172857761383, 0.29651185870170593, 0.26909124851226807, 0.23677867650985718, 0.1350763440132141, 0.06892912089824677]
EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al, 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm.	[17, 0, 1, 19, 11, 82, 26, 33, 34, 15]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7941867113113403, 0.5738368630409241, 0.5233256220817566, 0.231820210814476, 0.22086326777935028, 0.20785552263259888, 0.09491286426782608, 0.37309738993644714, 0.1802356243133545, 0.44051393866539]
Each channel was manually transcribed and timed, then annotated with dialogue act and adjacency pair information (Shriberg et al, 2004).	[12, 46, 21, 3, 38, 1, 9, 0, 22, 69]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6778678894042969, 0.46490025520324707, 0.15414950251579285, 0.16080442070960999, 0.2591032087802887, 0.1602744162082672, 0.24797117710113525, 0.2366182655096054, 0.07905491441488266, 0.1584588885307312]
Each dialogue is divided into short, clearly defined dialogue acts Initiations I and Acknowledgments A based on the top of the hierarchy given in Carletta et al (1997).	[261, 96, 47, 249, 62, 13, 119, 142, 53, 293]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22620715200901031, 0.2653694748878479, 0.27057504653930664, 0.12040319293737411, 0.271639347076416, 0.04883000999689102, 0.08322277665138245, 0.06388256698846817, 0.06536129862070084, 0.07777723670005798]
Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders.	[57, 0, 66, 45, 124, 63, 56, 135, 4, 65]	[1, 0, 0, 0, 0, 0, 1, 1, 0, 0]	[0.7490671277046204, 0.25383368134498596, 0.08616197109222412, 0.0955507680773735, 0.0955507680773735, 0.14482654631137848, 0.6861207485198975, 0.6861207485198975, 0.30408522486686707, 0.07273662835359573]
Each instance represents w i, the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al (1999) and Florian et al (2004), as described below.	[10, 80, 81, 89, 163, 72, 109, 71, 139, 53]	[0, 1, 0, 1, 0, 0, 0, 0, 1, 0]	[0.44637739658355713, 0.5371118783950806, 0.06648155301809311, 0.5005059838294983, 0.06692420691251755, 0.08823021501302719, 0.057137686759233475, 0.22566092014312744, 0.5937024354934692, 0.08111438155174255]
Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009).	[51, 164, 127, 53, 150, 126, 107, 125, 138, 154]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7511125206947327, 0.4482615888118744, 0.12283146381378174, 0.18490809202194214, 0.1157311201095581, 0.1639053225517273, 0.1322820633649826, 0.11659200489521027, 0.14479954540729523, 0.07168462872505188]
Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model.	[37, 106, 35, 25, 54, 33, 60, 112, 80, 118]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43718987703323364, 0.1854386329650879, 0.1339614987373352, 0.07942011207342148, 0.09603065997362137, 0.2601635754108429, 0.12632052600383759, 0.059839051216840744, 0.07830923795700073, 0.31792038679122925]
Earlier examples of such approaches include lexc (Karttunen et al, 1992), FASTR (Jacquemin,2001), HABIL (Alegria et al, 2004), and Mul ti flex discussed below.	[2, 60, 7, 94, 13, 38, 28, 101, 34, 44]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40558651089668274, 0.5118950009346008, 0.07321768999099731, 0.05112648755311966, 0.05511504039168358, 0.06219334155321121, 0.051772087812423706, 0.12940636277198792, 0.05097611993551254, 0.04635673016309738]
Edit disfluency detection systems that rely exclusively on word-based information have been presented by Heeman et al (Heeman et al, 1996) and Charniak and Johnson (Charniak and Johnson, 2001).	[0, 186, 10, 35, 164, 133, 30, 19, 1, 94]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48904964327812195, 0.10951454937458038, 0.1492336541414261, 0.14146865904331207, 0.07552730292081833, 0.06515465676784515, 0.05764357000589371, 0.0529392845928669, 0.0532819889485836, 0.2837728261947632]
Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995).	[21, 22, 140, 0, 100, 330, 413, 364, 12, 17]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 1]	[0.7306158542633057, 0.07677005976438522, 0.09074486792087555, 0.6800787448883057, 0.07057055830955505, 0.0696905329823494, 0.33828961849212646, 0.18888837099075317, 0.1715194284915924, 0.6665749549865723]
Eisner (1996) algorithm with non-projective rewriting and second order features.	[167, 38, 24, 64, 23, 160, 202, 97, 32, 119]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2300644963979721, 0.24888287484645844, 0.35083723068237305, 0.14576229453086853, 0.1375688910484314, 0.05932185798883438, 0.04429898411035538, 0.2361486405134201, 0.09369353204965591, 0.06159582361578941]
Eisner (2002) uses closed semirings that are also equipped with a Kleene closure operator.	[28, 181, 182, 91, 196, 33, 198, 29, 37, 180]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3051513135433197, 0.2864115536212921, 0.10769299417734146, 0.05158906802535057, 0.10165557265281677, 0.04799337312579155, 0.06758245080709457, 0.08514756709337234, 0.04982328787446022, 0.06516189873218536]
Emotional connotation works exactly in the same way, but in this case word-emotion associations are taken from (Mohammad and Turney, 2010).	[72, 73, 70, 30, 32, 26, 33, 5, 145, 40]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5244374871253967, 0.6398909687995911, 0.3951924443244934, 0.14101560413837433, 0.16277284920215607, 0.16878262162208557, 0.07582176476716995, 0.07108898460865021, 0.2170962393283844, 0.17561893165111542]
Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting.	[127, 85, 120, 37, 183, 89, 155, 130, 4, 47]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5663229823112488, 0.3651844561100006, 0.24351148307323456, 0.34442782402038574, 0.4314332604408264, 0.43390926718711853, 0.12123120576143265, 0.0716366320848465, 0.05416342616081238, 0.22241441905498505]
English to French Lexical-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Dorr, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998).	[96, 129, 41, 3, 7, 308, 60, 307, 56, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.522869348526001, 0.14227481186389923, 0.07740335166454315, 0.18405219912528992, 0.1636343002319336, 0.07933579385280609, 0.05003950744867325, 0.05662446469068527, 0.09642987698316574, 0.049769580364227295]
Esuli and Sebastiani (2006) used the method to cover objective (N) cases.	[65, 4, 87, 66, 136, 106, 50, 56, 86, 55]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09164761006832123, 0.10105036944150925, 0.3287397623062134, 0.06383832544088364, 0.12344255298376083, 0.09725741297006607, 0.1435103863477707, 0.22060246765613556, 0.04994110018014908, 0.19624318182468414]
Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English.	[173, 155, 148, 24, 128, 159, 157, 129, 167, 145]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.3519971966743469, 0.3036881983280182, 0.09608516842126846, 0.1415516585111618, 0.5264402031898499, 0.12301573157310486, 0.08259832859039307, 0.13250628113746643, 0.10870502144098282, 0.12826208770275116]
Even with a more feasible method, pairwise (Kreel, 1998), which is employed in (Kudo and Matsumoto, 2000), we can not train a classifier in a reasonable time, because we have a large number of samples that belong to the non-entity class in this formulation.	[6, 40, 47, 48, 32, 3, 45, 33, 51, 58]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5872554183006287, 0.6412474513053894, 0.06828417629003525, 0.15410898625850677, 0.13464629650115967, 0.20963022112846375, 0.056382715702056885, 0.08742521703243256, 0.1399495005607605, 0.057288553565740585]
Ever since Morris and Hirst (1991)'s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics, with applications in summarization (Barzilay and Elhadad, 1997), information retrieval (Salton and Allan, 1994), and text understanding (Kozima, 1993).	[64, 17, 9, 66, 1, 13, 7, 62, 36, 68]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3444896936416626, 0.13270695507526398, 0.2651713788509369, 0.45456579327583313, 0.2947177290916443, 0.19436998665332794, 0.1791694015264511, 0.22366070747375488, 0.06574688106775284, 0.15824095904827118]
Every English word has either 0 or 1 alignments (Melamed, 1997).	[125, 129, 140, 74, 45, 149, 88, 63, 71, 73]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.44616106152534485, 0.42085617780685425, 0.5100052356719971, 0.05463051795959473, 0.04850231483578682, 0.15406294167041779, 0.3840909004211426, 0.06507564336061478, 0.06802064180374146, 0.04547424241900444]
"Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as ""arc factorization"" for nonprojective dependency parsing (McDonald and Satta, 2007)."	[35, 39, 3, 1, 216, 165, 48, 164, 215, 65]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5142349600791931, 0.789513885974884, 0.5044652223587036, 0.2639567255973816, 0.43412888050079346, 0.2739885449409485, 0.1400231122970581, 0.12029371410608292, 0.2952207624912262, 0.12964025139808655]
Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003).	[29, 212, 27, 138, 30, 33, 97, 5, 31, 189]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7663509249687195, 0.3519892692565918, 0.39807188510894775, 0.40392595529556274, 0.07986445724964142, 0.3432373106479645, 0.14406168460845947, 0.19390535354614258, 0.2844257950782776, 0.23987367749214172]
Except for the addition of a tag parameter p to the SHIFT transition, this is equivalent to the system described in Nivre (2009), which thanks to the SWAP transition can handle arbitrary non-projective trees.	[120, 114, 1, 117, 64, 115, 21, 62, 72, 69]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7156828045845032, 0.7538962960243225, 0.5095834136009216, 0.20808997750282288, 0.3275974690914154, 0.37056422233581543, 0.16322889924049377, 0.18645422160625458, 0.09314801543951035, 0.05264715477824211]
Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006)).	[17, 12, 16, 30, 66, 4, 5, 18, 136, 14]	[1, 1, 0, 0, 1, 0, 1, 0, 0, 0]	[0.6876124739646912, 0.6742817163467407, 0.21725597977638245, 0.35382944345474243, 0.569328784942627, 0.3790581524372101, 0.5831041932106018, 0.12741883099079132, 0.18368837237358093, 0.3736787438392639]
Feinstein and Cicchetti (1990), followed by Di Eugenio and Glass (2004) proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement.	[28, 5, 81, 27, 10, 0, 87, 98, 122, 12]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7370795011520386, 0.3564480245113373, 0.40317007899284363, 0.3301001191139221, 0.11686147004365921, 0.5271276235580444, 0.33303067088127136, 0.09349443018436432, 0.42226600646972656, 0.20724783837795258]
Finally, our work is similar to that of Cherry and Lin (2003) in our use of the conditional probability of a link given the co-occurrence of the linked words.	[41, 173, 172, 163, 57, 175, 21, 109, 185, 17]	[1, 1, 1, 0, 0, 0, 0, 1, 0, 0]	[0.6338065266609192, 0.7416672110557556, 0.5963057279586792, 0.450857937335968, 0.40578585863113403, 0.30460497736930847, 0.19241079688072205, 0.5320234298706055, 0.12100793421268463, 0.056508954614400864]
"Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from ""meme"" tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005)."	[47, 0, 36, 106, 457, 237, 44, 77, 35, 170]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21226882934570312, 0.5863355398178101, 0.11145168542861938, 0.06562855839729309, 0.3962082266807556, 0.12830932438373566, 0.04918159544467926, 0.06703165173530579, 0.04408816248178482, 0.049410272389650345]
Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence.	[20, 19, 17, 18, 45, 62, 31, 64, 26, 34]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6249435544013977, 0.30823594331741333, 0.5150262117385864, 0.3403484523296356, 0.43304499983787537, 0.30047130584716797, 0.3229818642139435, 0.10148601979017258, 0.33995386958122253, 0.1564791053533554]
Finally, the rule composing method (Galley et al, 2006) is used to compose two or more minimal GHKM or SPMT rules having shared states to form larger rules.	[53, 154, 205, 84, 95, 4, 177, 7, 164, 31]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5783058404922485, 0.24222294986248016, 0.39790600538253784, 0.12330617755651474, 0.20062614977359772, 0.14274416863918304, 0.19672352075576782, 0.2542932629585266, 0.22481223940849304, 0.2222769558429718]
Finally, the scripts determine whether CBn is the same as CPn, known as the principle of cheapness (Strube and Hahn, 1999).	[107, 216, 18, 39, 77, 416, 49, 31, 437, 375]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.10396148264408112, 0.5500553250312805, 0.3552037477493286, 0.07843246310949326, 0.11517839133739471, 0.09701624512672424, 0.06496895849704742, 0.16644766926765442, 0.10728450119495392, 0.592726469039917]
Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates.	[106, 143, 26, 12, 6, 100, 22, 32, 15, 37]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.729432225227356, 0.5106785893440247, 0.5066028237342834, 0.30833643674850464, 0.3819766044616699, 0.2554866373538971, 0.25071850419044495, 0.0867890790104866, 0.07511305809020996, 0.1403415948152542]
Finally, we are developing an interface to a new large-vocabulary version of the Gemini parser (Dowding et al, 1993) which will allow us to use semantic parse information as features in the individual sub-class classifiers, and also to extract entity and event representations from the classified utterances for automatic addition of entries to calendars and to-do lists.	[150, 19, 96, 36, 126, 25, 10, 15, 12, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22400954365730286, 0.15737295150756836, 0.20000167191028595, 0.2839682102203369, 0.22473867237567902, 0.07439795881509781, 0.15071426331996918, 0.04798833653330803, 0.15220008790493011, 0.04943676292896271]
Finally, when evaluated on the datasets of the recent ACL 07 MT workshop (Callison-Burch et al, 2007).	[6, 107, 9, 84, 60, 105, 11, 8, 61, 10]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.42437243461608887, 0.7898001074790955, 0.08660727739334106, 0.1380475014448166, 0.046896062791347504, 0.1565553992986679, 0.27461913228034973, 0.05094410479068756, 0.23090818524360657, 0.5506049990653992]
Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.	[91, 6, 31, 67, 112, 94, 26, 37, 101, 34]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5405914187431335, 0.5003408789634705, 0.24362657964229584, 0.22914224863052368, 0.09739644080400467, 0.4177340269088745, 0.2627924680709839, 0.058111924678087234, 0.1432379186153412, 0.19006209075450897]
First, a quasi logical form allows the under-specification of several types of information, such as anaphoric references, ellipsis and semantic relations (Alshawi and Crouch, 1992).	[2, 8, 62, 37, 128, 6, 27, 119, 137, 1]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.18041743338108063, 0.22676968574523926, 0.21440549194812775, 0.703303337097168, 0.11013663560152054, 0.13853709399700165, 0.09368577599525452, 0.05489323288202286, 0.17014969885349274, 0.13123595714569092]
First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b).	[124, 15, 97, 85, 52, 116, 2, 126, 1, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39547884464263916, 0.09055207669734955, 0.4247957468032837, 0.057778168469667435, 0.16965970396995544, 0.28243300318717957, 0.22781625390052795, 0.06997425109148026, 0.16142526268959045, 0.4260071814060211]
First, statistically unreliable translation pairs (Johnson et al2007) are filtered out.	[4, 130, 32, 1, 29, 12, 11, 17, 50, 218]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21044668555259705, 0.4311387538909912, 0.05388319492340088, 0.07932031154632568, 0.08256236463785172, 0.39948347210884094, 0.0817493349313736, 0.18677103519439697, 0.06177123636007309, 0.04906003922224045]
First, these parsers are among the best in the literature, with a test performance of 90.7 F1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)).	[166, 10, 5, 137, 146, 141, 142, 134, 67, 30]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7593708634376526, 0.5484569668769836, 0.5063302516937256, 0.07119031995534897, 0.11777527630329132, 0.1194734051823616, 0.27133291959762573, 0.0863170251250267, 0.06643471866846085, 0.05677713826298714]
First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al, 2007).	[102, 201, 2, 188, 12, 21, 78, 141, 270, 255]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.43446120619773865, 0.052916269749403, 0.4552728533744812, 0.10062416642904282, 0.07165028899908066, 0.6638751029968262, 0.06927932798862457, 0.20031845569610596, 0.06462059915065765, 0.33123520016670227]
First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies.	[29, 15, 128, 73, 42, 107, 61, 130, 113, 8]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29171326756477356, 0.5006119012832642, 0.13104163110256195, 0.07634478807449341, 0.26639312505722046, 0.0796782448887825, 0.07004158198833466, 0.05941099300980568, 0.07480797171592712, 0.05756330117583275]
Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005).	[98, 14, 0, 32, 51, 55, 22, 146, 145, 224]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.15777698159217834, 0.373824805021286, 0.7248377203941345, 0.195700004696846, 0.14984996616840363, 0.15432241559028625, 0.15917523205280304, 0.06990870833396912, 0.05976501479744911, 0.09473755210638046]
Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.	[101, 102, 164, 100, 88, 174, 165, 167, 118, 90]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40735822916030884, 0.2548081874847412, 0.4610392451286316, 0.31907233595848083, 0.2484630048274994, 0.0631636530160904, 0.32225126028060913, 0.16697958111763, 0.08099965751171112, 0.4661177396774292]
Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as.	[119, 91, 32, 33, 23, 17, 110, 137, 88, 24]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10902981460094452, 0.2035232037305832, 0.20806990563869476, 0.06441962718963623, 0.12428059428930283, 0.1334313005208969, 0.08119756728410721, 0.06525484472513199, 0.07384717464447021, 0.09955024719238281]
Following (Punyakanok et al, 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem.	[10, 5, 163, 6, 161, 67, 156, 75, 109, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3107956349849701, 0.15111984312534332, 0.05199846252799034, 0.05922563374042511, 0.06817179918289185, 0.06455086916685104, 0.35571110248565674, 0.043004196137189865, 0.051425836980342865, 0.0772136002779007]
Following (Yao et al, 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al, 2005) and dependency parsing (Nivre et al, 2004).	[15, 62, 10, 1, 61, 128, 116, 66, 6, 65]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.046504367142915726, 0.08514983206987381, 0.3226454555988312, 0.24167926609516144, 0.06351521611213684, 0.07854979485273361, 0.057644400745630264, 0.18152977526187897, 0.16836753487586975, 0.17914481461048126]
Following Callison-Burch (2009), we treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators.	[52, 113, 53, 4, 16, 54, 48, 50, 57, 58]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5616602301597595, 0.6786488890647888, 0.17689937353134155, 0.10133205354213715, 0.13524997234344482, 0.2775753438472748, 0.08016272634267807, 0.26631224155426025, 0.08232258260250092, 0.07067887485027313]
Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase.	[49, 99, 7, 39, 9, 56, 104, 40, 116, 89]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20522312819957733, 0.3738628029823303, 0.11511534452438354, 0.08518192172050476, 0.09747625142335892, 0.12626367807388306, 0.21062509715557098, 0.1653018295764923, 0.05335672199726105, 0.11180063337087631]
Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another.	[45, 95, 50, 171, 9, 32, 119, 115, 143, 34]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7750832438468933, 0.501141369342804, 0.33460327982902527, 0.060453806072473526, 0.22820572555065155, 0.44623953104019165, 0.18508075177669525, 0.16184595227241516, 0.1537126749753952, 0.42997443675994873]
Following Kudo et al (2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab2, to our POS tagging task.	[1, 8, 24, 46, 0, 76, 38, 168, 34, 15]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15455712378025055, 0.08774184435606003, 0.06050274521112442, 0.1525486409664154, 0.09663689881563187, 0.17237713932991028, 0.08241773396730423, 0.04709118232131004, 0.059277404099702835, 0.05941713601350784]
Following Lee et al.(2010) we used only the training sections for each language.	[164, 176, 72, 111, 202, 199, 64, 51, 169, 80]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5569370985031128, 0.09617684036493301, 0.06199542433023453, 0.13474981486797333, 0.11871560662984848, 0.13945209980010986, 0.09529421478509903, 0.06206433102488518, 0.06115080788731575, 0.0705895721912384]
Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.	[183, 197, 184, 200, 222, 92, 158, 176, 218, 182]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 1]	[0.7931594252586365, 0.6999176740646362, 0.7375484108924866, 0.614326536655426, 0.3334285020828247, 0.05337918549776077, 0.2511843740940094, 0.18439163267612457, 0.46923160552978516, 0.6803067326545715]
Following models were applied: n-gram posteriors (Zens and Ney, 2006b), sentence length model, a 6-gram LM and single word lexicon models in both normal and inverse direction.	[43, 44, 81, 38, 114, 89, 56, 113, 152, 133]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7700750231742859, 0.5476418733596802, 0.26006704568862915, 0.3036198019981384, 0.1126367449760437, 0.21491169929504395, 0.4813743233680725, 0.07055101543664932, 0.04459788277745247, 0.25960952043533325]
Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.	[77, 112, 137, 29, 107, 75, 186, 33, 69, 59]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7616496086120605, 0.27107957005500793, 0.21081729233264923, 0.13077609241008759, 0.12816676497459412, 0.2891695201396942, 0.13077609241008759, 0.05767069756984711, 0.12504327297210693, 0.047238562256097794]
Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization.	[167, 168, 172, 224, 223, 2, 166, 189, 162, 131]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.8045818209648132, 0.787889838218689, 0.7885927557945251, 0.44540461897850037, 0.6022673845291138, 0.12251301109790802, 0.35517677664756775, 0.20738478004932404, 0.08443117141723633, 0.11971080303192139]
For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009).	[48, 6, 85, 35, 78, 60, 23, 27, 165, 115]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.12013846635818481, 0.1792270690202713, 0.23110100626945496, 0.5380301475524902, 0.20321884751319885, 0.10166867822408676, 0.16971024870872498, 0.20022231340408325, 0.05425567552447319, 0.0493939146399498]
For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al,1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al, 2002) respectively.	[3, 12, 9, 1, 7, 0, 4, 14, 10, 15]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6807954907417297, 0.10706448554992676, 0.14451982080936432, 0.06552492082118988, 0.23012182116508484, 0.2269241064786911, 0.12835025787353516, 0.05881344527006149, 0.16320647299289703, 0.1442347913980484]
For English, the average human performance on pp-attachment for the (v, n1, p, n2) problem formulation is just 88.2% when given only the four head-words, but increases to 93.2% when given the full sentence (Ratnaparkhi et al, 1994).	[100, 108, 107, 105, 81, 30, 89, 80, 27, 39]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 1]	[0.7693339586257935, 0.7524798512458801, 0.5678765177726746, 0.5852391123771667, 0.14552322030067444, 0.28525617718696594, 0.06817900389432907, 0.058173149824142456, 0.1930530071258545, 0.5891609787940979]
For English, we use the default tagger setting from Finkel et al (2005).	[39, 61, 43, 41, 30, 79, 73, 74, 10, 64]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4396629333496094, 0.05869552865624428, 0.08879739046096802, 0.052250321954488754, 0.07366620004177094, 0.09469566494226456, 0.1514039784669876, 0.04532656446099281, 0.230647012591362, 0.16887450218200684]
For MST Parser, we use 1st order features and a projective decoder (Eisner, 1996).	[19, 8, 64, 230, 34, 53, 32, 86, 1, 233]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06715106219053268, 0.04909104108810425, 0.17088472843170166, 0.0685836672782898, 0.06644682586193085, 0.053507380187511444, 0.05814389884471893, 0.06382143497467041, 0.25025320053100586, 0.23649072647094727]
For Proper HeadWordMatch mentioned in (Lee et al, 2011), the Pronoun distance which indicates sentence distance limit between a pronoun and its antecedent.	[63, 37, 11, 26, 58, 57, 46, 71, 82, 75]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7946851849555969, 0.14244329929351807, 0.07028130441904068, 0.06194775551557541, 0.28887128829956055, 0.04963133484125137, 0.0664171576499939, 0.055290136486291885, 0.05365779250860214, 0.05920637398958206]
For annotating speech repairs, we have extended the scheme proposed by Bear et al (1992) so that it better deals with overlapping and ambiguous repairs.	[1, 4, 112, 158, 10, 31, 13, 182, 68, 177]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28804585337638855, 0.28804585337638855, 0.04836491495370865, 0.4535193145275116, 0.4886857867240906, 0.3963387608528137, 0.3865530490875244, 0.23589421808719635, 0.25855764746665955, 0.24344991147518158]
For better comparison with previous work we implemented three model extensions, borrowed from Headden III et al (2009).	[39, 4, 90, 150, 15, 8, 12, 193, 189, 103]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6834834814071655, 0.42565464973449707, 0.3716551661491394, 0.08684171736240387, 0.0497581847012043, 0.5218093991279602, 0.4751722514629364, 0.08261473476886749, 0.28639084100723267, 0.15608663856983185]
For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999).	[40, 45, 130, 18, 0, 42, 20, 13, 106, 29]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.35044607520103455, 0.2803500294685364, 0.1883196234703064, 0.29787296056747437, 0.6279588937759399, 0.06395638734102249, 0.16051515936851501, 0.19484375417232513, 0.3532159626483917, 0.11069868505001068]
For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7).	[193, 7, 258, 6, 1, 175, 261, 273, 149, 269]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7557247281074524, 0.7961261868476868, 0.15987013280391693, 0.10278691351413727, 0.2663569152355194, 0.30542632937431335, 0.3797186613082886, 0.07860450446605682, 0.3779999315738678, 0.2217157930135727]
For convenience, we identify part-whole relations in Rule 12 based on the output produced by ReVerb (Fader et al 2011), an open information extraction system.	[0, 12, 1, 14, 2, 212, 6, 8, 210, 36]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8039380311965942, 0.735280454158783, 0.2602887749671936, 0.218622624874115, 0.22140111029148102, 0.2570609450340271, 0.08063694834709167, 0.4412424862384796, 0.07212527096271515, 0.04622701182961464]
For coreference resolution, MUC (Vilain et al 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation.	[5, 9, 17, 0, 99, 25, 86, 97, 93, 29]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06551401317119598, 0.10759986937046051, 0.07278463244438171, 0.28249391913414, 0.27090153098106384, 0.06188688054680824, 0.10413489490747452, 0.07874660193920135, 0.19410784542560577, 0.046954717487096786]
For coreference resolution, we report the performance in terms of recall, precision, and F1-measure using the commonly-used model theoretic MUC scoring program (Vilain et al, 1995).	[80, 59, 2, 26, 0, 102, 29, 5, 97, 1]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7793674468994141, 0.6602379679679871, 0.2858736515045166, 0.08857685327529907, 0.32156750559806824, 0.5023637413978577, 0.22793155908584595, 0.05247253179550171, 0.11631210893392563, 0.12179727107286453]
For detailed info. of the corpora and these scores, refer to (Emerson, 2005).	[98, 24, 13, 17, 123, 86, 127, 12, 16, 31]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5991495251655579, 0.618607223033905, 0.2676891088485718, 0.29279303550720215, 0.09214618057012558, 0.31260284781455994, 0.2731649577617645, 0.30018121004104614, 0.19778750836849213, 0.26765039563179016]
For dialogue initiative annotation, we used the well-known utterance-based rules for allocation of control from (Walker and Whittaker, 1990).	[54, 46, 27, 15, 2, 19, 28, 8, 55, 195]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6677440404891968, 0.6998224258422852, 0.6309141516685486, 0.04945759475231171, 0.2538883686065674, 0.08831654489040375, 0.1334950029850006, 0.21568749845027924, 0.07590268552303314, 0.11298947781324387]
For example, Brill and Moore (2000) developed a generative model including contextual substitution rules; and Toutanova and Moore (2002) further improved the model by adding pronunciation factors into the model.	[36, 113, 0, 33, 114, 21, 3, 121, 168, 42]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6967654228210449, 0.43338990211486816, 0.5521355867385864, 0.15951086580753326, 0.23708398640155792, 0.1467636376619339, 0.06396815925836563, 0.05841906741261482, 0.3038782477378845, 0.08620228618383408]
For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees.	[58, 74, 134, 66, 55, 60, 38, 33, 73, 68]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5870639085769653, 0.15060538053512573, 0.16747158765792847, 0.2593212127685547, 0.07798095047473907, 0.39892926812171936, 0.13382723927497864, 0.05608806386590004, 0.060610082000494, 0.20296713709831238]
For example, Riloff and Shepherd (Riloff and Shepherd, 1997) developed a statistical co-occurrence model for semantic lexicon induction that was designed with these structures in mind.	[187, 180, 12, 34, 22, 28, 38, 40, 0, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4537007808685303, 0.1423269510269165, 0.18987956643104553, 0.12594209611415863, 0.09348021447658539, 0.11051494628190994, 0.0961960181593895, 0.1382206827402115, 0.48136162757873535, 0.061370592564344406]
For example, Sudo et al (2003) used patterns consisting of a path from a verb to any of its descendents (direct or indirect) while Bunescuand Mooney (2005) suggest the shortest path between the items being related.	[10, 24, 84, 107, 130, 27, 31, 11, 109, 87]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2735074460506439, 0.16408585011959076, 0.07927881181240082, 0.13432474434375763, 0.2540300488471985, 0.06501475721597672, 0.10653383284807205, 0.1882518082857132, 0.12741145491600037, 0.05798637121915817]
For example, Table 5 shows the abstract of Resnik (1999) and 5 selected sentences that cite it in AAN.	[78, 53, 68, 77, 59, 13, 3, 20, 109, 115]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5597987771034241, 0.05731375515460968, 0.054195206612348557, 0.41982966661453247, 0.04913802072405815, 0.12319100648164749, 0.0540204755961895, 0.06033611297607422, 0.06278139352798462, 0.05747871845960617]
For example, VerbNet (derived from Levin? s [1993] work, Kipper et al, 2008) is widely used for a number of semantic processing tasks, including semantic role labeling (Swier and Stevenson, 2004), the creation of semantic parse trees (Shi and Mihalcea, 2005), and implicit argument resolution (Gerber and Chai, 2010).	[0, 28, 36, 7, 33, 5, 19, 87, 1, 6]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7553706169128418, 0.5504457354545593, 0.22410979866981506, 0.44955605268478394, 0.3689190447330475, 0.14403198659420013, 0.10220519453287125, 0.6618043184280396, 0.08290519565343857, 0.05818983167409897]
For example, Zhao and Vogel (2002), Utiyama and Isahara (2003), and Munteanu and Marcu (2005) all acquire their comparable corpora from a collection of news articles which are either downloaded from the Web or archived by LDC.	[28, 22, 114, 111, 118, 26, 36, 30, 196, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24284611642360687, 0.08698038756847382, 0.07948581874370575, 0.098520927131176, 0.27986079454421997, 0.1498899608850479, 0.05654256045818329, 0.4956289827823639, 0.08138548582792282, 0.3442554473876953]
For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004).	[17, 0, 14, 4, 1, 6, 12, 18, 11, 2]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5709187984466553, 0.7846676707267761, 0.303447961807251, 0.0483839176595211, 0.49226775765419006, 0.21689938008785248, 0.3063134551048279, 0.06237347796559334, 0.3169753849506378, 0.2720663845539093]
For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees.	[31, 66, 121, 143, 55, 116, 151, 61, 20, 214]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.31426095962524414, 0.10563906282186508, 0.11160879582166672, 0.2112656831741333, 0.11727575212717056, 0.6350377798080444, 0.12308492511510849, 0.06215125694870949, 0.1108437031507492, 0.07540041208267212]
For example, given a source phrase f and a target phrase e, the phrase pair (f, e) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word in side one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase.	[106, 105, 121, 108, 113, 119, 102, 132, 155, 147]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7677784562110901, 0.7028912901878357, 0.5479973554611206, 0.2029389888048172, 0.2337687462568283, 0.34070929884910583, 0.21602407097816467, 0.1365242451429367, 0.283984899520874, 0.2566288709640503]
For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%.	[0, 1, 52, 14, 71, 45, 16, 39, 5, 59]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7667972445487976, 0.5999147295951843, 0.41207292675971985, 0.20609250664710999, 0.20381450653076172, 0.26347604393959045, 0.31464293599128723, 0.11649031192064285, 0.20818889141082764, 0.16655784845352173]
For example, motivated by the fact that some coreference relations are harder to identify than the others (see Harabagiu et al (2001)), Ng and Cardie (2002a) present a method for mining easy positive instances, in an attempt to avoid the inclusion of hard training instances that may complicate the acquisition of an accurate coreference model.	[0, 50, 21, 72, 35, 24, 33, 60, 19, 22]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7195529341697693, 0.12198224663734436, 0.18433678150177002, 0.07849283516407013, 0.0859547108411789, 0.08053341507911682, 0.08524183928966522, 0.0511939711868763, 0.06238474324345589, 0.07615502178668976]
For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).	[174, 49, 325, 40, 276, 12, 327, 23, 128, 4]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6170524954795837, 0.32371455430984497, 0.20397727191448212, 0.10376802831888199, 0.5163878798484802, 0.17241129279136658, 0.24892614781856537, 0.11838725209236145, 0.33200550079345703, 0.07734470069408417]
For example, the best systems in RTE2 and RTE3 (Giampiccolo et al, 2007) have an accuracy 10% higher than the others but they generally use resources that are not publicly available.	[122, 126, 138, 128, 28, 129, 45, 7, 112, 109]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6148641705513, 0.3873520493507385, 0.3515103757381439, 0.40795478224754333, 0.2967526912689209, 0.4409162104129791, 0.07125420868396759, 0.05415656790137291, 0.04638873040676117, 0.23010402917861938]
For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995).	[108, 0, 1, 13, 104, 61, 115, 105, 50, 17]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5015386939048767, 0.5925652980804443, 0.19184115529060364, 0.22417157888412476, 0.22272421419620514, 0.34863877296447754, 0.4783270061016083, 0.10967005044221878, 0.109896719455719, 0.14362585544586182]
For example, we included the examination of a body part belonging to a person in the domain, and this was expressed through a Saxon genitive in English but a prepositional phrase (with the subsidiary NPs in the reverse order) in the other languages. To test our assumptions about efficiency and scalability we inferred a larger Tbox, subcategorisation frames and mappings using a pre-existing data set of verb frames for English encoded using the COMLEX subcategorisation frame inventory (Grishmanet al, 1994).	[14, 34, 8, 33, 31, 129, 37, 5, 36, 67]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24733658134937286, 0.7288529872894287, 0.15587013959884644, 0.205559641122818, 0.07573387026786804, 0.06150073558092117, 0.09488297998905182, 0.08080320805311203, 0.2799181640148163, 0.045580364763736725]
For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing.	[248, 161, 421, 21, 118, 71, 70, 66, 39, 335]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3326997756958008, 0.22528322041034698, 0.09246081113815308, 0.14211967587471008, 0.10049670934677124, 0.09173066169023514, 0.11048337817192078, 0.2679910659790039, 0.23090048134326935, 0.15089204907417297]
For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives).	[44, 0, 16, 3, 20, 18, 15, 50, 24, 6]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5225686430931091, 0.8039162755012512, 0.268011212348938, 0.07583478838205338, 0.16787569224834442, 0.2693653404712677, 0.10026492178440094, 0.12204419076442719, 0.07910748571157455, 0.08128010481595993]
For highly inflecting languages more generally, morphological analysis is often treated as a segment and-normalise problem, amenable to analysis by weighted finite state transducer (wFST), for example, Creutz and Lagus (2002) for Finnish.	[153, 33, 5, 10, 203, 202, 152, 16, 23, 63]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18700799345970154, 0.4481276571750641, 0.2691110670566559, 0.18683815002441406, 0.41752588748931885, 0.4264935553073883, 0.1951167732477188, 0.2996797561645508, 0.18338659405708313, 0.16915582120418549]
For instance, (Mihalcea et al, 2007) use an English corpus annotated for subjectivity along with parallel text to build a subjectivity classifier for Romanian.	[8, 149, 15, 152, 84, 83, 145, 148, 2, 101]	[0, 1, 1, 0, 1, 1, 0, 1, 0, 0]	[0.4077322483062744, 0.7127256393432617, 0.5618743300437927, 0.42226919531822205, 0.5559330582618713, 0.7305265069007874, 0.44421258568763733, 0.5310190916061401, 0.10380113124847412, 0.4476948082447052]
For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs.	[0, 53, 187, 184, 22, 52, 26, 9, 61, 49]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7549828886985779, 0.20409879088401794, 0.1734868437051773, 0.0842721164226532, 0.2821209728717804, 0.4414965808391571, 0.0798788070678711, 0.2805495858192444, 0.06839948147535324, 0.09906668961048126]
For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001).	[41, 80, 99, 107, 8, 20, 42, 67, 135, 109]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.623546302318573, 0.17234905064105988, 0.05174924433231354, 0.16296643018722534, 0.17372028529644012, 0.04908723011612892, 0.06779005378484726, 0.06761042028665543, 0.07941161096096039, 0.051321521401405334]
For instance, Hughes and Ramage (2007) constructed a graph which represented various types of word relations from WordNet, and compared random-walk similarity to similarity assessments from human subject trials.	[62, 31, 4, 30, 60, 58, 33, 182, 38, 178]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.293788343667984, 0.18945403397083282, 0.376742422580719, 0.29105862975120544, 0.27614885568618774, 0.07832475751638412, 0.2650958299636841, 0.06294181942939758, 0.05923828110098839, 0.14968504011631012]
For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models.	[1, 0, 25, 37, 34, 6, 27, 148, 124, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07572223246097565, 0.23903532326221466, 0.08919769525527954, 0.06411072611808777, 0.07151992619037628, 0.08011665940284729, 0.33587050437927246, 0.04814813658595085, 0.21544544398784637, 0.07280953973531723]
For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004).	[68, 63, 1, 60, 12, 24, 20, 64, 17, 62]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6986827850341797, 0.7801305651664734, 0.36748644709587097, 0.25259050726890564, 0.22167252004146576, 0.17869843542575836, 0.10297829657793045, 0.20541471242904663, 0.08806567639112473, 0.23210982978343964]
For instance, in part-of-speech tagging (Brill, 1995), when the tag of one word is changed, it changes the answers to questions for nearby words.	[225, 99, 151, 28, 51, 150, 49, 200, 246, 165]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7301570177078247, 0.6135990023612976, 0.3525207042694092, 0.4053512215614319, 0.17745491862297058, 0.4250113070011139, 0.4245801270008087, 0.1958695650100708, 0.46793973445892334, 0.5066728591918945]
For instance, many systems used the pruning strategy described in (Xue and Palmer, 2004) and other systems used the soft pruning rules described in (Pradhan et al, 2005a).	[10, 9, 14, 75, 13, 62, 73, 70, 15, 76]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3263763189315796, 0.18342605233192444, 0.07148576527833939, 0.05377814173698425, 0.04721475765109062, 0.06637728959321976, 0.051557544618844986, 0.06192148104310036, 0.07496017962694168, 0.1511198878288269]
For instance, work on word sense disambiguation i corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case.	[98, 119, 46, 14, 16, 15, 10, 96, 62, 61]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.572777509689331, 0.5258355140686035, 0.19427315890789032, 0.45922017097473145, 0.08586829900741577, 0.15069036185741425, 0.2291676104068756, 0.10987532883882523, 0.1693567931652069, 0.12836745381355286]
For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)-stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them.	[44, 11, 43, 7, 8, 1, 2, 14, 33, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3217529356479645, 0.13032487034797668, 0.07647637277841568, 0.05268944054841995, 0.25137531757354736, 0.06546115130186081, 0.050511281937360764, 0.0858137235045433, 0.12078793346881866, 0.05458839237689972]
For model estimation we use the TADM3 software (Malouf, 2002).	[83, 89, 20, 66, 110, 4, 114, 14, 24, 10]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7108398079872131, 0.1636141687631607, 0.15822608768939972, 0.13322287797927856, 0.2137787640094757, 0.05135473236441612, 0.12922069430351257, 0.04818360134959221, 0.06592509895563126, 0.08272604644298553]
For more details of the second-order parsing algorithm, see (Carreras, 2007).	[12, 51, 54, 20, 9, 27, 10, 73, 4, 111]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19875001907348633, 0.7207790017127991, 0.05361184850335121, 0.05932534858584404, 0.17247168719768524, 0.17067691683769226, 0.09847544878721237, 0.2228100597858429, 0.06420498341321945, 0.17362886667251587]
For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods.	[137, 2, 17, 33, 118, 163, 45, 28, 148, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4548151195049286, 0.25393030047416687, 0.23767635226249695, 0.2373219132423401, 0.2047193944454193, 0.2052571028470993, 0.059130389243364334, 0.11296864598989487, 0.05560503900051117, 0.04762635752558708]
For non pronominal mentions, we used the number and gender data (Bergsma and Lin, 2006) provided by the task organizers and queried it for the head word of the mention.	[131, 85, 137, 119, 84, 78, 21, 138, 136, 22]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6024056673049927, 0.5666093826293945, 0.25947046279907227, 0.39421698451042175, 0.2543540596961975, 0.055417247116565704, 0.15562941133975983, 0.061962250620126724, 0.24897626042366028, 0.07389361411333084]
For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al, 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only.	[157, 89, 152, 156, 166, 163, 162, 53, 22, 172]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.42281392216682434, 0.6114458441734314, 0.583950936794281, 0.17336410284042358, 0.09304732829332352, 0.1779356598854065, 0.0995275154709816, 0.08117114007472992, 0.08538318425416946, 0.07412099838256836]
For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others.	[158, 1, 8, 0, 60, 17, 74, 31, 144, 37]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29845601320266724, 0.052889738231897354, 0.2682929039001465, 0.1939658224582672, 0.12174411863088608, 0.0821419283747673, 0.3677763044834137, 0.05539723485708237, 0.08281887322664261, 0.05794892832636833]
For our baseline SRL model, we adopt the features used in other state-of-the-art SRL systems, which include the seven baseline features from the original work of Gildea and Jurafsky (2002) ,additional features taken from Pradhan et al (2005), and feature combinations which are inspired by the system in Xue and Palmer (2004).	[62, 81, 10, 75, 82, 2, 83, 78, 48, 72]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7758409380912781, 0.2721661925315857, 0.40729400515556335, 0.14272460341453552, 0.4607675075531006, 0.290622353553772, 0.06261911243200302, 0.290622353553772, 0.0709456279873848, 0.3751852214336395]
For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005).	[110, 119, 120, 22, 37, 38, 65, 66, 92, 101]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5561251044273376, 0.6926975846290588, 0.23645633459091187, 0.2336161881685257, 0.24482856690883636, 0.12978537380695343, 0.2953032851219177, 0.1901555061340332, 0.09273435920476913, 0.05655684322118759]
For parameter estimation, we use the open source TADM system (Malouf, 2002).	[83, 0, 25, 114, 24, 66, 4, 14, 20, 110]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7224382758140564, 0.7677410840988159, 0.18399865925312042, 0.16346092522144318, 0.06872788816690445, 0.07149749249219894, 0.050597015768289566, 0.050701797008514404, 0.05273530259728432, 0.07899688929319382]
For parsing, we mapped all unknown words to unknown word symbols, and applied the Viterbi algorithm as implemented in Schmid (2004), exploiting its ability to deal with highly-ambiguous grammars.	[0, 6, 5, 28, 27, 23, 26, 7, 25, 20]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.535108745098114, 0.31235021352767944, 0.11032517999410629, 0.08666503429412842, 0.20856687426567078, 0.07061564177274704, 0.07529465854167938, 0.06842391192913055, 0.14415612816810608, 0.38475659489631653]
For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round.	[85, 128, 91, 86, 127, 63, 95, 18, 1, 60]	[1, 1, 0, 1, 0, 0, 1, 0, 0, 1]	[0.758849024772644, 0.6413171291351318, 0.38688716292381287, 0.5929714441299438, 0.44408971071243286, 0.14074958860874176, 0.5766171813011169, 0.07744596153497696, 0.08136089146137238, 0.5233438611030579]
For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or - alternatively - Dan Melamed's GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999).	[376, 407, 32, 382, 8, 422, 388, 65, 19, 436]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7659590840339661, 0.4818437099456787, 0.28223347663879395, 0.6038360595703125, 0.22865675389766693, 0.09070833772420883, 0.4897545576095581, 0.2368733435869217, 0.22865675389766693, 0.18409544229507446]
For the HCP system, MET is done following Chiang (2007).	[254, 15, 25, 5, 10, 76, 42, 37, 110, 38]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.04156108945608139, 0.4710424244403839, 0.2924244701862335, 0.07117734849452972, 0.07117734849452972, 0.33636170625686646, 0.06011422351002693, 0.05640273541212082, 0.3414915204048157, 0.04903139919042587]
For the algorithm discussed in Section 4.1, we derived our descriptive properties using the output of the dependency analysis generated by the Minipar (Lin, 1994) dependency parser.	[15, 196, 1, 17, 19, 73, 6, 201, 160, 21]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5337387323379517, 0.3144877552986145, 0.26704370975494385, 0.14972884953022003, 0.15211200714111328, 0.0636790320277214, 0.050391875207424164, 0.11253844201564789, 0.05441780388355255, 0.13255175948143005]
For the alignment of supplemental data with candidate outputs, we apply M2M ALIGNER (Jiampojamarn et al, 2007).	[28, 37, 139, 144, 147, 51, 155, 160, 5, 33]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.370175302028656, 0.1533297896385193, 0.05306816101074219, 0.060018088668584824, 0.1400182545185089, 0.06279049813747406, 0.07274235039949417, 0.16710670292377472, 0.4852413535118103, 0.33929991722106934]
For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in relation extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.	[9, 2, 0, 74, 58, 52, 29, 39, 68, 56]	[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]	[0.796707034111023, 0.7983980178833008, 0.7851937413215637, 0.6632797718048096, 0.5986547470092773, 0.5848665833473206, 0.4705337584018707, 0.14094656705856323, 0.09623762220144272, 0.44847020506858826]
For the final system, feature models and training parameters were adapted from Hall et al (2007).	[8, 92, 5, 15, 56, 67, 3, 1, 27, 52]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7614249587059021, 0.6709287166595459, 0.04924742504954338, 0.5270664691925049, 0.13260239362716675, 0.13763932883739471, 0.07851387560367584, 0.05518990010023117, 0.07653222978115082, 0.11330008506774902]
For the part of speech tagging, the memory-based tagger MBT (Daelemans et al, 1996), trained on the Wall Street Journal corpus2, was used.	[0, 89, 1, 78, 3, 10, 12, 5, 4, 121]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7784632444381714, 0.22082006931304932, 0.6688746213912964, 0.3545094430446625, 0.25190994143486023, 0.38225987553596497, 0.12830574810504913, 0.2074950784444809, 0.0745416134595871, 0.055022355169057846]
For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000).	[1, 12, 85, 0, 4, 55, 125, 16, 151, 86]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6502756476402283, 0.5229011178016663, 0.4542650878429413, 0.5951539278030396, 0.16299723088741302, 0.07819368690252304, 0.16266337037086487, 0.055750563740730286, 0.09574642032384872, 0.18502388894557953]
For the task of subjective expression detection, Choi et al (2006) and Breck et al (2007) used syntactic features in a sequence model.	[145, 30, 41, 14, 173, 13, 74, 63, 44, 38]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2604053318500519, 0.0633724108338356, 0.06851933896541595, 0.05861980840563774, 0.2558257579803467, 0.13151706755161285, 0.05978630483150482, 0.22941376268863678, 0.10819867998361588, 0.06244112178683281]
For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work.	[206, 62, 68, 34, 136, 43, 12, 124, 137, 58]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6858649849891663, 0.14348718523979187, 0.0724591538310051, 0.052857931703329086, 0.08853326737880707, 0.05433966964483261, 0.06487811356782913, 0.05625121667981148, 0.06165452301502228, 0.06814158707857132]
For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).	[1, 7, 6, 18, 85, 0, 4, 13, 5, 41]	[1, 1, 0, 1, 0, 1, 0, 1, 0, 0]	[0.6525893211364746, 0.751571774482727, 0.3220791518688202, 0.5152435898780823, 0.07200302183628082, 0.5793835520744324, 0.36986759305000305, 0.5230789184570312, 0.11094462126493454, 0.10931524634361267]
For the word alignments, we chose MGIZA (Gaoand Vogel, 2008), using seven threads per MGIZA instance, with the parallel option ,i.e. one MGIZA in stance per pair direction running in parallel.	[0, 155, 29, 115, 37, 135, 191, 3, 61, 70]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6394446492195129, 0.5541322231292725, 0.1422865390777588, 0.22307895123958588, 0.07632065564393997, 0.23345212638378143, 0.09170209616422653, 0.12369883805513382, 0.05083427205681801, 0.09561104327440262]
For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole.	[47, 41, 33, 3, 61, 34, 16, 59, 60, 55]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5955278873443604, 0.5247281193733215, 0.1251852661371231, 0.3122623860836029, 0.2044011354446411, 0.4110575020313263, 0.3803994953632355, 0.06444517523050308, 0.061198439449071884, 0.050840482115745544]
For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system.	[6, 0, 8, 86, 90, 7, 1, 77, 18, 5]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7617853283882141, 0.6001541018486023, 0.12381580471992493, 0.20141597092151642, 0.28584912419319153, 0.317543089389801, 0.379160612821579, 0.27628201246261597, 0.0826706662774086, 0.28920844197273254]
For words that were unknown in our subtree set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization.	[133, 192, 122, 107, 184, 138, 35, 256, 146, 279]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7062167525291443, 0.686119019985199, 0.5113247632980347, 0.17124994099140167, 0.08652428537607193, 0.4084283113479614, 0.06063484027981758, 0.12664775550365448, 0.08820690959692001, 0.07211899757385254]
Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008).	[54, 158, 153, 271, 277, 242, 171, 16, 279, 197]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6424185633659363, 0.29159802198410034, 0.3803393542766571, 0.45776671171188354, 0.09723043441772461, 0.3590294122695923, 0.24543657898902893, 0.30763667821884155, 0.08831524103879929, 0.10535874217748642]
Fortunately, the state of the art in broad-coverage (Lin, 1993) and unsupervised (Klein and Manning, 2004) dependency parsing allows us to treat dependency parsing merely as a preprocessing step.	[10, 3, 11, 171, 99, 151, 21, 93, 145, 39]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6892673969268799, 0.3927658498287201, 0.42522743344306946, 0.1628764271736145, 0.256701797246933, 0.4413015842437744, 0.09724882990121841, 0.05537129193544388, 0.06435798853635788, 0.07728796452283859]
Forun aligned words, we simply assign a random POS and very low probability, which does not substantially affect transition probability estimates. In Step 6 we build a tagger by feeding the es ti mated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger.	[36, 46, 29, 1, 0, 128, 148, 101, 159, 6]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2751786410808563, 0.517920196056366, 0.22315576672554016, 0.3300095796585083, 0.4036135971546173, 0.27017760276794434, 0.27017760276794434, 0.19565896689891815, 0.44858410954475403, 0.17726217210292816]
Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes.	[132, 45, 7, 47, 93, 139, 91, 8, 9, 96]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11810198426246643, 0.05282072350382805, 0.1268680840730667, 0.06782608479261398, 0.30858567357063293, 0.15733809769153595, 0.16608572006225586, 0.07854503393173218, 0.08183305710554123, 0.1495826691389084]
Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007).	[8, 11, 21, 181, 18, 53, 40, 39, 132, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2297973781824112, 0.24633125960826874, 0.04078862816095352, 0.49290648102760315, 0.06686698645353317, 0.4563450515270233, 0.10602609068155289, 0.07213611900806427, 0.04292936995625496, 0.058308228850364685]
From the point of view of the evaluation of the algorithm, we should design an assessment scheme that would make our experimental results more directly comparable to those of Yarowsky and Wicentowski (2000), Schone and Jurafsky (2000) and others.	[124, 122, 60, 77, 102, 121, 113, 69, 112, 28]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6971436142921448, 0.6007915735244751, 0.33349549770355225, 0.06938507407903671, 0.0544377900660038, 0.07195162773132324, 0.18802179396152496, 0.06057693064212799, 0.09535562992095947, 0.08381642401218414]
Further details of the noisy channel model can be found in Johnson and Charniak (2004).	[0, 152, 1, 17, 28, 11, 154, 150, 46, 142]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7819374203681946, 0.6528508067131042, 0.25491639971733093, 0.1762784868478775, 0.4869018495082855, 0.21645289659500122, 0.19608764350414276, 0.42322322726249695, 0.1282634437084198, 0.4702581465244293]
Further linguistic markup is added using the morpha lemmatiser (Minnen et al, 2000) and the C&C named entity tagger (Curran and Clark, 2003) trained on the data from MUC-7.	[13, 29, 7, 28, 42, 156, 64, 27, 82, 92]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29762646555900574, 0.06769674271345139, 0.08136337250471115, 0.04760705307126045, 0.04360921308398247, 0.07872369885444641, 0.06212266907095909, 0.11587657034397125, 0.04630117863416672, 0.0521877147257328]
Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a sub translation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right.	[7, 1, 19, 34, 15, 121, 32, 22, 5, 13]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.739884614944458, 0.39046695828437805, 0.2886958122253418, 0.2260110229253769, 0.2186850756406784, 0.06034955754876137, 0.11266297847032547, 0.07449758052825928, 0.12487528473138809, 0.06143256649374962]
Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002).	[57, 48, 7, 0, 1, 87, 49, 47, 72, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33759844303131104, 0.3546845018863678, 0.11414341628551483, 0.11097867041826248, 0.0784064456820488, 0.13086701929569244, 0.13520151376724243, 0.12658481299877167, 0.05984131991863251, 0.051316529512405396]
Furthermore, we compared our fragment pair collection with Callison-Burch (2008)'s approach on the same MSR corpus, only about 21% of the extracted paraphrases appear on both sides, which shows the potential to combine different resources.	[94, 65, 1, 121, 2, 64, 39, 160, 77, 36]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15592005848884583, 0.14294227957725525, 0.2796342670917511, 0.04691188037395477, 0.09291230142116547, 0.11844430863857269, 0.04841742292046547, 0.05314040184020996, 0.06678880006074905, 0.09592917561531067]
G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).	[70, 69, 125, 0, 29, 15, 28, 12, 38, 119]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3213668465614319, 0.6011492609977722, 0.08122134953737259, 0.33975857496261597, 0.14694412052631378, 0.0696864053606987, 0.10515667498111725, 0.06349027156829834, 0.0625775083899498, 0.06976546347141266]
GRE has been dominated by Dale and Reiter's (1995) Incremental Algorithm (IA), one version of which, generalised to deal with non-disjunctive plural references, is shown in Algorithm 1 (van Deemter, 2002).	[267, 32, 16, 6, 57, 189, 244, 139, 77, 7]	[1, 1, 1, 0, 0, 0, 1, 0, 0, 0]	[0.6001450419425964, 0.6849521994590759, 0.5266967415809631, 0.32812148332595825, 0.2545134723186493, 0.08134448528289795, 0.6348659992218018, 0.3230319917201996, 0.11071481555700302, 0.15658023953437805]
Gale and Church (1993) based their align program on the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.	[4, 18, 90, 115, 312, 2, 16, 185, 173, 3]	[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]	[0.8045202493667603, 0.8045202493667603, 0.8024880886077881, 0.79450523853302, 0.5474122166633606, 0.5371885895729065, 0.5371885895729065, 0.5616931915283203, 0.10602611303329468, 0.2697666585445404]
Gamon et al (2008) train a decision tree model and a language model to correct errors in article and preposition usage.	[46, 51, 3, 75, 40, 168, 171, 0, 90, 76]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.443574458360672, 0.7155237197875977, 0.08486908674240112, 0.24529798328876495, 0.14926382899284363, 0.11642024666070938, 0.07318346202373505, 0.41312941908836365, 0.2609560191631317, 0.09666687995195389]
Gamon et al (2008) used word-based language models to detect and correct common ESL errors, while Leacock and Chodorow (2003) used part-of-speech bigram language models to identify potentially ungrammatical two-word sequences in ESL essays.	[44, 0, 46, 72, 55, 92, 169, 86, 3, 165]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4579969346523285, 0.5293210744857788, 0.1956578642129898, 0.059241220355033875, 0.06904026120901108, 0.05366376414895058, 0.12057509273290634, 0.07123581320047379, 0.04169538989663124, 0.05086444318294525]
Ge et al [1998] also present a statistical algorithm based on the study of statistical data in a large corpus and the application of a naive Bayes model.	[50, 6, 130, 49, 142, 2, 119, 1, 189, 194]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6885848641395569, 0.6575431823730469, 0.2480475753545761, 0.3438621163368225, 0.2407342940568924, 0.07160486280918121, 0.16211801767349243, 0.2851015627384186, 0.11504539847373962, 0.09413843601942062]
Generation with the resulting grammar can be compared best with head corner generation (Shieber et al, 1990).	[0, 325, 276, 238, 13, 268, 167, 93, 57, 23]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8232486248016357, 0.08120459318161011, 0.3775424063205719, 0.1230335682630539, 0.19568003714084625, 0.04496253281831741, 0.10004089772701263, 0.07929825782775879, 0.3902295231819153, 0.10205607116222382]
Generative Lexicon (Pustejovsky, 1991), for example have been proposed to facilitate computationally precise description of natural language syntax and semantics.	[0, 14, 41, 6, 79, 175, 16, 38, 40, 206]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7904399037361145, 0.15362784266471863, 0.059718113392591476, 0.08438447117805481, 0.0757090300321579, 0.18794646859169006, 0.30500277876853943, 0.057214993983507156, 0.19568581879138947, 0.20549023151397705]
Generative word alignment models including IBM models (Brown et al, 1993) and HMM word alignment models (Vogel et al, 1996) have been widely used in various types of Statistical Machine Translation (SMT) systems.	[19, 0, 3, 8, 111, 58, 137, 20, 4, 22]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6708933115005493, 0.7460817694664001, 0.10510937869548798, 0.06940686702728271, 0.20563648641109467, 0.0687105804681778, 0.08360130339860916, 0.2702215313911438, 0.05825050175189972, 0.10270627588033676]
Germann et al (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.	[4, 17, 118, 53, 93, 90, 92, 94, 91, 119]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5351928472518921, 0.5063944458961487, 0.15949198603630066, 0.13063491880893707, 0.2299889624118805, 0.41892749071121216, 0.08256100863218307, 0.12294058501720428, 0.07078000158071518, 0.2490430623292923]
Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles.	[104, 65, 527, 74, 577, 72, 71, 525, 106, 475]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7496657371520996, 0.5629006028175354, 0.13510705530643463, 0.2731286585330963, 0.26899757981300354, 0.09645366668701172, 0.20902644097805023, 0.7232240438461304, 0.12815503776073456, 0.11871712654829025]
Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).	[1, 14, 2, 0, 11, 130, 81, 106, 259, 67]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7817011475563049, 0.5725919008255005, 0.5772973299026489, 0.34185361862182617, 0.13454097509384155, 0.20838084816932678, 0.3302365839481354, 0.21873052418231964, 0.1898995190858841, 0.25731444358825684]
Given no constraint on maximum phrase length, the hierarchical phrase reordering model (Galley and Manning, 2008) also analyzes the adjacent bilingual phrases for bp and identifies its orientation as S.	[66, 65, 92, 58, 62, 184, 181, 63, 2, 12]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7429775595664978, 0.7570592164993286, 0.6535778045654297, 0.4374292194843292, 0.42411646246910095, 0.21854008734226227, 0.3507567346096039, 0.4076959192752838, 0.21201780438423157, 0.19167086482048035]
Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)	[9, 1, 236, 8, 12, 230, 61, 22, 186, 237]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.3567263185977936, 0.1808207780122757, 0.1580643653869629, 0.06052793562412262, 0.062033332884311676, 0.15824845433235168, 0.10368696600198746, 0.5941068530082703, 0.06814629584550858, 0.04912995547056198]
Given that state-of-the-art edit detection performs at about 80% f-measure (Johnson and Charniak, 2004), much of the benefit derived here from oracle repair detection should be realizable in practice.	[156, 14, 147, 35, 148, 17, 55, 33, 71, 5]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5191928148269653, 0.55997633934021, 0.16930219531059265, 0.11842703074216843, 0.0841599628329277, 0.10454796254634857, 0.06097448244690895, 0.053225137293338776, 0.04774557426571846, 0.055313535034656525]
Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix.	[31, 127, 28, 146, 143, 16, 49, 134, 4, 117]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23572325706481934, 0.5690792798995972, 0.2899404764175415, 0.08491161465644836, 0.07038628309965134, 0.08251924812793732, 0.08448883146047592, 0.06328961998224258, 0.0694313570857048, 0.05427517741918564]
Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm.	[0, 199, 51, 2, 23, 7, 54, 12, 32, 72]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7489246726036072, 0.6792730689048767, 0.10072863101959229, 0.30114129185676575, 0.13987164199352264, 0.11009212583303452, 0.1307227611541748, 0.05001482367515564, 0.1989336460828781, 0.08466348052024841]
Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.	[335, 328, 0, 321, 337, 315, 26, 326, 319, 327]	[0, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.4874293804168701, 0.15791241824626923, 0.7080223560333252, 0.3079521656036377, 0.35772761702537537, 0.5197770595550537, 0.15895739197731018, 0.11175710707902908, 0.1628350466489792, 0.05613373965024948]
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.	[4, 0, 144, 113, 125, 115, 7, 116, 58, 129]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 1]	[0.6040902137756348, 0.8426924347877502, 0.175950825214386, 0.6982295513153076, 0.23464564979076385, 0.41895797848701477, 0.12349147349596024, 0.21576055884361267, 0.06215066835284233, 0.5545504689216614]
Grosz et al (1995) give the following example of the Am SHIFT pattern.	[305, 86, 280, 316, 336, 296, 195, 315, 209, 251]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3514631986618042, 0.07198227941989899, 0.12580206990242004, 0.33551448583602905, 0.2357891947031021, 0.21247202157974243, 0.047899115830659866, 0.058539003133773804, 0.0476326160132885, 0.29154446721076965]
Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose.	[110, 107, 25, 42, 16, 108, 15, 4, 26, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2767043709754944, 0.2320319414138794, 0.0928027331829071, 0.05927835404872894, 0.0857623741030693, 0.05000053346157074, 0.09320588409900665, 0.053473200649023056, 0.08706404268741608, 0.05561701953411102]
H&S10 refers to the results of Huang and Sagae (2010).	[62, 94, 14, 45, 32, 26, 31, 47, 97, 153]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.062178440392017365, 0.05319760739803314, 0.08918358385562897, 0.07396120578050613, 0.0536164753139019, 0.048155419528484344, 0.052790600806474686, 0.050201766192913055, 0.12147663533687592, 0.049905601888895035]
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes.	[58, 30, 31, 1, 32, 56, 16, 2, 14, 25]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07072227448225021, 0.2221718430519104, 0.11854168772697449, 0.05546695366501808, 0.07941989600658417, 0.0923752710223198, 0.05463949590921402, 0.10585947334766388, 0.05007816478610039, 0.0466446727514267]
Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity.	[3, 208, 86, 26, 131, 83, 4, 85, 121, 126]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.8043243885040283, 0.7294718027114868, 0.7095626592636108, 0.39017826318740845, 0.2999194860458374, 0.6325349807739258, 0.09752734005451202, 0.11231313645839691, 0.25223907828330994, 0.3896423280239105]
Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.	[19, 7, 31, 200, 196, 14, 185, 20, 162, 115]	[1, 0, 1, 0, 0, 0, 0, 1, 0, 0]	[0.541004478931427, 0.22253528237342834, 0.624250590801239, 0.30756357312202454, 0.45908355712890625, 0.29996946454048157, 0.28792062401771545, 0.5641495585441589, 0.12057898938655853, 0.38483545184135437]
Here A0 represents the seller, and A1 represents the things sold (CoNLL 2008 shared task, Surdeanu et al, 2008).	[31, 182, 110, 30, 152, 161, 98, 136, 8, 135]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5759791731834412, 0.21378189325332642, 0.3464985489845276, 0.37613925337791443, 0.05455918237566948, 0.0717000812292099, 0.12504804134368896, 0.2882862389087677, 0.0914590060710907, 0.1610574722290039]
Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results.	[50, 62, 23, 18, 60, 17, 20, 73, 72, 59]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33395856618881226, 0.2592617869377136, 0.15774787962436676, 0.1465039849281311, 0.10154049843549728, 0.048942483961582184, 0.058754149824380875, 0.3678032457828522, 0.13308419287204742, 0.04946943372488022]
Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al, 2009) and adaptors grammar (Johnsonand Goldwater, 2009).	[11, 3, 0, 89, 64, 27, 26, 158, 14, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5941929817199707, 0.06588616967201233, 0.22817184031009674, 0.22133971750736237, 0.06631829589605331, 0.17592130601406097, 0.10202014446258545, 0.059239499270915985, 0.06861405074596405, 0.21211007237434387]
Hobbs and Shieber (1987) extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach.	[227, 85, 19, 11, 218, 62, 72, 5, 10, 160]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.3661482036113739, 0.12475155293941498, 0.27412763237953186, 0.5441412925720215, 0.17700856924057007, 0.1481027603149414, 0.06399045884609222, 0.10168763250112534, 0.10168763250112534, 0.05957111343741417]
Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance.	[0, 41, 33, 36, 1, 12, 2, 3, 46, 40]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.634610116481781, 0.3434954285621643, 0.21132966876029968, 0.06338812410831451, 0.2411823868751526, 0.06868119537830353, 0.10443376749753952, 0.0981125459074974, 0.08355813473463058, 0.047977786511182785]
Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation.	[186, 93, 117, 50, 0, 60, 16, 168, 1, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08986743539571762, 0.0700354054570198, 0.06990688294172287, 0.07407991588115692, 0.09912192076444626, 0.09735666215419769, 0.06645996868610382, 0.05573105439543724, 0.06209683045744896, 0.06929050385951996]
However allowing reordering in translation is computationally expensive and in some cases even provably NP-complete (Knight, 1999).	[81, 148, 5, 11, 142, 135, 63, 109, 137, 140]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.5685021877288818, 0.6361958980560303, 0.35738083720207214, 0.35738083720207214, 0.43212786316871643, 0.05020203813910484, 0.5775512456893921, 0.2601684331893921, 0.45335298776626587, 0.09784436970949173]
However, Kim and Hovy (2004) and Andreevskaiaand Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance.	[123, 120, 119, 7, 122, 42, 183, 129, 150, 143]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.639896810054779, 0.6017090678215027, 0.26917117834091187, 0.1410113126039505, 0.12528574466705322, 0.0664023607969284, 0.2117541879415512, 0.2713198959827423, 0.13493748009204865, 0.6420489549636841]
However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations.	[185, 101, 184, 18, 181, 8, 182, 96, 19, 9]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6453509330749512, 0.6387573480606079, 0.6120951175689697, 0.5764213800430298, 0.37335842847824097, 0.284473180770874, 0.14376969635486603, 0.4235481917858124, 0.3566930294036865, 0.2939658463001251]
However, as was noted by (Yarowsky and Ngai, 2001), most words tend to have at most two pos.	[52, 44, 53, 6, 43, 79, 10, 78, 81, 67]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4302264153957367, 0.06657668203115463, 0.08071932941675186, 0.14528045058250427, 0.18578201532363892, 0.04346587508916855, 0.04357783496379852, 0.048228081315755844, 0.0614672526717186, 0.12388038635253906]
However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al, 1991) and separation of senses (Dyvik, 1998).	[44, 6, 25, 110, 46, 100, 37, 58, 52, 45]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42596912384033203, 0.5935580134391785, 0.2117336541414261, 0.2898107171058655, 0.31573185324668884, 0.10476861149072647, 0.23465047776699066, 0.36889776587486267, 0.3419034481048584, 0.2466469258069992]
However, cross framework parser evaluation is a difficult problem: previous attempts to evaluate the C & C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score.	[139, 160, 43, 22, 7, 2, 153, 12, 104, 53]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6857938766479492, 0.6489905714988708, 0.43152695894241333, 0.37460389733314514, 0.6680533289909363, 0.42183852195739746, 0.3777712881565094, 0.14299865067005157, 0.11511625349521637, 0.0543045736849308]
However, developing such grammars has been made much more efficient with the emergence of the Grammar Matrix (Bender et al,2002).	[12, 8, 116, 94, 2, 17, 73, 89, 4, 69]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6026252508163452, 0.26409369707107544, 0.32129716873168945, 0.16574245691299438, 0.4265396296977997, 0.07398975640535355, 0.04356470704078674, 0.13997985422611237, 0.20168615877628326, 0.29403600096702576]
However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm.	[117, 93, 137, 145, 66, 163, 51, 30, 52, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07588989287614822, 0.09061986953020096, 0.137839674949646, 0.1433572769165039, 0.16960978507995605, 0.05391886457800865, 0.07569002360105515, 0.057639192789793015, 0.04872468486428261, 0.04810021072626114]
However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks.	[167, 14, 15, 13, 177, 175, 164, 6, 0, 156]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6137527227401733, 0.7160441279411316, 0.1749747097492218, 0.23097771406173706, 0.23320013284683228, 0.12588554620742798, 0.07844927161931992, 0.185418501496315, 0.4182676076889038, 0.04833139851689339]
However, other such modules, e.g., those from NLTK (Loper and Bird, 2002), can be used for such assignments.	[110, 112, 61, 174, 202, 84, 134, 43, 206, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7050113081932068, 0.6518166065216064, 0.07666120678186417, 0.43490204215049744, 0.3178526759147644, 0.04876423254609108, 0.26928529143333435, 0.08318977802991867, 0.3373216390609741, 0.06962834298610687]
However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al,2009b).	[23, 27, 24, 28, 152, 0, 58, 137, 31, 26]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7460446953773499, 0.3441145420074463, 0.1010410338640213, 0.12712863087654114, 0.11617784202098846, 0.3719289004802704, 0.13645541667938232, 0.1073635146021843, 0.052067600190639496, 0.05082983896136284]
However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007).	[28, 0, 19, 156, 59, 41, 12, 2, 146, 32]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.5832606554031372, 0.7673009634017944, 0.13061171770095825, 0.2919262945652008, 0.5140113830566406, 0.07975129783153534, 0.19958634674549103, 0.07717756927013397, 0.4474290907382965, 0.1018633246421814]
However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998).	[139, 158, 11, 194, 48, 45, 8, 33, 157, 210]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3638206124305725, 0.1872881054878235, 0.07331321388483047, 0.3401869833469391, 0.28437578678131104, 0.06731075793504715, 0.05979525297880173, 0.05548893287777901, 0.04948398843407631, 0.07132793962955475]
However, these two models either require post processing to calculate the positive/negative coverage in a document for polarity identification (Mei et al, 2007) or require some kind of supervised setting in which review text should contain ratings for aspects of interest (Titov and McDonald, 2008a).	[107, 7, 68, 177, 186, 2, 43, 115, 16, 18]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.09460050612688065, 0.05904482677578926, 0.53544020652771, 0.10520905256271362, 0.26383984088897705, 0.08471902459859848, 0.42631348967552185, 0.06898918747901917, 0.12928587198257446, 0.0784805417060852]
However, they are quite rare, even in monolingual contexts (Stevenson and Wilks, 2001, e.g.), and they are not able to integrate and use knowledge coming from corpus and other resources during the learning process.	[427, 123, 221, 110, 59, 242, 193, 197, 47, 367]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3853393495082855, 0.3007872402667999, 0.04981953278183937, 0.08447178453207016, 0.3185696005821228, 0.15429192781448364, 0.15255549550056458, 0.24220247566699982, 0.08511881530284882, 0.10703950375318527]
However, this approach follows that of Katz and Giesbrecht (2006) in assuming that literal meanings are compositional.	[75, 112, 22, 34, 37, 121, 13, 60, 110, 62]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6828685998916626, 0.6263055205345154, 0.08236448466777802, 0.2756670415401459, 0.2847902476787567, 0.060669124126434326, 0.050879232585430145, 0.16340313851833344, 0.32755741477012634, 0.147108793258667]
However, this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier, 2003) between the CCGBank and an earlier version of the PropBank.	[84, 67, 108, 92, 93, 100, 116, 97, 68, 74]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5593775510787964, 0.6517750024795532, 0.11465137451887131, 0.0849609449505806, 0.3730321526527405, 0.05979252606630325, 0.06496422737836838, 0.15226632356643677, 0.22049325704574585, 0.13847362995147705]
Huang and Chiang (2007) describe a variation of cube pruning called cube growing, and they apply it to a source-tree to target string translator.	[110, 114, 13, 68, 122, 117, 119, 118, 70, 95]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6240854859352112, 0.5720433592796326, 0.09000461548566818, 0.6291614174842834, 0.1425817310810089, 0.21830061078071594, 0.22560569643974304, 0.2578507363796234, 0.0870070606470108, 0.23311495780944824]
Huang et al (2006) study a TSG-based tree-to-string alignment model.	[109, 32, 100, 67, 2, 8, 78, 16, 45, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07101918011903763, 0.20721396803855896, 0.061961445957422256, 0.07878277450799942, 0.07460758090019226, 0.07460758090019226, 0.059401340782642365, 0.06301411986351013, 0.0788068100810051, 0.12106246501207352]
Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser.	[4, 105, 14, 103, 142, 45, 85, 3, 123, 121]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7661781311035156, 0.5934534668922424, 0.05998324602842331, 0.08275818824768066, 0.329355388879776, 0.30270418524742126, 0.3338872790336609, 0.0658063068985939, 0.265053927898407, 0.4129422605037689]
Human annotators demonstrate frequent disagreement about the number of segments and exactly where the transitions between segments occur, while still demonstrating statistically significant agreement (Passonneau and Litman, 1997).	[43, 120, 430, 230, 409, 131, 503, 129, 269, 93]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7280905246734619, 0.7369247674942017, 0.06886995583772659, 0.2835186719894409, 0.10529030859470367, 0.05404103174805641, 0.050914935767650604, 0.13515904545783997, 0.15306782722473145, 0.07378046214580536]
Hwa (1999) used a variant of the inside-outside algorithm presented in Pereira and Schabes (1992) to exploit a partially labeled out-of-domain tree bank, and found an advantage to adaptation over direct grammar induction.	[121, 31, 104, 82, 51, 91, 89, 61, 125, 33]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6049938797950745, 0.5840136408805847, 0.14243604242801666, 0.21146902441978455, 0.1496361792087555, 0.06752898544073105, 0.2761227488517761, 0.21322141587734222, 0.18246996402740479, 0.23579226434230804]
Hwa et al (2002) found that human translations from Chinese to English preserved only 39-42% of the unlabeled Chinese dependencies.	[97, 70, 109, 68, 89, 98, 33, 58, 64, 100]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7041762471199036, 0.5334032773971558, 0.18986359238624573, 0.0901048555970192, 0.04743766412138939, 0.09565000236034393, 0.09203767776489258, 0.06147634610533714, 0.4085756838321686, 0.05392209067940712]
Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.	[25, 53, 263, 177, 258, 9, 52, 194, 4, 14]	[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]	[0.4225713312625885, 0.3686125874519348, 0.48512136936187744, 0.26579833030700684, 0.5839806199073792, 0.19504587352275848, 0.06466276198625565, 0.6081883311271667, 0.1634754240512848, 0.14297351241111755]
Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008).	[27, 151, 1, 29, 77, 115, 44, 68, 23, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15746305882930756, 0.24869778752326965, 0.1804129034280777, 0.05236073210835457, 0.061714135110378265, 0.15630188584327698, 0.06500986218452454, 0.07806608825922012, 0.07666457444429398, 0.07598843425512314]
I declare that Korean common nouns have both the RESTR (ICTION) for normal semantics and the QUALIA-ST (RUCTURE), which in turn has the AGENTIVE and TELIC attributes, adopting the basic idea from Pustejovsky (1991).	[211, 219, 159, 97, 402, 372, 283, 347, 422, 15]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7269977331161499, 0.06668417155742645, 0.075015127658844, 0.06562710553407669, 0.08627138286828995, 0.04771687835454941, 0.09870408475399017, 0.07985232770442963, 0.06837782263755798, 0.06861633062362671]
I-TRIE is a non-deterministic left branching trie with weights on rule entry as in Charniak et al (1998).	[78, 76, 79, 72, 74, 102, 71, 132, 9, 38]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3521502614021301, 0.36582955718040466, 0.3123878836631775, 0.23075883090496063, 0.07357610017061234, 0.06262747943401337, 0.19537417590618134, 0.08732883632183075, 0.047928325831890106, 0.04683973267674446]
ICTCLAS Segmenter: this model, trained by Zhang et al (2003), is a hierarchicalHMM segmenter that incorporates parts-of speech (POS) information into the probability models and generates multiple HMM mod els for solving segmentation ambiguities.	[49, 24, 28, 38, 19, 72, 42, 26, 37, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17873495817184448, 0.08733183890581131, 0.09239999949932098, 0.08081947267055511, 0.10160119831562042, 0.1324882060289383, 0.10945669561624527, 0.08636581897735596, 0.06538455933332443, 0.06199564412236214]
Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm.	[105, 12, 1, 35, 69, 31, 32, 9, 65, 19]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.6877403259277344, 0.7162953615188599, 0.5924880504608154, 0.6310083270072937, 0.4699854552745819, 0.5239845514297485, 0.3932514786720276, 0.3869052827358246, 0.43467533588409424, 0.4120243787765503]
In (Brown et al, 1993), three alignment models are described that include fertility models, these are IBM Models 3, 4, and 5.	[496, 352, 378, 216, 217, 131, 140, 210, 524, 134]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 1]	[0.6880339980125427, 0.6732112765312195, 0.34191426634788513, 0.5023922324180603, 0.2372957468032837, 0.19932971894741058, 0.4211970269680023, 0.21908468008041382, 0.200636088848114, 0.5493476390838623]
In (Foster and Kuhn, 2007), two kinds of linear mixture were described: linear mixture of language models (LMs), and linear mixture of translation models (TMs). Some of the results reported above involved linear TM mixtures, but none of them involved linear LM mixtures.	[94, 89, 66, 49, 90, 91, 2, 26, 63, 138]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6311084032058716, 0.4997676610946655, 0.378092497587204, 0.32246124744415283, 0.5768796801567078, 0.2380998432636261, 0.28986656665802, 0.3080129325389862, 0.45130762457847595, 0.16047047078609467]
In (Gale and Church, 1991) and (Brown et al, 1991), the authors start from the fact that the length of a source text sentence is highly correlated with the length of its target text translation: short sentences tend to have short translations, and long sentences tend to have long translations.	[7, 86, 112, 10, 100, 26, 8, 90, 11, 1]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5803259611129761, 0.37469562888145447, 0.3797171115875244, 0.21061860024929047, 0.37162721157073975, 0.11204979568719864, 0.3491455912590027, 0.06716764718294144, 0.12305358052253723, 0.14188849925994873]
In (Goldberg et al, 2006), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well.	[55, 24, 1, 56, 3, 2, 5, 20, 4, 33]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6257022619247437, 0.45861169695854187, 0.19855336844921112, 0.1166185587644577, 0.33495575189590454, 0.05667613074183464, 0.051845621317625046, 0.2008281946182251, 0.04859476909041405, 0.05855441093444824]
In (Goldsmith, 2001) a recursive structure is proposed, such that stems can consist of a sub-stem and a suffix.	[256, 320, 572, 43, 597, 601, 215, 205, 461, 269]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.4831841289997101, 0.4504445791244507, 0.06477883458137512, 0.18573428690433502, 0.31391441822052, 0.5131871700286865, 0.16319812834262848, 0.2618044912815094, 0.19480010867118835, 0.4021639823913574]
In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space.	[29, 82, 52, 72, 86, 24, 140, 85, 104, 66]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7225784659385681, 0.6623470187187195, 0.3566904067993164, 0.4749947190284729, 0.11409361660480499, 0.07897307723760605, 0.3157491385936737, 0.12929168343544006, 0.1397712528705597, 0.5290209650993347]
In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented.	[326, 305, 325, 273, 284, 304, 337, 288, 33, 19]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5914785861968994, 0.5378023386001587, 0.4487875699996948, 0.5324962735176086, 0.22688907384872437, 0.32201263308525085, 0.2084071934223175, 0.1981506198644638, 0.3431779444217682, 0.05360091105103493]
In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003).	[5, 51, 46, 38, 92, 45, 47, 36, 44, 43]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.520122766494751, 0.7683084011077881, 0.5367880463600159, 0.09720320999622345, 0.26400697231292725, 0.27319514751434326, 0.20965364575386047, 0.11353591084480286, 0.07016845792531967, 0.3163556456565857]
In (Mihalcea et al, 2007), different shortcomings of lexicon-based translation scheme was discussed for the more semantic-oriented task subjective analysis, instead the authors proposed to use a parallel-corpus, apply the classifier in the source language and use the corresponding sentences in the target language to train a new classifier.	[32, 34, 15, 2, 8, 12, 13, 30, 173, 101]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4364296793937683, 0.6946073174476624, 0.5890708565711975, 0.36420485377311707, 0.3057638704776764, 0.21242201328277588, 0.11641697585582733, 0.21569493412971497, 0.09081974625587463, 0.44496583938598633]
In (Moschitti et al, 2007) it was shown that the use of TK improves QC of 1.2 percent points, i.e. from 90.6 to 91.8: further analysis of these fragments may help us to device compact, less sparse syntactic features and design more accurate models for the task.	[26, 71, 28, 136, 109, 125, 140, 3, 128, 106]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4042101204395294, 0.21810327470302582, 0.1209927573800087, 0.09293121844530106, 0.1698453575372696, 0.09002342075109482, 0.06446977704763412, 0.05962780863046646, 0.27968698740005493, 0.2689743638038635]
In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S	[30, 98, 53, 25, 201, 165, 180, 182, 167, 199]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15382711589336395, 0.0585571825504303, 0.08701132982969284, 0.04582033306360245, 0.36786821484565735, 0.09595483541488647, 0.05422034487128258, 0.07223117351531982, 0.27718597650527954, 0.1978560984134674]
In (Shieber, 1988), a chart-based bottom-up generator is presented which is devoid of an indexing scheme: all word edges leave and enter the same vertex and as a result, interactions must be considered explicitly between new edges and all edges currently in the chart.	[112, 52, 19, 35, 60, 61, 230, 268, 247, 126]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08691959828138351, 0.08480646461248398, 0.05710345506668091, 0.09975913912057877, 0.1323334127664566, 0.14573341608047485, 0.15358348190784454, 0.08023184537887573, 0.0841641053557396, 0.08399833738803864]
In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010).	[160, 117, 162, 93, 203, 176, 192, 165, 5, 94]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16885782778263092, 0.4123850166797638, 0.25860702991485596, 0.33513179421424866, 0.26413780450820923, 0.17389675974845886, 0.16204242408275604, 0.2328428328037262, 0.05486017465591431, 0.24302202463150024]
In ASGRE, this has been translated into a criterion which determines the adequacy of an attribute set, implemented in its most straightforward form in Full Brevity algorithms which select the smallest attribute set that uniquely refers to the intended referent (Dale, 1989).	[60, 63, 66, 64, 57, 61, 71, 58, 48, 97]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.563876211643219, 0.5649629831314087, 0.20607662200927734, 0.42110535502433777, 0.42252594232559204, 0.1899152249097824, 0.06408330798149109, 0.33421096205711365, 0.11882548779249191, 0.0727543905377388]
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem.	[105, 1, 13, 112, 52, 9, 109, 114, 53, 143]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.6271748542785645, 0.3500608205795288, 0.15073736011981964, 0.10661172866821289, 0.21034058928489685, 0.16815419495105743, 0.43970245122909546, 0.07201604545116425, 0.15679654479026794, 0.5154241323471069]
In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set.	[115, 89, 1, 138, 110, 97, 70, 168, 132, 121]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6370453834533691, 0.3622656762599945, 0.3940560221672058, 0.16084113717079163, 0.08404242992401123, 0.42072269320487976, 0.09226876497268677, 0.11955908685922623, 0.16276267170906067, 0.08497273176908493]
In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000).	[69, 11, 63, 56, 23, 15, 50, 126, 55, 12]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6273123621940613, 0.5945502519607544, 0.3407481610774994, 0.22692491114139557, 0.17208026349544525, 0.13022810220718384, 0.1693698912858963, 0.1142805814743042, 0.31915122270584106, 0.09962284564971924]
In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained segmentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes using alignments with a part-of-speech tagged English corpus.	[2, 24, 23, 27, 41, 11, 8, 12, 44, 26]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.6433186531066895, 0.5836975574493408, 0.515264093875885, 0.39361658692359924, 0.5585342645645142, 0.39570164680480957, 0.17477060854434967, 0.10145755112171173, 0.07902029156684875, 0.13967189192771912]
In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.	[4, 148, 89, 77, 87, 96, 80, 13, 84, 114]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 1]	[0.7575350403785706, 0.5861355066299438, 0.5152631998062134, 0.43883052468299866, 0.6911145448684692, 0.2797882556915283, 0.2646394670009613, 0.2052222043275833, 0.17917901277542114, 0.6193898916244507]
In a different approach, Hwa et al (2002) aligned the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees.	[121, 29, 113, 98, 123, 71, 55, 33, 52, 87]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3090857267379761, 0.29803580045700073, 0.15528753399848938, 0.10148774087429047, 0.20810405910015106, 0.10827966779470444, 0.06533456593751907, 0.0504550039768219, 0.1100679412484169, 0.06810857355594635]
In a recent paper on the SRL on verbal predicates for English, (Toutanova et al, 2005) pointed out that one potential flaw in a SRL system where each argument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard constraint that they do not have the same label.	[9, 41, 45, 31, 38, 121, 15, 6, 128, 7]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7071952223777771, 0.5061624050140381, 0.15044741332530975, 0.4139971435070038, 0.14350783824920654, 0.3553016185760498, 0.1010349690914154, 0.23130671679973602, 0.051887381821870804, 0.5863862037658691]
In a similar vein, Pang et al (2003) used a corpus of alternative English translations of Chinesenews stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.	[1, 116, 4, 96, 16, 33, 15, 90, 32, 0]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7513366937637329, 0.7416394352912903, 0.35794001817703247, 0.4020387530326843, 0.12591521441936493, 0.3309074640274048, 0.18276354670524597, 0.44452381134033203, 0.0825444683432579, 0.525946855545044]
In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000).	[101, 29, 28, 39, 32, 30, 37, 36, 1, 11]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5899874567985535, 0.5907593965530396, 0.5265218615531921, 0.36551886796951294, 0.17074449360370636, 0.1701730340719223, 0.16036522388458252, 0.45031067728996277, 0.07675109803676605, 0.07957976311445236]
In addition to lexical translation, our system models structural changes and changes to feature values, for although dependency structures are fairly well preserved across languages (Fox, 2002), there are certainly many instances where the structure must be modified.	[128, 103, 146, 147, 127, 131, 29, 9, 8, 110]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6649482250213623, 0.5217734575271606, 0.40959808230400085, 0.22393347322940826, 0.2659502327442169, 0.08407646417617798, 0.1055692732334137, 0.3790828585624695, 0.07918575406074524, 0.23251082003116608]
In addition to the IE tasks in the biomedical domain, negation scope learning has attracted increasing attention in some natural language processing (NLP) tasks, such as sentiment classification (Turney, 2002).	[84, 151, 126, 1, 81, 80, 78, 125, 87, 31]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3312920033931732, 0.2109915018081665, 0.07413282990455627, 0.06484644114971161, 0.13625194132328033, 0.06002932786941528, 0.04853137582540512, 0.0665210634469986, 0.04955944046378136, 0.05321580916643143]
In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003).	[83, 134, 11, 102, 113, 100, 135, 7, 86, 14]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.48134785890579224, 0.3848845362663269, 0.31817150115966797, 0.30201584100723267, 0.5219916105270386, 0.30669400095939636, 0.2673110067844391, 0.05594336986541748, 0.2747504413127899, 0.04795968160033226]
In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set.	[165, 0, 167, 198, 208, 115, 179, 186, 169, 185]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48955923318862915, 0.6116008162498474, 0.4868481159210205, 0.2953985035419464, 0.17435838282108307, 0.05802110582590103, 0.37160202860832214, 0.12239136546850204, 0.14744071662425995, 0.05981050431728363]
In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding.	[144, 160, 149, 11, 171, 24, 46, 146, 14, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.45240268111228943, 0.24405437707901, 0.29799237847328186, 0.20691649615764618, 0.09402155131101608, 0.13273587822914124, 0.37709009647369385, 0.2066614031791687, 0.1076488271355629, 0.1307748407125473]
In addition to these finite state pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al, 1995).	[84, 103, 152, 22, 397, 102, 80, 60, 6, 409]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.2786364257335663, 0.6856306195259094, 0.6632269024848938, 0.16963475942611694, 0.22888191044330597, 0.49167415499687195, 0.29451698064804077, 0.04189043864607811, 0.13922733068466187, 0.1920996755361557]
In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction.	[85, 81, 11, 82, 80, 0, 147, 79, 72, 150]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.5411646366119385, 0.5563711524009705, 0.10013755410909653, 0.10874918848276138, 0.5526297092437744, 0.2899242043495178, 0.04978455975651741, 0.20222876965999603, 0.048987314105033875, 0.05856519192457199]
In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations.	[73, 52, 16, 3, 107, 20, 13, 67, 25, 54]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39098450541496277, 0.07255154103040695, 0.3139474391937256, 0.44720658659935, 0.24317315220832825, 0.056077007204294205, 0.0749797374010086, 0.07044235616922379, 0.0885913074016571, 0.06392979621887207]
In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR).	[178, 164, 32, 179, 65, 13, 206, 26, 139, 130]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6397159695625305, 0.21796296536922455, 0.06202825531363487, 0.17901688814163208, 0.16288580000400543, 0.06513095647096634, 0.06437330693006516, 0.16076000034809113, 0.0679999440908432, 0.1157134398818016]
In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio of source and target sentence has to be smaller than 2.	[81, 313, 219, 292, 88, 151, 59, 116, 161, 63]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4621099531650543, 0.159964919090271, 0.06236525624990463, 0.11614380776882172, 0.16597029566764832, 0.09178426116704941, 0.20911239087581635, 0.09772643446922302, 0.12273742258548737, 0.12117816507816315]
In addition, we plan to experiment with assigning costs to planning operators in a metric planning problem (Hoffmann, 2002) in order to model the cognitive cost of an RE (Krahmer et al, 2003) and compute minimal-cost instruction sequences.	[265, 71, 343, 306, 205, 316, 317, 252, 244, 120]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5943167805671692, 0.1777854561805725, 0.4935188889503479, 0.2611923813819885, 0.47918862104415894, 0.1628158837556839, 0.14033348858356476, 0.3566319942474365, 0.2360961139202118, 0.2523399591445923]
In addition, we show in Table 7 the F-score results provided by Snow et al (2007) for their SVM-based system and for the mapping-based approach of Navigli (2006), denoted by ODE.	[21, 123, 44, 29, 2, 131, 65, 77, 7, 43]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5115821361541748, 0.7098062038421631, 0.4400104880332947, 0.5082741975784302, 0.2537872791290283, 0.14265382289886475, 0.052353180944919586, 0.14051523804664612, 0.1141449362039566, 0.48389190435409546]
In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation.	[20, 1, 5, 0, 39, 3, 92, 88, 29, 50]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6251751780509949, 0.11213558167219162, 0.09982499480247498, 0.24478214979171753, 0.12583622336387634, 0.2293415665626526, 0.08400758355855942, 0.09503741562366486, 0.06281641870737076, 0.2528481185436249]
In all our experiments, we use two popular performance measures, B-CUBED Fmeasure (Bagga and Baldwin, 1998) and CEAF F measure (Luo, 2005), to evaluate the co reference resolution result.	[15, 0, 1, 4, 16, 6, 2, 5, 3, 13]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6284079551696777, 0.8055392503738403, 0.40048933029174805, 0.3369506299495697, 0.20090803503990173, 0.06002822145819664, 0.2408575564622879, 0.0525762178003788, 0.047185853123664856, 0.08948976546525955]
In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006).	[7, 0, 5, 156, 1, 27, 40, 170, 160, 17]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3068496286869049, 0.7141989469528198, 0.055608995258808136, 0.20560000836849213, 0.12063653022050858, 0.4550577700138092, 0.09901729226112366, 0.11142171919345856, 0.1805674433708191, 0.15736542642116547]
"In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that ""common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks"" (Sundheim 1995: 16)."	[302, 134, 57, 90, 38, 0, 6, 26, 211, 103]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05025345832109451, 0.05872699245810509, 0.0755562111735344, 0.09519311040639877, 0.08316099643707275, 0.25586625933647156, 0.18562844395637512, 0.05709952488541603, 0.054263077676296234, 0.0541919581592083]
In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores.	[77, 76, 78, 9, 49, 33, 72, 45, 57, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7691730260848999, 0.734307050704956, 0.3984232246875763, 0.16150030493736267, 0.24047210812568665, 0.10782130807638168, 0.1500222086906433, 0.0757904127240181, 0.41320642828941345, 0.07593739777803421]
In attribute extraction, typically one must choose between the precise results of rich patterns (involving punctuation and parts-of-speech) applied to small corpora (Berland and Charniak, 1999) and the high-coverage results of superficial patterns applied to web-scale data, e.g. via the Google API (Almuhareb and Poesio, 2004).	[38, 124, 55, 29, 15, 16, 99, 34, 37, 8]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5824750065803528, 0.5223273038864136, 0.3987571597099304, 0.09438096731901169, 0.05271454527974129, 0.057192131876945496, 0.20094241201877594, 0.09260404109954834, 0.14588041603565216, 0.08560435473918915]
In both cases, we report PARSEVAL labeled bracket scores (Magerman, 1995), with the brackets labeled by syntactic categories but not grammatical functions.	[160, 95, 5, 158, 46, 144, 93, 159, 161, 106]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6375069618225098, 0.479236900806427, 0.27117717266082764, 0.06245042011141777, 0.06720627844333649, 0.21502497792243958, 0.10288745164871216, 0.0542687252163887, 0.05087431147694588, 0.0421273335814476]
In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003).	[61, 32, 107, 114, 20, 62, 110, 40, 19, 51]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.27339258790016174, 0.19280119240283966, 0.1138182207942009, 0.05800595134496689, 0.0842280164361, 0.30599549412727356, 0.10325105488300323, 0.26669618487358093, 0.17921893298625946, 0.12121853977441788]
In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004), while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005).	[98, 12, 7, 46, 106, 16, 32, 9, 31, 99]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3399678170681, 0.320942759513855, 0.10536063462495804, 0.13749302923679352, 0.09764716774225235, 0.06150573492050171, 0.1755513697862625, 0.281655877828598, 0.24709923565387726, 0.10181427001953125]
In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations.	[44, 0, 3, 15, 20, 37, 16, 1, 47, 55]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3735933303833008, 0.6958673000335693, 0.07452823966741562, 0.12509311735630035, 0.21976731717586517, 0.06610390543937683, 0.16389355063438416, 0.07233298569917679, 0.06064146012067795, 0.0751032829284668]
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT.	[39, 84, 3, 131, 108, 144, 69, 83, 29, 135]	[1, 1, 0, 0, 0, 1, 0, 1, 0, 0]	[0.5092610716819763, 0.713929295539856, 0.4659435451030731, 0.26156672835350037, 0.2907286584377289, 0.6019685864448547, 0.3371293842792511, 0.5831299424171448, 0.4713338315486908, 0.12277832627296448]
In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text.	[324, 93, 92, 15, 184, 47, 377, 4, 7, 270]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38910654187202454, 0.596966028213501, 0.292849063873291, 0.20834152400493622, 0.15541990101337433, 0.2251785695552826, 0.07715900987386703, 0.10668952018022537, 0.08458656817674637, 0.12054119259119034]
In general a statistical machine translation system is composed of three components: a language model, a translation model, and a decoder (Brown et al, 1988). The language model tells how probable a given sentence is in the source language, the translation model indicates how likely it is that a particular target sentence is a translation of a given source sentence, and the decoder is what actually takes a source sentence as input and produces its translation as output.	[47, 36, 41, 139, 54, 44, 86, 128, 59, 5]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29123398661613464, 0.13487382233142853, 0.1719522923231125, 0.18727247416973114, 0.12232842296361923, 0.10487499088048935, 0.06499253213405609, 0.09152641892433167, 0.09735293686389923, 0.17367805540561676]
In general, these studies focus on issues regarding annotating speculation and approach the problem of recognizing speculation as a text classification problem, using the well-known bag of words method (Light et al 2004, Medlock and Briscoe, 2007) or simple substring matching (Light et al, 2004).	[139, 154, 10, 33, 146, 4, 157, 77, 147, 100]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17003796994686127, 0.46564939618110657, 0.359494686126709, 0.21494309604167938, 0.1691308468580246, 0.05943099409341812, 0.07223831862211227, 0.31757721304893494, 0.11574842780828476, 0.1760169267654419]
In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out.	[109, 165, 111, 113, 101, 68, 62, 162, 112, 40]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5954706072807312, 0.7422950863838196, 0.4054858684539795, 0.34715524315834045, 0.05431680381298065, 0.20168060064315796, 0.05908402428030968, 0.3112524747848511, 0.1124357357621193, 0.06799623370170593]
In order to examine the cross-lingual applicability of our methods, we also use Jacy, an HPSG-based grammar of Japanese (Siegel and Bender, 2002).	[191, 35, 1, 23, 12, 18, 13, 183, 22, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6561705470085144, 0.26782307028770447, 0.1412678062915802, 0.17336633801460266, 0.10644785314798355, 0.061630766838788986, 0.10954657942056656, 0.44072452187538147, 0.3571034371852875, 0.17521566152572632]
In order to extract source-target character mappings, we use m2m-aligner (Jiampojamarn et al, 2007), which implements a forward-backward algorithm to sum over probabilities of possible character sequence mappings, and uses Expectation Maximization to learn mapping probabilities.	[78, 67, 22, 58, 80, 74, 61, 59, 147, 70]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5426802039146423, 0.3793203830718994, 0.19058646261692047, 0.1116584986448288, 0.1214318573474884, 0.206809401512146, 0.27482643723487854, 0.23457220196723938, 0.05211765691637993, 0.17657920718193054]
In order to extract the linguistic features necessary for the ME model, all sentences containing the target word were automatically part of-speech (POS) tagged using the Brill POS tagger (Brill, 1992).	[15, 73, 12, 9, 98, 7, 2, 0, 86, 30]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.5689259171485901, 0.061796244233846664, 0.17229615151882172, 0.11246948689222336, 0.21860823035240173, 0.15652455389499664, 0.2959502637386322, 0.5324551463127136, 0.2852649390697479, 0.0491916723549366]
In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009).	[241, 13, 21, 64, 4, 40, 51, 222, 229, 53]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6956386566162109, 0.1059907078742981, 0.1903533935546875, 0.32785549759864807, 0.052511606365442276, 0.07192600518465042, 0.08919049799442291, 0.06375723332166672, 0.317035973072052, 0.057333722710609436]
In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs.	[128, 0, 2, 24, 57, 35, 27, 152, 144, 29]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6087035536766052, 0.5864450931549072, 0.24090762436389923, 0.09993807226419449, 0.12633125483989716, 0.08027275651693344, 0.0720476359128952, 0.058450549840927124, 0.06110464408993721, 0.055001161992549896]
In order to obtain the word alignment satisfying the ITG constraint, Wu (1997) propose a DPalgorithm, and we (Chao and Li, 2007) have transferred the constraint to four simple position judgment procedures in an explicit way, so that we can incorporate the ITG constraint as a feature into a log linear word alignment model (Moore, 2005).	[57, 172, 20, 176, 31, 3, 84, 29, 88, 82]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6830990314483643, 0.17725171148777008, 0.13164040446281433, 0.238399937748909, 0.1432834416627884, 0.08968435227870941, 0.08597339689731598, 0.04642947018146515, 0.06776686757802963, 0.06491038203239441]
In order to reduce the number of candidate arguments in the identification step, I apply the filtering technique of Xue and Palmer (2004), trivially adopted to the dependency syntax formalism.	[26, 43, 56, 50, 23, 49, 60, 31, 69, 28]	[0, 1, 0, 1, 0, 0, 1, 0, 0, 0]	[0.3755077123641968, 0.542852520942688, 0.1488359421491623, 0.5946546196937561, 0.25832927227020264, 0.15035943686962128, 0.5214846134185791, 0.06124463304877281, 0.04574958235025406, 0.292346328496933]
In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).	[0, 79, 86, 3, 44, 43, 140, 1, 128, 116]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7038663029670715, 0.34847018122673035, 0.31726324558258057, 0.07697652280330658, 0.10586820542812347, 0.055487826466560364, 0.05462130904197693, 0.0487537682056427, 0.1195036843419075, 0.11588022112846375]
In our approach, N-best candidates for each training example are produced with the CRF++ software (Kudo et al, 2004).	[113, 159, 10, 33, 1, 0, 76, 168, 36, 90]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08557114005088806, 0.2787579596042633, 0.13662229478359222, 0.14307405054569244, 0.049185581505298615, 0.10659053921699524, 0.24136978387832642, 0.05615038424730301, 0.053712084889411926, 0.14555686712265015]
In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly difficult, as hard constraints often ended up breaking examples that were working without such constraints, due to exceptions, sub-regularities and acceptable variation in the data.	[351, 330, 22, 331, 336, 370, 291, 282, 406, 405]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5405807495117188, 0.24406273663043976, 0.09542033821344376, 0.11181486397981644, 0.41479891538619995, 0.06694550812244415, 0.05626042187213898, 0.09861880540847778, 0.057485684752464294, 0.07337626814842224]
In our experiments we use the bibliographic citation dataset described in (Peng and McCallum, 2004).	[65, 98, 85, 111, 101, 5, 114, 24, 90, 93]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.7898329496383667, 0.700305163860321, 0.5375238656997681, 0.5501963496208191, 0.13556477427482605, 0.04557870328426361, 0.1392042189836502, 0.048677850514650345, 0.11802031099796295, 0.08747221529483795]
In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006).	[58, 41, 46, 25, 135, 127, 34, 138, 33, 122]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16463367640972137, 0.2124616503715515, 0.08712244033813477, 0.06765217334032059, 0.20523834228515625, 0.15491309762001038, 0.1547672003507614, 0.15597675740718842, 0.43408483266830444, 0.1312723606824875]
In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts.	[11, 20, 63, 45, 140, 39, 7, 62, 1, 41]	[0, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.47096049785614014, 0.6312997937202454, 0.30388036370277405, 0.4416565001010895, 0.36019366979599, 0.655232310295105, 0.44163641333580017, 0.2278471440076828, 0.344677209854126, 0.10529434680938721]
In parsing, Bohnet and Nivre (2012) and Bohnet et al (2013) propose a model for joint morphological analysis, part-of speech tagging and dependency parsing using a Usingeval.pl from Buchholz and Marsi (2006).	[0, 160, 15, 2, 1, 12, 165, 8, 161, 135]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6690131425857544, 0.4709322452545166, 0.3728885352611542, 0.45803284645080566, 0.3931839168071747, 0.4238358438014984, 0.08366209268569946, 0.1810179501771927, 0.11340449005365372, 0.1165783628821373]
In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures.	[18, 26, 6, 1, 5, 31, 88, 68, 8, 32]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5734934210777283, 0.09719835966825485, 0.1816544085741043, 0.1455085724592209, 0.06195352226495743, 0.1414734423160553, 0.2500482201576233, 0.09164635092020035, 0.08127482980489731, 0.0916420966386795]
In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov + bag-of-words model for language generation.	[44, 0, 188, 70, 185, 25, 14, 160, 12, 58]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.44706735014915466, 0.7660127282142639, 0.22653068602085114, 0.09945975244045258, 0.10961739718914032, 0.1681629866361618, 0.08845476061105728, 0.43425342440605164, 0.08249756693840027, 0.11196983605623245]
In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables.	[53, 18, 7, 50, 16, 0, 13, 5, 14, 3]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.274829626083374, 0.22059261798858643, 0.04776119813323021, 0.10991597920656204, 0.07120931893587112, 0.6911301016807556, 0.21579746901988983, 0.0509633794426918, 0.04968354105949402, 0.10166783630847931]
In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).	[24, 277, 319, 282, 48, 19, 50, 303, 305, 271]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5654409527778625, 0.37148189544677734, 0.12604187428951263, 0.08755792677402496, 0.11046958714723587, 0.053687598556280136, 0.3176249563694, 0.05973908305168152, 0.2570333480834961, 0.06032930314540863]
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale.	[20, 71, 137, 13, 132, 131, 21, 10, 23, 14]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6810394525527954, 0.36282598972320557, 0.3663736879825592, 0.2134237289428711, 0.2739942967891693, 0.3708285391330719, 0.2525169253349304, 0.23262928426265717, 0.1849328577518463, 0.18546009063720703]
In previous work, Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them in a large corpus.	[1, 134, 54, 13, 16, 43, 14, 77, 18, 100]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7400393486022949, 0.608045220375061, 0.329180508852005, 0.18083208799362183, 0.28167232871055603, 0.2766154110431671, 0.2207992821931839, 0.06699422001838684, 0.3095315992832184, 0.06697122752666473]
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	[27, 261, 28, 54, 222, 120, 264, 4, 145, 15]	[1, 1, 1, 0, 1, 1, 0, 0, 0, 0]	[0.6822696924209595, 0.6361274719238281, 0.5222460031509399, 0.18681691586971283, 0.5129059553146362, 0.5843905806541443, 0.29781171679496765, 0.13825297355651855, 0.17129528522491455, 0.14065296947956085]
"In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on ""diluted dictionaries"" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ)."	[24, 21, 14, 23, 174, 19, 4, 0, 29, 140]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7623627185821533, 0.7533526420593262, 0.09150996804237366, 0.25737443566322327, 0.10098961740732193, 0.06547187268733978, 0.07659383863210678, 0.06206607073545456, 0.047758039087057114, 0.44276735186576843]
In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004).	[0, 21, 61, 11, 155, 19, 18, 10, 2, 58]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5613692998886108, 0.8004647493362427, 0.10266532748937607, 0.3689391016960144, 0.15732547640800476, 0.10414906591176987, 0.08423085510730743, 0.06530563533306122, 0.06503567844629288, 0.0727563351392746]
In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008).	[6, 0, 72, 3, 127, 24, 67, 60, 46, 84]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3926253318786621, 0.27901023626327515, 0.49902594089508057, 0.24842433631420135, 0.060102880001068115, 0.13458529114723206, 0.43783140182495117, 0.28014397621154785, 0.06815218925476074, 0.0731290951371193]
In some projects (e.g. OntoNotes (Hovy et al 2006)), the percentage of agreements between two annotators is used, but a number of more complex measures are available (for a comprehensive survey see (Artstein and Poesio, 2008)).	[26, 25, 67, 60, 47, 65, 68, 69, 41, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3910083770751953, 0.0896085873246193, 0.05892059579491615, 0.05740899592638016, 0.11339902132749557, 0.10328403860330582, 0.13181564211845398, 0.11973820626735687, 0.04884903505444527, 0.058233872056007385]
In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or epsilon-cycles in FSMs (Eisner, 2002).	[221, 220, 219, 213, 112, 52, 90, 208, 187, 164]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.620649516582489, 0.4831409454345703, 0.26511016488075256, 0.1642625480890274, 0.08411040902137756, 0.07558907568454742, 0.05717151612043381, 0.2973279356956482, 0.06245904788374901, 0.11848653852939606]
In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995).	[495, 276, 326, 94, 451, 246, 85, 32, 33, 322]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6766185164451599, 0.4738682806491852, 0.3038860857486725, 0.05844062939286232, 0.19330357015132904, 0.0753202959895134, 0.05154402181506157, 0.4076389968395233, 0.0704803615808487, 0.2528269588947296]
In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).	[19, 197, 46, 0, 54, 1, 43, 36, 32, 28]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6258364915847778, 0.3085584342479706, 0.6209295988082886, 0.2137521654367447, 0.10495816171169281, 0.09752313792705536, 0.10724247992038727, 0.07190608978271484, 0.057543955743312836, 0.18379510939121246]
In that table, TBL stands for Brill ' s transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al (1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+ (CPD+ENS) Train Test 950 Kw 150 Kw.	[0, 5, 19, 106, 115, 122, 177, 15, 23, 160]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6501728296279907, 0.37160709500312805, 0.4031519293785095, 0.11843038350343704, 0.17629654705524445, 0.23640680313110352, 0.4798778295516968, 0.26431143283843994, 0.16193737089633942, 0.10284716635942459]
In the automatic evaluation of machine translation, paraphrases may help to alleviate problems presented by the fact that there are often alternative and equally valid ways of translating a text (Pang et al, 2003).	[152, 100, 32, 4, 181, 8, 15, 86, 136, 13]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5364910960197449, 0.6359433531761169, 0.23955485224723816, 0.17715653777122498, 0.1001441553235054, 0.39922845363616943, 0.09832413494586945, 0.05701851099729538, 0.05803816393017769, 0.08290699869394302]
In the case of English, partially motivated by Message Understanding Conferences (MUCs) (Grishman and Sundheim, 1996), a number of coreference resolution methods have been proposed.	[6, 7, 5, 2, 8, 161, 159, 29, 40, 12]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4793712794780731, 0.6390793323516846, 0.39880862832069397, 0.4612806439399719, 0.04523962363600731, 0.11285265535116196, 0.05287995561957359, 0.2664145827293396, 0.3621419370174408, 0.21199733018875122]
In the case of text messages, text-to-speech synthesis may be particularly useful for the visually impaired; automatic translation has also been considered (e.g., Aw et al, 2006).	[37, 137, 21, 159, 16, 33, 121, 161, 29, 22]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4979039430618286, 0.4398808181285858, 0.21992114186286926, 0.17040719091892242, 0.2206035703420639, 0.38746607303619385, 0.05832168832421303, 0.12918461859226227, 0.14112994074821472, 0.05035882070660591]
In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword.	[10, 15, 14, 54, 33, 107, 76, 0, 24, 1]	[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.5506625175476074, 0.7050508856773376, 0.1724288910627365, 0.502707302570343, 0.38618746399879456, 0.12749525904655457, 0.18974031507968903, 0.59615159034729, 0.06494654715061188, 0.1279992312192917]
In the first one, unsupervised evaluation, systems' answers were evaluated according to: (1) V Measure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al, 2009).	[2, 189, 7, 66, 209, 226, 51, 21, 11, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09427958726882935, 0.13737241923809052, 0.10548461228609085, 0.4697582423686981, 0.16504395008087158, 0.15513859689235687, 0.06757649034261703, 0.05930113047361374, 0.06540405750274658, 0.06404690444469452]
In the future, we will consider making an increase the context-size, which helped Toutanova et al (2003).	[99, 7, 14, 10, 90, 117, 137, 151, 42, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.3775915801525116, 0.3647078275680542, 0.07268552482128143, 0.08538299053907394, 0.10103495419025421, 0.1072603091597557, 0.04634670913219452, 0.042767543345689774, 0.5163598656654358, 0.046569567173719406]
In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005).	[52, 50, 53, 22, 54, 15, 61, 24, 20, 49]	[0, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.4067084789276123, 0.4821595549583435, 0.5261963605880737, 0.054771650582551956, 0.2662471830844879, 0.5443723201751709, 0.2468268871307373, 0.23295192420482635, 0.09689479321241379, 0.3738456070423126]
In the late 1990s, Chris Mellish implemented the first stochastic text planner (Mellish et al 1998).	[0, 4, 3, 130, 35, 33, 53, 115, 123, 133]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3303958773612976, 0.6789005994796753, 0.22962014377117157, 0.25536975264549255, 0.05496934428811073, 0.34462493658065796, 0.19928109645843506, 0.053343404084444046, 0.04517029598355293, 0.05507807806134224]
In the part-of-speech literature, whether taggers are based on a rule-based approach (Klein and Simmons, 1963), (Brill, 1992), (Voutilainen, 1993), or on a statistical one (Bahl and Mercer, 1976), (Leech et al, 1983), (Merialdo, 1994), (DeRose, 1988), (Church, 1989), (Cutting et al, 1992), there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones.	[9, 20, 102, 1, 97, 5, 12, 0, 93, 22]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7600504755973816, 0.16482675075531006, 0.3454245626926422, 0.12351952493190765, 0.10300184786319733, 0.1412757784128189, 0.2770213186740875, 0.6501527428627014, 0.20964710414409637, 0.23833109438419342]
In the recent years, there have been a number of papers considering this or similar problems: (Brown et al, 1990), (Dagan et al, 1993), (Kay et al, 1993), (Fung et al, 1993).	[5, 12, 10, 20, 45, 7, 72, 42, 33, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7685684561729431, 0.06981310248374939, 0.06513669341802597, 0.06758963316679001, 0.27092233300209045, 0.09051230549812317, 0.054763663560152054, 0.05701488256454468, 0.44978052377700806, 0.07585341483354568]
In their work on automatically identifying idiom types, Fazly and Stevenson (2006) - henceforth FS06 - show that an idiomatic VNC tends to have one (or at most a small number of) canonical form (s), which are its most preferred syntactic patterns.	[144, 152, 3, 150, 151, 35, 39, 41, 52, 181]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7058423757553101, 0.4368125796318054, 0.3203016519546509, 0.43403834104537964, 0.14503449201583862, 0.10338260233402252, 0.35043758153915405, 0.11032546311616898, 0.28019827604293823, 0.13564608991146088]
In this bakeoff, our basic model is based on the framework described in the work of Ratnaparkhi (1996) which was applied for English POS tagging.	[14, 0, 86, 12, 80, 4, 1, 2, 83, 34]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5648787617683411, 0.7462829351425171, 0.4633370637893677, 0.19969362020492554, 0.3967447578907013, 0.0800737515091896, 0.08978559821844101, 0.1733311414718628, 0.06284550577402115, 0.04695766046643257]
In this bakeoff, our models built for the tasks are similar to that in the work of Ng and Low (2004).	[18, 15, 56, 16, 141, 57, 7, 142, 67, 143]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5815764665603638, 0.24915984272956848, 0.053982701152563095, 0.14941556751728058, 0.13792602717876434, 0.06636298447847366, 0.14973345398902893, 0.10016807168722153, 0.22571279108524323, 0.5981335043907166]
In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner.	[15, 0, 5, 113, 27, 6, 69, 7, 68, 77]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49178576469421387, 0.5129032731056213, 0.30359166860580444, 0.10762246698141098, 0.17694252729415894, 0.18775463104248047, 0.08442191034555435, 0.16043871641159058, 0.4221947491168976, 0.04603132605552673]
In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005).	[0, 12, 2, 1, 124, 95, 13, 3, 4, 98]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8105347752571106, 0.48968783020973206, 0.35664522647857666, 0.48311591148376465, 0.26956939697265625, 0.35147538781166077, 0.08890640735626221, 0.06312140822410583, 0.049796201288700104, 0.09328902512788773]
In this paper we use TEASE (Szpektor et al, 2004), a state of-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules.	[3, 20, 24, 4, 6, 32, 179, 147, 65, 178]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7367636561393738, 0.353346586227417, 0.4241771101951599, 0.11296159774065018, 0.18765628337860107, 0.10038705170154572, 0.05422278866171837, 0.04880987107753754, 0.2410803884267807, 0.0803636685013771]
In this paper, we consider all constraints and measures evaluated by Kuhlmann and Nivre (2006) with some minor variations.	[4, 27, 83, 21, 118, 12, 142, 127, 18, 104]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3535393476486206, 0.4723196029663086, 0.11861603707075119, 0.1744491457939148, 0.14067015051841736, 0.19360412657260895, 0.27534112334251404, 0.1585715413093567, 0.09146377444267273, 0.09476710855960846]
In this paper, we developed probability models for the multi-level transfer rules presented in (Galley et al, 2004), showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests.	[26, 177, 173, 76, 167, 52, 143, 2, 38, 161]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4900274872779846, 0.3898244798183441, 0.2661721408367157, 0.07891383022069931, 0.2996445894241333, 0.0886608138680458, 0.05164926126599312, 0.07404255121946335, 0.05967974662780762, 0.14453275501728058]
In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002).	[6, 20, 0, 10, 39, 42, 51, 91, 26, 244]	[1, 1, 0, 1, 0, 0, 1, 1, 0, 0]	[0.6920772790908813, 0.6418638229370117, 0.20969749987125397, 0.5287538170814514, 0.3999122083187103, 0.21431471407413483, 0.5022072196006775, 0.5226730704307556, 0.15953539311885834, 0.0794994905591011]
In this paper, we evaluate the truth of these assumptions on the MRS expressions which the ERGcomputes for the sentences in the Redwoods Treebank (Oepen et al, 2002).	[131, 57, 13, 76, 75, 29, 52, 55, 87, 34]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7047422528266907, 0.15246257185935974, 0.14681196212768555, 0.14657524228096008, 0.2146887630224228, 0.13383620977401733, 0.1643180102109909, 0.14117029309272766, 0.05739723518490791, 0.05928104743361473]
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al, 2009).	[47, 10, 0, 4, 6, 1, 100, 148, 140, 49]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6438013911247253, 0.48112544417381287, 0.649296760559082, 0.31147387623786926, 0.41020911931991577, 0.15324543416500092, 0.0908588171005249, 0.08787399530410767, 0.04735653102397919, 0.08521687984466553]
In this project, a broad-coverage LFG grammar and parser for English was employed (see Riezler et al (2002)).	[31, 35, 14, 1, 142, 3, 17, 39, 26, 12]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5402355790138245, 0.7645974159240723, 0.28411680459976196, 0.249777153134346, 0.17017510533332825, 0.23867355287075043, 0.22570852935314178, 0.13784800469875336, 0.06257271021604538, 0.1338033825159073]
In this sense, our model behaves like a phrase based model, less sensitive to discontinuous phrases (Wellington et al, 2006).	[32, 166, 34, 3, 36, 169, 162, 13, 163, 31]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4747265875339508, 0.6588791608810425, 0.14387334883213043, 0.06739474833011627, 0.30025461316108704, 0.29661083221435547, 0.3846485912799835, 0.21895623207092285, 0.09661205112934113, 0.31651827692985535]
In this sense, they are functionally similar to the REALPRO system (Lavoie and Rambow, 1997).	[9, 49, 28, 11, 19, 15, 14, 8, 0, 1]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.592460036277771, 0.061927441507577896, 0.09261758625507355, 0.3646439015865326, 0.19332903623580933, 0.18333503603935242, 0.06186142936348915, 0.32999366521835327, 0.37001633644104004, 0.08161204308271408]
In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text (Marcus et al, 1994).	[150, 6, 3, 24, 17, 9, 5, 10, 0, 55]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5324003100395203, 0.5147238373756409, 0.10012233257293701, 0.14461730420589447, 0.11343555152416229, 0.0645870789885521, 0.3917708992958069, 0.11848897486925125, 0.14948515594005585, 0.06571175158023834]
In transliteration extraction, mining translations or transliterations from the ever-growing multilingual Web has become an active research topic, for example, by exploring query logs (Brill et al., 2001) and parallel (Nie et al, 1999) or comparable corpora (Sproat et al, 2006).	[23, 27, 24, 7, 135, 0, 28, 1, 105, 9]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.754888117313385, 0.3765380382537842, 0.10676415264606476, 0.2844843566417694, 0.5609608292579651, 0.36989325284957886, 0.13160060346126556, 0.09085247665643692, 0.08765222877264023, 0.10951349139213562]
Incrementality is not strict here in the sense of (Nivre, 2004), because sometimes more than one word is needed before parts of the frame are constructed and out put: into the right, for instance, needs to wait for a word like leg that completes the chunk.	[23, 11, 74, 113, 76, 82, 119, 64, 84, 25]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.0998438224196434, 0.5749174356460571, 0.10668521374464035, 0.054474905133247375, 0.5923080444335938, 0.06626462191343307, 0.0480981208384037, 0.3832453787326813, 0.29964542388916016, 0.06965571641921997]
Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).	[36, 37, 46, 20, 21, 17, 45, 58, 77, 19]	[1, 0, 1, 0, 1, 0, 0, 0, 0, 0]	[0.6575262546539307, 0.22224028408527374, 0.6822644472122192, 0.2856694757938385, 0.6842929720878601, 0.35299620032310486, 0.36445602774620056, 0.33012655377388, 0.21186979115009308, 0.1592395305633545]
Inspired by Chiang (2010), we adopt a fuzzy way to label every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006).	[70, 38, 83, 50, 14, 69, 62, 33, 49, 10]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5485007166862488, 0.23589971661567688, 0.35984674096107483, 0.16699621081352234, 0.3264406621456146, 0.3635305166244507, 0.31324303150177, 0.28582218289375305, 0.13672040402889252, 0.3901209235191345]
Inspired by HMM word alignment (Vogel et al, 1996), our second distance measure is based on jump width.	[0, 21, 8, 3, 123, 61, 12, 44, 132, 4]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6582496166229248, 0.7344760894775391, 0.053033966571092606, 0.12429031729698181, 0.19468878209590912, 0.46855372190475464, 0.06732400506734848, 0.49947887659072876, 0.22251808643341064, 0.10202615708112717]
Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF.	[199, 263, 276, 329, 198, 40, 281, 333, 14, 322]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6175044775009155, 0.4781620502471924, 0.1953931599855423, 0.3541708290576935, 0.12324536591768265, 0.0764487013220787, 0.4330466389656067, 0.13235972821712494, 0.11745358258485794, 0.09110207855701447]
Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics.	[30, 127, 100, 2, 83, 106, 21, 65, 71, 126]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5766028761863708, 0.6408630013465881, 0.3381744921207428, 0.19490966200828552, 0.1324549913406372, 0.15014787018299103, 0.1327478140592575, 0.24788200855255127, 0.06576426327228546, 0.20254525542259216]
Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al, 2011).	[169, 160, 181, 155, 176, 150, 154, 166, 170, 163]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7052210569381714, 0.6392941474914551, 0.20988251268863678, 0.23249241709709167, 0.3970094621181488, 0.16020800173282623, 0.07160253077745438, 0.2866533398628235, 0.1580471247434616, 0.128599613904953]
Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003).	[12, 117, 4, 132, 131, 87, 22, 11, 10, 17]	[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]	[0.6569598317146301, 0.7254558205604553, 0.500220000743866, 0.43343377113342285, 0.5372859239578247, 0.4628943204879761, 0.7173155546188354, 0.17097604274749756, 0.3627636730670929, 0.4058457016944885]
Inverted semrel structure from a definition of motorist Researchers who produced spreading activation networks from MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks.	[1, 8, 0, 89, 47, 169, 121, 19, 115, 129]	[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.47030824422836304, 0.5328720808029175, 0.37520337104797363, 0.07568430155515671, 0.0796133354306221, 0.07412149012088776, 0.5065237879753113, 0.05742960423231125, 0.050318438559770584, 0.08494096994400024]
Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).	[6, 15, 217, 3, 34, 49, 164, 0, 7, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0669238269329071, 0.26298707723617554, 0.1515161246061325, 0.15779578685760498, 0.09148386865854263, 0.043905116617679596, 0.056299544870853424, 0.2453676015138626, 0.060285717248916626, 0.04549971595406532]
Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information.	[594, 42, 593, 303, 308, 199, 353, 206, 335, 229]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6721529364585876, 0.5084471702575684, 0.0922280102968216, 0.13285142183303833, 0.0866815447807312, 0.0518050380051136, 0.08321214467287064, 0.055000122636556625, 0.13493917882442474, 0.2808457612991333]
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable.	[144, 119, 173, 196, 208, 31, 158, 153, 141, 62]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5200967788696289, 0.14590035378932953, 0.05731379985809326, 0.06349387764930725, 0.05205899849534035, 0.057060837745666504, 0.07601746916770935, 0.3777152895927429, 0.3118826746940613, 0.051489245146512985]
It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer.	[164, 98, 109, 201, 206, 213, 195, 36, 91, 137]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4934975802898407, 0.6264464259147644, 0.2499648630619049, 0.11977706849575043, 0.058453790843486786, 0.3371617794036865, 0.38750773668289185, 0.09684249013662338, 0.49796104431152344, 0.20786841213703156]
It can be applied to complicated models such IBM Model-4 (Brown et al, 1993).	[343, 142, 134, 524, 353, 133, 131, 621, 176, 595]	[0, 0, 1, 0, 0, 1, 0, 0, 1, 0]	[0.2727232277393341, 0.4185921251773834, 0.6061710119247437, 0.25498366355895996, 0.10155657678842545, 0.568570077419281, 0.2868484854698181, 0.04429164528846741, 0.5154324769973755, 0.4149078130722046]
It continues sampling a specification tree for each text specification until it finds one which successfully reads all of the input examples. The second baseline Aggressive is a state-of the-art semantic parsing framework (Clarke et al, 2010). The framework repeatedly predicts hidden structures (specification trees in our case) using a structure learner, and trains the structure learner based on the execution feedback of its predictions.	[86, 96, 87, 93, 143, 91, 165, 84, 4, 33]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.585848331451416, 0.2820890545845032, 0.1696770191192627, 0.4599655568599701, 0.10320630669593811, 0.14199797809123993, 0.060977786779403687, 0.06911299377679825, 0.1423873007297516, 0.05863712355494499]
It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox,2002).	[136, 4, 0, 1, 19, 138, 89, 59, 18, 127]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7911545038223267, 0.721716582775116, 0.24181921780109406, 0.07107187062501907, 0.09790436923503876, 0.14623254537582397, 0.19650505483150482, 0.07985144108533859, 0.07199549674987793, 0.19905991852283478]
It has been previously attempted by Cucerzan and Yarowsky in their language independent NER work which used morphological and contextual evidences (Cucerzan and Yarowsky, 1999).	[0, 2, 196, 7, 9, 5, 4, 163, 17, 197]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7627410888671875, 0.1924288421869278, 0.21894486248493195, 0.08013203740119934, 0.10274160653352737, 0.05114923045039177, 0.12478379905223846, 0.07678191363811493, 0.20235709846019745, 0.09997113794088364]
It has been successfully applied for text content such as news articles, scientific papers (Teufel and Moens, 2002) that follow a discourse structure.	[505, 37, 75, 71, 129, 336, 27, 36, 58, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3956480026245117, 0.2054087221622467, 0.08990899473428726, 0.44954580068588257, 0.24681195616722107, 0.2682133615016937, 0.49361592531204224, 0.10983552038669586, 0.06905491650104523, 0.09876125305891037]
It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc.	[16, 56, 28, 104, 2, 19, 6, 90, 25, 100]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.174936905503273, 0.18134288489818573, 0.2799813449382782, 0.165294349193573, 0.05290078744292259, 0.06313467025756836, 0.11875469982624054, 0.08802025020122528, 0.18936631083488464, 0.13873694837093353]
It has been used in a variety of difficult classification tasks such as part-of-speech tagging (Ratnaparkhi, 1996), prepositional phrase attachment (Ratnaparkhi et al, 1994) and named entity tagging (Borthwick et al, 1998), and achieves state of the art performance.	[8, 9, 4, 0, 110, 20, 21, 117, 27, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15326637029647827, 0.11454152315855026, 0.08528178930282593, 0.1099589467048645, 0.06629952043294907, 0.10684452205896378, 0.16223284602165222, 0.056713733822107315, 0.0508170947432518, 0.07798934727907181]
It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b).	[55, 8, 13, 11, 84, 3, 24, 9, 95, 97]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7105826735496521, 0.37375059723854065, 0.09763485938310623, 0.14877530932426453, 0.0939847007393837, 0.04583411663770676, 0.0948268249630928, 0.10124602168798447, 0.2992928624153137, 0.25024518370628357]
It is also considered to be the anchor of discourse relations, in the sense of the Penn Discourse Treebank (PDT) (Prasad et al, 2008).	[0, 113, 109, 6, 2, 120, 75, 46, 69, 90]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8189741373062134, 0.11943889409303665, 0.050480589270591736, 0.04895174875855446, 0.05434221029281616, 0.05578594654798508, 0.052991822361946106, 0.05471067130565643, 0.05447841435670853, 0.05565841123461723]
It is also the case that we thought PP attachment might be improved because of the increased coverage of preposition noun and preposition-verb combinations that work such as (Hindle and Rooth, 1993) show to be so important.	[69, 170, 80, 77, 211, 50, 42, 167, 85, 71]	[1, 1, 0, 1, 0, 1, 1, 0, 0, 0]	[0.6492253541946411, 0.5204110145568848, 0.4739683270454407, 0.6756691932678223, 0.29717931151390076, 0.5796231627464294, 0.6119245886802673, 0.06005245819687843, 0.2089301198720932, 0.46402886509895325]
It is an important and growing field of natural language processing with applications in areas such as transfer based machine translation (Riezler and Maxwell, 2006) and sentence condensation (Riezler et al, 2003).	[39, 172, 159, 153, 15, 14, 1, 82, 150, 135]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4586534798145294, 0.20362181961536407, 0.12479066103696823, 0.16013142466545105, 0.40122663974761963, 0.0860716700553894, 0.11796759814023972, 0.06494205445051193, 0.07362000644207001, 0.1101750060915947]
It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)).	[3, 113, 135, 55, 115, 96, 26, 129, 117, 5]	[1, 1, 0, 0, 1, 0, 0, 1, 0, 0]	[0.7884962558746338, 0.7906087636947632, 0.13256637752056122, 0.13278095424175262, 0.6559015512466431, 0.17933182418346405, 0.29424187541007996, 0.5227948427200317, 0.151027113199234, 0.3212272822856903]
It is exactly this mismatch between structure of the traversal and Pereira and Warren (1983) point out that Earley deduction is not restricted to a left-to-right expansion of goals, but this suggestion was not followed up with a specific a lgorithm addressing the problems discussed here.	[4, 20, 65, 1, 21, 122, 42, 163, 120, 149]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6843207478523254, 0.09998733550310135, 0.39188069105148315, 0.16098296642303467, 0.3171246647834778, 0.32172223925590515, 0.1096312552690506, 0.06504647433757782, 0.05683125555515289, 0.05436702072620392]
It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003).	[19, 50, 9, 53, 78, 77, 49, 58, 52, 84]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4043980836868286, 0.5080811977386475, 0.11753955483436584, 0.20723684132099152, 0.21719005703926086, 0.37353572249412537, 0.2190999537706375, 0.25270453095436096, 0.1206335797905922, 0.32058945298194885]
It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000).	[113, 0, 33, 15, 1, 118, 9, 89, 37, 110]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.249383807182312, 0.24543121457099915, 0.05361267924308777, 0.04991188645362854, 0.09739918261766434, 0.05585267022252083, 0.08688612282276154, 0.13433684408664703, 0.2712579369544983, 0.04973533749580383]
It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003).	[17, 19, 5, 137, 59, 6, 62, 48, 63, 55]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5353314876556396, 0.3479745388031006, 0.24408358335494995, 0.24554187059402466, 0.06904441118240356, 0.36012253165245056, 0.047699157148599625, 0.1712009161710739, 0.061552707105875015, 0.05667415261268616]
It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ.	[0, 6, 12, 13, 10, 3, 1, 2, 4, 18]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7239035367965698, 0.15980128943920135, 0.09557023644447327, 0.09059211611747742, 0.0844392329454422, 0.09233501553535461, 0.09803710877895355, 0.14501585066318512, 0.1967642605304718, 0.19148238003253937]
It presupposes fine grained methods for the identification of cohesive ties 76 between (sentence) units in a text; describing the computational basis for developing such methods is outside of the scope of this paper (howeveb see (Kennedy and Boguraev, 1996), (Fellbaum, 1999), (Kelleb 1994)), as is the complete framework for lexical cohesion analysis we have developed.	[9, 161, 10, 28, 164, 1, 18, 17, 149, 128]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5293211936950684, 0.3176431953907013, 0.3457086384296417, 0.3542148470878601, 0.18547241389751434, 0.1503170281648636, 0.13687968254089355, 0.21558722853660583, 0.13715827465057373, 0.5067236423492432]
It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.).	[169, 27, 141, 153, 158, 50, 160, 5, 97, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1587902158498764, 0.27102234959602356, 0.05011501535773277, 0.15047724545001984, 0.19871243834495544, 0.056576039642095566, 0.06451692432165146, 0.14433342218399048, 0.16244111955165863, 0.06076115369796753]
It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007).	[37, 33, 35, 194, 47, 31, 180, 65, 229, 251]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5089558362960815, 0.4572739899158478, 0.45164546370506287, 0.47738319635391235, 0.27378979325294495, 0.11185523867607117, 0.10616005957126617, 0.05001380667090416, 0.07917379587888718, 0.11877880990505219]
It was first suggested by Leacock et al (1998).	[49, 249, 63, 277, 157, 83, 52, 279, 213, 47]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6175951957702637, 0.3067236542701721, 0.05406598746776581, 0.051727280020713806, 0.0539512112736702, 0.05072460696101189, 0.05214538052678108, 0.05278197303414345, 0.22457954287528992, 0.3451816141605377]
It was noted by Dyer et al (2008) that the standard distance-based reordering model needs to be redefined for lattice input.	[156, 22, 3, 84, 155, 145, 76, 37, 6, 68]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1481381505727768, 0.057660605758428574, 0.08960781246423721, 0.1244879737496376, 0.17741070687770844, 0.1674160361289978, 0.08963432163000107, 0.12626956403255463, 0.05474705994129181, 0.1483313888311386]
JACY (Siegel and Bender, 2002) is a hand-crafted Japanese HPSG grammar that provides semantic information as well as linguistically motivated analysis of complex constructions.	[66, 194, 19, 110, 90, 1, 29, 9, 115, 53]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6560553908348083, 0.5922412872314453, 0.2412678748369217, 0.20340469479560852, 0.17312325537204742, 0.16809257864952087, 0.4709671139717102, 0.08411777764558792, 0.19867326319217682, 0.3923225998878479]
Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments.	[13, 62, 5, 21, 88, 23, 31, 63, 4, 8]	[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.5185626745223999, 0.3959062695503235, 0.5457033514976501, 0.513077974319458, 0.08130079507827759, 0.42034584283828735, 0.09855041652917862, 0.2231573909521103, 0.049776140600442886, 0.06785313040018082]
Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings.	[4, 96, 32, 73, 74, 101, 190, 54, 185, 55]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5290804505348206, 0.49865689873695374, 0.2985798418521881, 0.06216087564826012, 0.06826921552419662, 0.07797832041978836, 0.1323399394750595, 0.08716341108083725, 0.245973601937294, 0.09601062536239624]
Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing.	[8, 5, 20, 117, 120, 39, 3, 75, 112, 114]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6376508474349976, 0.3717140853404999, 0.2655103802680969, 0.26951953768730164, 0.1360585242509842, 0.42889872193336487, 0.16123263537883759, 0.05321814864873886, 0.07464154064655304, 0.09338445961475372]
Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees.	[8, 0, 20, 36, 70, 18, 22, 114, 19, 23]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7208719849586487, 0.7445322871208191, 0.08223028481006622, 0.32657280564308167, 0.23593270778656006, 0.05908595398068428, 0.29604262113571167, 0.5552631616592407, 0.06629300117492676, 0.23211947083473206]
Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting.	[40, 33, 19, 31, 179, 23, 16, 122, 2, 53]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49602779746055603, 0.33627039194107056, 0.1406671553850174, 0.04808888956904411, 0.15524940192699432, 0.048709508031606674, 0.04374682903289795, 0.0460478812456131, 0.11505787074565887, 0.06981067359447479]
Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language.	[19, 18, 12, 20, 45, 93, 123, 41, 6, 87]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5497488975524902, 0.6704520583152771, 0.2403828650712967, 0.3330383598804474, 0.05581254884600639, 0.09616154432296753, 0.45088544487953186, 0.4590824544429779, 0.08766234666109085, 0.1849813163280487]
Johnson et al, (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher's exact test.	[61, 117, 80, 79, 69, 102, 48, 1, 203, 9]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6172536611557007, 0.583156406879425, 0.24419863522052765, 0.3522517681121826, 0.3630112111568451, 0.1250629872083664, 0.15893305838108063, 0.09178728610277176, 0.08265989273786545, 0.07506056874990463]
Just one system adopts the FullBrevity approach of Dale (1989), while the majority (11 systems) adopt the Dale and Reiterconvention of always adding TYPE.	[10, 28, 97, 4, 5, 8, 3, 84, 7, 85]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4530877470970154, 0.5391671657562256, 0.07964805513620377, 0.234463632106781, 0.0600825697183609, 0.07731503993272781, 0.06049509346485138, 0.058174535632133484, 0.14539922773838043, 0.05295157432556152]
Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms.	[3, 21, 124, 105, 22, 51, 48, 139, 135, 97]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7964628338813782, 0.7095115184783936, 0.3790912926197052, 0.17578120529651642, 0.5772000551223755, 0.3591561019420624, 0.3131040930747986, 0.07332058250904083, 0.0935056209564209, 0.23663829267024994]
Kaplan et al (1989) present a framework for translation based on the description and correspondence concepts of Lexical Functional Grammar (Kaplan and Bresnan, 1982).	[22, 23, 6, 17, 39, 65, 9, 85, 45, 104]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6905132532119751, 0.6184261441230774, 0.40708377957344055, 0.11483459919691086, 0.1501602977514267, 0.08307711780071259, 0.20172350108623505, 0.08812499046325684, 0.0768912211060524, 0.14295661449432373]
Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed.	[5, 33, 118, 18, 119, 36, 116, 95, 167, 34]	[1, 0, 0, 0, 1, 0, 0, 0, 1, 0]	[0.7996048331260681, 0.43386411666870117, 0.2571355104446411, 0.13581930100917816, 0.5581099390983582, 0.32459187507629395, 0.3575803339481354, 0.4417884647846222, 0.6897751688957214, 0.42234861850738525]
Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy.	[94, 10, 11, 63, 51, 2, 24, 141, 64, 38]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6614671945571899, 0.16297291219234467, 0.13970384001731873, 0.23240499198436737, 0.17286354303359985, 0.1681451052427292, 0.1681451052427292, 0.11839570105075836, 0.05668783560395241, 0.17470437288284302]
Kessler et al (1997) mention that parsing and word-sense disambiguation can also benefit from genre classification.	[10, 34, 37, 8, 130, 18, 54, 73, 33, 180]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2763770818710327, 0.6816771030426025, 0.2006109207868576, 0.1350492238998413, 0.07733282446861267, 0.12035603821277618, 0.11030764877796173, 0.11528050154447556, 0.0644877478480339, 0.06197366863489151]
Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing.	[12, 200, 26, 20, 192, 0, 1, 2, 39, 61]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3707585632801056, 0.39427605271339417, 0.15702734887599945, 0.17513854801654816, 0.2422703504562378, 0.47898975014686584, 0.2318163365125656, 0.07697586715221405, 0.31287550926208496, 0.053549591451883316]
Klein et al (2003) also applied the related Conditional Markov Models for combining classifiers.	[65, 20, 15, 2, 55, 14, 0, 83, 7, 64]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.4657239615917206, 0.48384296894073486, 0.22416098415851593, 0.09536589682102203, 0.0782337561249733, 0.06396575272083282, 0.5536472797393799, 0.06117688864469528, 0.1354159116744995, 0.047744378447532654]
Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds.	[23, 83, 17, 191, 94, 82, 104, 120, 109, 89]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47208911180496216, 0.4623814821243286, 0.36833634972572327, 0.33692291378974915, 0.2076721042394638, 0.15511128306388855, 0.15675102174282074, 0.43209394812583923, 0.08957743644714355, 0.09868544340133667]
Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)).	[2, 74, 88, 171, 170, 9, 17, 161, 16, 147]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7231831550598145, 0.7409256100654602, 0.29656070470809937, 0.05758722126483917, 0.09381581097841263, 0.15765085816383362, 0.0443604551255703, 0.1734561175107956, 0.22715121507644653, 0.06872529536485672]
Kobayashi et al (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation.	[0, 30, 2, 4, 17, 28, 29, 25, 27, 14]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5543302893638611, 0.6688265204429626, 0.24323497712612152, 0.26727408170700073, 0.09605655819177628, 0.12491551041603088, 0.06414259970188141, 0.05595307797193527, 0.05074981972575188, 0.0673002228140831]
Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers.	[163, 157, 49, 107, 168, 155, 171, 162, 169, 7]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7464540004730225, 0.3131710886955261, 0.38314908742904663, 0.16211161017417908, 0.425987184047699, 0.09623071551322937, 0.34963953495025635, 0.08518549054861069, 0.07046981900930405, 0.12490175664424896]
Kooand Collins (2010) propose a third-order graph based parser.	[49, 171, 175, 167, 130, 81, 0, 26, 124, 80]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.5426867604255676, 0.4650638997554779, 0.3770619034767151, 0.341007798910141, 0.11920743435621262, 0.30199137330055237, 0.7037204504013062, 0.1193910762667656, 0.14171889424324036, 0.13543669879436493]
Koomen et al (2005) combined several SRL outputs using ILP method.	[51, 40, 34, 6, 9, 48, 4, 5, 65, 53]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6443374752998352, 0.27681636810302734, 0.10645654797554016, 0.0974031612277031, 0.10905331373214722, 0.07308543473482132, 0.08250852674245834, 0.06930725276470184, 0.06930725276470184, 0.07518885284662247]
Koomen et al (2005) used a 2 layer architecture similar to ours.	[31, 15, 3, 28, 4, 63, 9, 17, 7, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21610689163208008, 0.2502615749835968, 0.06367805600166321, 0.06698878109455109, 0.04732299968600273, 0.06367805600166321, 0.041369833052158356, 0.06159365177154541, 0.0543859601020813, 0.059233684092760086]
Kozareva et al (2008) test their approach on relatively simple and objective categories like states, countries (both closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to a general category, like food, and to a stereotypical property, like sweet (Veale and Hao, 2007).	[132, 130, 182, 82, 83, 131, 165, 160, 151, 66]	[1, 1, 0, 0, 1, 1, 0, 0, 0, 0]	[0.7028313279151917, 0.612485408782959, 0.3209889233112335, 0.26729825139045715, 0.5666759014129639, 0.5500642657279968, 0.43492621183395386, 0.4487212300300598, 0.08252774924039841, 0.05739887058734894]
Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000).	[57, 1, 33, 167, 21, 143, 61, 156, 105, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1888362020254135, 0.11335495859384537, 0.06219670549035072, 0.0810810849070549, 0.04635756462812424, 0.1996213048696518, 0.09113837033510208, 0.05203070864081383, 0.08099644631147385, 0.05851374939084053]
Kudo and Matsumoto (Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task (Sang and Buchholz, 2000).	[1, 48, 23, 5, 20, 3, 6, 24, 2, 38]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5006943941116333, 0.5834642052650452, 0.11822134256362915, 0.4208501875400543, 0.09028899669647217, 0.3199145197868347, 0.17436331510543823, 0.27620410919189453, 0.3535878360271454, 0.09562662243843079]
Kudo et al (2004) use SVMs to morphologically tag Japanese.	[12, 28, 84, 81, 72, 46, 96, 30, 8, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3549829125404358, 0.16561006009578705, 0.35625797510147095, 0.21243514120578766, 0.2832052409648895, 0.07955943793058395, 0.054606229066848755, 0.07624455541372299, 0.08088521659374237, 0.1439862847328186]
Kurohashi and Nagao (1994) used a similar data structure in their rule-based method.	[276, 37, 0, 293, 250, 45, 215, 8, 19, 303]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.643510639667511, 0.7253742218017578, 0.7824391722679138, 0.08679500967264175, 0.10968530923128128, 0.08211216330528259, 0.04342920333147049, 0.19402393698692322, 0.19402393698692322, 0.045975212007761]
Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements.	[160, 176, 10, 38, 183, 142, 105, 42, 203, 162]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47619643807411194, 0.05621970444917679, 0.06566576659679413, 0.14848355948925018, 0.13663838803768158, 0.0407903678715229, 0.05714879930019379, 0.12780901789665222, 0.04715564474463463, 0.16550202667713165]
L&L demonstrated with a system called RAP that a (manually-tuned) weight-based scheme for integrating pronoun interpretation preferences can achieve high performance on real data, in their case, 86% accuracy on a corpus of computer training manuals. Dagan et al (1995) then developed a post processor based on predicate-argument statistics that was used to override RAP's decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in per Kennedy and Boguraev (1996, henceforth, K&B) adapted L&L's algorithm to rely on far less syntactic analysis (noun phrase identification and rudimentary grammatical role marking), with performance in the 75% range on mixed genres.	[144, 7, 24, 26, 19, 143, 16, 44, 9, 162]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4018319249153137, 0.12076651304960251, 0.11641533672809601, 0.08087225258350372, 0.08157018572092056, 0.16945861279964447, 0.1872386932373047, 0.05318904668092728, 0.1033826693892479, 0.08848952502012253]
LLW stands for Levenshtein with learned weights, which is a modification of RY proposed by Mann and Yarowsky (2001).	[128, 129, 124, 118, 125, 115, 35, 36, 156, 157]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3516150414943695, 0.31267160177230835, 0.2145567089319229, 0.3625444173812866, 0.16950318217277527, 0.23133456707000732, 0.0528266541659832, 0.06688077002763748, 0.0528266541659832, 0.06688077002763748]
Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus.	[41, 97, 45, 54, 29, 13, 93, 58, 3, 64]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39045292139053345, 0.12210338562726974, 0.19004395604133606, 0.08863669633865356, 0.10008244961500168, 0.23721376061439514, 0.06249751150608063, 0.10611441731452942, 0.05991397425532341, 0.07448253780603409]
Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj).	[168, 190, 101, 208, 175, 142, 136, 56, 189, 45]	[1, 1, 0, 1, 1, 1, 0, 0, 0, 0]	[0.6824017763137817, 0.7044861912727356, 0.3310708999633789, 0.5835576057434082, 0.5503306984901428, 0.5520265698432922, 0.05439208075404167, 0.10700530558824539, 0.288432240486145, 0.36524200439453125]
Lappin and Leass (1994), for example, use several heuristics to filter out expletive pronouns, including a check for patterns including modal adjectives.	[67, 179, 39, 173, 286, 341, 212, 289, 263, 82]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5916785001754761, 0.6434733271598816, 0.23448769748210907, 0.5025066137313843, 0.1458768993616104, 0.06265538185834885, 0.15529517829418182, 0.43476879596710205, 0.06412523239850998, 0.09083489328622818]
Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system.	[70, 185, 47, 60, 184, 169, 210, 97, 211, 202]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13575150072574615, 0.10140401870012283, 0.05757983401417732, 0.08499908447265625, 0.07060495764017105, 0.1393825262784958, 0.0821135863661766, 0.10909286886453629, 0.052387308329343796, 0.05943313613533974]
Lauer (1995) compared a dependency model with adjacency models, and found that the dependency model is better.	[137, 69, 154, 149, 18, 128, 178, 146, 143, 135]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6246874928474426, 0.08523911237716675, 0.31760531663894653, 0.4799198806285858, 0.24247582256793976, 0.17838338017463684, 0.24247582256793976, 0.307284951210022, 0.1573449820280075, 0.1646309643983841]
Let us start by setting the syntactic relation that we want to focus on for the purposes of this study: following Guevara (2010) and Baroni and Zamparelli (2010), I model the semantic composition of adjacent Adjective-Noun pairs expressing attributive modification of a nominal head.	[15, 108, 2, 17, 99, 0, 51, 25, 43, 30]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.3509752154350281, 0.23389005661010742, 0.4037860333919525, 0.11409188061952591, 0.4680250585079193, 0.823904275894165, 0.10381481796503067, 0.1080634668469429, 0.07195577025413513, 0.07525475323200226]
Lex PageRank (Erkan and Radev, 2004) is one of such methods.	[115, 71, 52, 143, 135, 0, 3, 75, 53, 12]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5122741460800171, 0.52606600522995, 0.3735436201095581, 0.5707972645759583, 0.15072128176689148, 0.47239235043525696, 0.12380489706993103, 0.16338275372982025, 0.29351502656936646, 0.09567940980195999]
Lexical knowledge (for unknown words) and the word lemma (for known words) provide, w.h.p, one sided error (Mikheev, 1997).	[241, 225, 52, 12, 264, 222, 92, 133, 276, 17]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5899980068206787, 0.6280853152275085, 0.4256853759288788, 0.3439736068248749, 0.2849563956260681, 0.1572820246219635, 0.091898612678051, 0.07813016325235367, 0.43260878324508667, 0.16079743206501007]
Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).	[52, 169, 12, 185, 104, 48, 9, 101, 211, 41]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1047440767288208, 0.07164855301380157, 0.08970292657613754, 0.08100276440382004, 0.17686225473880768, 0.10021871328353882, 0.06386729329824448, 0.05468989908695221, 0.05624472349882126, 0.07672256976366043]
Li et al (2004) propose a letter-to-letter n-gram transliteration model for Chinese-English transliteration in an attempt to allow for the encoding of more contextual information.	[156, 56, 55, 120, 171, 58, 148, 47, 67, 66]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2527327239513397, 0.3374960422515869, 0.13657742738723755, 0.07972096651792526, 0.19537319242954254, 0.1413017362356186, 0.13266220688819885, 0.10138135403394699, 0.08430854976177216, 0.11403451859951019]
Li et al (2012) and Bohnet and Nivre (2012) use joint models for POS tagging and dependency parsing, significantly outperforming their pipeline counterparts.	[160, 12, 15, 2, 0, 10, 135, 1, 14, 165]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.5476791262626648, 0.5512074828147888, 0.15589499473571777, 0.30770349502563477, 0.4474599063396454, 0.5449765920639038, 0.11597059667110443, 0.06121555715799332, 0.0671880692243576, 0.07237929850816727]
Like (Marton and Resnik, 2008), we find that the XP+feature obtains a significant improvement of 1.08 BLEU over the baseline.	[80, 81, 97, 82, 121, 98, 101, 41, 94, 95]	[1, 1, 1, 0, 1, 0, 0, 1, 0, 0]	[0.7290809154510498, 0.6286323666572571, 0.7111297845840454, 0.2708212733268738, 0.539211094379425, 0.3337370455265045, 0.4853053689002991, 0.6306103467941284, 0.25122395157814026, 0.30100953578948975]
Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w|s), where w is an NL sentence and s is a world state containing a set of possible MR logical forms that can be matched to w.	[69, 118, 27, 84, 30, 28, 102, 90, 4, 108]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31889575719833374, 0.3171824514865875, 0.28868788480758667, 0.05828843265771866, 0.10432780534029007, 0.06781808286905289, 0.07741495221853256, 0.07876218110322952, 0.10797077417373657, 0.0812976211309433]
Like McCarthy et al (2004) we use k= 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998).	[3, 2, 63, 32, 56, 31, 97, 37, 50, 75]	[1, 0, 0, 1, 0, 0, 0, 1, 0, 0]	[0.5174021124839783, 0.3250387907028198, 0.38932541012763977, 0.5477615594863892, 0.45974355936050415, 0.1002693772315979, 0.12783126533031464, 0.5882337093353271, 0.1156420037150383, 0.3145897388458252]
Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.	[23, 76, 28, 119, 68, 52, 34, 98, 135, 37]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3065670430660248, 0.23380590975284576, 0.227609783411026, 0.328134685754776, 0.12414812296628952, 0.18469741940498352, 0.06276001781225204, 0.06487783789634705, 0.0692301020026207, 0.10170360654592514]
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples.	[72, 13, 40, 27, 39, 75, 83, 29, 36, 11]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.560155987739563, 0.6102614998817444, 0.6064011454582214, 0.2767035961151123, 0.05670066922903061, 0.3478679060935974, 0.09750676155090332, 0.05968675762414932, 0.12459426373243332, 0.04980563372373581]
Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data.	[37, 157, 115, 233, 212, 177, 149, 4, 173, 196]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7699393630027771, 0.6545093059539795, 0.12876510620117188, 0.18361088633537292, 0.2876262664794922, 0.17111851274967194, 0.06886418163776398, 0.05314641445875168, 0.07479295879602432, 0.12620066106319427]
Lu et al (2008) introduced a generative semantic parsing model using a hybrid-tree framework.	[0, 99, 175, 47, 25, 58, 70, 64, 54, 12]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6704172492027283, 0.3569479286670685, 0.11896105110645294, 0.09111440926790237, 0.054395418614149094, 0.0618332102894783, 0.07304824143648148, 0.05459035560488701, 0.05292563512921333, 0.056874051690101624]
MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)).	[25, 3, 12, 2, 29, 23, 121, 115, 58, 65]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.5047738552093506, 0.04751672223210335, 0.20404864847660065, 0.08708545565605164, 0.30917930603027344, 0.5733397603034973, 0.05146905034780502, 0.050265807658433914, 0.04528849944472313, 0.056828293949365616]
Macherey et al (2008) use methods from computational geometry to compute the upper envelope.	[46, 137, 60, 33, 187, 67, 89, 38, 189, 6]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5378175973892212, 0.2772737443447113, 0.20248009264469147, 0.0628465935587883, 0.20479953289031982, 0.10378917306661606, 0.09235220402479172, 0.04866573214530945, 0.05282343178987503, 0.047008778899908066]
Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents.	[67, 22, 59, 96, 11, 62, 10, 14, 168, 30]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2199317067861557, 0.4881938695907593, 0.1449945867061615, 0.28130635619163513, 0.08019399642944336, 0.13571836054325104, 0.08745132386684418, 0.07527344673871994, 0.1630028933286667, 0.049445003271102905]
Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality.	[169, 157, 32, 192, 166, 43, 199, 78, 73, 49]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6943687796592712, 0.3665788173675537, 0.1464102566242218, 0.27720165252685547, 0.05574614554643631, 0.06652900576591492, 0.28995466232299805, 0.28442445397377014, 0.057482361793518066, 0.09669096767902374]
Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous in stance of that same token in a previous sentence of the same document.	[60, 33, 4, 27, 47, 66, 18, 51, 17, 48]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.373217910528183, 0.5211037397384644, 0.2600039839744568, 0.048721104860305786, 0.260497510433197, 0.08694534748792648, 0.07800848037004471, 0.109052874147892, 0.08352772146463394, 0.15981751680374146]
Mani and Wilson (2000) and Ahn et al (2005b) also perform limited semantic class disambiguation.	[152, 18, 142, 5, 3, 149, 13, 11, 134, 71]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07219219952821732, 0.07217761874198914, 0.14577580988407135, 0.0728723555803299, 0.057899486273527145, 0.15069052577018738, 0.05084628239274025, 0.1037587970495224, 0.04757922887802124, 0.09639311581850052]
Mani and Wilson (2000) worked on news and introduced an annotation scheme for temporal expressions, and a method for using explicit temporal expressions to assign activity times to the entirety of an article.	[1, 10, 149, 4, 30, 141, 24, 38, 142, 63]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7701765894889832, 0.6770914793014526, 0.5397208333015442, 0.22162090241909027, 0.18668735027313232, 0.2987756133079529, 0.20478078722953796, 0.057912677526474, 0.15631063282489777, 0.19329005479812622]
Mann and Yarowsky (2001) present a method for inducing translation lexicons based on trasduction modules of cognate pairs via bridge languages.	[52, 173, 0, 45, 56, 166, 28, 177, 55, 176]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6131138801574707, 0.6131138801574707, 0.8102789521217346, 0.314402312040329, 0.3626282811164856, 0.314402312040329, 0.3342048227787018, 0.3626282811164856, 0.21216735243797302, 0.21216735243797302]
Mansour et al (2007) combine a lexicon-based tagger (such as MorphTagger (Bar-Haim et al, 2005)), and a character-based tagger (such as the data-driven ArabicSVM (Diab et al, 2004)), which includes character features as part of its classification model, in order to extend the set of analyses suggested by the analyzer.	[43, 24, 92, 36, 40, 4, 46, 79, 12, 35]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4901720881462097, 0.05979553982615471, 0.08349711447954178, 0.1865820586681366, 0.1914711892604828, 0.06614672392606735, 0.08581733703613281, 0.15794730186462402, 0.05924926698207855, 0.051052823662757874]
Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshawand Marcus, 1995), but determining sub-NP structure is rarely addressed.	[8, 22, 25, 47, 26, 11, 24, 30, 34, 28]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6906409859657288, 0.35312220454216003, 0.20093579590320587, 0.4164021611213684, 0.3553493916988373, 0.14656929671764374, 0.41598212718963623, 0.3179158568382263, 0.05857028067111969, 0.11662501841783524]
Many different feature functions were explored in (Och et al, 2004).	[84, 207, 3, 2, 37, 11, 112, 8, 45, 30]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7653430104255676, 0.08888810127973557, 0.1425403654575348, 0.10464286059141159, 0.08647871762514114, 0.10885923355817795, 0.26961031556129456, 0.05415581166744232, 0.05064142495393753, 0.22029615938663483]
Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.	[113, 114, 63, 76, 10, 25, 95, 111, 117, 156]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2362414300441742, 0.5351759791374207, 0.0633154883980751, 0.11385360360145569, 0.0746936947107315, 0.06661295145750046, 0.2635079026222229, 0.16448961198329926, 0.07069002091884613, 0.062034185975790024]
Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation (Tran et al, 1996), sense disambiguation in speech synthesis (Yarowsky, 1995), and relation tagging in information retrieval (Marsh and Perzanowski, 1999).	[13, 166, 6, 87, 10, 165, 104, 21, 37, 187]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11612904816865921, 0.0713641420006752, 0.23553749918937683, 0.05517006292939186, 0.23927804827690125, 0.07618358731269836, 0.06092581897974014, 0.19046328961849213, 0.13338811695575714, 0.2153518795967102]
Many relationship classification methods utilize some language-dependent preprocessing, like deep or shallow parsing, part of speech tagging and 228 named entity annotation (Pantel et al, 2004).	[11, 88, 36, 5, 218, 9, 1, 138, 82, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29898738861083984, 0.0942106693983078, 0.21204450726509094, 0.4458538293838501, 0.04750029742717743, 0.0667729303240776, 0.05026692524552345, 0.04953983053565025, 0.18332040309906006, 0.06118084862828255]
Many studies in natural language processing are concerned with how to generate definite descriptions that evoke a discourse entity already introduced in the context. A solution to this problem has been initially proposed by Dale (1989) in terms of distinguishing descriptions and distin guishable entities.	[61, 5, 8, 67, 2, 69, 4, 60, 41, 3]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5253250002861023, 0.5685675144195557, 0.1254473179578781, 0.09112125635147095, 0.36463794112205505, 0.061682943254709244, 0.2311318814754486, 0.06326596438884735, 0.05815192684531212, 0.06973531097173691]
Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper.	[24, 55, 0, 69, 1, 20, 22, 2, 135, 255]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36063826084136963, 0.10381520539522171, 0.26313093304634094, 0.3048535883426666, 0.11042231321334839, 0.07102907449007034, 0.1762574017047882, 0.0911707654595375, 0.213465616106987, 0.213465616106987]
Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs.	[133, 3, 14, 19, 124, 22, 45, 36, 43, 117]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4481028914451599, 0.1708347201347351, 0.0958641842007637, 0.3445450961589813, 0.3764115571975708, 0.1147252544760704, 0.1889999508857727, 0.07899704575538635, 0.08255527913570404, 0.09238675981760025]
Matsuzaki et al (2005) introduced a model for such learning: PCFG-LA.	[20, 10, 1, 3, 13, 136, 66, 4, 61, 114]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6344821453094482, 0.4923975169658661, 0.38406428694725037, 0.16214203834533691, 0.10300752520561218, 0.23538818955421448, 0.22962446510791779, 0.09150063246488571, 0.0731198713183403, 0.0834893062710762]
Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al, 1996),.	[31, 304, 244, 289, 341, 77, 208, 18, 303, 179]	[1, 0, 0, 1, 0, 0, 0, 1, 0, 0]	[0.7010572552680969, 0.1363954395055771, 0.10572998970746994, 0.595024824142456, 0.07712922245264053, 0.05336257070302963, 0.2026200145483017, 0.6966591477394104, 0.07912980020046234, 0.07675305753946304]
McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.	[1, 104, 49, 24, 201, 35, 55, 12, 45, 54]	[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.373566597700119, 0.6711628437042236, 0.23924338817596436, 0.21810708940029144, 0.2973676919937134, 0.4193418622016907, 0.19661380350589752, 0.5174437761306763, 0.08644059300422668, 0.08857554942369461]
McDonald et al (2007) also dealt with sentiment analysis, via the global joint-structural approach.	[18, 52, 30, 26, 158, 32, 0, 22, 39, 38]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.4143711030483246, 0.3785931468009949, 0.3196755647659302, 0.13314160704612732, 0.3163462281227112, 0.08323965221643448, 0.6692098379135132, 0.4284490942955017, 0.10091250389814377, 0.07321640849113464]
Melamed (1997b) measures the semantic entropy of words using bi texts.	[139, 46, 143, 141, 40, 124, 28, 145, 149, 55]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5306411981582642, 0.3366086483001709, 0.11697179824113846, 0.06533107161521912, 0.4903028905391693, 0.4559333920478821, 0.049339659512043, 0.09689650684595108, 0.04857653006911278, 0.14811401069164276]
Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al, 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (Kubler and Zhekova, 2009).	[1, 78, 185, 6, 2, 0, 145, 12, 10, 7]	[1, 0, 0, 0, 1, 1, 0, 0, 0, 0]	[0.5788875818252563, 0.21819713711738586, 0.22255326807498932, 0.22747938334941864, 0.5332229733467102, 0.5249171257019043, 0.12752291560173035, 0.16081225872039795, 0.23296010494232178, 0.10929587483406067]
Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulteim Walde, 2006 for German semantic verb classes.	[510, 570, 25, 42, 554, 81, 29, 87, 66, 88]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13992154598236084, 0.1325838416814804, 0.24053356051445007, 0.2916729748249054, 0.14734356105327606, 0.13032880425453186, 0.09332098811864853, 0.08562920242547989, 0.17955736815929413, 0.1248299703001976]
MindNet (Richardson et al, 1998) is both an extraction methodology and a lexical ontology different from a word net since it was created automatically from a dictionary and its structure is based on such resources.	[47, 1, 54, 29, 40, 4, 3, 25, 92, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3979601263999939, 0.22371190786361694, 0.12391863018274307, 0.17170363664627075, 0.11610583961009979, 0.1993802785873413, 0.21197909116744995, 0.16535918414592743, 0.14768269658088684, 0.21216069161891937]
Model Overall Unknown Word Accuracy Accuracy Baseline, 96.72% 84.5% J Ratnaparkhi 96.63% 85.56% (1996) Table 3 Baseline model performance This table also shows the results reported in Ratnaparkhi (1996: 142) for Convenience.	[91, 65, 63, 8, 50, 104, 83, 87, 102, 100]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5794230103492737, 0.49471691250801086, 0.25147587060928345, 0.3199460804462433, 0.27899089455604553, 0.11735601723194122, 0.16545145213603973, 0.443143367767334, 0.38776567578315735, 0.177786186337471]
Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly.	[40, 526, 356, 551, 546, 55, 501, 185, 537, 31]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7422501444816589, 0.3811381459236145, 0.4775092899799347, 0.34703919291496277, 0.3507099747657776, 0.26863572001457214, 0.18235719203948975, 0.1124507486820221, 0.09116916358470917, 0.14983810484409332]
Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011).	[2, 128, 11, 117, 116, 133, 31, 138, 16, 105]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6625122427940369, 0.7374390959739685, 0.17427803575992584, 0.46848851442337036, 0.5798748135566711, 0.24101632833480835, 0.12420572340488434, 0.18765868246555328, 0.05380126088857651, 0.048549022525548935]
Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types.	[30, 89, 160, 79, 122, 62, 153, 123, 12, 80]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.43369659781455994, 0.3862263858318329, 0.08166808634996414, 0.09362634271383286, 0.09020069986581802, 0.2561591863632202, 0.5207411050796509, 0.05929005891084671, 0.06302407383918762, 0.17073729634284973]
Moore (2005) proposes a similar framework, but with more features and a different search method.	[20, 31, 3, 130, 118, 117, 146, 32, 95, 29]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4964538812637329, 0.06116476655006409, 0.04690571874380112, 0.2648923993110657, 0.0661085844039917, 0.052032649517059326, 0.19128479063510895, 0.34996944665908813, 0.13500167429447174, 0.12054382264614105]
More details on the probability model used by ProAlign are available in (Cherry and Lin, 2003).	[17, 65, 118, 0, 15, 92, 164, 51, 35, 125]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.12985241413116455, 0.14190275967121124, 0.051811665296554565, 0.8170925378799438, 0.4175148904323578, 0.288263201713562, 0.4419618248939514, 0.1667940616607666, 0.04360531270503998, 0.29598504304885864]
More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009).	[117, 181, 212, 17, 13, 213, 109, 179, 123, 192]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.5577413439750671, 0.6207290291786194, 0.21063114702701569, 0.39669081568717957, 0.5417909026145935, 0.059688128530979156, 0.10120807588100433, 0.21798546612262726, 0.18466567993164062, 0.11978840827941895]
More linguistic knowledge has been explored by Hulth (2003).	[0, 49, 2, 191, 44, 4, 52, 183, 176, 125]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7460491061210632, 0.6831408739089966, 0.2788269519805908, 0.3908478915691376, 0.049199312925338745, 0.06137996166944504, 0.2946559190750122, 0.13100485503673553, 0.041401736438274384, 0.04307795315980911]
More recent examples of similar techniques include the Resolver system (Yates and Etzioni, 2009) and Poon and Domingos's USP system (Poon and Domingos, 2009).	[147, 210, 189, 67, 53, 218, 191, 229, 217, 219]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6875807046890259, 0.2622823715209961, 0.06922697275876999, 0.1384967416524887, 0.45052117109298706, 0.12701360881328583, 0.38095271587371826, 0.0868743434548378, 0.08818566799163818, 0.2615690529346466]
More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus.	[197, 3, 209, 98, 7, 5, 165, 194, 195, 43]	[1, 1, 0, 1, 0, 1, 0, 0, 1, 0]	[0.8059689402580261, 0.7145994901657104, 0.21434588730335236, 0.7419219017028809, 0.3674370348453522, 0.5255556106567383, 0.17822058498859406, 0.3211943209171295, 0.7334527969360352, 0.1922997683286667]
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word.	[3, 143, 14, 104, 50, 43, 22, 47, 111, 21]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.8006658554077148, 0.557133674621582, 0.3168826103210449, 0.0952155813574791, 0.43302273750305176, 0.10096937417984009, 0.29878872632980347, 0.23976680636405945, 0.06895023584365845, 0.5943800210952759]
More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression.	[14, 80, 164, 169, 168, 178, 171, 76, 184, 30]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5468494892120361, 0.5255803465843201, 0.19486606121063232, 0.11025501042604446, 0.14194093644618988, 0.4185968041419983, 0.2399825155735016, 0.13866807520389557, 0.06623806059360504, 0.1439477950334549]
More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches.	[7, 23, 28, 29, 98, 31, 61, 63, 110, 40]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5662994980812073, 0.06368210911750793, 0.11152984946966171, 0.09767605364322662, 0.07261855900287628, 0.06785399466753006, 0.05635498836636543, 0.05671722814440727, 0.21818764507770538, 0.07694400101900101]
More suitable ways could be bilingual chunk parsing, and refining the bracketing grammar as described in [Wu 1997].	[26, 223, 5, 10, 6, 168, 27, 165, 176, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.348775178194046, 0.25603434443473816, 0.273006409406662, 0.273006409406662, 0.30383530259132385, 0.32357004284858704, 0.2940327227115631, 0.062371354550123215, 0.16405631601810455, 0.4948408305644989]
Moreover, as stressed in previous research, using syntactic dependencies seems to be particularly well suited to coping with the problem of linguistic variation across languages (Hwa et al, 2002).	[106, 31, 29, 47, 32, 95, 58, 33, 35, 18]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5151947140693665, 0.47585219144821167, 0.3266047537326813, 0.06188604608178139, 0.22924299538135529, 0.2730303704738617, 0.1034095287322998, 0.07220761477947235, 0.3339322507381439, 0.12221536040306091]
Moreover, our ranking model is related to reranking (Shen et al, 2004) in SMT as well.	[6, 2, 5, 7, 153, 0, 53, 61, 40, 17]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4806824028491974, 0.11672662198543549, 0.0691382884979248, 0.05156446993350983, 0.2430887073278427, 0.3857787847518921, 0.1954897791147232, 0.30023184418678284, 0.04486830532550812, 0.05315888300538063]
Moreover, the assigned tag applies to the whole blog post while a finer grained sentiment extraction is needed (McDonald et al, 2007).	[6, 23, 35, 38, 72, 8, 5, 42, 20, 25]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4430809020996094, 0.143530935049057, 0.250799298286438, 0.07509328424930573, 0.05295518413186073, 0.07214046269655228, 0.07307570427656174, 0.05256577953696251, 0.05235544592142105, 0.08310579508543015]
"Moreover, the domain is another example of ""found test material"" in the sense of (Hirschman et al, 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA."	[5, 42, 9, 41, 40, 10, 13, 167, 76, 84]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4419485032558441, 0.5635283589363098, 0.3529413938522339, 0.36763864755630493, 0.26613083481788635, 0.09520813077688217, 0.13081049919128418, 0.3357025384902954, 0.07260845601558685, 0.10537964105606079]
Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al (2006).	[82, 62, 7, 135, 114, 33, 13, 18, 137, 119]	[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.48568952083587646, 0.7331464886665344, 0.3928948938846588, 0.3711709678173065, 0.12496204674243927, 0.05302877351641655, 0.12082286924123764, 0.5893279314041138, 0.47874581813812256, 0.10923441499471664]
Munteanu and Marcu (2006) proposed a method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora.	[1, 0, 163, 14, 3, 20, 10, 19, 50, 11]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.7603744864463806, 0.7876257300376892, 0.5191779136657715, 0.39205774664878845, 0.45294564962387085, 0.5696713924407959, 0.3718765377998352, 0.1977318525314331, 0.36209943890571594, 0.29469361901283264]
My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al, 1993), but that other features might improve parsing of other languages or even other English genres.	[43, 584, 588, 78, 5, 13, 398, 58, 8, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31997257471084595, 0.17774271965026855, 0.37766537070274353, 0.1873772293329239, 0.11069151014089584, 0.11069151014089584, 0.06827015429735184, 0.2448168396949768, 0.13508298993110657, 0.13508298993110657]
Navigli (2006) has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource.	[21, 3, 22, 17, 32, 8, 40, 76, 88, 91]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.6566057801246643, 0.5554844737052917, 0.4472828507423401, 0.6316582560539246, 0.5807686448097229, 0.4746180474758148, 0.4639555513858795, 0.3981027603149414, 0.2291833609342575, 0.17663177847862244]
Nearly all existing unification grammars of this kind use either term unification (the kind of unification used in resolution theorem provers, and hence provided as a primitive in PROLOG) or some version of the graph unification proposed by Kay (1985) and Shieber (1984).	[81, 28, 26, 48, 27, 91, 19, 35, 12, 46]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7575485110282898, 0.5402501225471497, 0.27038365602493286, 0.23712722957134247, 0.07116599380970001, 0.09230054914951324, 0.0819491595029831, 0.06830938160419464, 0.07743918150663376, 0.31548556685447693]
Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006).	[40, 41, 112, 117, 61, 103, 39, 94, 43, 2]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15790235996246338, 0.11581505835056305, 0.04672830179333687, 0.045919641852378845, 0.06722613424062729, 0.17657610774040222, 0.05015072226524353, 0.12059983611106873, 0.06316505372524261, 0.06811043620109558]
Nevertheless, these metrics give us a point of comparison with Schone and Jurafsky (2001) who, using a vocabulary of English words occurring at least 10 times in a 6.7 million word newswire corpus, report F1 of 88.1 for conflation sets based only on suffixation, and 84.5 for circumfixation.	[169, 29, 11, 8, 174, 177, 134, 129, 106, 51]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7628132104873657, 0.3968697190284729, 0.45861196517944336, 0.3295131325721741, 0.2509240508079529, 0.0663338303565979, 0.1959647387266159, 0.12032373994588852, 0.06360328197479248, 0.055166568607091904]
Next, the target-specific polarity of adjectives is determined using Hearst-like patterns. Kanayama and Nasukawa (2006) introduce polar atoms: minimal human-understandable syntactic structures that specify polarity of clauses.	[2, 48, 16, 7, 17, 65, 192, 191, 158, 151]	[1, 0, 0, 1, 0, 1, 0, 0, 0, 0]	[0.7931624054908752, 0.46492353081703186, 0.36692604422569275, 0.5675683617591858, 0.4042024314403534, 0.5311521291732788, 0.3127044141292572, 0.15389502048492432, 0.33350443840026855, 0.28141331672668457]
Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs.	[17, 12, 1, 28, 104, 15, 16, 19, 159, 98]	[0, 0, 1, 0, 1, 0, 0, 1, 0, 1]	[0.4919177293777466, 0.3801460266113281, 0.5884748697280884, 0.28821811079978943, 0.5588105320930481, 0.33525583148002625, 0.3386656939983368, 0.541661262512207, 0.2928052246570587, 0.5906775593757629]
Next, we further show that the dependency analysis model of (Kudo and Matsumoto, 2002) applied to the results of identifying compound functional expressions significantly outperforms the one applied to the results without identifying compound functional expressions.	[122, 127, 108, 15, 126, 36, 24, 21, 104, 90]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6554010510444641, 0.6021692752838135, 0.42491614818573, 0.5258112549781799, 0.25185561180114746, 0.10923206806182861, 0.13304367661476135, 0.04814139008522034, 0.1857018917798996, 0.10727788507938385]
Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems.	[134, 108, 35, 36, 0, 39, 74, 41, 98, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4944290518760681, 0.35003402829170227, 0.474468469619751, 0.19006073474884033, 0.16263382136821747, 0.059465035796165466, 0.15441101789474487, 0.14085879921913147, 0.2775338590145111, 0.050159845501184464]
Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN).	[92, 25, 58, 20, 91, 88, 89, 64, 18, 19]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.759353756904602, 0.6095353364944458, 0.5472198128700256, 0.2771144211292267, 0.10797476768493652, 0.20148348808288574, 0.14915451407432556, 0.4682925045490265, 0.13107505440711975, 0.06319019198417664]
Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features.	[12, 31, 110, 18, 25, 99, 9, 97, 2, 42]	[1, 1, 0, 1, 0, 1, 0, 0, 0, 0]	[0.7292665839195251, 0.7103352546691895, 0.4033973813056946, 0.5882167816162109, 0.36614009737968445, 0.6105735898017883, 0.2334021031856537, 0.18878933787345886, 0.0656198188662529, 0.07023382186889648]
Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases.	[64, 3, 131, 85, 32, 68, 86, 113, 144, 45]	[1, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.7660399079322815, 0.21577905118465424, 0.6972127556800842, 0.4371839165687561, 0.18877317011356354, 0.591814398765564, 0.28874415159225464, 0.11547568440437317, 0.11603104323148727, 0.33685415983200073]
Ng et al (2003) address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus.	[24, 16, 34, 37, 1, 6, 25, 142, 148, 0]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.531726598739624, 0.5423775315284729, 0.2676558792591095, 0.10098214447498322, 0.22719745337963104, 0.5074805021286011, 0.16668105125427246, 0.21346858143806458, 0.1449604481458664, 0.371890664100647]
Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser.	[128, 147, 103, 144, 12, 82, 49, 78, 152, 74]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.523686408996582, 0.0984775573015213, 0.32301005721092224, 0.26133954524993896, 0.1399904191493988, 0.058231137692928314, 0.10321498662233353, 0.14043432474136353, 0.13078564405441284, 0.06289748847484589]
NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns.	[32, 3, 8, 31, 49, 171, 45, 17, 181, 47]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5518476963043213, 0.18501365184783936, 0.46012404561042786, 0.1131737157702446, 0.06811758130788803, 0.413613498210907, 0.07502159476280212, 0.14617584645748138, 0.1532459557056427, 0.07145605236291885]
Not all PFAs can be determinized, as discussed by (Mohri, 1997).	[283, 506, 80, 160, 596, 361, 214, 285, 592, 200]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.7648605704307556, 0.7663723826408386, 0.04529383033514023, 0.21925953030586243, 0.04868325963616371, 0.04610090330243111, 0.26909008622169495, 0.26287609338760376, 0.5460039973258972, 0.343820184469223]
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002).	[53, 32, 21, 94, 137, 143, 25, 18, 90, 75]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5684331655502319, 0.07717369496822357, 0.04187937080860138, 0.07627657055854797, 0.05018717050552368, 0.04985338822007179, 0.05585538223385811, 0.05537357181310654, 0.08420352637767792, 0.04866545647382736]
Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007).	[104, 99, 25, 89, 102, 130, 2, 4, 88, 54]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7075971364974976, 0.6523405909538269, 0.14284956455230713, 0.04373953118920326, 0.4376167953014374, 0.3514860272407532, 0.04415102303028107, 0.04829155281186104, 0.04856954514980316, 0.07017035037279129]
Note that the notion of density here is not to be confused with the conceptual density used by Agirre and Rigau (1996), which is essentially a semantic similarity measure by itself.	[183, 53, 8, 161, 193, 0, 32, 238, 44, 41]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23555079102516174, 0.3101005256175995, 0.20189154148101807, 0.16840019822120667, 0.1329168677330017, 0.24260777235031128, 0.04854824021458626, 0.0749029815196991, 0.17883720993995667, 0.07308506965637207]
Note that this use of local contexts is similar to the dynamic features in (Kudo and Matsumoto, 2002) 4.	[136, 61, 127, 123, 125, 121, 64, 137, 135, 42]	[1, 1, 0, 1, 1, 1, 0, 0, 0, 0]	[0.7496919631958008, 0.7095116972923279, 0.04741120710968971, 0.6020402312278748, 0.587060809135437, 0.6186718344688416, 0.41509562730789185, 0.18810181319713593, 0.07140980660915375, 0.04918772354722023]
Now back in machine translation, we do find some work addressing such concern: Frederking and Nirenburg (1994) develop a multi-engine MT or MEMT architecture which operates by combining outputs from three different engines based on the knowledge it has about inner workings of each of the component engines.	[72, 8, 64, 3, 75, 62, 79, 5, 91, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47422072291374207, 0.2830897271633148, 0.11543697118759155, 0.09280788898468018, 0.34721997380256653, 0.09539352357387543, 0.4928126633167267, 0.30252474546432495, 0.06657330691814423, 0.23716849088668823]
Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc.	[103, 102, 5, 13, 73, 2, 82, 0, 1, 22]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6812629699707031, 0.2859441637992859, 0.1388542354106903, 0.25272417068481445, 0.19409723579883575, 0.09976880997419357, 0.05121053382754326, 0.2634930908679962, 0.10270976275205612, 0.18634162843227386]
Obtained percent agreement of 0.988 and coefficient (Carletta, 1996) of 0.975 suggest high convergence of both annotations.	[84, 43, 46, 42, 50, 49, 78, 61, 65, 83]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2256459891796112, 0.46473413705825806, 0.2086380422115326, 0.24342599511146545, 0.3215804696083069, 0.20155756175518036, 0.04470117762684822, 0.12877295911312103, 0.07452493906021118, 0.07393615692853928]
Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words.	[0, 74, 69, 14, 61, 3, 68, 103, 83, 5]	[1, 1, 0, 1, 0, 0, 1, 0, 1, 0]	[0.8088493943214417, 0.7342773675918579, 0.2654755413532257, 0.7374061942100525, 0.0855608806014061, 0.22595825791358948, 0.7134726643562317, 0.2181786298751831, 0.5698602795600891, 0.48265713453292847]
Of particular interest is our prior work Lu et al (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form.	[53, 0, 36, 1, 27, 195, 13, 30, 37, 17]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.5618758201599121, 0.47883182764053345, 0.4387854039669037, 0.2683198153972626, 0.37553176283836365, 0.5538706183433533, 0.08984280377626419, 0.10315416008234024, 0.09097959846258163, 0.40734735131263733]
Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003).	[194, 14, 185, 61, 4, 200, 189, 190, 60, 13]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7386177182197571, 0.742309033870697, 0.25323784351348877, 0.6889210343360901, 0.17131221294403076, 0.15768960118293762, 0.20557419955730438, 0.19829390943050385, 0.22961290180683136, 0.1160445511341095]
Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al, 1998) and others.	[112, 24, 31, 159, 134, 107, 7, 84, 2, 154]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7972090840339661, 0.7007889151573181, 0.16116710007190704, 0.4350171387195587, 0.2684387266635895, 0.08945409953594208, 0.04642365500330925, 0.05464237928390503, 0.13559864461421967, 0.16416044533252716]
Of these models, the most related one to SeededLDA is the Labeled LDA model (Ramage et al 2009).	[26, 13, 192, 37, 30, 181, 19, 191, 31, 3]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7579750418663025, 0.08351708203554153, 0.1985418051481247, 0.37989261746406555, 0.3796496093273163, 0.12383148074150085, 0.05242515727877617, 0.07902383804321289, 0.1264760047197342, 0.11397527158260345]
On V1.1, we use the same training and testing data and capture the same NE classes as (Kazama et al 2002).	[44, 23, 1, 34, 113, 126, 37, 10, 22, 130]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3418329656124115, 0.09501077979803085, 0.14735345542430878, 0.11804332584142685, 0.07546192407608032, 0.0707552507519722, 0.08720506727695465, 0.045353543013334274, 0.050718266516923904, 0.0678941160440445]
On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007).	[124, 8, 1, 154, 180, 7, 11, 157, 83, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.339529424905777, 0.3243827819824219, 0.22371287643909454, 0.16779208183288574, 0.0809478685259819, 0.10888856649398804, 0.09324398636817932, 0.07407868653535843, 0.22513283789157867, 0.051188431680202484]
On another way, an application can combine active learning (Arora et al, 2009) and crowd sourcing, asking non-expertise such as workers of Amazon Mechanical Turk to label crucial alignment links that can improve the system with low cost, which is now a promising methodology in NLP areas (Callison-Burch, 2009).	[164, 13, 170, 31, 2, 27, 168, 22, 42, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4671604633331299, 0.23178689181804657, 0.16351954638957977, 0.09830915927886963, 0.4263451099395752, 0.06145210936665535, 0.08514860272407532, 0.12633663415908813, 0.05144653096795082, 0.11704641580581665]
On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries.	[52, 35, 36, 111, 51, 144, 49, 54, 48, 55]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6833558678627014, 0.1438879370689392, 0.09275828301906586, 0.17271138727664948, 0.14760614931583405, 0.10435663908720016, 0.061050597578287125, 0.06905627250671387, 0.49571287631988525, 0.05645636469125748]
On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006).	[125, 169, 2, 166, 20, 98, 63, 153, 195, 179]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19265985488891602, 0.19667285680770874, 0.1056039035320282, 0.26377177238464355, 0.05564044415950775, 0.10006822645664215, 0.05973999574780464, 0.07535143196582794, 0.059403158724308014, 0.07314454764127731]
On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995).	[0, 182, 22, 33, 9, 169, 55, 92, 104, 71]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7685269117355347, 0.49955490231513977, 0.49955490231513977, 0.22939294576644897, 0.3746223449707031, 0.3746223449707031, 0.273884117603302, 0.31105560064315796, 0.25349318981170654, 0.43267956376075745]
Once they had completed all tasks in sequence using one system, they filled out a questionnaire to assess user satisfaction by rating 8-9 statements, similar to those in (Walker et al, 1997), on a scale of 1-5, where 5 indicated highest satisfaction.	[111, 113, 104, 152, 161, 20, 28, 53, 102, 106]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3443101942539215, 0.28916284441947937, 0.19747227430343628, 0.24355769157409668, 0.05298381671309471, 0.25851303339004517, 0.20806945860385895, 0.07666032761335373, 0.14945881068706512, 0.11776645481586456]
One common approach involves the design of heuristic rules to identify specific types of (non) anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)).	[29, 64, 59, 537, 72, 529, 132, 544, 119, 176]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6750488877296448, 0.27792564034461975, 0.5426058173179626, 0.36822062730789185, 0.37238478660583496, 0.46520087122917175, 0.27454301714897156, 0.06983225792646408, 0.1063867062330246, 0.06359690427780151]
One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003).	[1, 48, 11, 84, 70, 2, 105, 7, 35, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2900794744491577, 0.0948876217007637, 0.2617538571357727, 0.20831400156021118, 0.12462564557790756, 0.25801968574523926, 0.15218757092952728, 0.06548415124416351, 0.06305158883333206, 0.07908052206039429]
One could use ILP-based decoding in the style of Finkel and Manning (2008) and Song et al (2012) to attempt to explicitly find the optimal C with choice of a marginalized out, but we did not explore this option.	[63, 15, 18, 14, 57, 16, 62, 59, 42, 65]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43735048174858093, 0.5522412061691284, 0.1402309238910675, 0.17073582112789154, 0.16927799582481384, 0.15918059647083282, 0.19082006812095642, 0.12092410773038864, 0.16158214211463928, 0.05526099354028702]
One of the earliest attempts at extracting 'interrupted collocations' (i.e. non-contiguous collocations, including VPCs), was that of Smadja (1993).	[610, 161, 572, 210, 280, 396, 2, 12, 134, 481]	[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.25681352615356445, 0.5160871148109436, 0.34277841448783875, 0.052616894245147705, 0.05187106505036354, 0.05553523078560829, 0.41856688261032104, 0.41856688261032104, 0.601146399974823, 0.23871327936649323]
One of the first definitions was suggested by Pereira and Warren (1983).	[112, 160, 163, 74, 75, 130, 17, 76, 19, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07682882994413376, 0.09375777095556259, 0.39649447798728943, 0.08322630077600479, 0.154209166765213, 0.053260788321495056, 0.06771688163280487, 0.05829932540655136, 0.08711499720811844, 0.1548701971769333]
One of the first works in the area of comparable corpora mining was based on word co-occurrence based approach (Rapp, 1995).	[3, 56, 11, 5, 14, 50, 20, 10, 19, 46]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4650333821773529, 0.44908684492111206, 0.05967393144965172, 0.05095392465591431, 0.08770406991243362, 0.13162259757518768, 0.14513470232486725, 0.09949012100696564, 0.10927281528711319, 0.05681455507874489]
One of the most popular alignment tools is m2maligner (Jiampojamarn et al, 2007), which is released as an open source software.	[21, 105, 37, 49, 80, 147, 139, 144, 143, 160]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5489652752876282, 0.08508596569299698, 0.23668594658374786, 0.10701309889554977, 0.09920630604028702, 0.05252472683787346, 0.10048335790634155, 0.07134746015071869, 0.29615548253059387, 0.052464816719293594]
One opossibility is the example-based combiner in Brill and Wu (1998, Sec. 3.2).	[82, 59, 32, 1, 29, 45, 26, 54, 49, 38]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2758021354675293, 0.5411341190338135, 0.137472465634346, 0.043709706515073776, 0.3627204895019531, 0.10511171072721481, 0.4952203631401062, 0.05679741129279137, 0.0604080930352211, 0.047557052224874496]
One reason for this is that there are more of characters than words (Gale and Church, 1991).	[158, 149, 148, 170, 147, 7, 24, 143, 21, 111]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6086122393608093, 0.7048286199569702, 0.7630279660224915, 0.5804805755615234, 0.45121458172798157, 0.1390485018491745, 0.048040151596069336, 0.08376534283161163, 0.05289978161454201, 0.04569626599550247]
One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).	[14, 32, 13, 20, 39, 98, 46, 104, 44, 155]	[0, 0, 1, 0, 0, 0, 0, 0, 1, 0]	[0.3681267499923706, 0.2339267134666443, 0.5141345262527466, 0.164123997092247, 0.26925206184387207, 0.28954505920410156, 0.0853215754032135, 0.06846196204423904, 0.6838497519493103, 0.048166196793317795]
Opinionfinder (Wilson et al, 2005) is a system for mining opinions from text.	[8, 15, 3, 62, 9, 1, 11, 13, 10, 65]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4068695306777954, 0.09055354446172714, 0.14279140532016754, 0.31466585397720337, 0.04225761443376541, 0.16883820295333862, 0.0855059027671814, 0.053051117807626724, 0.303087055683136, 0.0867147371172905]
Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008).	[7, 0, 1, 157, 121, 17, 160, 110, 13, 132]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.4241458773612976, 0.4904504716396332, 0.20696711540222168, 0.14849808812141418, 0.06524039059877396, 0.09275412559509277, 0.523776113986969, 0.06447644531726837, 0.12132242321968079, 0.05103164538741112]
Options for identifying interesting classes include manually created methods (WordNet (Miller et al, 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al, 2006).	[139, 209, 8, 27, 153, 14, 197, 196, 172, 211]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7169468402862549, 0.33817532658576965, 0.11569266021251678, 0.1372581422328949, 0.4032444953918457, 0.07941218465566635, 0.09370043873786926, 0.10713835060596466, 0.08945571631193161, 0.1155025064945221]
Oracle BLEU scores computed over k-best lists (Och et al, 2004) show that many high quality hypotheses are produced by first-pass SMT decoding.	[60, 162, 0, 1, 160, 66, 65, 68, 63, 5]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18621841073036194, 0.289588987827301, 0.1699610948562622, 0.050931721925735474, 0.18735907971858978, 0.12716223299503326, 0.12205929309129715, 0.12029238790273666, 0.33149027824401855, 0.060689181089401245]
Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006).	[82, 163, 81, 156, 127, 25, 80, 85, 2, 7]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5005197525024414, 0.08072973787784576, 0.4345422089099884, 0.06877129524946213, 0.07681551575660706, 0.1597510725259781, 0.07832859456539154, 0.19381141662597656, 0.1167231872677803, 0.16710412502288818]
Other measures have been proposed that utilize the text in WordNet's definitional glosses, such as Extended Lesk (Banerjee and Pedersen, 2003) and later the Gloss Vectors (Patwardhan and Pedersen, 2006) method.	[90, 48, 12, 91, 173, 54, 192, 17, 118, 49]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7778704166412354, 0.2358568161725998, 0.06350504606962204, 0.5035305023193359, 0.2380564957857132, 0.4827136695384979, 0.4098653197288513, 0.1591896414756775, 0.16105687618255615, 0.05753792077302933]
Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.	[5, 15, 1, 100, 75, 24, 77, 134, 144, 23]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.6577783226966858, 0.274649053812027, 0.49869614839553833, 0.24835601449012756, 0.2504817247390747, 0.4638271927833557, 0.1523645520210266, 0.17928989231586456, 0.6943966150283813, 0.19537213444709778]
Other recent approaches use Gibbs sampler for learning the SCFG by exploring a fixed grammar having pre-defined rule templates (Blunsom et al, 2008) or by reasoning over the space of derivations (Blunsom et al, 2009).	[22, 48, 4, 111, 42, 26, 40, 32, 38, 89]	[1, 0, 0, 0, 1, 0, 1, 0, 0, 0]	[0.7620430588722229, 0.36167144775390625, 0.31433039903640747, 0.11952223628759384, 0.6628829836845398, 0.18697193264961243, 0.5574758052825928, 0.17699739336967468, 0.3251184821128845, 0.08272980898618698]
Other work incorporating syntactic and linguistic information into IR includes early research by (Smeaton, O Donnell and Kelledy, 1995), who employed tree structured analytics (TSAs) resembling dependency trees, the use of syntax to detect paraphrases for question answering (QA) (Lin and Pantel, 2001), and semantic role labelling in QA (Shen and Lapata, 2007).	[41, 161, 19, 0, 34, 40, 55, 241, 140, 38]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32923954725265503, 0.3559431731700897, 0.28291159868240356, 0.4906575381755829, 0.19244737923145294, 0.216586172580719, 0.24559661746025085, 0.12785698473453522, 0.14865101873874664, 0.10867778956890106]
Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006).	[1, 7, 35, 3, 9, 18, 64, 305, 33, 48]	[1, 1, 1, 0, 0, 0, 0, 0, 1, 0]	[0.6136050224304199, 0.6136050224304199, 0.643997848033905, 0.4842122793197632, 0.4842122793197632, 0.2842436134815216, 0.31395554542541504, 0.10017649084329605, 0.7253947257995605, 0.3165438771247864]
Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al, 2001).	[36, 151, 87, 154, 45, 14, 12, 30, 71, 9]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.6441038846969604, 0.15437427163124084, 0.06789734214544296, 0.18722018599510193, 0.04994681477546692, 0.05043353512883186, 0.05615939572453499, 0.05989312380552292, 0.050101350992918015, 0.5280356407165527]
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy.	[40, 56, 41, 16, 198, 3, 74, 31, 111, 38]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.44562777876853943, 0.3127184212207794, 0.31771793961524963, 0.5465410947799683, 0.10373646020889282, 0.10670565813779831, 0.48781707882881165, 0.3754724860191345, 0.07127979397773743, 0.049785055220127106]
Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model.	[4, 109, 23, 22, 136, 63, 182, 37, 176, 87]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4129008650779724, 0.5066928267478943, 0.25563478469848633, 0.1113094761967659, 0.5246817469596863, 0.12196315079927444, 0.04702875018119812, 0.05697140842676163, 0.05440027639269829, 0.2804657816886902]
Our POS results, gathered using a Twitter-specific tagger (Gimpel et al, 2011), echo those of Ashok et al (2013) who looked at predict 14 Of course, simply inserting garbage isn't going to lead to more re-tweets, but adding more information generally involves longer text.	[19, 98, 12, 97, 11, 41, 0, 7, 1, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6953574419021606, 0.054722338914871216, 0.1919715702533722, 0.16635437309741974, 0.2422771453857422, 0.2432902306318283, 0.13723601400852203, 0.058993615210056305, 0.08044213056564331, 0.045382123440504074]
Our aggregation technique does, however, presuppose consistency of opinions, in a similar way to Thomas et al (2006).	[122, 114, 112, 20, 139, 17, 131, 41, 21, 35]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6835048794746399, 0.18368728458881378, 0.07632115483283997, 0.05667654797434807, 0.0486345998942852, 0.05264059826731682, 0.13146330416202545, 0.08855116367340088, 0.07024560123682022, 0.04313939809799194]
Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997).	[0, 187, 3, 190, 22, 184, 18, 67, 2, 35]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8176629543304443, 0.4355124235153198, 0.4050929546356201, 0.31728672981262207, 0.20861229300498962, 0.2469906359910965, 0.3668394982814789, 0.15191516280174255, 0.12211059033870697, 0.10152684152126312]
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English.	[105, 13, 3, 130, 144, 25, 111, 106, 49, 67]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3819066882133484, 0.2943717837333679, 0.4860987365245819, 0.2931228280067444, 0.33416885137557983, 0.16081751883029938, 0.10515862703323364, 0.30597299337387085, 0.2053213268518448, 0.06767509132623672]
Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD.	[0, 1, 8, 30, 11, 117, 13, 27, 20, 152]	[1, 1, 0, 1, 0, 1, 0, 1, 0, 0]	[0.7215784788131714, 0.5627921223640442, 0.045435305684804916, 0.682928740978241, 0.4118858873844147, 0.6451664566993713, 0.2500443458557129, 0.5783441066741943, 0.449448823928833, 0.4037032425403595]
Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006).	[129, 0, 26, 9, 16, 86, 154, 172, 171, 11]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7521287798881531, 0.11618256568908691, 0.09284289926290512, 0.08071635663509369, 0.046003684401512146, 0.06916961073875427, 0.37776386737823486, 0.3040335774421692, 0.1959553360939026, 0.05653860792517662]
Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009).	[45, 205, 15, 216, 54, 56, 82, 3, 106, 27]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.537203848361969, 0.053807277232408524, 0.2011909782886505, 0.17236407101154327, 0.21553534269332886, 0.043174102902412415, 0.062017880380153656, 0.05172764137387276, 0.11561215668916702, 0.07397333532571793]
Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006).	[185, 173, 96, 103, 87, 160, 134, 158, 95, 100]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37872961163520813, 0.2675599753856659, 0.35938560962677, 0.4092346429824829, 0.3190409541130066, 0.3090524971485138, 0.1453605592250824, 0.27875539660453796, 0.10156866163015366, 0.054249752312898636]
Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007).	[0, 3, 185, 187, 84, 194, 109, 95, 162, 88]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.811935544013977, 0.07140722870826721, 0.35330599546432495, 0.22223345935344696, 0.09889445453882217, 0.108260877430439, 0.2664549946784973, 0.05999274551868439, 0.05413748323917389, 0.1422353982925415]
Our error correction system implements a correction validation mechanism as proposed in (Gamon et al., 2008).	[13, 1, 165, 48, 181, 175, 168, 0, 44, 187]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17752909660339355, 0.09854338318109512, 0.056647974997758865, 0.08026517927646637, 0.2047291100025177, 0.052634093910455704, 0.10289427638053894, 0.2730289697647095, 0.20979025959968567, 0.1319751888513565]
Our hierarchical systems consist of a syntax-augmented system (SAMT) that includes target-language syntactic categories (Zollmann and Venugopal, 2006) and a Hiero-style system with a single non-terminal (Chiang, 2007).	[12, 1, 56, 3, 11, 27, 5, 25, 44, 47]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6328025460243225, 0.3269402086734772, 0.06435241550207138, 0.16209721565246582, 0.35852864384651184, 0.05714317411184311, 0.05988633632659912, 0.09690289199352264, 0.06875739246606827, 0.06401631981134415]
Our hybrid machine translation system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003), and b) one or several other MT systems, e.g. Moses (Koehn et al, 2007), or Joshua (Li et al., 2009).	[6, 10, 1, 0, 100, 25, 89, 22, 23, 88]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7384769320487976, 0.6861604452133179, 0.0958268940448761, 0.24839596450328827, 0.35601720213890076, 0.09849243611097336, 0.11953719705343246, 0.12953127920627594, 0.05752727389335632, 0.2525864243507385]
Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) .	[28, 110, 107, 108, 23, 38, 44, 114, 113, 118]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5974006652832031, 0.11003097146749496, 0.09659284353256226, 0.07618866115808487, 0.1539497971534729, 0.45818957686424255, 0.08054699003696442, 0.06880778074264526, 0.09679111838340759, 0.2498273253440857]
Our input string representation for a candidate pair is formed by first aligning the source and target words using M2M-aligner (Jiampojamarn et al., 2007).	[11, 78, 144, 147, 51, 121, 97, 9, 26, 160]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08104182034730911, 0.05458589643239975, 0.07864972949028015, 0.10308751463890076, 0.06899137794971466, 0.05202950909733772, 0.05695051699876785, 0.09364956617355347, 0.08765093237161636, 0.07146433740854263]
Our labelled set is then generated from pairs with LCSR 0.58 (using the cutoff from Melamed (1999)).	[166, 68, 165, 421, 29, 132, 74, 5, 16, 308]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5992405414581299, 0.5243868827819824, 0.13683775067329407, 0.048975955694913864, 0.0465332567691803, 0.3081530034542084, 0.05404887720942497, 0.0462232269346714, 0.0462232269346714, 0.05977814644575119]
Our main purely unsupervised results are with a flat clustering (Clark, 2000) that groups words having similar context distributions, according to Kullback Leibler divergence.	[20, 21, 17, 36, 68, 22, 6, 25, 11, 30]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7532903552055359, 0.582318902015686, 0.3107263743877411, 0.3005935549736023, 0.3927414119243622, 0.2604421079158783, 0.2044820487499237, 0.1570703536272049, 0.17297770082950592, 0.3754950761795044]
Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation).	[30, 70, 28, 37, 2, 39, 32, 44, 101, 29]	[1, 1, 1, 0, 0, 0, 0, 0, 1, 1]	[0.6785199046134949, 0.6480461955070496, 0.5811653137207031, 0.16987130045890808, 0.2230221927165985, 0.35192516446113586, 0.23047283291816711, 0.1114250123500824, 0.5321831703186035, 0.5066908597946167]
Our model is inspired by Centering (Grosz et al, 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences.	[32, 0, 35, 218, 9, 52, 10, 44, 6, 46]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38517284393310547, 0.5368325710296631, 0.16023777425289154, 0.13705481588840485, 0.11525067687034607, 0.22116541862487793, 0.43049895763397217, 0.17446617782115936, 0.1325550377368927, 0.3120776414871216]
Our narrative schemas differ slightly from Chambers and Jurafsky (2009).	[146, 63, 39, 128, 226, 56, 159, 54, 137, 118]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6755883693695068, 0.5550613403320312, 0.045637473464012146, 0.35365647077560425, 0.25558823347091675, 0.32162684202194214, 0.17925943434238434, 0.051905009895563126, 0.31215471029281616, 0.16374468803405762]
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009).	[53, 66, 42, 55, 4, 25, 45, 23, 70, 59]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3875768780708313, 0.4320998191833496, 0.3784896731376648, 0.08561842888593674, 0.14765368402004242, 0.06335070729255676, 0.12479303777217865, 0.05894692242145538, 0.2440137267112732, 0.17793291807174683]
Our performance of the single-phase CRF with maximum likelihood training is 69.44%, which agrees with (Settles, 2004) who also uses similar settings.	[53, 22, 69, 32, 10, 0, 4, 73, 63, 64]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14529851078987122, 0.3661889433860779, 0.21813912689685822, 0.08119714260101318, 0.05761753395199776, 0.09889809042215347, 0.04373696446418762, 0.05096907541155815, 0.32981032133102417, 0.2773023247718811]
Our research in compositionality is motivated by the hypothesis that a special treatment of se­ mantically non-compositional expressions can im­ prove results in various Natural Language Process­ ing (NPL) tasks, as shown for example by Acosta et al.(2011), who utilized MWEs in Information Re­ trieval (IR).	[1, 3, 0, 15, 217, 38, 39, 21, 6, 10]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3316425681114197, 0.5513727068901062, 0.3837311267852783, 0.2150093913078308, 0.18989421427249908, 0.2953512370586395, 0.10871417075395584, 0.08447705954313278, 0.09547081589698792, 0.07539615780115128]
Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used.	[178, 40, 191, 113, 180, 114, 172, 58, 126, 171]	[1, 0, 0, 0, 0, 1, 0, 0, 1, 0]	[0.7262765169143677, 0.20760196447372437, 0.1208343431353569, 0.32069382071495056, 0.35948464274406433, 0.5543951988220215, 0.0672832652926445, 0.16809508204460144, 0.5472510457038879, 0.08800749480724335]
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	[144, 168, 213, 151, 171, 32, 148, 201, 141, 152]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7909207344055176, 0.4651089310646057, 0.2753710150718689, 0.423133909702301, 0.4026601314544678, 0.07149437069892883, 0.3828321099281311, 0.0641595870256424, 0.06764452904462814, 0.1157170906662941]
Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database (Baker et al, 1998).	[34, 16, 39, 60, 0, 32, 10, 11, 17, 18]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3141646385192871, 0.4889396131038666, 0.19656889140605927, 0.07349556684494019, 0.42661190032958984, 0.1937536597251892, 0.1579992175102234, 0.12173271179199219, 0.08932709693908691, 0.0649561733007431]
Our unseen test data consisted of 207 French English sentence pairs from the Hansards corpus (Och and Ney, 2000b).	[26, 19, 32, 6, 12, 17, 7, 28, 8, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18298199772834778, 0.12902747094631195, 0.1214924156665802, 0.06221335753798485, 0.0729268342256546, 0.2047751098871231, 0.05409399792551994, 0.16404259204864502, 0.19781740009784698, 0.07002588361501694]
Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways.	[23, 62, 6, 11, 30, 36, 28, 56, 18, 74]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.516213059425354, 0.4300713539123535, 0.2223999798297882, 0.1259835958480835, 0.10671397298574448, 0.10595984756946564, 0.05296217277646065, 0.07552419602870941, 0.07015582919120789, 0.32485517859458923]
Ourbaseline model, which we used to evaluate the effects of using morphology, was Model 1 (Collins, 1999) with a simple POS tag set containing almost no morphological information.	[101, 20, 118, 115, 77, 31, 15, 120, 70, 19]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47778570652008057, 0.5538053512573242, 0.18600666522979736, 0.23339702188968658, 0.3762555718421936, 0.49476927518844604, 0.0892210379242897, 0.22072847187519073, 0.10204412043094635, 0.08819559961557388]
Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003).	[6, 19, 12, 5, 15, 82, 3, 26, 98, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25322815775871277, 0.1441265344619751, 0.08169644325971603, 0.0827484279870987, 0.057567715644836426, 0.08038895577192307, 0.044753916561603546, 0.05178660526871681, 0.12144392728805542, 0.057106323540210724]
Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).	[23, 173, 183, 182, 21, 142, 70, 156, 59, 51]	[1, 0, 0, 1, 1, 0, 0, 0, 0, 1]	[0.7938745021820068, 0.4617765545845032, 0.3764999210834503, 0.7795236706733704, 0.6634112000465393, 0.10159260779619217, 0.4108303189277649, 0.10887704789638519, 0.07683113217353821, 0.7551789879798889]
POS tagger in this work, we note that taggers optimized specifically for social media are now available and would likely have resulted in higher tagging accuracy (e.g. Owoputi et al (2013)).	[16, 136, 162, 27, 131, 7, 0, 18, 159, 76]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7813448905944824, 0.7359150052070618, 0.16203027963638306, 0.15375825762748718, 0.06112115830183029, 0.14212541282176971, 0.19503910839557648, 0.345542311668396, 0.06428395211696625, 0.060240816324949265]
POS tags of the focus word itself are also included, to aid sense disambiguations related to syntactic differences (Stevenson and Wilks, 2001).	[59, 104, 89, 78, 215, 478, 216, 18, 49, 317]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6030977368354797, 0.16210655868053436, 0.12131471931934357, 0.10547333210706711, 0.060757167637348175, 0.07612329721450806, 0.18187518417835236, 0.04881016165018082, 0.057272277772426605, 0.0515972338616848]
Pairwise mean kappa scores were calculated by comparing a coder's segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53-54).	[367, 390, 493, 267, 392, 389, 69, 61, 109, 135]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6144747734069824, 0.47827768325805664, 0.08549069613218307, 0.12033297121524811, 0.0747581198811531, 0.06055998429656029, 0.07006626576185226, 0.06926686316728592, 0.19073523581027985, 0.12062633037567139]
Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts.	[27, 106, 1, 108, 104, 2, 7, 0, 4, 64]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19468635320663452, 0.053291354328393936, 0.1576317995786667, 0.08551263809204102, 0.07140494138002396, 0.06992295384407043, 0.22330376505851746, 0.4527709186077118, 0.0532030388712883, 0.17103217542171478]
Pang et al (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences.	[3, 20, 83, 15, 96, 95, 101, 0, 13, 181]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5376876592636108, 0.40578365325927734, 0.11432140320539474, 0.12883491814136505, 0.08061904460191727, 0.07682930678129196, 0.0878162607550621, 0.35131439566612244, 0.11474017053842545, 0.07913766801357269]
Pang et al (2003) use word lattices as paraphrase representations from semantically equivalent translations sets.	[1, 9, 34, 19, 30, 16, 125, 24, 181, 4]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.702454686164856, 0.6982224583625793, 0.3447251617908478, 0.18065102398395538, 0.4538644552230835, 0.26981568336486816, 0.24167391657829285, 0.11507084220647812, 0.3259642720222473, 0.12288117408752441]
Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work.	[10, 30, 161, 54, 15, 181, 58, 52, 55, 152]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.27913767099380493, 0.13169293105602264, 0.16067451238632202, 0.20521368086338043, 0.06824582070112228, 0.07845193147659302, 0.08071589469909668, 0.09298205375671387, 0.0792769342660904, 0.04835591837763786]
Pantel et al (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance.	[31, 0, 48, 101, 18, 15, 23, 11, 100, 82]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6872174143791199, 0.8154636025428772, 0.3487508296966553, 0.1829586923122406, 0.29262304306030273, 0.32249313592910767, 0.3403245508670807, 0.09909976273775101, 0.17977656424045563, 0.20173366367816925]
Paraphrase generation methods that operate both on a single monolingual corpus or on parallel corpus are discussed by Madnani and Dorr 2010.	[137, 130, 119, 121, 198, 408, 140, 576, 314, 293]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.785514771938324, 0.7802338004112244, 0.5878322124481201, 0.32438698410987854, 0.48402079939842224, 0.514181911945343, 0.21810282766819, 0.35123297572135925, 0.2448897361755371, 0.2956917881965637]
Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011).	[0, 2, 17, 261, 10, 101, 9, 87, 13, 113]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8197011947631836, 0.7602385878562927, 0.31439170241355896, 0.07972913235425949, 0.27650028467178345, 0.04471239820122719, 0.050540991127491, 0.051149435341358185, 0.06915716081857681, 0.04913828521966934]
Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000).	[12, 45, 2, 43, 29, 28, 33, 16, 53, 32]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18944299221038818, 0.46446511149406433, 0.11083559691905975, 0.045414723455905914, 0.09702044725418091, 0.07545003294944763, 0.2981022596359253, 0.04816364124417305, 0.050970107316970825, 0.0586223304271698]
Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).	[122, 8, 116, 156, 15, 190, 30, 150, 179, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4567316770553589, 0.2761734426021576, 0.1232873871922493, 0.16896778345108032, 0.3286113440990448, 0.24036644399166107, 0.1077582836151123, 0.09765885025262833, 0.06713584810495377, 0.07903091609477997]
Percent agreement with the majonty opinion for each text We took then the RS-trees built by the analysts and used our formalizaUon of RST (Marcu, 1996, Marcu, 1997b) to assocmte with each.	[140, 136, 138, 135, 52, 139, 95, 134, 37, 137]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.7208279371261597, 0.5894486904144287, 0.5344043374061584, 0.5277562141418457, 0.34803658723831177, 0.7298935055732727, 0.054750993847846985, 0.49699774384498596, 0.08868037909269333, 0.44147565960884094]
Pereira et al (1993) used clustering to build an unlabeled hierarchy of nouns.	[137, 135, 25, 29, 5, 30, 2, 127, 76, 110]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6886851787567139, 0.23731772601604462, 0.10525673627853394, 0.18696632981300354, 0.13774573802947998, 0.06001052260398865, 0.18448656797409058, 0.14088577032089233, 0.0515533983707428, 0.1336621791124344]
Perhaps the most immediately promising resource is the CoNLL shared task data from 2008 (Surdeanu et al, 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predicates.	[3, 0, 8, 10, 89, 279, 9, 124, 296, 87]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6276871562004089, 0.8193588256835938, 0.39767521619796753, 0.24793380498886108, 0.16423964500427246, 0.3663710057735443, 0.4736343324184418, 0.06994058936834335, 0.0939401239156723, 0.06933167576789856]
Pevzner and Hearst (2002) propose the alternative metric called WindowDiff.	[6, 2, 146, 270, 156, 155, 150, 159, 1, 4]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6687827706336975, 0.3048141300678253, 0.14720147848129272, 0.10582712292671204, 0.17121319472789764, 0.07500363141298294, 0.06171908974647522, 0.05960340425372124, 0.13875019550323486, 0.12785704433918]
Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection.	[7, 105, 187, 17, 1, 159, 181, 6, 14, 158]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.05760357528924942, 0.10388742387294769, 0.06330302357673645, 0.04390417039394379, 0.12669740617275238, 0.6100577712059021, 0.20888908207416534, 0.09919358789920807, 0.08019068092107773, 0.20829488337039948]
Presuming a tripartite event structure (Moens and Steedman, 1988) consisting of a preparation phase (dynamic phase theta dyn), a culmination point (boundary tau) and a consequent state (static phase phi stat), there are three possibilities for aspect to select.	[88, 36, 259, 395, 384, 4, 10, 66, 17, 266]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5528988838195801, 0.745429277420044, 0.17714861035346985, 0.14522169530391693, 0.08597845584154129, 0.14560551941394806, 0.14560551941394806, 0.16578643023967743, 0.06054125726222992, 0.14343984425067902]
Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992).	[44, 15, 50, 95, 56, 115, 63, 12, 116, 90]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.7920999526977539, 0.7643941640853882, 0.21000875532627106, 0.13207164406776428, 0.18244192004203796, 0.41866016387939453, 0.08962910622358322, 0.05907274782657623, 0.6569876074790955, 0.4172561764717102]
Previous investigations can be found in works such as (Gale and Church, 1993).	[7, 21, 51, 93, 273, 103, 213, 50, 231, 227]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7139158248901367, 0.7139158248901367, 0.45285362005233765, 0.18385249376296997, 0.23739710450172424, 0.09758884459733963, 0.06557644158601761, 0.16856953501701355, 0.05388547107577324, 0.15393702685832977]
Previous shared tasks have shown that frame-semantic SRL of running text is a hard problem (Baker et al, 2007), partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available.	[62, 15, 5, 80, 83, 28, 55, 27, 58, 51]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.671686053276062, 0.3362279236316681, 0.3168927729129791, 0.12798067927360535, 0.22737520933151245, 0.35322946310043335, 0.09528801590204239, 0.1643894761800766, 0.13295020163059235, 0.16618642210960388]
Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort.	[7, 113, 133, 154, 5, 29, 78, 130, 77, 105]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5655983686447144, 0.6811622977256775, 0.4045661687850952, 0.1610623598098755, 0.12885840237140656, 0.08718110620975494, 0.17263272404670715, 0.08826472610235214, 0.15648390352725983, 0.22527576982975006]
Previous work (Branavan et al, 2009) is only able to handle low-level instructions.	[12, 57, 182, 121, 54, 30, 29, 26, 0, 101]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5987992882728577, 0.06634611636400223, 0.21339812874794006, 0.16567349433898926, 0.06728983670473099, 0.058496057987213135, 0.06510987132787704, 0.08632917702198029, 0.35203343629837036, 0.05328066274523735]
Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)	[25, 83, 6, 257, 4, 14, 331, 53, 259, 92]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.2715989053249359, 0.31147119402885437, 0.23584604263305664, 0.6854929327964783, 0.06702659279108047, 0.07234741747379303, 0.10956862568855286, 0.22683002054691315, 0.09342639148235321, 0.12360385805368423]
Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al (1993) and Rooth et al (1999).	[6, 99, 25, 82, 45, 81, 83, 40, 86, 30]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10655444860458374, 0.6178550124168396, 0.10359162092208862, 0.11451198160648346, 0.25663769245147705, 0.10458151251077652, 0.10272993892431259, 0.09506087005138397, 0.0616726391017437, 0.10436046868562698]
Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar.	[24, 30, 15, 92, 3, 25, 21, 20, 91, 38]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7044093608856201, 0.7595941424369812, 0.35489946603775024, 0.2999739646911621, 0.2710171043872833, 0.3421177566051483, 0.408332496881485, 0.5168280601501465, 0.09408487379550934, 0.14484095573425293]
Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries.	[11, 14, 212, 39, 25, 183, 28, 131, 118, 0]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.43701910972595215, 0.08494739979505539, 0.3692722022533417, 0.5450329780578613, 0.22121557593345642, 0.3554835915565491, 0.10678770393133163, 0.05469740927219391, 0.06895680725574493, 0.3666205108165741]
Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007).	[0, 7, 5, 4, 1, 30, 28, 129, 14, 201]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7251913547515869, 0.41384297609329224, 0.16487760841846466, 0.3158419132232666, 0.26865434646606445, 0.10813309252262115, 0.20116712152957916, 0.05235686153173447, 0.09785089641809464, 0.1094190925359726]
Quantitatively, subjective sentences in the product reviews amount to 78% (McDonald et al, 2007), while subjective sentences in the movie review dataset are only about 25% (Mao and Lebanon, 2006).	[30, 46, 85, 149, 159, 99, 16, 14, 28, 9]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5422037243843079, 0.08770006150007248, 0.07803460210561752, 0.09353408217430115, 0.18790321052074432, 0.05877257138490677, 0.08676587790250778, 0.1689436137676239, 0.05800827965140343, 0.0546792633831501]
R5 Superset of rules from (Xu et al., 2009).	[26, 213, 130, 68, 188, 208, 152, 180, 170, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19450677931308746, 0.3052482604980469, 0.163523331284523, 0.16823726892471313, 0.1669992059469223, 0.04852115735411644, 0.044689491391181946, 0.2294590026140213, 0.08913620561361313, 0.05493304505944252]
Ramshaw and Marcus (1995), Munoz et al (1999), Argamon et al (1998), Daelemans et al (1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences.	[31, 35, 107, 96, 7, 106, 45, 44, 117, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0888921394944191, 0.07156532257795334, 0.04919379949569702, 0.07981575280427933, 0.07440707832574844, 0.07348611205816269, 0.07314729690551758, 0.09353096038103104, 0.06975913047790527, 0.08273205161094666]
Rapp (1995) suggested his idea about the usage of context vectors in order to find the words that are the translation of each other in comparable corpora.	[56, 16, 10, 17, 5, 7, 9, 2, 31, 46]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5658809542655945, 0.07952738553285599, 0.24143420159816742, 0.21342408657073975, 0.06256914138793945, 0.07310574501752853, 0.23561403155326843, 0.04935097694396973, 0.280206561088562, 0.05594627559185028]
Ratnaparkhi et al (1994) created a benchmark dataset of 27,937 quadruples (v, n1, p, n2), extracted from the Wall Street Journal.	[87, 40, 69, 9, 81, 78, 140, 80, 30, 89]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.715691089630127, 0.4229818880558014, 0.36082929372787476, 0.05685945227742195, 0.07053180038928986, 0.2629680335521698, 0.061985719949007034, 0.07169798761606216, 0.05984990671277046, 0.0621640644967556]
Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web.	[2, 0, 10, 15, 4, 1, 12, 77, 81, 110]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7511893510818481, 0.6295111775398254, 0.21707864105701447, 0.20413236320018768, 0.3888493478298187, 0.23478356003761292, 0.6052443385124207, 0.07720673084259033, 0.16202965378761292, 0.33387255668640137]
Ravichandran et al (2005) applied LSH to the task of noun clustering.	[92, 0, 199, 103, 24, 35, 2, 97, 10, 140]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39450040459632874, 0.14524072408676147, 0.06854917109012604, 0.3148018419742584, 0.2637123763561249, 0.09855905175209045, 0.09847798198461533, 0.11315957456827164, 0.12256690114736557, 0.08932996541261673]
Ravin and Kazi (1999) further refined the method of solving co-reference through measuring context similarity and integrated it into Nominator (Wacholder et al, 1997), which was one of the first successful systems for named entity recognition and co-reference resolution.	[63, 123, 165, 60, 2, 159, 205, 34, 65, 41]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18246276676654816, 0.06508138030767441, 0.09527144581079483, 0.09352635592222214, 0.09734437614679337, 0.06546951085329056, 0.08144722133874893, 0.07222628593444824, 0.08067221194505692, 0.09070640057325363]
Readability studies have shown that disfluencies (fillers and speech repairs) may be deleted from transcripts without compromising meaning (Jones et al, 2003), and deleting repairs prior to parsing has been shown to improve its accuracy (Charniak and Johnson, 2001).	[33, 21, 4, 26, 79, 5, 186, 35, 36, 173]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6095836162567139, 0.5044140815734863, 0.2730739116668701, 0.1604776680469513, 0.24424277245998383, 0.4242699146270752, 0.09833914786577225, 0.24128377437591553, 0.06358674168586731, 0.064907968044281]
Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al, 2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and n-best alignments instead of 1-best alignments (Venugopal et al, 2008).	[135, 138, 22, 118, 38, 32, 117, 76, 79, 96]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6292243599891663, 0.20844559371471405, 0.13105528056621552, 0.2561284005641937, 0.06495678424835205, 0.10327903926372528, 0.1812105029821396, 0.07659945636987686, 0.22484096884727478, 0.07025142759084702]
Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling.	[13, 17, 118, 123, 121, 43, 3, 0, 124, 6]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.714684784412384, 0.26203322410583496, 0.12989656627178192, 0.25579872727394104, 0.16569267213344574, 0.056520652025938034, 0.07541164010763168, 0.16846853494644165, 0.07875525951385498, 0.13529205322265625]
Recent work has formalised NLG algorithms for referring expression generation in terms of algorithms for finding an appropriate subgraph of a graph representing the domain knowledge [Krahmer et al, 2003].	[72, 301, 4, 8, 69, 151, 19, 23, 165, 38]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5787725448608398, 0.6108101606369019, 0.3444995880126953, 0.3444995880126953, 0.22215056419372559, 0.12098220735788345, 0.06404668092727661, 0.06398191303014755, 0.24381636083126068, 0.12727107107639313]
Recent work on frame-semantic parsing in which sentences may contain multiple frames to be recognized along with their arguments has used the SemEval 07 data (Baker et al, 2007).	[0, 31, 29, 15, 25, 22, 14, 73, 51, 1]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7633671760559082, 0.05961989611387253, 0.054614562541246414, 0.16239668428897858, 0.12680384516716003, 0.05420739948749542, 0.07927227020263672, 0.04613013565540314, 0.19866977632045746, 0.13243980705738068]
Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.	[6, 225, 240, 228, 23, 301, 88, 130, 266, 150]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.46025705337524414, 0.4039614498615265, 0.10524481534957886, 0.6686096787452698, 0.10642096400260925, 0.3475401699542999, 0.1917673647403717, 0.09687235951423645, 0.07547220587730408, 0.16097897291183472]
Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008).	[4, 35, 20, 7, 2, 11, 34, 160, 154, 9]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.737399160861969, 0.75982266664505, 0.29289552569389343, 0.5910407900810242, 0.5217292308807373, 0.2970545291900635, 0.1986929476261139, 0.12092253565788269, 0.24861782789230347, 0.05383932963013649]
Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al, 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al, 2004).	[138, 1, 12, 0, 28, 9, 5, 131, 44, 77]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6154224276542664, 0.0658632442355156, 0.06357253342866898, 0.36655986309051514, 0.09960122406482697, 0.303780198097229, 0.05966406315565109, 0.10431550443172455, 0.09112028032541275, 0.04756779968738556]
Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al, 2005) has received growing interest.	[32, 17, 39, 11, 10, 18, 0, 20, 15, 68]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13935892283916473, 0.11747758090496063, 0.06050393357872963, 0.2245008945465088, 0.16048681735992432, 0.054013241082429886, 0.32672280073165894, 0.05051146075129509, 0.056427810341119766, 0.04992052912712097]
Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined.	[334, 289, 301, 300, 237, 412, 186, 203, 227, 175]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15048913657665253, 0.2060607522726059, 0.2156122773885727, 0.19897493720054626, 0.05921925976872444, 0.07126828283071518, 0.052948132157325745, 0.08204496651887894, 0.052508849650621414, 0.05051330849528313]
Regarding (i), Schone and Jurafsky (2001) compare the semantic vector of a phrase p and the vectors of its component words in two ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not.	[152, 129, 184, 137, 157, 14, 9, 126, 48, 151]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7046069502830505, 0.31535181403160095, 0.20962589979171753, 0.35496899485588074, 0.516865611076355, 0.06236601248383522, 0.06512296199798584, 0.051815249025821686, 0.06891360878944397, 0.2911621928215027]
Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline.	[37, 102, 13, 115, 66, 60, 1, 4, 67, 14]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7178477048873901, 0.609806478023529, 0.28471022844314575, 0.23861894011497498, 0.37768298387527466, 0.3281310498714447, 0.3974589407444, 0.340772420167923, 0.10367865860462189, 0.2532186210155487]
Reiter and Dale (1992) describe a fast algorithm for generating referring expressions in the context of a natural language generation system.	[89, 245, 0, 6, 16, 196, 2, 191, 18, 63]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46628910303115845, 0.7859092354774475, 0.3352881073951721, 0.2520378828048706, 0.2859145700931549, 0.49266812205314636, 0.050961799919605255, 0.10369691252708435, 0.10488790273666382, 0.15969344973564148]
Related work includes learning ensembles of POS taggers, as in the work of Brill and Wu (1998), where an ensemble consisting of a unigram model, an N-gram model, a transformation-based model, and an MEMM for POS tagging achieves substantial results beyond the individual taggers.	[37, 35, 33, 22, 34, 23, 4, 55, 6, 59]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2622317671775818, 0.14805427193641663, 0.08591382205486298, 0.36178484559059143, 0.058175601065158844, 0.09997570514678955, 0.2725990116596222, 0.18023578822612762, 0.09752067178487778, 0.2961062490940094]
Restrictions on possible relations are attached to the words ,e.g., expressed as valencies in the case of dependency relations, yielding a strictly lexicalized grammar in the sense of Schabes et al (1988).	[192, 50, 146, 238, 141, 171, 46, 27, 129, 113]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3712678849697113, 0.2538797855377197, 0.09288594871759415, 0.2458629012107849, 0.048973675817251205, 0.07420986145734787, 0.08452608436346054, 0.0524614155292511, 0.0921773612499237, 0.05031914636492729]
Riedel and Clarke (2006) showed that dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-the shelf optimizer if a cutting plane approach is used.	[206, 0, 2, 128, 4, 3, 207, 13, 56, 146]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24719052016735077, 0.76759272813797, 0.0970938503742218, 0.3506217300891876, 0.08374695479869843, 0.12162072956562042, 0.28045037388801575, 0.1284826397895813, 0.09353093057870865, 0.17641453444957733]
Riezler et al (2002) report on our WSJ parsing experiments.	[2, 94, 75, 124, 93, 22, 33, 86, 14, 132]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5154407024383545, 0.4406522512435913, 0.3273787498474121, 0.09695682674646378, 0.06282748281955719, 0.04236664995551109, 0.10987254232168198, 0.06905757635831833, 0.05899425968527794, 0.06789299845695496]
Riezler et al (2003) applied linguistically rich LFG grammars to a sentence compression system.	[159, 34, 1, 19, 17, 14, 151, 26, 2, 15]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7232411503791809, 0.5515603423118591, 0.2120693027973175, 0.12164650857448578, 0.09592273831367493, 0.31567707657814026, 0.3221578001976013, 0.0461108461022377, 0.04793965443968773, 0.3888346254825592]
Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision.	[3, 176, 31, 81, 138, 182, 28, 51, 27, 156]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2910134494304657, 0.5970268845558167, 0.4404303729534149, 0.1701918989419937, 0.27128663659095764, 0.13311906158924103, 0.10971459746360779, 0.14271096885204315, 0.05023053660988808, 0.47596806287765503]
Riloff et al (Riloff et al, 2003) applied bootstrapping to recognise subjective noun keywords and classify sentences as subjective or objective in newswire texts.	[2, 42, 20, 52, 201, 137, 1, 56, 5, 184]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30859023332595825, 0.28974100947380066, 0.1413249969482422, 0.20778264105319977, 0.15406404435634613, 0.308538019657135, 0.08321365714073181, 0.0668482631444931, 0.0784994587302208, 0.12999826669692993]
Riloff et al, (2003) have conducted experiments that use Bag Of-Words (BoW) as features to generate a Naive Bayes subjectivity classifier for the MPQA corpus in English.	[169, 165, 161, 4, 166, 129, 174, 160, 34, 168]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.6801632046699524, 0.6543322801589966, 0.6382333040237427, 0.5556599497795105, 0.41738662123680115, 0.6667045950889587, 0.1718740463256836, 0.37285009026527405, 0.1635212004184723, 0.05552227422595024]
Romano et al (2006) and Sekine (2006) used syntactic paraphrases to obtain patterns for extracting relations.	[24, 142, 4, 56, 27, 46, 74, 55, 5, 29]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5212488174438477, 0.6638693809509277, 0.12276168167591095, 0.19764234125614166, 0.07422342151403427, 0.05157090723514557, 0.10429858416318893, 0.3908165395259857, 0.08720237761735916, 0.10740933567285538]
Rosario and Hearst (2001) utilize neural networks to classify compounds according to their domain-specific relation taxonomy.	[0, 89, 44, 107, 10, 3, 52, 128, 29, 120]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8020293712615967, 0.5260118842124939, 0.3027442991733551, 0.3625151813030243, 0.38434556126594543, 0.0529354028403759, 0.07618137449026108, 0.10277841240167618, 0.11282046884298325, 0.052807699888944626]
Rosti et al (2007) look at sentence-level combinations (as well as word and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists.	[20, 27, 163, 5, 166, 85, 86, 49, 70, 71]	[1, 1, 1, 1, 0, 1, 0, 1, 0, 0]	[0.6078226566314697, 0.6044173240661621, 0.5770518183708191, 0.6650304794311523, 0.4093932509422302, 0.5786091089248657, 0.22826895117759705, 0.5359746813774109, 0.14180897176265717, 0.31102630496025085]
Rule size and lexicalization affect parsing complexity whether the grammar is binarized explicitly (Zhang et al, 2006) or implicitly binarized using Early-style intermediate symbols (Zollmann et al, 2006).	[20, 62, 5, 2, 135, 32, 18, 10, 33, 119]	[1, 1, 0, 0, 0, 1, 1, 0, 0, 0]	[0.7515271902084351, 0.6210067868232727, 0.350873738527298, 0.17754146456718445, 0.22006618976593018, 0.5066053867340088, 0.5775415897369385, 0.06555298715829849, 0.054541315883398056, 0.06270047277212143]
SCL is the structural correspondence learning technique of Blitzer et al (2006).	[22, 0, 14, 98, 32, 51, 20, 55, 145, 222]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6356967091560364, 0.7806702852249146, 0.5912713408470154, 0.17984728515148163, 0.25981858372688293, 0.26407018303871155, 0.3539486527442932, 0.23959729075431824, 0.1757628172636032, 0.2814956307411194]
SR-AW finds the sense of each word that is most related or most similar to those of its neighbors in the sentence, according to any of the ten measures available in WordNet::Similarity (Pedersen et al, 2004).	[73, 11, 25, 20, 1, 7, 55, 42, 72, 9]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6737500429153442, 0.6839805245399475, 0.1921830177307129, 0.05539865419268608, 0.42910000681877136, 0.22374403476715088, 0.15890493988990784, 0.13559676706790924, 0.2121238261461258, 0.15764227509498596]
SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features.	[116, 45, 121, 19, 181, 177, 176, 2, 33, 133]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6347154974937439, 0.4728488028049469, 0.2662810683250427, 0.13737483322620392, 0.16988039016723633, 0.11756619811058044, 0.2464265525341034, 0.05420029163360596, 0.11069992929697037, 0.08481089025735855]
SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993).	[55, 78, 24, 79, 145, 43, 49, 31, 15, 46]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3035362660884857, 0.7271320223808289, 0.22103720903396606, 0.09717553108930588, 0.20021511614322662, 0.2112640142440796, 0.10433052480220795, 0.0783611387014389, 0.055383726954460144, 0.10196847468614578]
Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001).	[10, 13, 1, 7, 100, 67, 69, 85, 95, 17]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26131486892700195, 0.517650306224823, 0.19740064442157745, 0.18240967392921448, 0.06923695653676987, 0.05989064648747444, 0.06962104886770248, 0.07022516429424286, 0.05062852054834366, 0.16445966064929962]
Second, we employ a MIRA-based binary classifier (Ritter et al, 2010) to predict whether a message mentions a concert event.	[109, 140, 58, 162, 170, 131, 157, 117, 47, 57]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.456999272108078, 0.0681820958852768, 0.10647940635681152, 0.11917916685342789, 0.2022528201341629, 0.060772188007831573, 0.08070152997970581, 0.05138927325606346, 0.07528641074895859, 0.04578936845064163]
See (Chan et al, 2007) for the relevance of word sense disambiguation and (Chiang et al, 2009) for the role of prepositions in MT.	[24, 6, 0, 1, 147, 36, 7, 18, 14, 9]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6874293684959412, 0.5569024682044983, 0.21052542328834534, 0.08815676718950272, 0.11432305723428726, 0.05137046054005623, 0.05266738310456276, 0.06681177765130997, 0.05418938025832176, 0.0583273321390152]
See Callison-Burch et al (2007) for details on the human evaluation task.	[107, 15, 6, 33, 127, 48, 4, 16, 113, 54]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7591810822486877, 0.6316216588020325, 0.09452801197767258, 0.06366407871246338, 0.14728637039661407, 0.2000703364610672, 0.3208145201206207, 0.2726023495197296, 0.052092164754867554, 0.11629152297973633]
Segmentation for Japanese is a successful field of research, achieving the F-score of nearly 99% (Kudo et al, 2004).	[12, 123, 89, 112, 127, 0, 1, 134, 32, 168]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5043787956237793, 0.171731099486351, 0.0846840888261795, 0.10293703526258469, 0.08846157044172287, 0.20134152472019196, 0.08728605508804321, 0.0476495586335659, 0.057101376354694366, 0.052379071712493896]
Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (Max Ent) (Xue and Shen, 2003), support vector machine Now the second author is affiliated with NTT.	[74, 76, 67, 69, 65, 68, 3, 55, 115, 20]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19141192734241486, 0.5687593817710876, 0.08972331136465073, 0.25201672315597534, 0.11352378129959106, 0.2584809958934784, 0.07346288859844208, 0.09146638959646225, 0.07346288859844208, 0.05577213317155838]
Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (Max Ent) (Xue and Shen, 2003), support vector machine.	[72, 42, 49, 18, 62, 8, 59, 17, 74, 37]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23972125351428986, 0.19015184044837952, 0.2102932333946228, 0.3160414397716522, 0.08784426748752594, 0.1550973802804947, 0.1046433299779892, 0.46521681547164917, 0.050021909177303314, 0.06808925420045853]
Selectors are words which take the place of a given target word within its local context (Lin, 1997).	[5, 56, 34, 61, 38, 94, 44, 146, 145, 0]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6391074061393738, 0.6717694401741028, 0.6361076831817627, 0.3463549315929413, 0.22702573239803314, 0.3863981366157532, 0.1951359063386917, 0.12521691620349884, 0.32481133937835693, 0.46983957290649414]
Semantically, the compositionality of MWEs is gradual, ranging from fully com positional to idiomatic (Bannard et al, 2003).	[11, 38, 126, 107, 4, 208, 175, 40, 196, 108]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0899471715092659, 0.09750882536172867, 0.36793702840805054, 0.1852360963821411, 0.3351500332355499, 0.04929157346487045, 0.07470531761646271, 0.18893301486968994, 0.05818886309862137, 0.15533462166786194]
Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).	[65, 2, 27, 61, 63, 6, 21, 25, 55, 42]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.16859377920627594, 0.061228569597005844, 0.07766645401716232, 0.6253712773323059, 0.4681769907474518, 0.2457650750875473, 0.2457650750875473, 0.11962325870990753, 0.10432758182287216, 0.049504805356264114]
Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge.	[200, 192, 9, 38, 199, 226, 202, 201, 193, 231]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5013790726661682, 0.6738654971122742, 0.09974770247936249, 0.08261875808238983, 0.4904446303844452, 0.10345108807086945, 0.13409295678138733, 0.04619629681110382, 0.23894068598747253, 0.0875130146741867]
Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009).	[34, 32, 35, 10, 41, 50, 45, 49, 46, 37]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.800444483757019, 0.8131248950958252, 0.44389429688453674, 0.4936937987804413, 0.3343289792537689, 0.25457167625427246, 0.3464972972869873, 0.34199512004852295, 0.1898503303527832, 0.052048083394765854]
Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004).	[94, 105, 76, 52, 67, 53, 82, 96, 92, 43]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32863202691078186, 0.09644345194101334, 0.054543908685445786, 0.15105430781841278, 0.30225351452827454, 0.321679949760437, 0.2995432913303375, 0.11612870544195175, 0.1512964367866516, 0.1461319923400879]
Several lexical representation formalisms addressing these desiderata have been proposed, e.g. DATR [Evans and Gazdar 1989a, 1989b, 1990]; LRL [Copestake, 1992]; [Russell et al 1991].	[7, 11, 13, 2, 9, 6, 27, 40, 38, 10]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39073529839515686, 0.7064652442932129, 0.05321582406759262, 0.2544870376586914, 0.05035848170518875, 0.15927019715309143, 0.07286233454942703, 0.0675506740808487, 0.050380777567625046, 0.052384357899427414]
Shen et al (2007) have further shown that better results (97.3 % accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first.	[22, 0, 2, 1, 198, 197, 29, 14, 137, 5]	[1, 1, 1, 1, 1, 1, 0, 1, 0, 0]	[0.7056529521942139, 0.8071336150169373, 0.5939915180206299, 0.5168980360031128, 0.5635731220245361, 0.5168980360031128, 0.06654803454875946, 0.5096279382705688, 0.3374054431915283, 0.06567297875881195]
Shen et al (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first.	[31, 5, 59, 197, 101, 63, 28, 23, 13, 164]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.411855548620224, 0.07325318455696106, 0.06466025859117508, 0.1287069320678711, 0.07554847002029419, 0.07432929426431656, 0.1129472404718399, 0.0569697767496109, 0.08310253918170929, 0.06015247479081154]
Shieber et al (1990) show that a top-down evaluation strategy will fail for rules such as vP-*vp x, irrespective of the order of evaluation of the right-hand side categories in the rule.	[42, 49, 125, 335, 183, 93, 69, 41, 64, 213]	[0, 0, 1, 0, 1, 0, 0, 0, 0, 0]	[0.2683793008327484, 0.26898255944252014, 0.5014950633049011, 0.10180202126502991, 0.5265679359436035, 0.20668786764144897, 0.11388726532459259, 0.4215106964111328, 0.11014655977487564, 0.09812262654304504]
Shinyama and Sekine (2006) developed an approach to preemptively discover relations in a corpus and present them as tables with all the entity pairs in the table having the same relations between them.	[18, 68, 47, 33, 23, 143, 66, 16, 22, 29]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7042295932769775, 0.22815798223018646, 0.06248614564538002, 0.5097443461418152, 0.3172895610332489, 0.06831008195877075, 0.45818185806274414, 0.19668377935886383, 0.1387844830751419, 0.08524154871702194]
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates.	[56, 55, 58, 37, 205, 191, 20, 188, 17, 142]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5555657148361206, 0.7549245357513428, 0.334451824426651, 0.497238427400589, 0.25476279854774475, 0.0647503137588501, 0.09589686244726181, 0.15451650321483612, 0.3401893675327301, 0.0484953410923481]
Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships.	[124, 126, 108, 22, 146, 109, 61, 98, 134, 112]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4566286504268646, 0.42747294902801514, 0.10710173845291138, 0.1628829538822174, 0.06398486346006393, 0.3892460763454437, 0.08793310075998306, 0.05620915815234184, 0.07496045529842377, 0.11353439837694168]
Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.	[17, 86, 64, 56, 89, 19, 34, 0, 4, 69]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32401183247566223, 0.2177436649799347, 0.07756279408931732, 0.06189433112740517, 0.17029966413974762, 0.14817233383655548, 0.1699797809123993, 0.31356024742126465, 0.179768368601799, 0.09700753539800644]
Similarly to classical NLP tasks, such as Base Phrase Chunking (Ramshaw and Marcus, 1999) (BPC) or NER (Tjong Kim Sang, 2002), we formulate the MD task as a sequence classification problem, i.e. the classifier assigns to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.	[17, 86, 64, 0, 4, 89, 19, 80, 69, 73]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20780454576015472, 0.16639399528503418, 0.15853969752788544, 0.23925048112869263, 0.17951634526252747, 0.25342196226119995, 0.09247392416000366, 0.04612167924642563, 0.05234130471944809, 0.08624285459518433]
Similarly to its context-free counterpart, the re-estimation algorithm can be extended to handle partially parsed corpora (Pereira and Schabes, 1992).	[4, 0, 30, 21, 121, 24, 8, 82, 33, 120]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.7089071273803711, 0.7510190010070801, 0.48953208327293396, 0.4574737250804901, 0.4261263310909271, 0.0937952920794487, 0.33499786257743835, 0.22088737785816193, 0.5093665719032288, 0.31115394830703735]
Similarly, (Banea et al, 2008) propose a method based on machine translation to generate parallel texts, followed by a cross-lingual projection of subjectivity labels, which are used to train subjectivity annotation tools for Romanian and Spanish.	[21, 73, 112, 66, 9, 103, 96, 23, 128, 11]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5160605907440186, 0.5910046696662903, 0.2028317153453827, 0.46621477603912354, 0.16414287686347961, 0.09578355401754379, 0.28664615750312805, 0.2777487337589264, 0.1623576432466507, 0.2627565860748291]
Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other.	[15, 8, 10, 6, 75, 59, 36, 58, 150, 9]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6410076022148132, 0.12628968060016632, 0.5862447023391724, 0.3453739583492279, 0.3428480625152588, 0.2731943726539612, 0.0988025814294815, 0.324196994304657, 0.11627063155174255, 0.3168688714504242]
Similarly, Chen and Rambow (2003) argue that the kind of deep linguistic features we harvest from FrameNet is beneficial for the successful assignment of PropBank roles to constituents, in this case using TAGs generated from PropBank to generate the relevant features.	[58, 64, 2, 70, 239, 56, 68, 233, 99, 61]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5708901882171631, 0.3925485908985138, 0.4618445932865143, 0.30264174938201904, 0.4618445932865143, 0.32551196217536926, 0.32632243633270264, 0.350761741399765, 0.10177329182624817, 0.12112964689731598]
Similarly, Choi et al (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task.	[35, 100, 13, 34, 74, 82, 4, 12, 95, 97]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3622252941131592, 0.3904016315937042, 0.2142351269721985, 0.1115744486451149, 0.07145049422979355, 0.12196291238069534, 0.1338154375553131, 0.08128651231527328, 0.2315494269132614, 0.05214136093854904]
Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.	[4, 0, 19, 115, 12, 40, 117, 103, 3, 91]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5177773237228394, 0.8141054511070251, 0.13911591470241547, 0.3293997645378113, 0.20741461217403412, 0.25397631525993347, 0.09062135219573975, 0.09755143523216248, 0.0967777669429779, 0.60889732837677]
Similarly, the method of (Ng, 2005) ranks base models according to their performance on separate tuning set, and then uses the highest-ranked base model for predicting on test documents.	[40, 134, 95, 39, 119, 125, 115, 2, 124, 32]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42638951539993286, 0.5424203872680664, 0.48549455404281616, 0.08121906965970993, 0.082930788397789, 0.10810722410678864, 0.10472191870212555, 0.09765426814556122, 0.11263922601938248, 0.05103566497564316]
"Since German is a language that makes productive use of ""closed"" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009)."	[58, 3, 102, 13, 73, 111, 37, 54, 25, 24]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38310569524765015, 0.15892119705677032, 0.43622198700904846, 0.24833019077777863, 0.0960032194852829, 0.08008884638547897, 0.06525866687297821, 0.18275119364261627, 0.0720672607421875, 0.30972734093666077]
Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia.	[137, 38, 56, 74, 71, 59, 124, 136, 51, 54]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.28472134470939636, 0.16406117379665375, 0.3763794004917145, 0.1309383064508438, 0.14621740579605103, 0.11408770829439163, 0.05678294971585274, 0.05021433159708977, 0.04941156134009361, 0.1419474482536316]
Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.	[10, 1, 156, 109, 19, 157, 44, 3, 70, 71]	[1, 0, 1, 0, 0, 0, 1, 1, 1, 0]	[0.6351947784423828, 0.48630857467651367, 0.5375382304191589, 0.3496604859828949, 0.12227310240268707, 0.2717171311378479, 0.6068054437637329, 0.516434907913208, 0.5089355707168579, 0.48631617426872253]
Since this method is not a contribution of this paper, we refer the reader to the fuller presentations in Goodman (1996) and Matsuzaki et al (2005).	[23, 137, 14, 95, 38, 91, 107, 4, 141, 7]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.631994366645813, 0.6400996446609497, 0.3504323959350586, 0.06635285168886185, 0.353500634431839, 0.15708082914352417, 0.14300714433193207, 0.07821203023195267, 0.12455840408802032, 0.08300299942493439]
Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002).	[31, 208, 16, 80, 40, 252, 274, 242, 7, 14]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6992351412773132, 0.6565579175949097, 0.3380480408668518, 0.26017144322395325, 0.43122437596321106, 0.6164108514785767, 0.2663617730140686, 0.12911435961723328, 0.09775423258543015, 0.09775423258543015]
Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity.	[292, 14, 125, 25, 76, 279, 121, 224, 11, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49706754088401794, 0.049577437341213226, 0.3395957350730896, 0.05272524803876877, 0.2633533477783203, 0.04502175375819206, 0.04917057976126671, 0.1842985600233078, 0.05127844586968422, 0.05612979829311371]
Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features.	[0, 67, 232, 4, 204, 5, 142, 57, 32, 223]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.8179807662963867, 0.7719646096229553, 0.27537739276885986, 0.17415034770965576, 0.2089698314666748, 0.31958118081092834, 0.5177185535430908, 0.09858234226703644, 0.17012344300746918, 0.14067430794239044]
Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner.	[160, 4, 58, 40, 205, 18, 1, 15, 128, 2]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5429199934005737, 0.6390134692192078, 0.24068738520145416, 0.17695850133895874, 0.1821047067642212, 0.16639311611652374, 0.159302219748497, 0.08923248946666718, 0.05379778519272804, 0.33799585700035095]
Snow et al (2006) use syntactic path patterns as features for supervised hyponymy and synonymy classifiers, whose training examples are derived automatically from WordNet.	[53, 50, 54, 52, 2, 66, 11, 70, 36, 38]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49941274523735046, 0.6525822281837463, 0.1587640941143036, 0.11949912458658218, 0.3325228691101074, 0.1145559772849083, 0.31105658411979675, 0.061557117849588394, 0.18706904351711273, 0.057323161512613297]
So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way.	[1, 8, 11, 0, 37, 48, 3, 156, 14, 135]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8088465929031372, 0.6409784555435181, 0.7464040517807007, 0.48713839054107666, 0.32762885093688965, 0.366020530462265, 0.44334790110588074, 0.07108872383832932, 0.13398803770542145, 0.1151687353849411]
Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006).	[63, 1, 0, 18, 6, 4, 201, 22, 198, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.610008955001831, 0.11446540057659149, 0.42538002133369446, 0.1876913160085678, 0.3015996217727661, 0.08118966966867447, 0.06089933589100838, 0.13300982117652893, 0.3058878481388092, 0.2526669204235077]
Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al, 1994), (Resnik, 1995)).	[13, 26, 27, 19, 102, 5, 103, 3, 4, 84]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2927252948284149, 0.09085946530103683, 0.17560487985610962, 0.04515061527490616, 0.27190962433815, 0.05617288500070572, 0.0582805834710598, 0.04529736936092377, 0.058285392820835114, 0.09232845902442932]
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b).	[113, 100, 29, 136, 13, 4, 107, 55, 31, 106]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7575628757476807, 0.23730455338954926, 0.21133150160312653, 0.208596333861351, 0.15337753295898438, 0.11964994668960571, 0.3916127383708954, 0.09390474110841751, 0.07183705270290375, 0.0619291253387928]
Some text planners take as their goal the delivery of a single fact, or a set of facts, but take into account that other information may need to be given first for each message to be understood, other information may need to be given after to counter misconceptions, and examples may be given to aide assimilation (for in stance, the RST text planners, such as Hovy 1988).	[8, 121, 16, 135, 94, 5, 71, 41, 43, 162]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4507060647010803, 0.4507060647010803, 0.093703992664814, 0.093703992664814, 0.06129319220781326, 0.09314446896314621, 0.153589129447937, 0.40485459566116333, 0.07173864543437958, 0.40485459566116333]
Some work such as (Kim and Hovy, 2006) has explored the connection to role labeling.	[29, 32, 44, 45, 24, 3, 49, 4, 157, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46674492955207825, 0.39290282130241394, 0.4995206892490387, 0.3010541498661041, 0.33421748876571655, 0.37161049246788025, 0.25067731738090515, 0.18569977581501007, 0.04411245137453079, 0.25303903222084045]
Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al, 1998), then the identification of locally uninstantiated roles (NIs).	[44, 19, 17, 65, 32, 30, 0, 27, 5, 10]	[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.5788280367851257, 0.5732427835464478, 0.49848076701164246, 0.5698548555374146, 0.1290740668773651, 0.07046306133270264, 0.35657018423080444, 0.5830547213554382, 0.11007960885763168, 0.11126133799552917]
Specifically, we assume MINIPAR-style (Lin, 1993) dependency trees where nodes represent text expressions and edges represent the syntactic relations between them.	[22, 115, 75, 23, 74, 81, 124, 72, 37, 32]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 1]	[0.7817644476890564, 0.5683997869491577, 0.576322078704834, 0.319828063249588, 0.4873374104499817, 0.2817579209804535, 0.0804257020354271, 0.2892807424068451, 0.19521789252758026, 0.6410865187644958]
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006).	[92, 44, 2, 45, 135, 76, 39, 116, 1, 50]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.6835936903953552, 0.22772599756717682, 0.16349975764751434, 0.11592639237642288, 0.24324896931648254, 0.11635090410709381, 0.16227227449417114, 0.5084055662155151, 0.05460647493600845, 0.09485264122486115]
Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005).	[0, 13, 1, 99, 70, 28, 56, 93, 9, 55]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5652430653572083, 0.4198096990585327, 0.24214601516723633, 0.11237915605306625, 0.05069396272301674, 0.08929865062236786, 0.20631077885627747, 0.34683337807655334, 0.07347893714904785, 0.05106491595506668]
Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003).	[17, 14, 3, 80, 21, 70, 36, 22, 41, 97]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22558040916919708, 0.13274967670440674, 0.04805848002433777, 0.07167316973209381, 0.05831344798207283, 0.060654789209365845, 0.08812419325113297, 0.09287495911121368, 0.05417425557971001, 0.04703713208436966]
Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.	[1, 40, 3, 53, 2, 153, 6, 64, 174, 74]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32326528429985046, 0.1855613738298416, 0.2199503779411316, 0.19352178275585175, 0.17866794764995575, 0.1895400732755661, 0.175528883934021, 0.4818645119667053, 0.13502828776836395, 0.15146172046661377]
Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training.	[61, 5, 11, 40, 74, 12, 25, 1, 72, 88]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7575613856315613, 0.4441486895084381, 0.18400901556015015, 0.32650408148765564, 0.13306966423988342, 0.1493406891822815, 0.24739469587802887, 0.22162148356437683, 0.19583243131637573, 0.11005260795354843]
Stemming is enabled (Lin and Och, 2004a).	[10, 106, 120, 149, 21, 14, 186, 26, 22, 166]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.04548116773366928, 0.07017995417118073, 0.06130913272500038, 0.05774884298443794, 0.05229173228144646, 0.04565820470452309, 0.05673209950327873, 0.06054272502660751, 0.05362800881266594, 0.037290025502443314]
Strube (1998)'s centering approach (whose sentence ordering is designated as SR2 in Table 2) also deals with and even prefers intra sentential anaphora, which raises the upper limit to a more acceptable 80.2%.	[2, 3, 94, 60, 149, 10, 68, 17, 66, 36]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.549202024936676, 0.30135905742645264, 0.3728621304035187, 0.14830438792705536, 0.2366006225347519, 0.35512012243270874, 0.20507767796516418, 0.12042432278394699, 0.06155175343155861, 0.06260467320680618]
Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007).	[2, 24, 32, 64, 75, 12, 69, 123, 142, 113]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15153780579566956, 0.0412302166223526, 0.20658807456493378, 0.045692939311265945, 0.046813447028398514, 0.1659393012523651, 0.15164367854595184, 0.07419486343860626, 0.04745844379067421, 0.08255466818809509]
Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.	[20, 220, 25, 4, 65, 5, 62, 49, 61, 55]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4337897002696991, 0.6051713228225708, 0.10642216354608536, 0.13009771704673767, 0.41229262948036194, 0.13234513998031616, 0.07536066323518753, 0.04944441840052605, 0.4751560389995575, 0.11897676438093185]
Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al, 2006), and employing document subcomponent information (McDonald et al, 2007).	[55, 147, 28, 148, 32, 165, 31, 143, 9, 142]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39618563652038574, 0.22271032631397247, 0.33221742510795593, 0.17045652866363525, 0.16521485149860382, 0.06744145601987839, 0.09510014951229095, 0.12732313573360443, 0.053948719054460526, 0.05158421769738197]
Substantial improvements have been made to parse western language such as English, and many powerful models have been proposed (Brill 1993, Collins 1997).	[92, 4, 88, 85, 116, 8, 104, 10, 1, 111]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37399429082870483, 0.3171424865722656, 0.04833617061376572, 0.06489790230989456, 0.32558178901672363, 0.06934723258018494, 0.23959732055664062, 0.29375454783439636, 0.05851747468113899, 0.07962735742330551]
Such an algorithm is presented by Graehl and Knight (2004).	[144, 11, 123, 112, 125, 109, 96, 95, 141, 36]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4263124465942383, 0.06743388622999191, 0.04843294993042946, 0.31749391555786133, 0.18004441261291504, 0.06538351625204086, 0.19765479862689972, 0.0659051164984703, 0.08530645072460175, 0.0641823559999466]
Such approaches have been shown to be effective in log-linear word alignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006).	[2, 63, 36, 176, 139, 46, 170, 113, 133, 186]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.533843994140625, 0.3329777419567108, 0.2150096446275711, 0.16504065692424774, 0.19648927450180054, 0.1375109702348709, 0.13875219225883484, 0.3239305317401886, 0.21736618876457214, 0.07951019704341888]
Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization.	[70, 147, 62, 1, 26, 76, 78, 82, 101, 77]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17673233151435852, 0.26269200444221497, 0.04463186860084534, 0.06995358318090439, 0.04871005564928055, 0.06865065544843674, 0.21853546798229218, 0.0534413680434227, 0.1775488406419754, 0.21575315296649933]
Supertags have been successfully applied to guide parsing in symbolic frameworks such as Lexicalised Tree-Adjoning grammar (Bangalore and Joshi, 1999).	[280, 7, 17, 45, 97, 101, 283, 278, 288, 339]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.40335288643836975, 0.5160732269287109, 0.5160732269287109, 0.11114505678415298, 0.048160482197999954, 0.09653356671333313, 0.10315442830324173, 0.04803003370761871, 0.048281386494636536, 0.10326585918664932]
Supervised classifiers have been used be fore for this task, notably by Katz and Giesbrecht (2006).	[68, 74, 31, 7, 11, 10, 102, 125, 1, 114]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14592841267585754, 0.316304087638855, 0.05304940417408943, 0.0707932561635971, 0.2117081731557846, 0.40088367462158203, 0.05951641499996185, 0.05082692950963974, 0.04317040368914604, 0.06509622931480408]
Suzuki et al (2009) and phrase-structure annotations in the case of Carreras et al (2008).	[161, 29, 41, 86, 81, 35, 43, 140, 36, 118]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10221803188323975, 0.20007586479187012, 0.28902533650398254, 0.45976927876472473, 0.12949277460575104, 0.11794418096542358, 0.0584973581135273, 0.11772128939628601, 0.06993620842695236, 0.08416110277175903]
Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al, 2000) verb lexicon, in addition to supervised parses.	[35, 34, 42, 38, 21, 172, 156, 210, 29, 178]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7888888716697693, 0.47720304131507874, 0.48636773228645325, 0.2708949148654938, 0.3363029956817627, 0.07723842561244965, 0.21974362432956696, 0.06870102137327194, 0.2262154519557953, 0.061430998146533966]
Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007).	[6, 12, 0, 4, 21, 158, 26, 125, 171, 19]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7268176674842834, 0.6347265243530273, 0.2605873644351959, 0.11776149272918701, 0.3379240036010742, 0.516525387763977, 0.2995058298110962, 0.13594950735569, 0.16157478094100952, 0.3561588525772095]
Syntax-driven (Galley et al, 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process.	[211, 212, 210, 0, 202, 106, 196, 207, 5, 1]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7330108880996704, 0.33431941270828247, 0.0921727791428566, 0.5955684781074524, 0.12029776722192764, 0.2955085039138794, 0.14808204770088196, 0.20794759690761566, 0.2110876739025116, 0.12255147099494934]
System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994).	[69, 25, 9, 88, 14, 64, 13, 28, 8, 58]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7124255895614624, 0.43578869104385376, 0.35029685497283936, 0.41114217042922974, 0.04864651709794998, 0.1678089201450348, 0.08069918304681778, 0.0818084180355072, 0.07087001204490662, 0.14230066537857056]
TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al, 2006) and question answering (Harabagiu and Hickl, 2006).	[104, 5, 25, 4, 2, 40, 1, 0, 18, 13]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6816354990005493, 0.0871082991361618, 0.0693449079990387, 0.4178803861141205, 0.06675079464912415, 0.05980774015188217, 0.07635624706745148, 0.3064706027507782, 0.08004505187273026, 0.11056340485811234]
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	[188, 119, 149, 133, 228, 32, 174, 134, 61, 111]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7565197348594666, 0.6940149068832397, 0.7674335241317749, 0.4029770493507385, 0.4750477969646454, 0.4673248529434204, 0.3436797559261322, 0.20864833891391754, 0.21301794052124023, 0.12022783607244492]
Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C & C POS tagger (Curran and Clark, 2003).	[48, 60, 137, 18, 81, 106, 5, 90, 61, 101]	[1, 0, 0, 1, 0, 0, 0, 1, 0, 0]	[0.5909279584884644, 0.2630375027656555, 0.24792420864105225, 0.5141773223876953, 0.31732824444770813, 0.11853872239589691, 0.05267780274152756, 0.5345733761787415, 0.38936617970466614, 0.21387021243572235]
Takamura et al (2005) used the spin model to extract word semantic orientation.	[61, 0, 175, 171, 12, 105, 1, 18, 28, 188]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7569668292999268, 0.3973957598209381, 0.0990569144487381, 0.09437393397092819, 0.2888602614402771, 0.09626797586679459, 0.0647648274898529, 0.04914969578385353, 0.3761913478374481, 0.09058763831853867]
Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less.	[25, 24, 116, 15, 160, 163, 21, 137, 9, 150]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6349417567253113, 0.10384201258420944, 0.5875415205955505, 0.09564121812582016, 0.39343759417533875, 0.26527658104896545, 0.14109684526920319, 0.05824850872159004, 0.2546427845954895, 0.2309304028749466]
Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples.	[51, 166, 88, 79, 65, 177, 15, 97, 26, 116]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1374804824590683, 0.12592487037181854, 0.2684172987937927, 0.05650097131729126, 0.04583881050348282, 0.05414856970310211, 0.04546886309981346, 0.05756019800901413, 0.2712816596031189, 0.0450880192220211]
Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.	[72, 108, 50, 25, 24, 59, 20, 140, 45, 26]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19027414917945862, 0.38527655601501465, 0.08810947835445404, 0.2595980167388916, 0.04724157601594925, 0.18963004648685455, 0.06589512526988983, 0.07487479597330093, 0.051282577216625214, 0.05990234762430191]
Tests sentence pairs were manually aligned and were marked with both sure and possible alignments (Och and Ney 2000a).	[90, 83, 86, 2, 73, 81, 87, 93, 13, 80]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7129358053207397, 0.6461226940155029, 0.22892382740974426, 0.3194890022277832, 0.19429977238178253, 0.4318101704120636, 0.08218248933553696, 0.090947225689888, 0.2295871376991272, 0.4343472719192505]
Teufel and Moens (2002) introduced AZ and applied it to computational linguistics papers.	[186, 187, 68, 183, 76, 2, 7, 67, 69, 99]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21595677733421326, 0.4159663915634155, 0.3275023400783539, 0.2690550684928894, 0.18707892298698425, 0.1895042359828949, 0.1895042359828949, 0.25409501791000366, 0.18726471066474915, 0.04811735823750496]
Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.	[118, 10, 37, 22, 26, 5, 33, 46, 7, 6]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4737114906311035, 0.5344541668891907, 0.30963239073753357, 0.5407848358154297, 0.4150840640068054, 0.389241099357605, 0.3407115936279297, 0.11042533069849014, 0.3282988965511322, 0.05923458933830261]
TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation word repetition and computes similarities between textual units based on the similarities of word space vectors.	[44, 1, 84, 78, 80, 161, 154, 73, 66, 83]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08362632244825363, 0.07925587892532349, 0.08607196062803268, 0.06222547963261604, 0.05819960683584213, 0.0472082756459713, 0.05532069504261017, 0.06461470574140549, 0.148883655667305, 0.049749717116355896]
The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998).	[125, 11, 36, 134, 38, 89, 24, 56, 90, 35]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.707366943359375, 0.27294445037841797, 0.4313478469848633, 0.24525098502635956, 0.4607830047607422, 0.4551548659801483, 0.2752264142036438, 0.059282317757606506, 0.06771629303693771, 0.06682518124580383]
The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.	[117, 116, 240, 68, 14, 51, 38, 150, 124, 76]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6997823119163513, 0.5141481757164001, 0.5430814027786255, 0.6087220907211304, 0.407776802778244, 0.05849050357937813, 0.0827312245965004, 0.1192174106836319, 0.05154997110366821, 0.1403491050004959]
The CCG parser has been extensively evaluated elsewhere (Clark and Curran, 2007), and arguably GRs or predicate-argument structures provide a more suitable test set for the CCG parser than PTB phrase-structure trees.	[254, 277, 255, 700, 306, 67, 199, 783, 803, 308]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40382882952690125, 0.7635140419006348, 0.26645627617836, 0.4087139964103699, 0.44028419256210327, 0.17183943092823029, 0.11004582047462463, 0.25558555126190186, 0.24511082470417023, 0.38928288221359253]
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second.	[124, 51, 166, 127, 165, 161, 172, 181, 159, 177]	[1, 1, 1, 1, 0, 0, 1, 0, 1, 0]	[0.6626554131507874, 0.6262241005897522, 0.5149560570716858, 0.5691469311714172, 0.3940851390361786, 0.47943955659866333, 0.5264225006103516, 0.21692480146884918, 0.6252673268318176, 0.1274108737707138]
The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011).	[133, 87, 5, 35, 36, 102, 19, 34, 197, 84]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6785598397254944, 0.6547489166259766, 0.12848569452762604, 0.23245574533939362, 0.179850235581398, 0.2087290734052658, 0.08249104022979736, 0.13005833327770233, 0.10298395901918411, 0.048108942806720734]
The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al, 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003).	[59, 4, 12, 1, 26, 0, 3, 101, 56, 146]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21916301548480988, 0.11070787906646729, 0.17188511788845062, 0.08175832033157349, 0.07693953812122345, 0.21810364723205566, 0.04786962643265724, 0.09456443786621094, 0.07542447745800018, 0.10869189351797104]
The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing.	[346, 0, 217, 2, 6, 405, 5, 56, 18, 118]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.7197769284248352, 0.7914113402366638, 0.41831645369529724, 0.5280861854553223, 0.7030301094055176, 0.08851052820682526, 0.09295032918453217, 0.2031548023223877, 0.09484601020812988, 0.09923785924911499]
The CogNIAC algorithm (Baldwin, 1997) was designed for high-precision AR.	[194, 0, 9, 196, 110, 108, 185, 164, 107, 11]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4894394278526306, 0.6209570169448853, 0.29822447896003723, 0.5082052946090698, 0.4355068802833557, 0.12369371950626373, 0.28942587971687317, 0.2726810872554779, 0.07196767628192902, 0.0746375098824501]
The Context-Sensitive Paraphrase Suggestion (CS-PS) model first finds a set of local paraphrases P of the input phrase K using the pivot-based method proposed by Bannard and Callison-Burch (2005).	[3, 41, 20, 5, 85, 126, 32, 40, 88, 111]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41679441928863525, 0.33731964230537415, 0.15289351344108582, 0.12472762167453766, 0.0877273827791214, 0.09497197717428207, 0.11180002242326736, 0.09658467024564743, 0.18550069630146027, 0.14623627066612244]
The Context-Sensitive extension (Krahmer and Theune, 2002) is able to generate referring expressions for the most salient entity in a context; the Boolean Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups.	[0, 169, 195, 121, 77, 1, 4, 7, 74, 292]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.720700740814209, 0.7158324122428894, 0.415720671415329, 0.1951783001422882, 0.128165140748024, 0.14679959416389465, 0.14679959416389465, 0.19861926138401031, 0.13186803460121155, 0.08414509892463684]
The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees.	[144, 12, 3, 67, 62, 137, 45, 132, 108, 149]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6640917062759399, 0.3021925687789917, 0.1863260716199875, 0.1135677620768547, 0.5061672925949097, 0.09090457856655121, 0.10068583488464355, 0.08069615811109543, 0.0597924143075943, 0.06848165392875671]
The GATE API (Application Programming Interface) is fully documented in Javadoc and also examples are given in the comprehensive User Guide (Cunningham et al, 2002b).	[110, 125, 123, 124, 91, 228, 29, 164, 134, 18]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6426676511764526, 0.38934680819511414, 0.20941200852394104, 0.1259746551513672, 0.06875212490558624, 0.06875212490558624, 0.10552169382572174, 0.17473329603672028, 0.17587944865226746, 0.07119996845722198]
The Generative Lexicon Theory (GLT) (Pustejovsky, 1991, 1994c) can be said to take advantage of both linguistic and conceptual approaches, providing a framework which arose from the integration of linguistic studies and of techniques found in AI.	[15, 422, 6, 96, 427, 14, 121, 92, 73, 120]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09944614768028259, 0.12280793488025665, 0.10042092204093933, 0.17685046792030334, 0.07597146183252335, 0.08115259557962418, 0.2573387324810028, 0.05546860024333, 0.05851588398218155, 0.17212016880512238]
The German V-O pairs were extracted from a syntactic analysis of the HGC carried out using the BitPar parser (Schmid, 2004). We used only V-O pairs because they cons ti tute far more sense-discriminative contexts than, for example, verb-subject pairs, but we plan to examine these and other grammatical relationships in future work.	[30, 6, 5, 4, 3, 9, 26, 1, 24, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.766803503036499, 0.10782390087842941, 0.0713767260313034, 0.1275230348110199, 0.08688072115182877, 0.06789910793304443, 0.06446922570466995, 0.14388014376163483, 0.06692445278167725, 0.22441145777702332]
The Gildea and Hockenmaier (2003) system uses features extracted from Combinatory Categorial Grammar (CCG) corresponding to the features that were used by G&J and G&P systems.	[0, 63, 8, 20, 1, 58, 86, 101, 75, 29]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.742225706577301, 0.6708730459213257, 0.7080791592597961, 0.1261659562587738, 0.10560160875320435, 0.0736653208732605, 0.10607047379016876, 0.049816057085990906, 0.06454510986804962, 0.13347403705120087]
The Grefenstette (1994) relation extractor produces context relations that are then lemmatised using the Minnen et al (2000) morphological analyser.	[109, 129, 164, 72, 94, 13, 134, 23, 57, 105]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4793108403682709, 0.12845945358276367, 0.11854603886604309, 0.3359622657299042, 0.26141682267189026, 0.24312062561511993, 0.10770166665315628, 0.11738184094429016, 0.051730550825595856, 0.2854200303554535]
The HMM model of (Mann and Yarowsky, 2001) is of distinctly different design than our PHMM model.	[114, 158, 37, 7, 140, 43, 34, 155, 164, 13]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37620294094085693, 0.08846502751111984, 0.08846502751111984, 0.0949893519282341, 0.0949893519282341, 0.0482153445482254, 0.047795992344617844, 0.045827265828847885, 0.0482153445482254, 0.05914868041872978]
The Infectious Diseases (ID) task of the BioNLPShared Task 2011 (Kim et al, 2011a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases.	[25, 3, 1, 21, 2, 26, 14, 4, 70, 11]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.734898030757904, 0.605394184589386, 0.23406335711479187, 0.15358228981494904, 0.08027756214141846, 0.10475222021341324, 0.062481749802827835, 0.08625676482915878, 0.10071845352649689, 0.13372455537319183]
The Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two strings is the ratio of the length of their LCS and the length of the longer string.	[147, 146, 423, 302, 77, 204, 149, 44, 144, 234]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8016508221626282, 0.7026342749595642, 0.111927330493927, 0.17962932586669922, 0.22836171090602875, 0.0671582818031311, 0.2173914611339569, 0.14333829283714294, 0.057282425463199615, 0.08624283969402313]
The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training.	[103, 77, 61, 97, 156, 92, 59, 98, 83, 138]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4718852639198303, 0.4319513738155365, 0.7160079479217529, 0.3613940477371216, 0.18713200092315674, 0.09865529090166092, 0.376406192779541, 0.09423267096281052, 0.16872717440128326, 0.07783134281635284]
The NP chunks in the shared task data are base-NP chunks which are non-recursive NPs, a definition first proposed by Ramshaw and Marcus (1995).	[33, 2, 43, 82, 44, 8, 47, 25, 24, 26]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48594415187835693, 0.5956552624702454, 0.2734248638153076, 0.12239129096269608, 0.47243356704711914, 0.19186227023601532, 0.16528251767158508, 0.059709664434194565, 0.06403454393148422, 0.05646921694278717]
The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.	[127, 1, 12, 224, 223, 0, 6, 8, 4, 211]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7671989798545837, 0.37455469369888306, 0.5786253809928894, 0.3631889224052429, 0.1970742791891098, 0.11207293719053268, 0.24075722694396973, 0.1615365743637085, 0.15731722116470337, 0.11519069969654083]
The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003).	[251, 357, 371, 211, 317, 358, 86, 4, 13, 279]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12243445962667465, 0.37597617506980896, 0.10196082293987274, 0.19034479558467865, 0.21972393989562988, 0.17512284219264984, 0.12053439766168594, 0.04849198833107948, 0.04849198833107948, 0.0823713168501854]
The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999).	[0, 169, 167, 1, 170, 151, 33, 112, 168, 5]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8335934281349182, 0.2781500816345215, 0.23281919956207275, 0.232171893119812, 0.15947265923023224, 0.14892272651195526, 0.07667820900678635, 0.11287066340446472, 0.07693904638290405, 0.07887528836727142]
The REVERB extractor (Fader et al 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities.	[120, 71, 80, 109, 21, 147, 54, 24, 205, 64]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07797163724899292, 0.3323011100292206, 0.0869944840669632, 0.2590920925140381, 0.29486119747161865, 0.07180838286876678, 0.048104409128427505, 0.07567615061998367, 0.11635351181030273, 0.16553594172000885]
The Redwoods tree bank provides deeper semantics expressed in the Minimum Recursion Semantics formalism (Copestake et al, 2001), but in the present experiments we have not explored this fully.	[12, 5, 1, 105, 111, 14, 116, 40, 8, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7929827570915222, 0.26528242230415344, 0.1924813687801361, 0.08940237760543823, 0.07403156906366348, 0.16950823366641998, 0.2387317270040512, 0.09932275861501694, 0.22139385342597961, 0.09776600450277328]
The STRAND scores are similar to those published by Resnik (1999).	[28, 21, 107, 127, 77, 48, 26, 80, 45, 1]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5205339193344116, 0.6994565725326538, 0.21017970144748688, 0.06792350113391876, 0.23443835973739624, 0.1358906626701355, 0.052317164838314056, 0.10670454055070877, 0.06431688368320465, 0.6538475751876831]
The SemEval-2012 CLTE task (Negri et al, 2012) asks participants to judge entailment pairs in four language combinations, defining four target entailment relations, for ward, backward, bidirectional and no entailment.	[3, 0, 96, 32, 1, 88, 33, 34, 24, 26]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.707915186882019, 0.7905507683753967, 0.10689542442560196, 0.25333139300346375, 0.15773430466651917, 0.09592177718877792, 0.16961541771888733, 0.2943510115146637, 0.06790662556886673, 0.24004821479320526]
The TCD-M5P-resources-only submission ranked 5th (among 17) in the ranking task, and 5th among 19 (tied with two other systems) in the scoring task (Callison-Burch et al, 2012). Unfortunately the TCD-M5P-all submission contained an error.13 Below are the official results for TCD-M5P-resources-only and the corrected results for TCD-M5P-all: In four cases in which Google n-grams formed the reference data, the scores were computed using the wrong language (Spanish instead of English) as the reference.	[297, 302, 298, 402, 301, 287, 408, 407, 360, 286]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.630801260471344, 0.596229076385498, 0.10139472782611847, 0.4506440758705139, 0.21043482422828674, 0.04948114976286888, 0.06363993138074875, 0.07328914850950241, 0.15808384120464325, 0.05229705944657326]
The TextRunner system (Banko and Etzioni, 2008) is trained using a CRF classifier on S-V-O tuples from a parsed corpus as positive examples, and tuples that violate phrasal structure as negative ones.	[122, 94, 131, 15, 100, 44, 69, 130, 65, 71]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4617602825164795, 0.3032374978065491, 0.4103585183620453, 0.29177719354629517, 0.3904232382774353, 0.05277647450566292, 0.4470406174659729, 0.10322453826665878, 0.1119837760925293, 0.15231111645698547]
The Total column presents the number of extracted NEs and generated hypotheses and the Average column shows the average numbers per text respectively.2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al, 2005).	[123, 61, 15, 128, 116, 62, 142, 140, 80, 100]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33876049518585205, 0.08095719665288925, 0.10767221450805664, 0.09130993485450745, 0.11939381808042526, 0.11083930730819702, 0.0864655002951622, 0.06888741999864578, 0.08057258278131485, 0.12753914296627045]
The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al, 2011).	[7, 204, 15, 269, 5, 245, 276, 6, 264, 250]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7963809370994568, 0.785837709903717, 0.33048170804977417, 0.2781520187854767, 0.1337479203939438, 0.0855206623673439, 0.19966937601566315, 0.05962930619716644, 0.07316184788942337, 0.08061966300010681]
The Twitter dataset uses a domain-dependent tag set of 25 tags that are described in (Gimpel et al, 2011).	[13, 61, 9, 15, 18, 20, 3, 29, 1, 36]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06988407671451569, 0.2651495039463043, 0.06434756517410278, 0.06983290612697601, 0.4739040732383728, 0.2106466442346573, 0.0834265723824501, 0.05959875136613846, 0.07198099046945572, 0.35691818594932556]
The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003).	[1, 21, 44, 4, 51, 23, 59, 9, 96, 55]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.682895302772522, 0.26557838916778564, 0.210825577378273, 0.35229530930519104, 0.2542600929737091, 0.07340008020401001, 0.10577992349863052, 0.2859940230846405, 0.13780267536640167, 0.13122425973415375]
The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008).	[448, 27, 34, 3, 15, 249, 43, 79, 83, 402]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.45675256848335266, 0.09497242420911789, 0.09944424033164978, 0.31289032101631165, 0.31289032101631165, 0.2678001821041107, 0.07524722814559937, 0.2590497136116028, 0.40396633744239807, 0.10555039346218109]
The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley's algorithm.	[247, 310, 33, 4, 9, 381, 198, 426, 22, 486]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6384363770484924, 0.1173379197716713, 0.18048985302448273, 0.09691682457923889, 0.09691682457923889, 0.3534891903400421, 0.07734210044145584, 0.09109171479940414, 0.06455893069505692, 0.09489394724369049]
The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007).	[146, 0, 35, 206, 6, 161, 122, 232, 70, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7490071058273315, 0.7910773754119873, 0.2198641300201416, 0.09596846252679825, 0.12898047268390656, 0.09776216000318527, 0.31812143325805664, 0.13507477939128876, 0.1686592996120453, 0.17797133326530457]
The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.	[76, 15, 51, 77, 3, 25, 70, 41, 38, 198]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4448506832122803, 0.3434860408306122, 0.2989102900028229, 0.10177415609359741, 0.2352411150932312, 0.3526867628097534, 0.2418198585510254, 0.2636357545852661, 0.17890305817127228, 0.11976414173841476]
The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations.	[3, 9, 35, 152, 0, 25, 23, 56, 34, 116]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.7414527535438538, 0.7414527535438538, 0.22438214719295502, 0.5528979301452637, 0.5762649178504944, 0.15051324665546417, 0.15166239440441132, 0.0969359427690506, 0.07706712931394577, 0.3273569941520691]
The approach of classifying identified events into whether they fall under negation or speculation was followed by Sauri and Pustejovsky (2009) and the participants of the BioNLP? 09 Shared Task (Kim et al., 2009).	[58, 0, 59, 162, 165, 120, 61, 1, 12, 161]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2455403208732605, 0.7099813222885132, 0.39867711067199707, 0.25651660561561584, 0.1064196452498436, 0.09664692729711533, 0.16194750368595123, 0.10061868280172348, 0.18830756843090057, 0.09076332300901413]
The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002).	[31, 107, 4, 110, 0, 1, 3, 25, 123, 17]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6316436529159546, 0.085860975086689, 0.4280175566673279, 0.05944708734750748, 0.5738574266433716, 0.08507216721773148, 0.24330495297908783, 0.07222308963537216, 0.296725869178772, 0.25110265612602234]
The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text.	[41, 73, 2, 40, 16, 6, 109, 1, 57, 102]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5820412635803223, 0.3313779830932617, 0.30058106780052185, 0.2722615599632263, 0.11972199380397797, 0.1737532615661621, 0.07732266932725906, 0.16160285472869873, 0.32194024324417114, 0.0601813904941082]
The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble-based methods.	[134, 11, 2, 132, 14, 23, 75, 136, 49, 74]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5606992244720459, 0.39854127168655396, 0.19965419173240662, 0.6106998324394226, 0.36750271916389465, 0.16188010573387146, 0.2732252776622772, 0.3518432378768921, 0.05012958124279976, 0.050120431929826736]
The base system is an HMM based tagger, similar to (Bikel et al, 1997).	[111, 5, 3, 1, 6, 27, 135, 97, 30, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3452862501144409, 0.147548645734787, 0.08671780675649643, 0.044318780303001404, 0.22338131070137024, 0.06674204766750336, 0.08074698597192764, 0.2413100153207779, 0.0865427628159523, 0.048916179686784744]
The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005).	[205, 34, 116, 334, 81, 149, 228, 319, 180, 213]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24425426125526428, 0.07244418561458588, 0.10608948767185211, 0.15563295781612396, 0.12047237157821655, 0.11926855891942978, 0.09132220596075058, 0.11707364022731781, 0.05563027039170265, 0.16884420812129974]
The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).	[61, 65, 12, 10, 74, 29, 125, 69, 64, 188]	[1, 0, 0, 0, 1, 0, 1, 1, 1, 0]	[0.8134716153144836, 0.27356505393981934, 0.28910109400749207, 0.0712248757481575, 0.6820250749588013, 0.12464752793312073, 0.748824417591095, 0.655301034450531, 0.5858002305030823, 0.2961568534374237]
The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model.	[110, 10, 20, 5, 14, 132, 1, 22, 21, 71]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6954507231712341, 0.32083553075790405, 0.6269504427909851, 0.3685421645641327, 0.13781900703907013, 0.3017469346523285, 0.10751789063215256, 0.1545681208372116, 0.1965857446193695, 0.22253820300102234]
The basic surface realisation algorithm used is a bot tom up, tabular realisation algorithm (Kay, 1996) optimised for TAGs.	[118, 113, 32, 8, 35, 191, 143, 187, 86, 7]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.319872111082077, 0.717022716999054, 0.04689095541834831, 0.16257885098457336, 0.15684553980827332, 0.05114872008562088, 0.04689095541834831, 0.0678994357585907, 0.06619109958410263, 0.15954969823360443]
The best result known to us is achieved by Toutanova [2002] by enriching the feature representation of the MaxEnt approach [Ratnaparkhi, 1996].	[91, 17, 90, 82, 81, 97, 95, 96, 83, 85]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5313634276390076, 0.35346174240112305, 0.21148516237735748, 0.25892174243927, 0.09804237633943558, 0.09940294176340103, 0.0627584159374237, 0.05621291697025299, 0.05595025792717934, 0.06357328593730927]
The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules.	[157, 174, 23, 31, 8, 0, 5, 179, 147, 158]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6837847828865051, 0.4196605384349823, 0.10703950375318527, 0.12910997867584229, 0.09495476633310318, 0.37501394748687744, 0.11682731658220291, 0.15060804784297943, 0.2418239265680313, 0.22606050968170166]
The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008).	[27, 23, 10, 119, 117, 111, 120, 67, 71, 77]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34857648611068726, 0.13538746535778046, 0.2921115458011627, 0.05898621678352356, 0.28871455788612366, 0.07747199386358261, 0.13162074983119965, 0.05091812089085579, 0.07726853340864182, 0.06629426777362823]
The bootstrapping methods for language independent NER of Cucerzan and Yarowsky (1999) have a similar effect.	[133, 0, 5, 2, 157, 17, 61, 65, 196, 122]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5741901993751526, 0.7367160320281982, 0.044024571776390076, 0.29126811027526855, 0.24282079935073853, 0.35250991582870483, 0.3618435263633728, 0.10587456822395325, 0.22966067492961884, 0.06418966501951218]
The candidate argument extraction method used for the FrameNet data, (as mentioned in 4) was adapted from the algorithm of Xue and Palmer (2004) applied to dependency trees.	[74, 7, 75, 15, 49, 50, 72, 9, 10, 76]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0854719802737236, 0.6183744668960571, 0.08981110900640488, 0.07763592153787613, 0.07343333959579468, 0.059260379523038864, 0.046497050672769547, 0.049165740609169006, 0.07838468998670578, 0.10187193751335144]
The closest previous work is the detailed manual analysis performed by Levy and Manning (2003).	[144, 137, 138, 135, 108, 40, 25, 2, 106, 152]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5333948731422424, 0.6936636567115784, 0.5888970494270325, 0.44138431549072266, 0.1825060397386551, 0.07490761578083038, 0.09236659109592438, 0.16105327010154724, 0.0834461897611618, 0.04947790876030922]
The cluster output can then be used as classes for selectional preferences (Pereira et al, 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994).	[16, 144, 38, 40, 17, 143, 20, 2, 138, 10]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5808669924736023, 0.08688704669475555, 0.3984333574771881, 0.2598625421524048, 0.21247762441635132, 0.23571594059467316, 0.18949411809444427, 0.4509977102279663, 0.06717132776975632, 0.0720759928226471]
The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier.	[59, 6, 75, 143, 21, 17, 9, 24, 155, 96]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24443194270133972, 0.2195196896791458, 0.4620639383792877, 0.24580387771129608, 0.08818599581718445, 0.12325676530599594, 0.05872822925448418, 0.0479617603123188, 0.08794468641281128, 0.05045943707227707]
The complete merging process and the conversion from the constituent representation to dependencies is detailed in (Surdeanu et al, 2008).	[76, 302, 75, 298, 141, 16, 111, 115, 238, 113]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4040624797344208, 0.3746725022792816, 0.3951634168624878, 0.12773145735263824, 0.24871863424777985, 0.3130675256252289, 0.09670127183198929, 0.2671273648738861, 0.19094282388687134, 0.07403166592121124]
The computation of grammatical relations from shallow parsers or chunkers is still at an early stage (Buchholz et al, 1999, Carroll et al, 1998) and there are few other robust semantic processors, and none in the medical domain.	[11, 3, 52, 10, 24, 134, 74, 37, 187, 122]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7245035171508789, 0.7278057932853699, 0.6304844617843628, 0.36760377883911133, 0.09103827178478241, 0.12805010378360748, 0.11926276236772537, 0.054156556725502014, 0.12280193716287613, 0.05543188750743866]
The concept of compatible coverage vectors for the locations of translated words becomes the notion of reachability between frontier nodes in the lattice (Dyer et al, 2008).	[60, 63, 59, 64, 77, 55, 139, 66, 43, 68]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4875779449939728, 0.19527970254421234, 0.38142129778862, 0.09944232553243637, 0.156137615442276, 0.47807037830352783, 0.06945344060659409, 0.10530704259872437, 0.08136563748121262, 0.06235616281628609]
The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and see also Kaplan and Kay (1994).	[246, 132, 7, 309, 12, 158, 256, 2, 5, 553]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2381950318813324, 0.3762020766735077, 0.19561487436294556, 0.06344261765480042, 0.04967164620757103, 0.06551680713891983, 0.056582920253276825, 0.08109740912914276, 0.08109740912914276, 0.046807415783405304]
The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG).	[21, 138, 17, 86, 159, 126, 18, 3, 4, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17455410957336426, 0.38373374938964844, 0.12210935354232788, 0.05967315658926964, 0.06810945272445679, 0.052814140915870667, 0.24735717475414276, 0.05726496875286102, 0.10366258025169373, 0.06440500915050507]
The core scheme is in principle identical with the MUC coreference scheme and is restricted to the annotation of coreference in the sense of (van Deemter and Kibble, 2000).	[0, 74, 102, 1, 5, 30, 28, 113, 111, 65]	[1, 1, 0, 0, 0, 1, 1, 0, 0, 0]	[0.7234005331993103, 0.7115963101387024, 0.22664324939250946, 0.38082924485206604, 0.38521531224250793, 0.5778912305831909, 0.5371139645576477, 0.2021184116601944, 0.1101715937256813, 0.087215855717659]
The correct rate of tagging of these models has reached 95%, in part by using a very large amount of training data (e.g., 1,000,000 words in Schmid, 1994).	[15, 14, 147, 12, 16, 4, 165, 0, 8, 19]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.581522524356842, 0.2232215851545334, 0.32090920209884644, 0.4079762101173401, 0.24154968559741974, 0.04601461812853813, 0.15333572030067444, 0.07024799287319183, 0.22633898258209229, 0.15412172675132751]
The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004).	[1, 84, 149, 0, 85, 90, 127, 12, 125, 106]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6755803227424622, 0.6336653828620911, 0.18112383782863617, 0.6756388545036316, 0.23233994841575623, 0.4908496141433716, 0.3432144522666931, 0.23154406249523163, 0.17295952141284943, 0.10459161549806595]
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1).	[21, 136, 103, 42, 131, 43, 13, 102, 4, 94]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4830668866634369, 0.3805667757987976, 0.25651443004608154, 0.09920476377010345, 0.4001386761665344, 0.09654087573289871, 0.43524980545043945, 0.18488867580890656, 0.04528715834021568, 0.061690229922533035]
The data used in our initial English-only experiments were a set of 554 consumer reviews described in (McDonald et al, 2007).	[120, 84, 99, 106, 156, 144, 103, 85, 16, 28]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5133287310600281, 0.18192313611507416, 0.10533258318901062, 0.06665898114442825, 0.07002299278974533, 0.173257514834404, 0.11425814032554626, 0.09722842276096344, 0.10762718319892883, 0.051179032772779465]
The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998).	[39, 3, 11, 6, 25, 32, 40, 21, 37, 80]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7730347514152527, 0.3788473606109619, 0.11639055609703064, 0.07528761774301529, 0.10331439971923828, 0.20960970222949982, 0.14423339068889618, 0.08712366968393326, 0.05821061134338379, 0.049299415200948715]
The data used was the Microsoft Research Beijing corpus from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and we used the same train/test split used in the competition.	[0, 13, 1, 53, 118, 119, 42, 40, 88, 8]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8267384767532349, 0.5533533692359924, 0.37913838028907776, 0.3085302412509918, 0.3509511947631836, 0.14698217809200287, 0.41118374466896057, 0.18271756172180176, 0.29485270380973816, 0.44130009412765503]
The dataset was created following the crowdsourcing methodology proposed in (Negri et al, 2011), which consists of the following steps.	[25, 164, 98, 2, 9, 27, 13, 90, 68, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14573989808559418, 0.1317061483860016, 0.08984135836362839, 0.06453344225883484, 0.06101101264357567, 0.04810280725359917, 0.049988292157649994, 0.0539315864443779, 0.09625548869371414, 0.10317417234182358]
The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice.	[50, 52, 16, 55, 8, 47, 44, 31, 21, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18867555260658264, 0.3614997863769531, 0.064815953373909, 0.06451836228370667, 0.17388978600502014, 0.07212329655885696, 0.07713381946086884, 0.05167215317487717, 0.0796210914850235, 0.13172894716262817]
The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006).	[86, 125, 7, 91, 115, 96, 118, 18, 135, 120]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.5993285775184631, 0.22097162902355194, 0.1064898893237114, 0.08620613813400269, 0.09781485050916672, 0.1146778017282486, 0.1597796231508255, 0.6666226983070374, 0.15908688306808472, 0.05909854173660278]
The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies.	[72, 83, 74, 88, 76, 52, 59, 124, 38, 71]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6569588780403137, 0.6090434193611145, 0.13668493926525116, 0.4047883152961731, 0.6020015478134155, 0.10420622676610947, 0.2436937689781189, 0.16829265654087067, 0.10977818816900253, 0.440528929233551]
The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al, 2004).	[75, 5, 80, 98, 1, 79, 6, 126, 76, 101]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4663334786891937, 0.543095052242279, 0.17773152887821198, 0.19317546486854553, 0.21744081377983093, 0.3652305603027344, 0.1106674075126648, 0.07661784440279007, 0.06920111924409866, 0.05853016674518585]
The development of the WordFrame model was motivated by work originally presented in Yarowsky and Wicentowski (2000).	[110, 47, 129, 4, 36, 28, 56, 130, 93, 33]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11355645954608917, 0.11829765886068344, 0.3069344758987427, 0.3645598292350769, 0.11298170685768127, 0.048582419753074646, 0.05157093331217766, 0.09671369194984436, 0.1317632496356964, 0.04459408298134804]
The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states.	[136, 3, 26, 94, 90, 34, 0, 1, 160, 49]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5750985145568848, 0.05225399509072304, 0.3241618573665619, 0.6073527336120605, 0.3585990369319916, 0.06106996908783913, 0.17223139107227325, 0.1264677792787552, 0.05716588720679283, 0.09115278720855713]
The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus.	[35, 0, 25, 29, 23, 24, 80, 28, 83, 6]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7499638199806213, 0.5435636043548584, 0.590843915939331, 0.22500622272491455, 0.40261679887771606, 0.4693993628025055, 0.22807183861732483, 0.04267938435077667, 0.22839364409446716, 0.18165211379528046]
The end result of our selection and aggregation module (see section 6.2) is a fully specified logical form which is to be sent to the Semantic-Head Driven Generation component of Gemini (Shieber et al, 1990).	[0, 18, 182, 99, 54, 57, 56, 30, 270, 108]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8397920727729797, 0.613507091999054, 0.2647484540939331, 0.32694950699806213, 0.1511675864458084, 0.3988876938819885, 0.2510184645652771, 0.2931443154811859, 0.12381089478731155, 0.09471659362316132]
The entity grid approach has already been applied to many applications relying on local coherence estimation: summary rating (Barzilay and Lapata, 2005), essay scoring (Burstein et al, 2010) or story generation (McIntyre and Lapata,2010).	[0, 133, 6, 93, 26, 199, 63, 122, 171, 82]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3475014567375183, 0.5139212608337402, 0.35524362325668335, 0.38602215051651, 0.4977303445339203, 0.19841662049293518, 0.09710080921649933, 0.08663355559110641, 0.12901492416858673, 0.12946976721286774]
The evaluation data comes from the WSI task of SemEval-2007 (Agirre and Soroa, 2007).	[187, 0, 84, 194, 3, 185, 8, 6, 12, 162]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6220319271087646, 0.7909664511680603, 0.49337780475616455, 0.18558555841445923, 0.23325718939304352, 0.22603930532932281, 0.36915522813796997, 0.09090792387723923, 0.04792128875851631, 0.23053763806819916]
The explanation for such a behavior is: since we are not throwing away any infrequent word pairs, PMI will rank pairs with low frequency counts higher (Church and Hanks, 1989).	[52, 51, 37, 40, 38, 46, 45, 99, 81, 61]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.5394256114959717, 0.6321026086807251, 0.21879100799560547, 0.5821375846862793, 0.5915548801422119, 0.25348228216171265, 0.04901409149169922, 0.06467079371213913, 0.15744268894195557, 0.12674126029014587]
The extractive approach is represented by MEAD*, which is adapted from the open source summarization framework MEAD (Radev et al., 2000).	[18, 20, 1, 53, 19, 54, 15, 3, 0, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08652769029140472, 0.27698469161987305, 0.10249669849872589, 0.1978909820318222, 0.05407555773854256, 0.10118236392736435, 0.2554229199886322, 0.13673678040504456, 0.3366718292236328, 0.2643905282020569]
The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim.	[28, 29, 72, 99, 40, 33, 160, 64, 37, 27]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.428476482629776, 0.12212992459535599, 0.16358503699302673, 0.08474226295948029, 0.26297542452812195, 0.21023443341255188, 0.06281313300132751, 0.18713171780109406, 0.111203633248806, 0.08592110127210617]
The feature semantic class used by Cardie and Wagstaff (1999) seems to be a domain-dependent one which can only be used for the MUC domain and similar ones.	[86, 18, 8, 77, 167, 150, 125, 116, 47, 121]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47254475951194763, 0.3129737973213196, 0.1676284521818161, 0.04632124677300453, 0.06965210288763046, 0.05893896147608757, 0.05730080232024193, 0.04716593772172928, 0.0538933202624321, 0.24947461485862732]
The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features.	[45, 44, 46, 29, 2, 14, 10, 19, 25, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29061242938041687, 0.08355101197957993, 0.09644579887390137, 0.10818573832511902, 0.07777837663888931, 0.12958410382270813, 0.07744617015123367, 0.055230092257261276, 0.057094406336545944, 0.06393105536699295]
The features used here are grouped according to variables, which define feature sub-spaces as in Charniak and Johnson (2001) and Zhang and Weng (2005).	[55, 65, 53, 85, 51, 82, 83, 84, 52, 60]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5292297005653381, 0.5315033197402954, 0.24543261528015137, 0.4708181321620941, 0.17804645001888275, 0.40561550855636597, 0.3680911064147949, 0.43040353059768677, 0.38012853264808655, 0.2297152429819107]
The features we use to build the classifier are generated from the templates of Ng and Low (2004).	[88, 113, 20, 104, 35, 89, 24, 45, 4, 23]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6891741156578064, 0.7529928088188171, 0.29296162724494934, 0.33590832352638245, 0.23684723675251007, 0.48116227984428406, 0.17896124720573425, 0.04836029186844826, 0.40331125259399414, 0.05466786399483681]
The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002).	[64, 66, 182, 147, 118, 65, 379, 57, 373, 81]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29418033361434937, 0.5308775901794434, 0.17609469592571259, 0.1112741082906723, 0.0438462495803833, 0.0963587760925293, 0.15063625574111938, 0.0496574304997921, 0.06547729671001434, 0.05855473875999451]
The first attempt to detect sub-sentential fragments from comparable sentences is (Munteanuand Marcu, 2006).	[0, 160, 1, 158, 163, 50, 19, 2, 20, 14]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7113761305809021, 0.6652861833572388, 0.3952043652534485, 0.4734290540218353, 0.34908339381217957, 0.4357374310493469, 0.3058615028858185, 0.12377327680587769, 0.31234225630760193, 0.08844874799251556]
The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008).	[1, 7, 3, 2, 4, 28, 5, 11, 19, 66]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7655856609344482, 0.7461927533149719, 0.6842488646507263, 0.13997220993041992, 0.31605517864227295, 0.4040950834751129, 0.37626659870147705, 0.057430680841207504, 0.10926777869462967, 0.0872037410736084]
The first demonstration of using charts for generation appeared in Shieber (1988).	[59, 147, 85, 87, 105, 93, 51, 61, 211, 126]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30084216594696045, 0.1668887585401535, 0.062015123665332794, 0.049525320529937744, 0.20357060432434082, 0.061002202332019806, 0.1165088564157486, 0.06038951501250267, 0.05068698897957802, 0.084972083568573]
The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank:2 TreeTagger (Schmid, 1994), which uses decision trees, TnT (Brants, 2000), which uses a hidden Markov model, SVMtool (Gimenez and Ma`rquez, 2004), which is based on support vector machines, and ACOPOST (Schroder, 2002), implementing the memory-based model of Daelemans et al (1996).	[15, 10, 146, 78, 8, 158, 54, 53, 126, 25]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4717854857444763, 0.17966945469379425, 0.22528810799121857, 0.4525804817676544, 0.12719497084617615, 0.06688972562551498, 0.1742853820323944, 0.04444701224565506, 0.0503164567053318, 0.3422389030456543]
The first length based algorithm was proposed in (Brown et al, 1991).	[49, 144, 56, 16, 15, 14, 115, 135, 94, 124]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2370150089263916, 0.12247546017169952, 0.20002958178520203, 0.09847025573253632, 0.09761814028024673, 0.30630889534950256, 0.46446120738983154, 0.1838323026895523, 0.05263673514127731, 0.0498526357114315]
The first method is based on a decision threshold (Dagan and Itai, 1994): the algorithm rejects decisions taken when the difference of the maximum likelihood among the competing senses is not big enough.	[208, 153, 324, 167, 162, 546, 548, 550, 221, 545]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.650743842124939, 0.25454193353652954, 0.18269804120063782, 0.30774351954460144, 0.31248173117637634, 0.1072564646601677, 0.10866223275661469, 0.06534378230571747, 0.17564170062541962, 0.09373345971107483]
The first of these is Minimality, defined as the proportion of descriptions produced by a system that are maximally brief, as per the original definition in Dale (1989).	[5, 11, 3, 6, 2, 25, 49, 61, 7, 119]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42302048206329346, 0.24530211091041565, 0.05817577615380287, 0.05181167647242546, 0.17009389400482178, 0.06433873623609543, 0.07016252726316452, 0.07296042889356613, 0.04901382699608803, 0.07044991105794907]
The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al (2009).	[49, 398, 132, 24, 23, 185, 21, 187, 330, 513]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35748291015625, 0.4615306854248047, 0.2129974067211151, 0.14730249345302582, 0.06649220734834671, 0.2743660509586334, 0.05569733679294586, 0.096426822245121, 0.349001944065094, 0.138455331325531]
The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007).	[75, 76, 48, 127, 63, 46, 125, 7, 79, 19]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6098533868789673, 0.24769467115402222, 0.2011677771806717, 0.2011677771806717, 0.4147854149341583, 0.043098945170640945, 0.043098945170640945, 0.12172722816467285, 0.09467847645282745, 0.2144564390182495]
The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files.	[112, 113, 103, 163, 66, 156, 22, 116, 160, 118]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20249512791633606, 0.14197023212909698, 0.20613917708396912, 0.2120387703180313, 0.0900539755821228, 0.06701002269983292, 0.20694762468338013, 0.25898003578186035, 0.15109948813915253, 0.187158465385437]
The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).	[189, 185, 7, 165, 153, 199, 175, 46, 119, 26]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4582306146621704, 0.5124147534370422, 0.176277294754982, 0.5121219754219055, 0.4382418990135193, 0.29702380299568176, 0.49878832697868347, 0.05405072122812271, 0.18576785922050476, 0.16135787963867188]
The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007).	[7, 12, 21, 57, 60, 48, 4, 41, 80, 106]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3612116873264313, 0.07834853231906891, 0.16484344005584717, 0.10414981842041016, 0.1883847862482071, 0.22511990368366241, 0.20391924679279327, 0.05756169930100441, 0.16887696087360382, 0.14354078471660614]
The following models are used as benchmark: (i) PYTHY (Toutanova et al, 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models.	[97, 3, 105, 12, 94, 7, 98, 13, 44, 113]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6223781108856201, 0.6266158819198608, 0.18673276901245117, 0.3902638852596283, 0.11530651152133942, 0.14849741756916046, 0.04480301961302757, 0.16650334000587463, 0.13544754683971405, 0.12232044339179993]
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006).	[116, 129, 40, 117, 144, 41, 4, 113, 115, 74]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4441384971141815, 0.5827462077140808, 0.14412136375904083, 0.12688089907169342, 0.07100999355316162, 0.24281568825244904, 0.18756920099258423, 0.3665836453437805, 0.2595464885234833, 0.10091812163591385]
The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007).	[57, 40, 53, 55, 86, 65, 72, 0, 70, 17]	[1, 1, 0, 1, 1, 1, 0, 0, 0, 0]	[0.7330758571624756, 0.7373447418212891, 0.4520656168460846, 0.60612553358078, 0.5401683449745178, 0.5873492956161499, 0.41604751348495483, 0.4157634377479553, 0.45247238874435425, 0.2799687385559082]
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009).	[172, 9, 5, 117, 36, 113, 104, 13, 122, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47133228182792664, 0.4873315691947937, 0.06502929329872131, 0.047019414603710175, 0.32489320635795593, 0.05473150312900543, 0.0659424215555191, 0.052536316215991974, 0.10879994183778763, 0.04806113243103027]
The generator is informed by a corpus study of embedded discourse units on two discourse annotated corpora: the RST Discourse Treebank (Carlson et al., 2001) and the Penn Discourse Treebank.	[16, 143, 158, 134, 140, 37, 28, 18, 25, 41]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6729400157928467, 0.38585031032562256, 0.1508343368768692, 0.15279752016067505, 0.1641131192445755, 0.13310985267162323, 0.10253887623548508, 0.13971605896949768, 0.09784757345914841, 0.053029175847768784]
The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001).	[48, 13, 87, 2, 8, 16, 28, 100, 14, 15]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43777400255203247, 0.16212373971939087, 0.05042114481329918, 0.07574938237667084, 0.05496642366051674, 0.27712416648864746, 0.18642938137054443, 0.059466149657964706, 0.14076006412506104, 0.4027637839317322]
The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al, 2005) roles projected onto it (Boxwell and White, 2008).	[323, 0, 353, 222, 400, 17, 127, 42, 5, 10]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7689997553825378, 0.3339821398258209, 0.18686559796333313, 0.6794896125793457, 0.05569934844970703, 0.08272143453359604, 0.14223532378673553, 0.12604057788848877, 0.056971292942762375, 0.056971292942762375]
The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by (Church, 1988), has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation (WSD).	[109, 50, 22, 44, 115, 57, 84, 36, 85, 110]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6546053886413574, 0.28279128670692444, 0.1525028645992279, 0.0815841555595398, 0.04914473742246628, 0.1523011475801468, 0.08505653589963913, 0.07279949635267258, 0.04903976991772652, 0.15822120010852814]
The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora.	[105, 35, 4, 12, 13, 3, 16, 8, 22, 71]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43071624636650085, 0.05003788322210312, 0.3513166904449463, 0.05486401543021202, 0.4114709794521332, 0.11727676540613174, 0.17146053910255432, 0.11387142539024353, 0.05052580311894417, 0.2581595480442047]
The idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in Riloff and Shepherd (1997) and Roark and Charniak (1998).	[158, 87, 26, 37, 46, 176, 48, 25, 149, 123]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4601610004901886, 0.5539184212684631, 0.35299152135849, 0.280985563993454, 0.42977580428123474, 0.12454766035079956, 0.4642614424228668, 0.06690685451030731, 0.0698150172829628, 0.3100665807723999]
The idea of searching a large corpus for specific lexico-syntactic phrases to indicate a semantic relation of interest was first described by Hearst (1992).	[2, 36, 18, 34, 59, 6, 153, 52, 84, 44]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.6351963877677917, 0.5738136172294617, 0.19753235578536987, 0.4743000268936157, 0.456792414188385, 0.2507140636444092, 0.18995335698127747, 0.09887463599443436, 0.4964641034603119, 0.6031684875488281]
The idea thata stem and stem+affix should be semantically similar has been exploited previously for morphological analysis (Schone and Jurafsky, 2000).	[4, 19, 6, 15, 39, 2, 3, 14, 31, 28]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7282181978225708, 0.3271973729133606, 0.11937905848026276, 0.10461119562387466, 0.1087643951177597, 0.06283138692378998, 0.07958260923624039, 0.06694670021533966, 0.1554892659187317, 0.05005233362317085]
The incremental algorithm (Reiter and Dale, 1992) is the most widely discussed attribute selection algorithm.	[63, 57, 126, 166, 6, 0, 117, 162, 5, 164]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.048463787883520126, 0.23578399419784546, 0.26230588555336, 0.0958707183599472, 0.1517544686794281, 0.05933501571416855, 0.16496895253658295, 0.07487592101097107, 0.05690362676978111, 0.07223264873027802]
The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart.	[73, 181, 97, 180, 15, 87, 99, 184, 106, 154]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12040114402770996, 0.057758793234825134, 0.060568783432245255, 0.0692811906337738, 0.08519106358289719, 0.09586285799741745, 0.04757917299866676, 0.05269278585910797, 0.051434315741062164, 0.04743532836437225]
The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al (2007), who successfully used sense information to boost state-of-the-art statistical MT.	[0, 9, 1, 19, 20, 104, 17, 3, 35, 50]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7635583281517029, 0.25591841340065, 0.08787266165018082, 0.12363740801811218, 0.08544518798589706, 0.051663778722286224, 0.49738967418670654, 0.12368632107973099, 0.10612095147371292, 0.11907604336738586]
The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al, 1994) for predictability.	[63, 9, 101, 82, 5, 100, 24, 4, 56, 30]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19130274653434753, 0.06042654439806938, 0.10998773574829102, 0.08408226072788239, 0.06572484225034714, 0.07967457175254822, 0.042959682643413544, 0.055357109755277634, 0.167144313454628, 0.05592722073197365]
The language model described by Chelba and Jelinek (1998) similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords.	[14, 82, 1, 4, 2, 83, 0, 18, 36, 28]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7118445038795471, 0.5973339676856995, 0.20313459634780884, 0.22542352974414825, 0.15381784737110138, 0.08387504518032074, 0.7357240319252014, 0.08034159243106842, 0.13441213965415955, 0.0595880001783371]
The largest step towards an automatically trainable spelling system was the statistical model for spelling errors (Brill and Moore, 2000).	[10, 97, 13, 64, 0, 3, 9, 22, 70, 132]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.39668476581573486, 0.41697102785110474, 0.13393040001392365, 0.4282407760620117, 0.7408207058906555, 0.18826302886009216, 0.17404790222644806, 0.09904088079929352, 0.25467613339424133, 0.14639730751514435]
The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004).	[24, 85, 33, 37, 1, 153, 130, 14, 146, 143]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.7670219540596008, 0.764649510383606, 0.6011596918106079, 0.08587560057640076, 0.2596111595630646, 0.6192767024040222, 0.38147661089897156, 0.3397064805030823, 0.35583698749542236, 0.27104097604751587]
The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.	[50, 18, 29, 58, 66, 24, 57, 80, 1, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4054346978664398, 0.23424647748470306, 0.0745960921049118, 0.1564822643995285, 0.05778001993894577, 0.10246497392654419, 0.06090864911675453, 0.06387408822774887, 0.1969851851463318, 0.22033673524856567]
The latter are small and simple (Alshawi et al, 2000): tree nodes are words, and there need be no other structure to recover or align.	[67, 86, 181, 69, 92, 24, 68, 170, 87, 140]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6165812015533447, 0.4437708854675293, 0.1152418851852417, 0.5573024749755859, 0.07940852642059326, 0.07858104258775711, 0.37354204058647156, 0.2525431215763092, 0.4012681543827057, 0.16320648789405823]
The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b).	[30, 23, 35, 52, 56, 21, 31, 46, 113, 29]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.7788764238357544, 0.1499379724264145, 0.20544132590293884, 0.09475753456354141, 0.06044270470738411, 0.11338064819574356, 0.07434865087270737, 0.2573690414428711, 0.6306720972061157, 0.14604341983795166]
The lexicon match is not based on direct unification of the target phrase's semantics with that of its head, a fundamental requirement of the bottom-up head-driven algorithm of Shieber et al (1989) and Van Noord (1990).	[0, 21, 20, 3, 152, 76, 81, 19, 92, 125]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7739222049713135, 0.6183449029922485, 0.3115594685077667, 0.29257285594940186, 0.4001957178115845, 0.3882919251918793, 0.3140736520290375, 0.08644343167543411, 0.10012776404619217, 0.12304043024778366]
The mapping for the dependents in the alternation can be taken from existing lexical resources (Dorr, 1997), learned from corpora (McCarthy, 2000) or learned from existing lexicons (Bond et al, 2002).	[37, 21, 91, 61, 43, 188, 219, 168, 31, 58]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17714321613311768, 0.08582821488380432, 0.05032781884074211, 0.05709950998425484, 0.056056778877973557, 0.05327684432268143, 0.0508788526058197, 0.06993214786052704, 0.1362476944923401, 0.04475255310535431]
The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best.	[203, 184, 187, 68, 250, 20, 257, 40, 198, 170]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4365088939666748, 0.43691372871398926, 0.2502965033054352, 0.09827587008476257, 0.34397077560424805, 0.19790172576904297, 0.10889476537704468, 0.04928981885313988, 0.29167264699935913, 0.13568657636642456]
The method was invented to meet the needs of applications using WYSIWYM editing (Power and Scott, 1998), which allow an author to control the content of an automatically generated text without prior training in knowledge engineering.	[33, 39, 38, 29, 43, 6, 31, 36, 46, 127]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5147091150283813, 0.5798439383506775, 0.548809826374054, 0.2649724781513214, 0.2602180540561676, 0.2539181709289551, 0.20203858613967896, 0.1183452159166336, 0.16695202887058258, 0.1384694129228592]
The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.	[32, 94, 130, 75, 51, 93, 11, 84, 154, 91]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.07162845879793167, 0.1018608808517456, 0.092422254383564, 0.2443457394838333, 0.6914429664611816, 0.3016138970851898, 0.06872522085905075, 0.06754078716039658, 0.29940444231033325, 0.05562296137213707]
The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008).	[0, 111, 138, 135, 146, 149, 152, 142, 116, 106]	[1, 0, 0, 1, 0, 0, 1, 0, 1, 0]	[0.6231642961502075, 0.2346377670764923, 0.2913128137588501, 0.5106084942817688, 0.2736703157424927, 0.2328813225030899, 0.5746483206748962, 0.08056826889514923, 0.6719440817832947, 0.41846203804016113]
The monologue side has been annotated with discourse relations, using an adaptation of the annotation guidelines of Carlson and Marcu (2001), whereas the dialogue side has been marked up with dialogue acts, using tags inspired by the schemes of Bunt (2000), Carletta et al. (1997) and Core and Allen (1997).	[29, 52, 128, 1, 49, 282, 302, 2, 5, 18]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0530906543135643, 0.059295229613780975, 0.05719014257192612, 0.05120167136192322, 0.06447912007570267, 0.0846124067902565, 0.19137558341026306, 0.05120167136192322, 0.30024054646492004, 0.2278459072113037]
The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work.	[0, 8, 145, 9, 41, 28, 36, 42, 62, 34]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7713606953620911, 0.6414713263511658, 0.3455125391483307, 0.08893618732690811, 0.22171959280967712, 0.04863019660115242, 0.061030834913253784, 0.052006181329488754, 0.08825987577438354, 0.07555913925170898]
The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.	[67, 65, 139, 134, 159, 56, 74, 229, 186, 206]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 1]	[0.146038219332695, 0.49031636118888855, 0.20347486436367035, 0.5192362666130066, 0.4300594925880432, 0.3728627562522888, 0.16459079086780548, 0.25371766090393066, 0.06988189369440079, 0.5192362666130066]
The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al, 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al, 2003).	[94, 133, 78, 121, 128, 123, 39, 71, 0, 76]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09600400179624557, 0.09893136471509933, 0.07684352993965149, 0.1232215091586113, 0.1335896998643875, 0.055298108607530594, 0.2591666579246521, 0.06417413800954819, 0.19159196317195892, 0.053232092410326004]
The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998).	[1, 10, 0, 44, 16, 2, 17, 78, 91, 68]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41244933009147644, 0.05043531209230423, 0.48829010128974915, 0.04325159266591072, 0.12138088792562485, 0.0631013959646225, 0.07692944258451462, 0.18812426924705505, 0.2384016066789627, 0.07403457164764404]
The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al, 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes.	[16, 4, 88, 45, 101, 10, 97, 0, 9, 33]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7224935293197632, 0.3737093210220337, 0.1274791806936264, 0.15912678837776184, 0.09432262182235718, 0.2794141471385956, 0.14528027176856995, 0.5992387533187866, 0.2966690957546234, 0.12355197966098785]
The most well-known of work from this period is that of Mann and Thompson (1988), Grosz and Sidner (1986b), Moore and Moser (1996), Polanyi and van den Berg (1996), and Asher and Lascarides (2003).	[401, 43, 478, 461, 583, 59, 604, 90, 582, 37]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 1]	[0.4524654448032379, 0.12374625355005264, 0.11575907468795776, 0.2741036117076874, 0.0563536211848259, 0.052435096353292465, 0.06329669803380966, 0.6482374668121338, 0.08246117830276489, 0.6651952266693115]
The other metric is ROUGE (Lin and Och, 2004), here named R.	[27, 162, 30, 5, 31, 115, 209, 181, 206, 210]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07093162834644318, 0.06577282398939133, 0.13576963543891907, 0.059464551508426666, 0.14681179821491241, 0.05239664018154144, 0.10681820660829544, 0.058087125420570374, 0.07093162834644318, 0.14681179821491241]
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009).	[190, 59, 29, 164, 221, 165, 188, 185, 60, 183]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3421892821788788, 0.16260144114494324, 0.1483047604560852, 0.05387295037508011, 0.05204277113080025, 0.04513275623321533, 0.05200891196727753, 0.057799845933914185, 0.2156648337841034, 0.06746871024370193]
The other tagger was the rule-based tagger of Brill (Brill, 1995).	[258, 36, 98, 78, 254, 16, 257, 182, 129, 179]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7101053595542908, 0.772402286529541, 0.3285498023033142, 0.1937023103237152, 0.4688984751701355, 0.245526522397995, 0.14874549210071564, 0.21491897106170654, 0.1102343499660492, 0.48150330781936646]
The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a).	[31, 111, 4, 48, 148, 32, 153, 2, 50, 159]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.756867527961731, 0.31736311316490173, 0.3679851293563843, 0.19148565828800201, 0.1933884173631668, 0.7635943293571472, 0.4968682527542114, 0.17395776510238647, 0.2447003573179245, 0.412850022315979]
The parser only used a subset of CCG, pureCCG (Eisner, 1996), consisting of the Application and Composition rules.	[22, 126, 40, 64, 147, 30, 95, 105, 94, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2863960862159729, 0.4759204387664795, 0.10549138486385345, 0.24296517670154572, 0.10054036974906921, 0.16593392193317413, 0.17151658236980438, 0.24932003021240234, 0.19833284616470337, 0.08490915596485138]
The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004).	[158, 86, 134, 160, 137, 133, 18, 121, 130, 164]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7350524067878723, 0.2294803112745285, 0.3710566759109497, 0.27131548523902893, 0.42105644941329956, 0.3271287679672241, 0.09923604875802994, 0.19791577756404877, 0.34434330463409424, 0.26691922545433044]
The pioneering work on fusion is Barzilay and McKeown (2005), which introduces the frame work used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output.	[113, 198, 130, 124, 128, 442, 144, 176, 426, 104]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.6710011959075928, 0.7593511343002319, 0.485483318567276, 0.29600003361701965, 0.15631753206253052, 0.2447061836719513, 0.2462444305419922, 0.4949645698070526, 0.5138165354728699, 0.22964239120483398]
The polarity of each word in arguments is derived from Multi-perspective Question Answering Opinion Corpus (MPQA) (Wilson et al, 2009).	[49, 91, 50, 92, 133, 24, 538, 131, 132, 23]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.765301525592804, 0.7279433012008667, 0.09127884358167648, 0.13987784087657928, 0.08884350210428238, 0.09112908691167831, 0.06102801486849785, 0.2526002526283264, 0.09588155895471573, 0.07239369302988052]
The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al, 1993), and proved highly beneficial also in our case.	[163, 75, 35, 104, 195, 301, 12, 113, 124, 83]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6288484334945679, 0.7691797018051147, 0.14007389545440674, 0.4085010290145874, 0.5518659353256226, 0.3005339503288269, 0.18408003449440002, 0.4568120539188385, 0.2184508591890335, 0.05574628338217735]
The precision observed for this task is comparable to precision obtained for Country-Capital and Country-Language in a previous single-language acquisition study (Davidov et al, 2007).	[55, 35, 113, 112, 25, 131, 56, 109, 117, 62]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5302249193191528, 0.2800440788269043, 0.11511087417602539, 0.06832718849182129, 0.18191979825496674, 0.09816601872444153, 0.07880786806344986, 0.06139116734266281, 0.05994980409741402, 0.05877089500427246]
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	[17, 208, 2, 9, 27, 31, 6, 261, 251, 207]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4729434847831726, 0.4344022572040558, 0.19573289155960083, 0.7018985152244568, 0.3875771164894104, 0.08925018459558487, 0.05815635249018669, 0.0886651799082756, 0.05701357498764992, 0.0625104084610939]
The problem of constructing an explicit theory of infhrence for DATR was originally addressed in (Evans and Gazdar, 1989a).	[0, 4, 68, 46, 52, 67, 45, 8, 44, 66]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7336132526397705, 0.28444916009902954, 0.15988825261592865, 0.088383749127388, 0.07902362197637558, 0.09476551413536072, 0.060215167701244354, 0.20093145966529846, 0.043816640973091125, 0.05620410293340683]
The problem, which was described in (Melamed, 1997) in a word-to-word alignment context, is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation.	[39, 149, 71, 27, 31, 143, 15, 125, 45, 74]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.45223596692085266, 0.14307962357997894, 0.06146906688809395, 0.07372169941663742, 0.15737859904766083, 0.12262021750211716, 0.051120467483997345, 0.06735533475875854, 0.06094210594892502, 0.0662018284201622]
The procedure is obtained by making some variations to the algorithm designed by Resnik (1995) for disambiguating noun groups.	[138, 0, 26, 56, 82, 111, 21, 84, 3, 131]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5972971320152283, 0.7771869897842407, 0.3244938552379608, 0.32815244793891907, 0.275138258934021, 0.10775569081306458, 0.19054076075553894, 0.14091856777668, 0.1374606490135193, 0.2892889678478241]
The recent availability of more corpora has enabled much information about dependency relations to be obtained by using a Japanese dependency analyzer such as KNP (Kurohashi and Nagao, 1994) or CaboCha (Kudo and Matsumoto,2002).	[127, 0, 5, 15, 12, 59, 1, 25, 145, 29]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5548810958862305, 0.6429455280303955, 0.08585909008979797, 0.09842534363269806, 0.09543124586343765, 0.2956044673919678, 0.23248468339443207, 0.12870822846889496, 0.33771100640296936, 0.4053204655647278]
The regulating aspects of semantic orientation of a text are natural language context information (Pang et al, 2002) language properties (Wiebe and Mihalcea, 2006), domain pragmatic knowledge (Aue and Gamon, 2005) and lastly most challenging is the time dimension (Read, 2005).	[115, 88, 114, 19, 89, 3, 53, 90, 21, 107]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13915123045444489, 0.13976509869098663, 0.2778579890727997, 0.0972580686211586, 0.187189981341362, 0.10243568569421768, 0.11713644862174988, 0.15351814031600952, 0.08834302425384521, 0.05272044986486435]
The respective dependency parse tree is included through following the shortest dependency path hypothesis (Bunescu and Mooney, 2005), by using the syntactical and dependency information of edges (e) and vertices (v).	[250, 0, 34, 249, 140, 14, 9, 226, 222, 36]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26055237650871277, 0.7999723553657532, 0.08861498534679413, 0.2534168064594269, 0.18273457884788513, 0.14584596455097198, 0.11706200242042542, 0.49210020899772644, 0.18971233069896698, 0.07651221007108688]
The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008).	[145, 141, 148, 7, 17, 95, 14, 6, 8, 157]	[1, 1, 0, 0, 0, 1, 1, 0, 0, 0]	[0.6917667984962463, 0.7094576954841614, 0.4394572675228119, 0.05963442847132683, 0.3260231018066406, 0.5287231206893921, 0.5171690583229065, 0.12044281512498856, 0.3748621940612793, 0.2552242577075958]
The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical.	[80, 0, 83, 78, 51, 9, 19, 70, 7, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6467667818069458, 0.283754825592041, 0.1261739730834961, 0.1007523164153099, 0.14495106041431427, 0.053422197699546814, 0.09365513175725937, 0.10590805113315582, 0.047086410224437714, 0.07645370811223984]
The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)	[1, 84, 133, 120, 38, 122, 209, 221, 129, 157]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06414451450109482, 0.5650420188903809, 0.06106811761856079, 0.211351677775383, 0.060452964156866074, 0.1628962755203247, 0.307370126247406, 0.22064831852912903, 0.11676093190908432, 0.0829920768737793]
The role of supervision is to permit some constituents to be built but not others (Pereira and Schabes, 1992).	[43, 124, 82, 126, 109, 108, 50, 113, 31, 115]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05482464283704758, 0.06612566858530045, 0.05099979043006897, 0.051375579088926315, 0.04620746150612831, 0.0775073766708374, 0.04501769691705704, 0.3044791519641876, 0.05058882385492325, 0.33786287903785706]
The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003).	[0, 211, 198, 135, 156, 197, 214, 137, 80, 170]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5417072772979736, 0.22025039792060852, 0.14449957013130188, 0.18512490391731262, 0.09380565583705902, 0.04943733662366867, 0.12991783022880554, 0.05323037877678871, 0.43885600566864014, 0.5205351114273071]
The same dataset has been used in other investigations, such as in (Li and Roth, 2002). The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in Table 1.	[165, 59, 160, 25, 151, 53, 173, 172, 169, 171]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7520674467086792, 0.23098036646842957, 0.210267573595047, 0.06583491712808609, 0.4666122794151306, 0.11287853121757507, 0.09572472423315048, 0.06255949288606644, 0.09607430547475815, 0.08834774047136307]
The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy.	[90, 94, 140, 137, 91, 130, 93, 10, 25, 87]	[1, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.6328965425491333, 0.7411233186721802, 0.48081037402153015, 0.5309203863143921, 0.5915018916130066, 0.05530847609043121, 0.08450878411531448, 0.11258066445589066, 0.10453958809375763, 0.49305111169815063]
The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998).	[111, 72, 4, 88, 27, 5, 18, 20, 35, 21]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6014596223831177, 0.058879390358924866, 0.20663557946681976, 0.0651320219039917, 0.08516427129507065, 0.04644453153014183, 0.05351285636425018, 0.07733526080846786, 0.042760368436574936, 0.23154030740261078]
The second direction concerns experiments on supertagging (Bangalore and Joshi, 1999) followed by a parsing stage the tagging stage associates to each word a supertag.	[247, 264, 245, 249, 243, 244, 112, 281, 164, 226]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.5282948017120361, 0.7685196399688721, 0.5097474455833435, 0.5846679210662842, 0.5875648260116577, 0.38100719451904297, 0.09831906855106354, 0.054012883454561234, 0.23425641655921936, 0.0869954451918602]
The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs.	[26, 178, 3, 33, 16, 19, 43, 54, 29, 2]	[0, 1, 0, 0, 1, 1, 0, 0, 0, 0]	[0.41900399327278137, 0.5391120314598083, 0.3903522789478302, 0.33599039912223816, 0.5324605107307434, 0.5564162135124207, 0.12246141582727432, 0.32275131344795227, 0.24032680690288544, 0.35445669293403625]
The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001).	[70, 41, 1, 18, 57, 147, 40, 4, 5, 12]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6713469624519348, 0.5466215014457703, 0.35705676674842834, 0.41220003366470337, 0.21860867738723755, 0.1358458548784256, 0.06797357648611069, 0.09962184727191925, 0.083244189620018, 0.16330531239509583]
The semantic similarity formula from (Corley and Mihalcea, 2005) defines the similarity of a pair of documents differently depending on with respect to which text it is computed.	[38, 19, 20, 47, 67, 35, 5, 21, 23, 56]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7109262943267822, 0.39657917618751526, 0.17681996524333954, 0.4948401153087616, 0.18930551409721375, 0.15485355257987976, 0.18529215455055237, 0.07025216519832611, 0.17789730429649353, 0.5519219636917114]
The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007).	[24, 31, 14, 25, 26, 46, 30, 29, 37, 54]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.46744421124458313, 0.46054673194885254, 0.3170933127403259, 0.19376850128173828, 0.19828389585018158, 0.13148683309555054, 0.5386682152748108, 0.05090469494462013, 0.11586438119411469, 0.1352633386850357]
The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al, 2005).	[80, 103, 4, 34, 45, 100, 50, 88, 59, 78]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.089438796043396, 0.14712966978549957, 0.13968999683856964, 0.27495113015174866, 0.06518765538930893, 0.05169682577252388, 0.15603145956993103, 0.15135866403579712, 0.20206794142723083, 0.12333853542804718]
The solution to the conditionalization problem is given in Section 3, using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a comparison to the MIRA learning algorithm used by McDonald et al (2005a).	[117, 78, 149, 70, 158, 159, 73, 26, 131, 12]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5606750249862671, 0.44397294521331787, 0.27288344502449036, 0.19665409624576569, 0.08407098799943924, 0.34472769498825073, 0.2044428586959839, 0.1298341602087021, 0.1061977967619896, 0.048825062811374664]
The split-merge smooth implementation of (Petrov et al, 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al, 2009) and for many other languages (Petrov and Klein, 2007).	[167, 170, 73, 18, 169, 9, 1, 178, 34, 181]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26526206731796265, 0.13865932822227478, 0.14633426070213318, 0.07609106600284576, 0.11068432033061981, 0.10061097890138626, 0.13991469144821167, 0.05883195251226425, 0.05386416241526604, 0.05747899040579796]
The structure of our treebank is inspired by the Redwoods treebank of English in which utterances are parsed and the annotator selects the best parse from the full analyses derived by the grammar (Oepen et al., 2002).	[18, 149, 55, 42, 1, 33, 57, 21, 111, 22]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.4771377146244049, 0.2926865816116333, 0.30624324083328247, 0.09831657260656357, 0.18592767417430878, 0.18052521347999573, 0.34438925981521606, 0.6030215620994568, 0.049022357910871506, 0.3311755359172821]
The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories.	[58, 55, 59, 57, 49, 54, 56, 18, 98, 31]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.6865851283073425, 0.7037889361381531, 0.19960036873817444, 0.40865960717201233, 0.24223841726779938, 0.42930352687835693, 0.5299282670021057, 0.12971556186676025, 0.3035750389099121, 0.05883792042732239]
The synonym matching is computed using WordNet (Fellbaum, 1998) and the paraphrase matching is computed using paraphrase tables (Callison-Burch et al, 2010).	[144, 6, 130, 70, 229, 175, 202, 10, 201, 238]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3660864233970642, 0.7810501456260681, 0.2889922559261322, 0.11895100772380829, 0.09326349943876266, 0.07044358551502228, 0.06867611408233643, 0.08975072205066681, 0.07687869668006897, 0.11142352223396301]
The task was extended to additional phrase types for the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing.	[0, 3, 79, 115, 82, 86, 114, 71, 62, 117]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7695119976997375, 0.44731923937797546, 0.22373826801776886, 0.16740241646766663, 0.17613349854946136, 0.13125820457935333, 0.2792389690876007, 0.4559793174266815, 0.07289119064807892, 0.09741999208927155]
The text unit's definition in Hearst (1994, 1997) and Foltz et al (1998) is generally task dependent, depending on what size gives the best results.	[105, 116, 86, 104, 40, 16, 13, 1, 69, 8]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.5036737322807312, 0.317135214805603, 0.3919453024864197, 0.0978303924202919, 0.10037488490343094, 0.1867673546075821, 0.604809045791626, 0.05719195678830147, 0.26893532276153564, 0.09903591871261597]
The texts were POS-tagged using TnT (Brants,2000).	[1, 0, 27, 10, 6, 51, 133, 151, 17, 23]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18739116191864014, 0.23792356252670288, 0.172867089509964, 0.06944501399993896, 0.04062448814511299, 0.06663128733634949, 0.06928060203790665, 0.10636937618255615, 0.08186990022659302, 0.10359936207532883]
The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000).	[46, 45, 42, 69, 15, 50, 25, 47, 38, 10]	[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]	[0.49970847368240356, 0.4967714548110962, 0.23311366140842438, 0.12707488238811493, 0.36057135462760925, 0.5086134076118469, 0.2803747355937958, 0.6231740117073059, 0.06649772822856903, 0.08644606918096542]
The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010).	[16, 43, 35, 243, 37, 47, 36, 14, 68, 192]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15175296366214752, 0.10385707020759583, 0.15768207609653473, 0.2768968641757965, 0.07149046659469604, 0.057191476225852966, 0.14329057931900024, 0.4183341860771179, 0.1704082041978836, 0.11577924340963364]
The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the lt-xml2tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al, 2000).	[117, 81, 82, 84, 18, 141, 101, 11, 62, 5]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2699265778064728, 0.07214219868183136, 0.14335225522518158, 0.07863413542509079, 0.3404391407966614, 0.17467279732227325, 0.10891618579626083, 0.09896829724311829, 0.046855270862579346, 0.3185073733329773]
The translation quality is measured by three MT evaluation metrics: TER (Snover et al, 2006), BLEU (Papineni et al, 2002), and METEOR (Lavie and Agarwal, 2007).	[7, 11, 26, 1, 13, 0, 12, 85, 4, 5]	[1, 1, 0, 0, 1, 0, 1, 0, 0, 0]	[0.7710989117622375, 0.7992976307868958, 0.056461457163095474, 0.22686751186847687, 0.6555535197257996, 0.26451754570007324, 0.5154590606689453, 0.05558188259601593, 0.1193651407957077, 0.08294208347797394]
The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010).	[52, 54, 55, 48, 195, 53, 30, 61, 74, 71]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5439496636390686, 0.09228385984897614, 0.55100417137146, 0.29330262541770935, 0.2328231781721115, 0.054347313940525055, 0.05309122055768967, 0.20192307233810425, 0.2215469628572464, 0.052330393344163895]
The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008).	[7, 31, 227, 79, 26, 33, 67, 215, 0, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.38704603910446167, 0.45357242226600647, 0.13621728122234344, 0.07729081809520721, 0.27638697624206543, 0.2225342094898224, 0.05098569020628929, 0.2812148928642273, 0.3658128082752228, 0.1334787756204605]
The use of PCFG is tied to the annotation principles of popular tree banks, such as the Penn Treebank (PTB) (Marcus et al, 1994), which are used as a data source for grammar extraction.	[9, 8, 7, 69, 0, 150, 24, 124, 3, 71]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.100179523229599, 0.07220529764890671, 0.3789055645465851, 0.22412408888339996, 0.19019950926303864, 0.40323030948638916, 0.10711485147476196, 0.10082148760557175, 0.07824932038784027, 0.09334690123796463]
The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002).	[13, 0, 15, 17, 1, 94, 104, 52, 39, 73]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6485453844070435, 0.5682279467582703, 0.17909343540668488, 0.07662631571292877, 0.14196893572807312, 0.1333855539560318, 0.29396140575408936, 0.19946295022964478, 0.14620131254196167, 0.10669295489788055]
The use of crowd sourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and Callison-Burch (2009).	[37, 170, 48, 35, 38, 88, 46, 43, 124, 41]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.128511443734169, 0.09160542488098145, 0.1281702220439911, 0.3367084562778473, 0.07386141270399094, 0.06106853112578392, 0.053754065185785294, 0.06853105872869492, 0.18267810344696045, 0.07100746035575867]
The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008).	[2, 119, 0, 12, 123, 184, 1, 103, 39, 41]	[1, 1, 0, 0, 1, 0, 0, 1, 1, 0]	[0.7562112212181091, 0.6955947279930115, 0.39767295122146606, 0.15817378461360931, 0.6832535862922668, 0.17385071516036987, 0.04182839021086693, 0.5806415677070618, 0.5486416220664978, 0.2515835464000702]
The vast space between these two extremes can still be explained in terms of compositional principles with mechanisms from GLT such as type coercion and sub selection (Pustejovsky, 1991, 1993).	[375, 271, 121, 387, 426, 276, 106, 238, 5, 73]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3594699203968048, 0.41045111417770386, 0.07371056079864502, 0.35313937067985535, 0.14955256879329681, 0.2822645306587219, 0.43493911623954773, 0.1894873082637787, 0.06384977698326111, 0.11103415489196777]
The work by Xu et al (2009) is the closest to our approach.	[125, 30, 117, 164, 209, 22, 31, 204, 2, 210]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5352766513824463, 0.295040488243103, 0.17739780247211456, 0.6607241034507751, 0.3157048523426056, 0.06527834385633469, 0.13029897212982178, 0.20227199792861938, 0.04768802225589752, 0.20113393664360046]
Their factors are taken from Ge et al (1998), with two exceptions.	[12, 54, 13, 1, 48, 92, 61, 109, 202, 38]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6432556509971619, 0.05613812059164047, 0.34571152925491333, 0.0435652881860733, 0.4727157652378082, 0.10800018906593323, 0.0781315341591835, 0.08904282003641129, 0.04898887872695923, 0.11657271534204483]
Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system.	[119, 0, 3, 14, 99, 13, 38, 2, 28, 128]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4100014865398407, 0.5127903819084167, 0.366921067237854, 0.23935173451900482, 0.2266962230205536, 0.27547362446784973, 0.23844704031944275, 0.211354598402977, 0.059986360371112823, 0.09719030559062958]
Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008).	[12, 34, 59, 24, 80, 51, 25, 87, 37, 109]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6704320311546326, 0.5234408974647522, 0.47525930404663086, 0.34422579407691956, 0.21586965024471283, 0.20265361666679382, 0.4780551791191101, 0.12495884299278259, 0.21509836614131927, 0.07577503472566605]
Theorem 5 in (Abney, 2002) provides a theoretical explanation for these results: if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T.	[179, 104, 124, 156, 132, 170, 175, 176, 70, 49]	[1, 0, 0, 0, 0, 0, 1, 1, 0, 0]	[0.6259436011314392, 0.15666672587394714, 0.3792126178741455, 0.42918071150779724, 0.4091893434524536, 0.2077137529850006, 0.524154007434845, 0.5012126564979553, 0.06793978810310364, 0.11812452226877213]
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).	[2, 234, 0, 241, 8, 254, 248, 25, 86, 64]	[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.5586219429969788, 0.47498536109924316, 0.7898995280265808, 0.64532470703125, 0.4350728392601013, 0.1419396698474884, 0.3723764717578888, 0.21544243395328522, 0.3957512676715851, 0.14528286457061768]
There are four kinds of chunk tags in the CoNLL-1999 dataset, namely IOB1, IOB2, IOE1, and IOE2 (Tjong Kim Sang and Veenstra, 1999).	[117, 103, 29, 98, 104, 110, 69, 115, 72, 150]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4505769908428192, 0.2130241096019745, 0.2502612769603729, 0.06770176440477371, 0.1104915514588356, 0.07281415909528732, 0.09398622065782547, 0.07519789785146713, 0.055993445217609406, 0.05038563162088394]
There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task.	[34, 238, 19, 10, 23, 240, 244, 335, 8, 56]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7733122110366821, 0.7342773675918579, 0.6181577444076538, 0.07215158641338348, 0.34483638405799866, 0.2582877278327942, 0.1526041030883789, 0.10444826632738113, 0.28051629662513733, 0.23296257853507996]
There has been a modest amount of previous work on improving probabilistic decision lists, as well as a fair amount of work in related fields, especially in transformation-based learning (Brill, 1995).	[131, 94, 25, 118, 40, 1, 8, 133, 136, 75]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0755387544631958, 0.07312990725040436, 0.3215593695640564, 0.1424340456724167, 0.45998960733413696, 0.11355474591255188, 0.11355474591255188, 0.3611648380756378, 0.2902331054210663, 0.48600828647613525]
There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat.	[56, 192, 242, 147, 244, 211, 159, 250, 137, 67]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6389012932777405, 0.6749967336654663, 0.40130168199539185, 0.3820151090621948, 0.10434900969266891, 0.15690752863883972, 0.14968255162239075, 0.1098235473036766, 0.07732436805963516, 0.07814565300941467]
There was one training set for each French-English, German-English, Italian-English, Spanish-English language combination (Negri et al, 2011).	[5, 33, 53, 172, 54, 86, 93, 74, 20, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2337178885936737, 0.406923770904541, 0.16161687672138214, 0.22128228843212128, 0.07262720167636871, 0.0795384868979454, 0.07643395662307739, 0.046609073877334595, 0.06407282501459122, 0.08159569650888443]
Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002).	[157, 158, 45, 81, 111, 92, 83, 91, 16, 46]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7481340169906616, 0.7189209461212158, 0.08670606464147568, 0.2892417311668396, 0.17911934852600098, 0.1412097066640854, 0.1333964616060257, 0.1116953194141388, 0.04523959755897522, 0.04758136719465256]
Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words.	[52, 25, 56, 77, 220, 0, 191, 24, 38, 135]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6867762804031372, 0.45422524213790894, 0.31556835770606995, 0.41130682826042175, 0.12137734889984131, 0.7906308174133301, 0.4501882493495941, 0.05056336522102356, 0.09638743102550507, 0.08374322205781937]
These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010).	[39, 2, 17, 12, 81, 4, 20, 15, 1, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4788128137588501, 0.17622117698192596, 0.13076643645763397, 0.08744130283594131, 0.22876790165901184, 0.20121055841445923, 0.14407159388065338, 0.2177266925573349, 0.09282609075307846, 0.07429608702659607]
These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.	[44, 159, 84, 20, 21, 30, 105, 31, 23, 122]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.28800636529922485, 0.12188868969678879, 0.04900575801730156, 0.12956230342388153, 0.4733271300792694, 0.05337359383702278, 0.05656097084283829, 0.08281021565198898, 0.6140121221542358, 0.0449078232049942]
These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc.	[2, 58, 60, 62, 123, 66, 16, 70, 69, 74]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29880082607269287, 0.34960296750068665, 0.21171091496944427, 0.3024904727935791, 0.12173505872488022, 0.09509578347206116, 0.3053710162639618, 0.046120818704366684, 0.05323067680001259, 0.050366826355457306]
These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009).	[16, 88, 4, 22, 192, 172, 162, 179, 132, 136]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5744942426681519, 0.36150258779525757, 0.05965014174580574, 0.41043341159820557, 0.061689868569374084, 0.0700991228222847, 0.06082851067185402, 0.4195897579193115, 0.22326138615608215, 0.16515572369098663]
These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002).	[25, 43, 47, 30, 37, 77, 49, 73, 4, 56]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22283221781253815, 0.6912227869033813, 0.09826484322547913, 0.19094936549663544, 0.05309015512466431, 0.06633548438549042, 0.12355369329452515, 0.30334892868995667, 0.21474865078926086, 0.19522957503795624]
These include rule-based systems [Krupka 1998], Hidden Markov Models (HMM) [Bikel et al 1997] and Maximum Entropy Models (MaxEnt) [Borthwick 1998].	[3, 10, 1, 6, 27, 30, 5, 111, 82, 135]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47616586089134216, 0.09379298985004425, 0.09487824141979218, 0.05377449095249176, 0.05608740448951721, 0.06276068091392517, 0.05678922310471535, 0.23783378303050995, 0.06142912432551384, 0.1864291876554489]
These intervals were computed following the boot strap technique described in (Koehn, 2004).	[195, 196, 63, 194, 25, 11, 113, 68, 115, 132]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5919228792190552, 0.5945425629615784, 0.3157428503036499, 0.3925004303455353, 0.49833881855010986, 0.183344304561615, 0.2662737965583801, 0.07985084503889084, 0.09472193568944931, 0.6416758894920349]
These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach.	[3, 47, 13, 103, 2, 43, 23, 16, 83, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30777084827423096, 0.19614799320697784, 0.16178935766220093, 0.21174922585487366, 0.18205909430980682, 0.26050710678100586, 0.155233234167099, 0.06721262633800507, 0.04785905405879021, 0.23608723282814026]
These methods consist of simple rules that can reliably assign a sense to certain word categories: one sense per collocation (Yarowsky, 1993), and one sense per discourse (Gale et al, 1992).	[0, 164, 1, 98, 43, 130, 158, 18, 189, 5]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.6780588626861572, 0.16484804451465607, 0.3967638313770294, 0.2927297055721283, 0.18141108751296997, 0.29013127088546753, 0.27827322483062744, 0.19214370846748352, 0.5544281005859375, 0.20192112028598785]
These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006).	[170, 10, 176, 123, 62, 179, 178, 171, 13, 112]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7151966094970703, 0.7966862916946411, 0.20669405162334442, 0.36706605553627014, 0.10658106952905655, 0.120058573782444, 0.05985642597079277, 0.4166940152645111, 0.09007827937602997, 0.3615611493587494]
These numbers compare favorably with the previous literature: (Filatova and Hovy 2001) obtained 82% accuracy on anchoring for a single type of event/topic on 172 clauses, while (Mani and Wilson 2000) obtained accuracy of 59.4% on anchoring over 663 verb contexts.	[44, 127, 52, 103, 102, 60, 115, 49, 59, 82]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14905448257923126, 0.0916113406419754, 0.08175196498632431, 0.2452586442232132, 0.2244851142168045, 0.05383835732936859, 0.1333482414484024, 0.09826729446649551, 0.05525277554988861, 0.10233574360609055]
These patterns are implemented as regular expressions using the JAPE language (Cunningham et al, 2002).	[177, 163, 38, 178, 28, 37, 39, 179, 4, 10]	[1, 1, 0, 0, 1, 1, 0, 0, 0, 0]	[0.7465938925743103, 0.6477866172790527, 0.44167855381965637, 0.44167855381965637, 0.6477866172790527, 0.6412089467048645, 0.07092812657356262, 0.07092812657356262, 0.220429927110672, 0.07354532182216644]
These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution.	[128, 125, 29, 145, 70, 160, 84, 13, 144, 88]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.588554859161377, 0.11005546152591705, 0.23837725818157196, 0.23913758993148804, 0.14727318286895752, 0.1670517772436142, 0.07264900952577591, 0.07618066668510437, 0.05063990131020546, 0.060604654252529144]
These two limitations define the matching problem of F-Score (Rosenberg and Hirschberg, 2007) which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality.	[73, 70, 2, 21, 75, 56, 120, 19, 236, 27]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5119755268096924, 0.40725135803222656, 0.33568423986434937, 0.08223912119865417, 0.18104159832000732, 0.3815116584300995, 0.17104797065258026, 0.1959482878446579, 0.05692172423005104, 0.600471019744873]
They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al 2011).	[43, 62, 28, 198, 210, 156, 194, 3, 88, 142]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4733882248401642, 0.1138305589556694, 0.06798188388347626, 0.19748644530773163, 0.09023203700780869, 0.100120410323143, 0.09446243941783905, 0.17269164323806763, 0.0695258378982544, 0.12734946608543396]
They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002).	[1, 7, 13, 57, 29, 17, 32, 2, 8, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7301007509231567, 0.7301007509231567, 0.46086594462394714, 0.33693253993988037, 0.08346402645111084, 0.07403340190649033, 0.05185295641422272, 0.04641532897949219, 0.04641532897949219, 0.0580151341855526]
They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step.	[1, 3, 75, 102, 27, 38, 35, 105, 71, 84]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29467445611953735, 0.5688110589981079, 0.09966158121824265, 0.12987597286701202, 0.21008360385894775, 0.17597241699695587, 0.06852659583091736, 0.06243138015270233, 0.06497710198163986, 0.21669642627239227]
They then later propose using Web counts as a baseline unsupervised method for many NLP tasks (Lapata and Keller, 2004).	[195, 0, 17, 88, 9, 1, 207, 10, 12, 134]	[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]	[0.7196304798126221, 0.6474772691726685, 0.7057726979255676, 0.4972243309020996, 0.5691283941268921, 0.3778952956199646, 0.7132272720336914, 0.24698498845100403, 0.32500171661376953, 0.22966304421424866]
Third, the knowledge of textual structure helps to interpret the meaning of entities in a text (Grosz and Sidner 1986).	[333, 379, 211, 103, 173, 79, 215, 89, 536, 113]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.39115774631500244, 0.35400813817977905, 0.20314669609069824, 0.3467937409877777, 0.22575116157531738, 0.08167622238397598, 0.2484588325023651, 0.09372204542160034, 0.0869038924574852, 0.15274180471897125]
This F-measure is based on the recall and precision figures reported in Figure 7.15 in Collins (1999).	[110, 51, 8, 11, 88, 23, 53, 47, 37, 43]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3530494272708893, 0.7614074349403381, 0.32386770844459534, 0.12469889223575592, 0.09062229096889496, 0.1574167162179947, 0.4477456510066986, 0.2263847291469574, 0.11246445029973984, 0.1638401597738266]
This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al, 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson, 2001).	[12, 73, 190, 276, 25, 0, 71, 89, 211, 257]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.529220461845398, 0.529220461845398, 0.5488051176071167, 0.4142434895038605, 0.3294321298599243, 0.594927966594696, 0.3790012001991272, 0.3294321298599243, 0.3326984941959381, 0.16095773875713348]
This approach is taken by Clark (2000), where the perplexity of a finite-state model is used to compare different category sets.	[60, 64, 8, 14, 4, 6, 66, 12, 38, 7]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.56984543800354, 0.3242258131504059, 0.08876387029886246, 0.052382081747055054, 0.04702666029334068, 0.04573992267251015, 0.08461849391460419, 0.04737141355872154, 0.07162589579820633, 0.06277993321418762]
This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word's translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999).	[137, 0, 1, 7, 2, 8, 20, 107, 43, 114]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48043739795684814, 0.7666441202163696, 0.11782129108905792, 0.11782129108905792, 0.051306042820215225, 0.051306042820215225, 0.37098896503448486, 0.1488347202539444, 0.08065741509199142, 0.08032376319169998]
This concept was introduced as lifting in (Kahane et al, 1998).	[97, 103, 106, 21, 101, 94, 34, 93, 7, 111]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.5978056788444519, 0.6034730672836304, 0.5099101662635803, 0.6344559192657471, 0.46441709995269775, 0.5346868634223938, 0.21774080395698547, 0.19967946410179138, 0.1080901026725769, 0.08167312294244766]
This confirms the results of Fox (2002) and Galley et al (2004) that many translation operations must span more than one parse tree node.	[130, 147, 80, 11, 25, 64, 32, 145, 37, 131]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4811449944972992, 0.29772108793258667, 0.0705341324210167, 0.0742257833480835, 0.5250810980796814, 0.06462705135345459, 0.10257667303085327, 0.12815755605697632, 0.05182519555091858, 0.09082894027233124]
This evaluation was also used in (Sha and Pereira, 2003).	[131, 89, 10, 39, 2, 11, 153, 114, 137, 96]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5251063108444214, 0.20179852843284607, 0.0846571996808052, 0.05401849001646042, 0.08033790439367294, 0.1799061894416809, 0.048037294298410416, 0.05196961760520935, 0.04698444902896881, 0.055328305810689926]
This figure shows the amount of time (excluding any startup overhead) spent parsing or bracketing using this system (the two lowest lines) versus the parsers of Collins (2003) and Charniak (2000) run with default settings.	[531, 105, 393, 143, 131, 262, 111, 305, 366, 411]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7711310386657715, 0.5291980504989624, 0.11877408623695374, 0.5241059064865112, 0.11956841498613358, 0.0437752939760685, 0.10533934831619263, 0.12872593104839325, 0.051952432841062546, 0.22263364493846893]
This final similarity function will then be embedded into a normal HAC algorithm to group the web pages into different namesakes where we compute the centroid-based distance between clusters (Mann and Yarowsky, 2003).	[56, 82, 1, 74, 51, 87, 53, 153, 17, 76]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33738231658935547, 0.19802436232566833, 0.08292455226182938, 0.18687759339809418, 0.19499477744102478, 0.2871849834918976, 0.1468309611082077, 0.04783943295478821, 0.044734518975019455, 0.16422168910503387]
This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way.	[71, 0, 5, 4, 218, 250, 19, 28, 33, 172]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7159435749053955, 0.6288195848464966, 0.24381087720394135, 0.26367998123168945, 0.33952122926712036, 0.23672868311405182, 0.4502447843551636, 0.29825666546821594, 0.05727909132838249, 0.12040986120700836]
This huge number of tokens can be explained by the fact that the lexicon used for tokenization and tagging integrates many multi-word expressions which are not part of these mantic lexicon for (Brill and Resnik, 1994) and 0.77 for (LauerandDras, 1994)), but a direct comparison is difficult inasmuch as only three-word sequences (V N P, for (Brill and Resnik, 1994) and N N N for (Lauer and Dras, 1994)) were used for evaluation in those works, and the language studied is English.	[60, 95, 47, 100, 80, 103, 79, 5, 117, 116]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17764025926589966, 0.06286684423685074, 0.07355166226625443, 0.08809954673051834, 0.14178453385829926, 0.08324576914310455, 0.12507429718971252, 0.06172974407672882, 0.06856408715248108, 0.10151661932468414]
This idea is demonstrated by Attardi (2006), who proposes a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency.	[0, 43, 17, 83, 94, 51, 6, 5, 36, 4]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6655606627464294, 0.4239831268787384, 0.11056970804929733, 0.14998836815357208, 0.20655593276023865, 0.08387575298547745, 0.06568075716495514, 0.06726972758769989, 0.05896701663732529, 0.047307029366493225]
This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods.	[75, 145, 24, 0, 33, 108, 64, 43, 85, 129]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6399811506271362, 0.7125118374824524, 0.10161031782627106, 0.414167195558548, 0.057260897010564804, 0.07466825842857361, 0.07092546671628952, 0.07579702883958817, 0.04451844468712807, 0.08297359198331833]
This is a purely syntactic resource, but we can also include this tree bank in the category of multistratal resources since the PropBank (Palmer et al, 2005 )andNomBank (Meyers et al, 2004) projects have an notated shallow semantic structures on top of it. Dependency-converted versions of the Penn Tree bank, PropBank and NomBank were used in the CoNLL-2008 Shared Task (Surdeanu et al, 2008), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics.	[130, 94, 65, 129, 78, 143, 103, 150, 99, 144]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1421532779932022, 0.1718526929616928, 0.1731407642364502, 0.07013457268476486, 0.11831562221050262, 0.07110284268856049, 0.08025603741407394, 0.06389974802732468, 0.07511124014854431, 0.05988113954663277]
This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007).	[83, 111, 17, 9, 104, 114, 121, 23, 18, 90]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5843890309333801, 0.24725580215454102, 0.06971552968025208, 0.12739214301109314, 0.10794533044099808, 0.15022233128547668, 0.0889926552772522, 0.07128987461328506, 0.05374881252646446, 0.2261110544204712]
This is achieved by adopting the scoring method of Swier and Stevenson (2004), in which we compute the portion Frame of frame slots that can be mapped to an extracted argument, and the portion% Sent of extracted arguments from the sentence that can be mapped to the frame.	[46, 192, 47, 140, 126, 43, 172, 176, 53, 157]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7920294404029846, 0.06049121543765068, 0.27897828817367554, 0.23884867131710052, 0.11838659644126892, 0.2165839821100235, 0.05647312104701996, 0.09662395715713501, 0.056892167776823044, 0.09995836764574051]
This is also the place where linguistic constraints can be applied, say to avoid non compositional phrases (Lin, 1999).	[0, 124, 99, 77, 5, 6, 7, 116, 3, 48]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6409746408462524, 0.4866117238998413, 0.0635489746928215, 0.2939905524253845, 0.12976619601249695, 0.1414433866739273, 0.3558465838432312, 0.34082165360450745, 0.17287205159664154, 0.21053582429885864]
This is consistent with results reportedby Evert and Krenn (2001).	[23, 9, 6, 19, 1, 20, 79, 71, 31, 36]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.3970872461795807, 0.2866772413253784, 0.23384834825992584, 0.27845218777656555, 0.26208993792533875, 0.24965709447860718, 0.6130793690681458, 0.08906711637973785, 0.050653956830501556, 0.04874105378985405]
This is contrasted with the all-paths bottom-up strategy in GEMINI (Dowding et al 1993) that finds all admissable edges of the grammar.	[11, 54, 83, 24, 27, 52, 65, 103, 81, 40]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3428138494491577, 0.3530443608760834, 0.15175418555736542, 0.061707984656095505, 0.048843394964933395, 0.046372849494218826, 0.06802777945995331, 0.12812559306621552, 0.0661836564540863, 0.04692574590444565]
This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008).	[137, 19, 61, 112, 4, 77, 140, 65, 7, 66]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5351244807243347, 0.1738862246274948, 0.06276093423366547, 0.17875613272190094, 0.08570675551891327, 0.13816188275814056, 0.1431981772184372, 0.06184524670243263, 0.14431880414485931, 0.05292752385139465]
This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms).	[2, 10, 22, 7, 3, 20, 6, 8, 0, 9]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2903684377670288, 0.6584165096282959, 0.08826535195112228, 0.10205453634262085, 0.06499361246824265, 0.0686655342578888, 0.05975290387868881, 0.10326114296913147, 0.1346101462841034, 0.07230747491121292]
This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task.	[49, 21, 15, 1, 2, 11, 35, 52, 9, 77]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15642720460891724, 0.37147584557533264, 0.2959539592266083, 0.08185102790594101, 0.22224637866020203, 0.17940405011177063, 0.29089125990867615, 0.07643751800060272, 0.1298394799232483, 0.08185102790594101]
This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers.	[101, 111, 102, 109, 110, 103, 105, 31, 30, 32]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7822072505950928, 0.5356163382530212, 0.14936497807502747, 0.39801931381225586, 0.44770875573158264, 0.2876076400279999, 0.464526891708374, 0.10358891636133194, 0.10611610859632492, 0.2202109843492508]
This is quite feasible using statistical taggers like those of Garside (1987), Church (1988) or Foster (1991) which achieve performance upwards of 97% on unrestricted text.	[0, 42, 34, 20, 75, 25, 29, 73, 69, 36]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7592687606811523, 0.5282938480377197, 0.1422455906867981, 0.06342349201440811, 0.06442604213953018, 0.1544262319803238, 0.0494769923388958, 0.043332673609256744, 0.04872216284275055, 0.07165462523698807]
This is the task of finding for a word in one language words of a similar meaning in a second language. The results of this can be used to aid manual construction of resources or directly aid translation. This task was first approached as a distributional similarity-like problem by Brown et al (1988).	[47, 17, 16, 58, 23, 40, 66, 59, 139, 74]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3964241147041321, 0.24687151610851288, 0.08310836553573608, 0.14835327863693237, 0.06382099539041519, 0.0588393472135067, 0.07480040192604065, 0.08331727981567383, 0.05965337157249451, 0.048719968646764755]
This kind of multi-level assessment corresponds to that described and used in Carletta et al, (1997).	[32, 291, 195, 190, 1, 2, 5, 176, 15, 224]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7997630834579468, 0.08337020128965378, 0.04553955793380737, 0.12053381651639938, 0.08646837621927261, 0.08646837621927261, 0.10223376005887985, 0.054143182933330536, 0.11667587608098984, 0.14019709825515747]
This knowledge could be especially helpful for cross document coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents.	[13, 0, 149, 5, 6, 1, 98, 45, 121, 162]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4697839021682739, 0.7891693115234375, 0.48900243639945984, 0.21072711050510406, 0.23923514783382416, 0.22656100988388062, 0.38929763436317444, 0.11296182125806808, 0.30851835012435913, 0.1730295866727829]
This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.	[36, 124, 80, 106, 99, 92, 97, 69, 93, 108]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35042884945869446, 0.5242995619773865, 0.21349665522575378, 0.11787574738264084, 0.4884088635444641, 0.1375727355480194, 0.1611231416463852, 0.1667829155921936, 0.08762834221124649, 0.15800124406814575]
This lexicalization procedure is commonly used in statistical parsing (Collins, 1996) and produces a dependency tree.	[1, 15, 123, 0, 19, 127, 33, 13, 158, 10]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.34586000442504883, 0.5446785092353821, 0.18300704658031464, 0.7870976328849792, 0.20352308452129364, 0.0643925815820694, 0.14572516083717346, 0.09721974283456802, 0.0813646912574768, 0.07815597951412201]
This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5).	[11, 23, 52, 13, 14, 82, 7, 31, 56, 63]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31393495202064514, 0.1565295308828354, 0.08772245794534683, 0.04927828535437584, 0.14526773989200592, 0.06032145023345947, 0.0673513188958168, 0.2506292760372162, 0.24333959817886353, 0.15585938096046448]
This measurement is called longest common subsequence ratio [Melamed, 1995].	[130, 129, 137, 191, 54, 45, 59, 3, 93, 49]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8190574645996094, 0.757114827632904, 0.36886459589004517, 0.12962211668491364, 0.05628193914890289, 0.17651629447937012, 0.05219833925366402, 0.04911569505929947, 0.04960120469331741, 0.05056307837367058]
This message constitutes the primary communicative or dis course goal (Grosz and Sidner, 1986) of the graphic and captures its main contribution to the overall dis course goal of the entire document.	[38, 339, 146, 274, 660, 651, 125, 321, 409, 729]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3040197789669037, 0.07515458762645721, 0.2200787514448166, 0.11736369878053665, 0.1168847605586052, 0.2878111004829407, 0.2570776045322418, 0.1095447689294815, 0.14239074289798737, 0.05855768918991089]
This method is a straightforward application of the n-best re-ranking approach described in Och et al (2004).	[54, 23, 167, 208, 58, 164, 53, 65, 13, 60]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.40815308690071106, 0.7456515431404114, 0.16953030228614807, 0.13211984932422638, 0.2101568728685379, 0.09400317817926407, 0.28426408767700195, 0.22301176190376282, 0.11929268389940262, 0.08143250644207001]
This method is called dual decomposition (DD) (Rush et al, 2010).	[29, 28, 196, 23, 0, 68, 69, 9, 40, 8]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5181555151939392, 0.23285841941833496, 0.20085209608078003, 0.17682388424873352, 0.2023535966873169, 0.136815145611763, 0.22975210845470428, 0.09344125539064407, 0.09887535125017166, 0.3384517729282379]
This method is similar to the one introduced by Mikolov et al (2013a) for estimating a translation matrix, only solved analytically.	[10, 12, 11, 7, 55, 49, 18, 41, 16, 66]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.052823506295681, 0.04545534774661064, 0.061953354626894, 0.1732608675956726, 0.15877625346183777, 0.09300753474235535, 0.09631329774856567, 0.10445569455623627, 0.0598176084458828, 0.2925282418727875]
This method, described in earlier work Wubben et al.(2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.	[57, 23, 12, 3, 46, 80, 85, 75, 73, 2]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46372315287590027, 0.0904945433139801, 0.21249113976955414, 0.2970491647720337, 0.11243314296007156, 0.3692329227924347, 0.4638696610927582, 0.4430941641330719, 0.05459389463067055, 0.06446874141693115]
This model is a version of DeSR (Attardi, 2006), a deterministic classifier-based Shift/Reduce parser.	[6, 36, 62, 18, 17, 7, 13, 20, 52, 9]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.7053784728050232, 0.6766150593757629, 0.5235103964805603, 0.6170843839645386, 0.07075698673725128, 0.2525145411491394, 0.25427836179733276, 0.18920695781707764, 0.10099944472312927, 0.16458767652511597]
This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment.	[171, 213, 216, 156, 11, 55, 182, 196, 20, 170]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6864671111106873, 0.5980504155158997, 0.4932281970977783, 0.25924333930015564, 0.20572903752326965, 0.1397411972284317, 0.10783296078443527, 0.2883240580558777, 0.0952669084072113, 0.15378911793231964]
This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem.	[1, 183, 7, 10, 27, 13, 165, 8, 53, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17314709722995758, 0.4880411922931671, 0.2105465680360794, 0.13544045388698578, 0.20364989340305328, 0.11437712609767914, 0.09488113224506378, 0.15418705344200134, 0.06504363566637039, 0.1315198689699173]
This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006).	[103, 7, 15, 88, 49, 50, 16, 97, 2, 19]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6186209917068481, 0.42895302176475525, 0.17415809631347656, 0.6093743443489075, 0.3702431321144104, 0.45016804337501526, 0.18170838057994843, 0.3442087173461914, 0.23723433911800385, 0.07118388265371323]
This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al, 2002).	[26, 21, 31, 5, 10, 1, 172, 57, 25, 186]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7039437294006348, 0.6669932007789612, 0.13710597157478333, 0.7496069669723511, 0.08195503056049347, 0.18090102076530457, 0.12267250567674637, 0.06715679913759232, 0.056234221905469894, 0.07071399688720703]
This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003).	[200, 1, 14, 93, 18, 17, 43, 87, 73, 77]	[0, 1, 0, 0, 1, 0, 0, 1, 0, 0]	[0.4776243269443512, 0.5168305039405823, 0.48880940675735474, 0.4222714304924011, 0.5042603611946106, 0.40465012192726135, 0.21868491172790527, 0.5009235739707947, 0.2475055605173111, 0.11193937808275223]
This result supports the use of salience, in line with the conclusion drawn in (Barzilay and Lapata, 2005).	[173, 3, 35, 183, 192, 184, 25, 92, 18, 157]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.32695940136909485, 0.2934816777706146, 0.12944073975086212, 0.17469395697116852, 0.09140782803297043, 0.16545246541500092, 0.4663425385951996, 0.05414953827857971, 0.07906565815210342, 0.1515730768442154]
This results in efficiently treating the well known problem originally described in Kay (1996), where one unnecessarily retains sub-optimal strings.	[16, 22, 4, 126, 132, 182, 9, 54, 68, 165]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06068898364901543, 0.10841826349496841, 0.19398421049118042, 0.06068898364901543, 0.10841826349496841, 0.05497264117002487, 0.04894479364156723, 0.05013066902756691, 0.05972162261605263, 0.05972162261605263]
This results in several aspects that distinguish the MH treebank from, e.g., the WSJ Penn tree bank annotation scheme (Marcus et al, 1994).	[0, 1, 24, 25, 7, 150, 69, 9, 8, 72]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6013761758804321, 0.6015015244483948, 0.2586774528026581, 0.181146502494812, 0.46364128589630127, 0.4712318778038025, 0.28603988885879517, 0.07474721968173981, 0.11589130759239197, 0.053912531584501266]
This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping.	[73, 71, 16, 32, 1, 11, 2, 28, 97, 3]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4801334738731384, 0.2610395848751068, 0.06327701359987259, 0.3751979470252991, 0.10442723333835602, 0.10442723333835602, 0.31039679050445557, 0.054565925151109695, 0.05320165678858757, 0.13996049761772156]
This simple feature is the lexical similarity between T and H computed using WordNet-based metrics as in (Corley and Mihalcea, 2005).	[24, 43, 7, 55, 20, 3, 47, 17, 67, 60]	[1, 1, 1, 1, 1, 0, 1, 0, 0, 1]	[0.728274405002594, 0.677109956741333, 0.5196653604507446, 0.5847263336181641, 0.5491942167282104, 0.48040151596069336, 0.5409358739852905, 0.22542880475521088, 0.1762576401233673, 0.509333610534668]
This suggests that a robust model of discourse structure could complement current robust interpretation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Strube and Muller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al (1998)).	[188, 35, 28, 27, 20, 161, 152, 156, 15, 129]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37851956486701965, 0.2812085747718811, 0.04884776100516319, 0.394063800573349, 0.40006008744239807, 0.27470147609710693, 0.3788139522075653, 0.0542033351957798, 0.0472731813788414, 0.047844402492046356]
This technical report addresses two problems found in (Mani et al, 2006): (1) feature vector duplication caused by the data normalization process (once fixed, the accuracy drops to 76.56% and 83.23%) and (2) a somewhat unrealistic evaluation scheme (we describe Mani et al (2007)'s results in Section 4.1).	[13, 79, 67, 150, 18, 8, 86, 81, 2, 35]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30316752195358276, 0.09106946736574173, 0.09101347625255585, 0.08482556790113449, 0.13200680911540985, 0.05715992674231529, 0.4115656018257141, 0.08399408310651779, 0.1475110501050949, 0.06789928674697876]
This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009).	[145, 189, 137, 8, 31, 133, 173, 67, 171, 138]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6381122469902039, 0.2623562812805176, 0.32124194502830505, 0.14060506224632263, 0.0933866947889328, 0.3798999786376953, 0.07448628544807434, 0.05664391815662384, 0.20247556269168854, 0.07786142826080322]
This type of similarity is reminiscent of relational analogies investigated in Turney (2008).	[192, 25, 7, 196, 197, 191, 6, 203, 21, 131]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6048341989517212, 0.0568130686879158, 0.06702829897403717, 0.4306454062461853, 0.09143976867198944, 0.2679329216480255, 0.03924214467406273, 0.18297074735164642, 0.06071106344461441, 0.1872631460428238]
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003).	[0, 9, 86, 15, 133, 2, 4, 113, 44, 152]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36280134320259094, 0.11938615143299103, 0.15806929767131805, 0.07243060320615768, 0.2866848111152649, 0.05213906243443489, 0.06376848369836807, 0.0537116602063179, 0.39020073413848877, 0.07415207475423813]
This use of model-theoretic interpretation represents an important extension to thesemantic grammars used in existing statistical spoken language interfaces, which rely on co-occurrences among lexically-determined semantic classes and slot fillers (Miller et al, 1996), in that the probability of an analysis is now also conditioned on the existence of denoted entities and relations in the world model.	[0, 104, 1, 109, 15, 119, 32, 19, 60, 108]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7590041756629944, 0.6935935616493225, 0.21644651889801025, 0.2470950484275818, 0.19268804788589478, 0.2091735601425171, 0.08130571991205215, 0.11534926295280457, 0.05071120709180832, 0.1544186770915985]
This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012).	[3, 32, 17, 10, 29, 42, 56, 251, 0, 78]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4524775445461273, 0.30978721380233765, 0.27194589376449585, 0.08691128343343735, 0.14049212634563446, 0.07071429491043091, 0.17100638151168823, 0.08374693244695663, 0.36337512731552124, 0.14033861458301544]
This was done using the SRI Language Modelling toolkit (Stolcke, 2002) employing linear interpolation and modified Kneser Ney discounting (Chen and Goodman, 1996).	[17, 3, 48, 6, 0, 41, 39, 101, 26, 35]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.30560728907585144, 0.29202502965927124, 0.40823647379875183, 0.19222885370254517, 0.6426425576210022, 0.25614723563194275, 0.2504136860370636, 0.41746604442596436, 0.11451093852519989, 0.08811374753713608]
This was the approach taken by Merlo and Stevenson (2001), who worked with a Decision Tree and selected linguistic cues to classify English verbs into three classes: unaccusative, unergative and object-drop.	[33, 402, 157, 3, 10, 276, 342, 143, 66, 432]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5492024421691895, 0.7467986941337585, 0.44899800419807434, 0.2093774676322937, 0.2093774676322937, 0.48271411657333374, 0.09521560370922089, 0.28210556507110596, 0.24516789615154266, 0.4008646607398987]
This work was inspired by adaptor grammars (Johnson et al, 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree.	[116, 59, 33, 32, 2, 3, 194, 12, 172, 201]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3215191960334778, 0.20388978719711304, 0.1438518762588501, 0.35602301359176636, 0.06607965379953384, 0.050059713423252106, 0.36291173100471497, 0.06313615292310715, 0.08201774954795837, 0.05067645385861397]
Though punctuation is usually entirely ignored in unsupervised parsing research, Seginer (2007) departs from this in one key aspect: the use of phrasal punctuation - punctuation symbols that often mark phrasal boundaries within a sentence.	[123, 145, 125, 7, 11, 3, 18, 4, 9, 6]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3856014907360077, 0.7064027786254883, 0.3738088309764862, 0.19710351526737213, 0.16476649045944214, 0.1049838736653328, 0.09049922972917557, 0.07222374528646469, 0.051061879843473434, 0.05337655544281006]
Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure.	[5, 7, 1, 3, 2, 8, 6, 4, 9, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7139008641242981, 0.4200201630592346, 0.2682522237300873, 0.1164785623550415, 0.2577815353870392, 0.17111939191818237, 0.20426440238952637, 0.14502377808094025, 0.07841113209724426, 0.18417033553123474]
Three experiments involving the Twitter language model confirm Bertoldi and Federico (2009)'s findings that the language model was most helpful.	[27, 45, 142, 126, 127, 38, 151, 54, 28, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3603230118751526, 0.23021718859672546, 0.06086999550461769, 0.29110172390937805, 0.059167925268411636, 0.08325119316577911, 0.06311682611703873, 0.1663854569196701, 0.04615149646997452, 0.09077706187963486]
Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll and Briscoe, 2001).	[178, 44, 21, 10, 173, 168, 23, 1, 82, 81]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48737624287605286, 0.14364314079284668, 0.1986543983221054, 0.07265797257423401, 0.055762551724910736, 0.08420808613300323, 0.0838245376944542, 0.33787286281585693, 0.23948802053928375, 0.08187946677207947]
Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al, 2005) together with the semantic frames from FrameNet (Baker et al, 1998) to create an inference mechanism to improve QA.	[37, 429, 386, 51, 79, 331, 255, 347, 345, 368]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.047392845153808594, 0.08730661869049072, 0.20780616998672485, 0.12709572911262512, 0.10190563648939133, 0.47134119272232056, 0.08282782137393951, 0.15337494015693665, 0.13088147342205048, 0.07223977148532867]
Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008).	[10, 16, 3, 106, 20, 90, 13, 72, 86, 34]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5305633544921875, 0.3583829998970032, 0.18354147672653198, 0.10729843378067017, 0.19179879128932953, 0.05848923325538635, 0.06411474943161011, 0.10014917701482773, 0.050121910870075226, 0.07244192808866501]
Thus, the features in Mani et al (2006) are augmented with those used to describe signals detailed in Derczynski and Gaizauskas (2010), with some slight changes.	[135, 57, 106, 54, 42, 67, 105, 56, 13, 76]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4735356271266937, 0.6349663138389587, 0.1304824948310852, 0.19242647290229797, 0.04823729768395424, 0.052914418280124664, 0.05423162877559662, 0.3542616665363312, 0.04549698531627655, 0.057379499077796936]
Tjong Kim Sang and Buchholz (2000) give an overview of the CoNLL shared task of chunking.	[0, 3, 117, 86, 79, 114, 62, 10, 82, 71]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 1]	[0.7753705382347107, 0.46167489886283875, 0.3017491102218628, 0.15081781148910522, 0.24120350182056427, 0.5421228408813477, 0.06254791468381882, 0.25437694787979126, 0.2505519986152649, 0.5770210027694702]
To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b).	[172, 87, 2, 27, 86, 5, 168, 80, 153, 105]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6980341076850891, 0.518356442451477, 0.17657041549682617, 0.07717151194810867, 0.16114458441734314, 0.24822811782360077, 0.15608586370944977, 0.10638107359409332, 0.08976274728775024, 0.06967130303382874]
To be particular, the method used by McCarthy and Carroll (2003) is formula (6).	[177, 73, 26, 54, 99, 135, 66, 88, 27, 165]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6541754603385925, 0.507687509059906, 0.1361989825963974, 0.08448866009712219, 0.12409470975399017, 0.047407351434230804, 0.2699487805366516, 0.3906394839286804, 0.049463964998722076, 0.06345552206039429]
To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees.	[0, 4, 162, 49, 71, 36, 168, 21, 33, 41]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6648151278495789, 0.4277491867542267, 0.5538870096206665, 0.22882305085659027, 0.46598511934280396, 0.45257583260536194, 0.11286619305610657, 0.30588892102241516, 0.04800993204116821, 0.07044811546802521]
To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side.	[105, 103, 5, 112, 31, 2, 77, 102, 27, 110]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4504282474517822, 0.18407149612903595, 0.22946381568908691, 0.25970908999443054, 0.08121174573898315, 0.19966179132461548, 0.24785515666007996, 0.16437208652496338, 0.12968215346336365, 0.05215802788734436]
To compute similarity for nouns we adopt conceptual density (cd) (Agirre and Rigau, 1996), a semantic similarity model previously applied to word sense disambiguation tasks.	[238, 0, 41, 193, 183, 251, 44, 132, 209, 155]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.08891250938177109, 0.6669472455978394, 0.3168734312057495, 0.5374462008476257, 0.18441231548786163, 0.1905481070280075, 0.04961147904396057, 0.05771581828594208, 0.2466880977153778, 0.06438595801591873]
To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	[4, 2, 129, 0, 543, 6, 36, 3, 187, 219]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1152392253279686, 0.06692693382501602, 0.18582797050476074, 0.18429943919181824, 0.08042672276496887, 0.09577962011098862, 0.13693846762180328, 0.0682167336344719, 0.3022337257862091, 0.3203068971633911]
To deal with such problems ,weadopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998).	[199, 9, 55, 14, 28, 144, 21, 27, 169, 187]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14261561632156372, 0.19843719899654388, 0.08347117155790329, 0.10572696477174759, 0.05052538588643074, 0.05809001997113228, 0.08908388763666153, 0.07370592653751373, 0.06333424150943756, 0.05309014022350311]
To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)): Heuristic 1: Whenever the first feedback loop requires the addition of the main verb of the question as a query keyword, generate all verb conjugations as well as its nominalizations.	[26, 29, 15, 28, 34, 25, 30, 41, 24, 68]	[1, 1, 0, 1, 0, 1, 0, 0, 0, 0]	[0.6857845187187195, 0.6978309750556946, 0.3541710674762726, 0.6038314700126648, 0.40592241287231445, 0.5430274605751038, 0.4032271206378937, 0.16171199083328247, 0.18079888820648193, 0.45965614914894104]
To determine whether semantic restrictions are being violated, domain information from ontologies/thesauri such as WordNet could be used and/or statistical techniques as used by Mason (2004).	[43, 144, 282, 60, 256, 88, 111, 117, 41, 6]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6092312335968018, 0.048033103346824646, 0.046328578144311905, 0.2545851171016693, 0.045881323516368866, 0.07859484851360321, 0.1015622466802597, 0.4155789017677307, 0.2113635540008545, 0.06370536983013153]
To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., Harabagiu et al (2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).	[16, 25, 3, 61, 24, 30, 48, 55, 19, 73]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6188634037971497, 0.3832518756389618, 0.07441142201423645, 0.07441142201423645, 0.27893221378326416, 0.19703924655914307, 0.08918231725692749, 0.07043173909187317, 0.06416161358356476, 0.23880814015865326]
To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994).	[93, 21, 99, 19, 98, 92, 25, 122, 83, 0]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5332072973251343, 0.5222232937812805, 0.1507006287574768, 0.11258435249328613, 0.06607987731695175, 0.36329779028892517, 0.05063682049512863, 0.0886068120598793, 0.12964630126953125, 0.5799685120582581]
To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents.	[139, 153, 8, 27, 164, 211, 30, 179, 64, 51]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3131718337535858, 0.6644947528839111, 0.04904044046998024, 0.052047036588191986, 0.07261544466018677, 0.04408835247159004, 0.05410028249025345, 0.18395473062992096, 0.1189282089471817, 0.05364971607923508]
To further narrow the search space, we only consider IV words which are morphophonemic ally similar to the OOV type, following settings in Han and Baldwin (2011).	[86, 50, 87, 100, 2, 132, 69, 204, 70, 198]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6285696029663086, 0.15781067311763763, 0.19234097003936768, 0.2987765371799469, 0.17899039387702942, 0.17264363169670105, 0.14017315208911896, 0.4812356233596802, 0.24309399724006653, 0.08164288848638535]
To gather examples from parallel corpora, we followed the approach in (Ng et al, 2003).	[120, 2, 72, 32, 85, 153, 30, 79, 74, 131]	[1, 0, 1, 0, 0, 0, 0, 1, 0, 0]	[0.6875056624412537, 0.2468624711036682, 0.6426957249641418, 0.25273066759109497, 0.38443422317504883, 0.21743132174015045, 0.4153803586959839, 0.5751577615737915, 0.09382462501525879, 0.14303351938724518]
To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy.	[10, 2, 65, 71, 84, 22, 6, 85, 25, 51]	[1, 0, 0, 0, 1, 0, 1, 0, 0, 0]	[0.6472693085670471, 0.4287533760070801, 0.3054240942001343, 0.36971548199653625, 0.6627473831176758, 0.2560907304286957, 0.6288621425628662, 0.2659023702144623, 0.18525175750255585, 0.13467217981815338]
To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005).	[135, 46, 67, 42, 106, 130, 123, 103, 31, 79]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4530509412288666, 0.059390317648649216, 0.10070665925741196, 0.0919555053114891, 0.1604088693857193, 0.06416095793247223, 0.05114859342575073, 0.08157559484243393, 0.06421160697937012, 0.09568697959184647]
To make them useful, the necessary preprocessing steps must have been done. The texts were first automatically segmented and tokenized and then they were part-of-speech tagged by TnT tagger (Brants, 2000), which was trained on the respective WILS training data.	[0, 1, 123, 145, 6, 10, 15, 37, 44, 147]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6856058239936829, 0.582004189491272, 0.3096725642681122, 0.38852012157440186, 0.30731236934661865, 0.12519660592079163, 0.35771644115448, 0.06357613950967789, 0.18091419339179993, 0.30107998847961426]
To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al, 1999).	[68, 22, 222, 118, 189, 151, 81, 29, 89, 229]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13883236050605774, 0.14569173753261566, 0.14569173753261566, 0.1555759757757187, 0.3588618338108063, 0.2816113233566284, 0.08147219568490982, 0.43528279662132263, 0.0521695651113987, 0.43528279662132263]
To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs.	[14, 138, 5, 111, 9, 124, 120, 102, 0, 130]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.744254469871521, 0.6908039450645447, 0.47063618898391724, 0.10372324287891388, 0.16253642737865448, 0.5148190855979919, 0.47587844729423523, 0.3201267719268799, 0.28806471824645996, 0.2621707618236542]
To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags.	[4, 157, 127, 174, 10, 17, 163, 139, 53, 82]	[1, 0, 0, 0, 0, 0, 1, 1, 0, 0]	[0.5567787885665894, 0.34574177861213684, 0.3164452016353607, 0.14185000956058502, 0.18580490350723267, 0.304979532957077, 0.5493425726890564, 0.5255617499351501, 0.05921662226319313, 0.13292564451694489]
To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation.	[106, 82, 81, 88, 117, 120, 86, 100, 133, 20]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7775822877883911, 0.5953481793403625, 0.38894763588905334, 0.3308480381965637, 0.14830823242664337, 0.30824998021125793, 0.4347541928291321, 0.12761695683002472, 0.0813220739364624, 0.13149389624595642]
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model.	[25, 178, 82, 64, 7, 81, 18, 181, 2, 84]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6282419562339783, 0.6166808009147644, 0.2837996482849121, 0.23125804960727692, 0.11641567200422287, 0.39227911829948425, 0.1804787814617157, 0.37612494826316833, 0.15222717821598053, 0.08602649718523026]
To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a).	[20, 109, 4, 26, 142, 122, 34, 1, 167, 107]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.615159809589386, 0.2701188921928406, 0.07794518023729324, 0.08050767332315445, 0.08751974999904633, 0.12816661596298218, 0.3342110812664032, 0.061268337070941925, 0.12389469146728516, 0.05201711505651474]
To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD).	[6, 191, 270, 155, 267, 146, 192, 156, 50, 2]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6607267260551453, 0.5688286423683167, 0.20495353639125824, 0.18807324767112732, 0.6712413430213928, 0.07701608538627625, 0.4248398542404175, 0.11583907157182693, 0.4125153124332428, 0.1292983591556549]
To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004).	[54, 145, 69, 53, 28, 27, 49, 47, 76, 119]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.47786274552345276, 0.5863391160964966, 0.2872788608074188, 0.3463825285434723, 0.09797745943069458, 0.10388612002134323, 0.1289118230342865, 0.15924102067947388, 0.0967881977558136, 0.05587735399603844]
To test this hypothesis, we tried two quite different training data sets, one from the cell phone domain and the other from the DVD player domain, both used in (Wu et al, 2009).	[142, 125, 159, 162, 157, 139, 161, 143, 147, 141]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7366647124290466, 0.44211748242378235, 0.4682839810848236, 0.3441462218761444, 0.30606725811958313, 0.27506476640701294, 0.38784652948379517, 0.5927260518074036, 0.11052043735980988, 0.09367547184228897]
To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al (2011)) automatically extracted from the ClueWeb09 web crawl, where each extraction comprises a predicate and two arguments.	[217, 151, 139, 147, 36, 80, 150, 9, 109, 125]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5404883027076721, 0.11642687022686005, 0.3077685534954071, 0.2454092651605606, 0.1104663610458374, 0.08284939080476761, 0.26728808879852295, 0.12442762404680252, 0.17469999194145203, 0.4678918421268463]
To this end, the GATE Gazetteer (Cunningham et al., 2002) was used, and only entities recognized by it automatically were considered.	[28, 35, 174, 53, 84, 222, 195, 50, 51, 43]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5792466998100281, 0.23364536464214325, 0.1996828317642212, 0.05284775048494339, 0.05805778130888939, 0.05805778130888939, 0.05284775048494339, 0.1387432962656021, 0.2312372922897339, 0.2848567068576813]
To this end, the Grammar Matrix project (Bender et al, 2002) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar for a language of their choice.	[14, 86, 8, 112, 6, 2, 16, 97, 113, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4586088955402374, 0.24754296243190765, 0.07017097622156143, 0.2798285484313965, 0.07842057198286057, 0.313510537147522, 0.09560727328062057, 0.04757809266448021, 0.3723628520965576, 0.11351866275072098]
To this end, we intend to implement a second-pass analysis that would rerank the candidates produced by fuzzy inverted generation by computing text similarities over short passages such as those propose din (Hatzivassiloglou et al, 1999).	[0, 39, 196, 38, 33, 36, 104, 12, 131, 190]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3680681586265564, 0.29861170053482056, 0.29861170053482056, 0.2536095082759857, 0.15260402858257294, 0.0527409203350544, 0.1053454577922821, 0.3590332567691803, 0.24457909166812897, 0.15260402858257294]
Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences.	[52, 51, 157, 127, 4, 42, 151, 129, 122, 165]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6520082950592041, 0.6066606640815735, 0.14113572239875793, 0.18031960725784302, 0.08453036844730377, 0.07234655320644379, 0.21462823450565338, 0.07416258007287979, 0.05170927196741104, 0.12387165427207947]
Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank.	[48, 27, 51, 91, 47, 93, 96, 49, 0, 28]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6781248450279236, 0.6882924437522888, 0.1077663004398346, 0.36759206652641296, 0.12296716868877411, 0.15070828795433044, 0.05753526836633682, 0.050927769392728806, 0.3856712877750397, 0.20513057708740234]
Toutanova and Moore (2002) extend Brill and Moore (2000) to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling.	[149, 74, 60, 115, 51, 129, 45, 19, 111, 145]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.803675651550293, 0.5222534537315369, 0.5599663853645325, 0.2983416020870209, 0.7329952120780945, 0.10881177335977554, 0.16578690707683563, 0.39566248655319214, 0.17011769115924835, 0.09057214111089706]
Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes.	[27, 1, 7, 15, 19, 5, 0, 29, 32, 33]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23025262355804443, 0.0832606628537178, 0.20961610972881317, 0.06373310834169388, 0.1400211900472641, 0.08266972750425339, 0.2379254549741745, 0.07443457096815109, 0.047266412526369095, 0.0712089091539383]
Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000).	[97, 27, 68, 19, 47, 70, 46, 73, 33, 117]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.5880286693572998, 0.6793421506881714, 0.17333172261714935, 0.12886106967926025, 0.3095262944698334, 0.32531607151031494, 0.20103520154953003, 0.1239623874425888, 0.5211953520774841, 0.045547351241111755]
Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005).	[111, 109, 108, 112, 114, 67, 113, 122, 104, 29]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.7986555099487305, 0.7397083640098572, 0.522996187210083, 0.6335997581481934, 0.4361446499824524, 0.262068510055542, 0.35273683071136475, 0.19662471115589142, 0.1883123368024826, 0.06531769037246704]
Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning (Wellington et al, 2006).	[6, 27, 7, 35, 4, 0, 55, 1, 179, 23]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.815445601940155, 0.24233795702457428, 0.14161786437034607, 0.33033788204193115, 0.05589358136057854, 0.512511134147644, 0.055400166660547256, 0.0840424969792366, 0.06245513632893562, 0.1035909503698349]
Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs.	[6, 187, 181, 7, 123, 25, 189, 21, 191, 121]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4235072135925293, 0.6641862988471985, 0.2851361930370331, 0.07124980539083481, 0.22596652805805206, 0.16334335505962372, 0.08774525672197342, 0.22742792963981628, 0.08316057920455933, 0.3633311986923218]
Two further problems are the treatment of unary rules and functors with what Shieber (1988) calls vestigial semantics, which we prefer to call identity semantics.	[176, 255, 14, 56, 21, 11, 162, 207, 260, 40]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2955945134162903, 0.215600848197937, 0.18816258013248444, 0.1275283247232437, 0.08413348346948624, 0.07700954377651215, 0.06044496223330498, 0.055879879742860794, 0.08975816518068314, 0.0957556888461113]
Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007).	[4, 48, 15, 0, 1, 9, 16, 58, 17, 92]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.801930844783783, 0.23908770084381104, 0.389367938041687, 0.17182648181915283, 0.07824645936489105, 0.3318592309951782, 0.06042934209108353, 0.1722749024629593, 0.14583316445350647, 0.06227150931954384]
Two main extensions from that work that we are making use of are: 1) proofs falling below a user defined cost threshold halt the search 2) a simple variable typing system reduces the number of axioms written and the size of the search space (Hobbs et al, 1988, pg 102).	[26, 261, 208, 78, 68, 221, 193, 126, 152, 252]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4190683364868164, 0.24628065526485443, 0.09463106095790863, 0.23426863551139832, 0.21337935328483582, 0.11347880959510803, 0.05990653857588768, 0.2906668186187744, 0.2807369828224182, 0.05641321465373039]
Two measures are used to evaluate the parses: lexical accuracy, which is the percentage of correctly tagged words compared to the extracted gold standard corpus (Watkinson and Manandhar, 2001) and average crossing bracket rate (CBR) (Goodman, 1996).	[39, 10, 4, 1, 51, 11, 17, 119, 43, 38]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7230719327926636, 0.6783828139305115, 0.14885294437408447, 0.46499112248420715, 0.373396098613739, 0.6643309593200684, 0.08096957206726074, 0.444084495306015, 0.16049696505069733, 0.19109828770160675]
Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al, 2008a), which are both based on the Wall Street Journal (WSJ) corpus.	[134, 16, 140, 158, 41, 18, 143, 27, 42, 89]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6954198479652405, 0.5822241306304932, 0.1357027143239975, 0.08780734241008759, 0.054136089980602264, 0.10168660432100296, 0.18450194597244263, 0.07780195027589798, 0.08619649708271027, 0.12075816839933395]
Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)).	[359, 381, 2, 354, 48, 36, 226, 248, 38, 220]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5146583318710327, 0.08359523862600327, 0.3793395161628723, 0.26457691192626953, 0.19004712998867035, 0.3234066963195801, 0.051996953785419464, 0.1941707283258438, 0.381789892911911, 0.11405517905950546]
Two-level formal lists based on that introduced by (Koskenniemi, 1983) (see also (Ritchie et al, 1992) and (Kaplan and Kay, 1994)) are widely used in practical NLP systems, and are deservedly regarded as something of a standard.	[16, 821, 668, 775, 632, 18, 659, 2, 5, 759]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7488270998001099, 0.5446795225143433, 0.42276209592819214, 0.5459034442901611, 0.39788058400154114, 0.2897573411464691, 0.35379618406295776, 0.23470951616764069, 0.23470951616764069, 0.27146679162979126]
USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation.	[105, 23, 34, 75, 12, 142, 26, 1, 14, 168]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7764339447021484, 0.10221109539270401, 0.080513134598732, 0.2877959609031677, 0.07320570200681686, 0.4698043167591095, 0.06306098401546478, 0.06642352789640427, 0.05956520512700081, 0.09183157235383987]
USP (Poon and Domingos, 2009) is based on Markov Logic Networks and attempts to create a full semantic parse in an unsupervised fashion.	[12, 1, 17, 245, 76, 16, 54, 58, 53, 0]	[1, 0, 1, 1, 1, 0, 0, 0, 1, 1]	[0.6504930257797241, 0.4839835464954376, 0.5690805912017822, 0.5998420715332031, 0.5944406390190125, 0.2294885814189911, 0.39160987734794617, 0.315954327583313, 0.5035309791564941, 0.7618738412857056]
Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al, 2005).	[135, 0, 36, 61, 109, 175, 186, 42, 6, 1]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15800559520721436, 0.6266128420829773, 0.06723136454820633, 0.05259956791996956, 0.05720015615224838, 0.45833611488342285, 0.05612145736813545, 0.0528135783970356, 0.10288338363170624, 0.08623282611370087]
Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure.	[103, 71, 184, 54, 46, 44, 70, 64, 142, 162]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36524486541748047, 0.6019693613052368, 0.33157065510749817, 0.07705520838499069, 0.08945271372795105, 0.09111015498638153, 0.16329774260520935, 0.057618338614702225, 0.05478847771883011, 0.23948125541210175]
Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the n-bests other than those in ORACLE.	[87, 25, 42, 128, 141, 8, 41, 27, 39, 64]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.15095770359039307, 0.33065277338027954, 0.0892893448472023, 0.3929186761379242, 0.05607477203011513, 0.08000275492668152, 0.10674189031124115, 0.07723351567983627, 0.06437408179044724, 0.5526482462882996]
Unlike Ng et al (2003) our algorithm works on monolingual corpora, which are much more abundant than parallel ones, and is fully automatic.	[27, 2, 106, 149, 73, 153, 54, 40, 130, 150]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06241385638713837, 0.09988178312778473, 0.043098222464323044, 0.41950052976608276, 0.18746043741703033, 0.08860940486192703, 0.0458909347653389, 0.3464374244213104, 0.05879250541329384, 0.08136777579784393]
Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems are not required to translate the whole context but just the target word.	[0, 9, 1, 19, 57, 66, 64, 102, 24, 35]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5212004780769348, 0.22542747855186462, 0.08691439032554626, 0.08868363499641418, 0.16584259271621704, 0.1813534051179886, 0.22427144646644592, 0.10894373059272766, 0.21190078556537628, 0.08604288101196289]
Unlike some of the previous work (e.g., (Titov and McDonald, 2008a)), we do not constrain aspect specific sentiment to be the same across the document.	[38, 44, 8, 45, 35, 18, 43, 139, 2, 178]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6559551954269409, 0.37446147203445435, 0.2134619653224945, 0.3648940622806549, 0.27966389060020447, 0.18401873111724854, 0.31534019112586975, 0.06300681829452515, 0.10433731973171234, 0.2599022090435028]
Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwateret al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001).	[1, 0, 5, 171, 230, 525, 172, 9, 170, 517]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3414948582649231, 0.7830696702003479, 0.3414948582649231, 0.14050476253032684, 0.21951620280742645, 0.14319895207881927, 0.08005832135677338, 0.2634134888648987, 0.28444966673851013, 0.11661679297685623]
Use of unification (a core operation in HPSG) in CG dates at least as far back as Karttunen (1986, 1989), Uszkoreit (1986), and Zeevat (1988).	[130, 24, 21, 25, 41, 128, 1, 199, 104, 126]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18728385865688324, 0.4678525924682617, 0.09143713116645813, 0.27667495608329773, 0.13248443603515625, 0.07636300474405289, 0.13392403721809387, 0.07307638227939606, 0.05816587433218956, 0.07671694457530975]
Using machine transliteration can resolve part of UNK translation (Knight and Graehl, 1997).	[0, 9, 166, 22, 36, 4, 122, 145, 5, 3]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8309211730957031, 0.668423056602478, 0.521123468875885, 0.40651997923851013, 0.20514513552188873, 0.3807596266269684, 0.2029862105846405, 0.33149290084838867, 0.16800600290298462, 0.04991643503308296]
Using the same models, Mann and Yarowsky (2001) induced over 90% of the Spanish-Portuguese cognate vocabulary.	[21, 132, 37, 17, 3, 126, 29, 158, 151, 16]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.4803515672683716, 0.49315744638442993, 0.2196410447359085, 0.0884600356221199, 0.06619299203157425, 0.21957485377788544, 0.1296284943819046, 0.2196410447359085, 0.26149991154670715, 0.5149704813957214]
Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies.	[98, 139, 167, 150, 122, 111, 34, 76, 72, 17]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5030143857002258, 0.6857866644859314, 0.07141371816396713, 0.13205836713314056, 0.20125959813594818, 0.1452644020318985, 0.2350376397371292, 0.09204348176717758, 0.09084364771842957, 0.1493988186120987]
Using these simple, language agnostic measures allows one to look for divergence types such as those described by Dorr (1994).	[19, 134, 23, 17, 25, 349, 273, 270, 4, 47]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6562926769256592, 0.5838027000427246, 0.682364821434021, 0.4552574157714844, 0.40927430987358093, 0.15828627347946167, 0.36551007628440857, 0.30277472734451294, 0.1739317923784256, 0.2329094111919403]
Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00.	[78, 27, 145, 6, 130, 69, 39, 104, 26, 134]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5203799605369568, 0.38492199778556824, 0.07570917159318924, 0.05286281555891037, 0.07801691442728043, 0.05210050940513611, 0.05576595291495323, 0.06556078791618347, 0.06999150663614273, 0.04989127069711685]
Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model.	[130, 14, 65, 21, 157, 129, 135, 32, 143, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.27988192439079285, 0.4407305419445038, 0.36783382296562195, 0.05434095486998558, 0.06949997693300247, 0.04679662734270096, 0.08798898756504059, 0.09986503422260284, 0.07948469370603561, 0.548558235168457]
Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007).	[30, 147, 144, 148, 0, 149, 4, 123, 150, 14]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.593381404876709, 0.4959201216697693, 0.31057238578796387, 0.22365637123584747, 0.1612367182970047, 0.3173990547657013, 0.05483658239245415, 0.0976022407412529, 0.06316134333610535, 0.053647201508283615]
Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost (Carreras et al., 2002), Hidden Markov Models (Bikel et al,), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used.	[54, 64, 7, 76, 79, 69, 52, 80, 82, 66]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23327603936195374, 0.09788641333580017, 0.32558509707450867, 0.10574470460414886, 0.08970983326435089, 0.09466084092855453, 0.0792405754327774, 0.10782358050346375, 0.14144039154052734, 0.06193540245294571]
Vieira & Poesio (2000), Harabagiu et al (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, other and definite NP anaphora, and MUC-style coreference resolution.	[0, 65, 72, 18, 17, 5, 21, 71, 30, 44]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6382291316986084, 0.4332084655761719, 0.06815942376852036, 0.12996028363704681, 0.09354571998119354, 0.3877832591533661, 0.07965770363807678, 0.1388894021511078, 0.10881617665290833, 0.129774272441864]
Vijay-Shanker and Joshi (1985) introduced the first TAG parser in a CYK-like algorithm.	[162, 20, 148, 442, 261, 7, 2, 89, 19, 95]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5864846706390381, 0.3226846754550934, 0.5136399865150452, 0.05455540865659714, 0.1814979761838913, 0.19676846265792847, 0.2054738849401474, 0.24534067511558533, 0.07774847000837326, 0.05126601457595825]
Virga and Khudanpur (2003) model this scoring function using a separate translation and language model, that is, s (e, f)= Pr (f |e) Pr (e).	[128, 54, 96, 68, 53, 58, 95, 52, 60, 123]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4515489935874939, 0.1299290955066681, 0.14540448784828186, 0.0477578230202198, 0.40402951836586, 0.21191343665122986, 0.2797452509403229, 0.13831792771816254, 0.05515037849545479, 0.24507731199264526]
We achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009).	[168, 71, 45, 151, 54, 48, 53, 113, 26, 101]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7550995945930481, 0.555778980255127, 0.06145545095205307, 0.051719386130571365, 0.06503301113843918, 0.06465240567922592, 0.13384491205215454, 0.05278884246945381, 0.058532845228910446, 0.08044693619012833]
We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can be realized by the fast portable realizer RealPro (Lavoie and Rambow, 1997).	[0, 11, 12, 38, 53, 7, 2, 14, 41, 23]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7069599032402039, 0.596795380115509, 0.2526155710220337, 0.3079815208911896, 0.26196572184562683, 0.3189312815666199, 0.06967421621084213, 0.2553096115589142, 0.14010848104953766, 0.13529303669929504]
We adopt the approach of Pangand Lee (Pang and Lee, 2005) described in Section 2 for feature rating estimation.	[145, 71, 130, 140, 99, 35, 37, 43, 89, 41]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6818024516105652, 0.628248929977417, 0.2885158956050873, 0.13576054573059082, 0.19472216069698334, 0.45931723713874817, 0.42229729890823364, 0.09414543956518173, 0.2688395082950592, 0.0979258343577385]
We adopt the common problem formulation for this task described by Merialdo (1994).	[17, 27, 64, 75, 61, 86, 14, 53, 120, 63]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3421317934989929, 0.4469161331653595, 0.2650383412837982, 0.16657133400440216, 0.17413713037967682, 0.14389081299304962, 0.32331058382987976, 0.4029541611671448, 0.09246950596570969, 0.0458524115383625]
We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences.	[4, 14, 11, 10, 7, 6, 15, 1, 32, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7571042776107788, 0.5784581303596497, 0.18240995705127716, 0.14660142362117767, 0.09360545873641968, 0.06245909258723259, 0.38241374492645264, 0.04821649193763733, 0.0540819950401783, 0.4069543182849884]
We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004).	[111, 11, 140, 108, 15, 94, 84, 67, 75, 72]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26882222294807434, 0.08845395594835281, 0.3206670582294464, 0.05401814356446266, 0.05267488583922386, 0.07204483449459076, 0.1350359320640564, 0.049386605620384216, 0.04863017424941063, 0.07199110835790634]
We also compare our unsupervised methods against the supervised method proposed by Katz and Giesbrecht (2006).	[103, 8, 27, 98, 42, 119, 32, 109, 31, 93]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.674894392490387, 0.1277075856924057, 0.5561964511871338, 0.15063925087451935, 0.18643637001514435, 0.07360745221376419, 0.16488783061504364, 0.18235953152179718, 0.09659187495708466, 0.05955632030963898]
We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features.	[118, 119, 12, 100, 117, 52, 86, 22, 2, 75]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6141130924224854, 0.3079627454280853, 0.12115222215652466, 0.22710642218589783, 0.42700785398483276, 0.25454679131507874, 0.11916177719831467, 0.11231851577758789, 0.13930198550224304, 0.27051469683647156]
We annotate the same set of 800 tweets mentioned previously with tags from the CoNLL shared task (Tjong Kim Sang and Buchholz,2000).	[0, 3, 62, 117, 96, 86, 79, 123, 118, 114]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7769712805747986, 0.4902198016643524, 0.08222585916519165, 0.10799768567085266, 0.38217398524284363, 0.132607564330101, 0.2345563918352127, 0.16080200672149658, 0.18730168044567108, 0.4816112816333771]
We applied the instance-based methodology to evaluate two state-of-the-art unsupervised acquisition algorithms, DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al, 2004), whose output is publicly available.	[3, 24, 4, 32, 74, 20, 66, 51, 147, 23]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7091161012649536, 0.5753588080406189, 0.12316452711820602, 0.11753886938095093, 0.5491980910301208, 0.38891786336898804, 0.14519357681274414, 0.05595846846699715, 0.13858647644519806, 0.051157526671886444]
We are aware of two methods that have been proposed for significance testing with BLUE: bootstrap resampling (Koehn, 2004b; Zhang et al, 2004) and the sign test (Collins et al, 2005).	[42, 27, 143, 158, 148, 147, 141, 142, 159, 127]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.8023236393928528, 0.7551459074020386, 0.5447995066642761, 0.18471112847328186, 0.11423929780721664, 0.25013676285743713, 0.10493195801973343, 0.06746628135442734, 0.46834808588027954, 0.0525088869035244]
We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins (1999).	[110, 24, 29, 70, 23, 26, 33, 12, 32, 84]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6617238521575928, 0.5578504800796509, 0.39003151655197144, 0.14604836702346802, 0.152121901512146, 0.24527262151241302, 0.23914773762226105, 0.08078529685735703, 0.05205659940838814, 0.11900924891233444]
We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to: (1) learn concepts with high precision compared to singly-anchored pat terns (Kozareva et al, 2008), (2) use only one seed instance for the discovery of new previously unknown terms, and (3) harvest knowledge with minimal supervision.	[180, 134, 66, 43, 119, 14, 29, 1, 52, 54]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.44407689571380615, 0.0964227169752121, 0.09831211715936661, 0.3140631914138794, 0.0792495533823967, 0.23418793082237244, 0.13296717405319214, 0.13191813230514526, 0.06442998349666595, 0.24113784730434418]
We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively.	[4, 270, 212, 155, 230, 1, 278, 150, 261, 227]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.547078549861908, 0.15680691599845886, 0.29582998156547546, 0.39891302585601807, 0.2533229887485504, 0.46997424960136414, 0.283517062664032, 0.06294389814138412, 0.12771110236644745, 0.19359277188777924]
We based our POS table lookup on NYU's COMLEX (Grishman et al 1994).	[160, 0, 19, 91, 125, 72, 129, 71, 110, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18133917450904846, 0.14357581734657288, 0.13033482432365417, 0.06569353491067886, 0.06596310436725616, 0.06264517456293106, 0.05451009422540665, 0.04894733428955078, 0.05762740224599838, 0.055283524096012115]
We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans.	[129, 113, 5, 135, 138, 19, 53, 146, 45, 46]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.24712017178535461, 0.08437655121088028, 0.22126594185829163, 0.19447201490402222, 0.05193014070391655, 0.12900179624557495, 0.09363652020692825, 0.6497368216514587, 0.19184020161628723, 0.10411210358142853]
We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005).	[173, 26, 175, 71, 38, 120, 44, 118, 46, 61]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8079754710197449, 0.28475138545036316, 0.09964977204799652, 0.05037713423371315, 0.0516979917883873, 0.05409368500113487, 0.05807769298553467, 0.04574655741453171, 0.3791573941707611, 0.06193832680583]
We believe that this relaxation can be done in that particular case, as adjectives are much more likely to convey opinions a priori than verbs (Wiebe et al 2004).	[305, 153, 522, 303, 334, 147, 284, 201, 177, 52]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5861329436302185, 0.05310649052262306, 0.26904070377349854, 0.15440118312835693, 0.16621889173984528, 0.07008200138807297, 0.06835076212882996, 0.0687224417924881, 0.046650759875774384, 0.048551250249147415]
We borrow the terminology and notation of PATR-II (Shieber, 1984), a minimal constraint-based formalism that extends context-free grammar.	[56, 32, 59, 55, 57, 85, 31, 24, 58, 62]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19680394232273102, 0.2614469528198242, 0.12032319605350494, 0.15953460335731506, 0.338950514793396, 0.08468200266361237, 0.20975425839424133, 0.06326015293598175, 0.08380952477455139, 0.07602649927139282]
We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrase based models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al, 2006).	[20, 16, 130, 9, 1, 129, 57, 131, 32, 136]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4639088213443756, 0.39498865604400635, 0.2835669219493866, 0.38915497064590454, 0.40931951999664307, 0.19504143297672272, 0.1929381638765335, 0.18384099006652832, 0.08034779131412506, 0.3880062997341156]
We built grammars using its implementation of the suffix array extraction method described in Lopez (2007).	[43, 22, 242, 2, 260, 87, 16, 142, 34, 161]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7435396313667297, 0.4042988717556, 0.44963929057121277, 0.31206491589546204, 0.35720786452293396, 0.10526901483535767, 0.09896459430456161, 0.23993271589279175, 0.19530978798866272, 0.27744051814079285]
We call this pseudo projective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al, 1998).	[34, 0, 33, 21, 9, 117, 41, 82, 50, 86]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36304834485054016, 0.7453778386116028, 0.43344417214393616, 0.40800774097442627, 0.3464692234992981, 0.14088478684425354, 0.4926939308643341, 0.1894291490316391, 0.2214105725288391, 0.07861015945672989]
We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999).	[0, 18, 130, 45, 40, 20, 15, 83, 9, 22]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7842382192611694, 0.6162939071655273, 0.5316982269287109, 0.27300208806991577, 0.43189647793769836, 0.19354955852031708, 0.35324910283088684, 0.09864070266485214, 0.05576252192258835, 0.05888822674751282]
We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering.	[1, 0, 32, 134, 8, 144, 9, 36, 91, 11]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4290347695350647, 0.763500452041626, 0.08737753331661224, 0.2917937934398651, 0.09160029143095016, 0.0906926766037941, 0.10361722111701965, 0.08621922880411148, 0.10995239019393921, 0.06987929344177246]
We classified coordination keys into 52 classes ac cording to the classification proposed by (Kurohashiand Nagao, 1994).	[65, 37, 272, 61, 304, 268, 73, 155, 36, 134]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.16491667926311493, 0.6099916696548462, 0.3729638159275055, 0.475065678358078, 0.1646457314491272, 0.11635364592075348, 0.24451079964637756, 0.07899283617734909, 0.0595652274787426, 0.11310040205717087]
We collected cue phrases for such a content shifted sentence detection from the dataset adapters can be found as the supplementary material 765 works of Chapman et al (2007), Light et al (2004) and Vincze et al (2008) and from the experiments of Farkas and Szarvas (2008) and Farkas et al (2009).	[2, 139, 148, 111, 106, 83, 127, 125, 57, 72]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05186927691102028, 0.07317408174276352, 0.0556989386677742, 0.07233093678951263, 0.08277042210102081, 0.06348705291748047, 0.08012697100639343, 0.11918952316045761, 0.1161855012178421, 0.08910082280635834]
We compare our P-Mod algorithm against the t-test measure, which, of all standard measures, yields the best results in general-language collocation extraction studies (Evert and Krenn, 2001), and also against the widely used C-value, which aims at enhancing the common frequency of occurrence measure by making it sensitive to nested terms (Frantzi et al, 2000).	[39, 89, 17, 9, 110, 83, 2, 70, 42, 74]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6160683035850525, 0.5733397006988525, 0.2200215756893158, 0.19944074749946594, 0.07838968932628632, 0.6329191327095032, 0.14522281289100647, 0.2040330469608307, 0.41590791940689087, 0.41469982266426086]
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008).	[40, 30, 6, 210, 63, 31, 42, 162, 90, 3]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6951384544372559, 0.6370901465415955, 0.22461003065109253, 0.18870390951633453, 0.048149533569812775, 0.09579683840274811, 0.05429413542151451, 0.0494418628513813, 0.10071717947721481, 0.06107708439230919]
We compared these results against an inverse IBM model 1 but the results were inconclusive which is consistent with the results presented in (Och et al, 2004) where no improvements were achieved using p (e|f).	[4, 84, 158, 48, 3, 161, 191, 46, 69, 17]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48136961460113525, 0.675974428653717, 0.272100567817688, 0.3263219892978668, 0.05120760202407837, 0.2438269853591919, 0.18241485953330994, 0.16138894855976105, 0.10639120638370514, 0.06124177202582359]
We compute the association score from a linear combination of two clues: surface similarity computed as Equation (2) and position difference based distortion score by following (He et al, 2008).	[46, 72, 142, 171, 59, 119, 141, 118, 135, 61]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5508698225021362, 0.16202503442764282, 0.27488499879837036, 0.09764519333839417, 0.28987351059913635, 0.2008405178785324, 0.13940411806106567, 0.26570919156074524, 0.09497855603694916, 0.0638018473982811]
We consider all possible phrase-pairs in the training data, then use Fisher's Exact Test to filter out pairs with low correlation (Johnson et al, 2007).	[117, 29, 79, 102, 203, 12, 9, 86, 1, 69]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4507845342159271, 0.4739859998226166, 0.3442690372467041, 0.1682155281305313, 0.16497324407100677, 0.478971004486084, 0.08590208739042282, 0.29647138714790344, 0.06919161230325699, 0.1475219577550888]
We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.	[322, 325, 328, 335, 314, 326, 26, 321, 212, 315]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 1]	[0.7285119891166687, 0.5320185422897339, 0.5034186244010925, 0.5882052779197693, 0.17330162227153778, 0.5320091247558594, 0.2890607416629791, 0.37681862711906433, 0.32938700914382935, 0.5380702018737793]
We consider the standard phrase-based approach to MT (Och and Ney, 2004).	[1, 10, 0, 405, 35, 25, 158, 5, 14, 28]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.6204784512519836, 0.6204784512519836, 0.1704970747232437, 0.18700142204761505, 0.09779056161642075, 0.05941561236977577, 0.596161425113678, 0.0783819630742073, 0.0783819630742073, 0.0674651637673378]
We contrast our work with (Galley et al, 2004), highlight some severe limitations of probability estimates computed from single derivations, and demonstrate that it is critical to account for many derivations for each sentence pair.	[49, 18, 46, 179, 48, 80, 43, 197, 72, 55]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22211456298828125, 0.32831886410713196, 0.0523805245757103, 0.1899968981742859, 0.06379404664039612, 0.05964130535721779, 0.05144369602203369, 0.04847145453095436, 0.22913849353790283, 0.22014667093753815]
We do not use any additional information to remove specific features using alignments or syntax (unlike, e.g. removing all but one Al+ in noun phrases (Lee, 2004)).	[24, 33, 14, 15, 32, 6, 8, 19, 18, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06032838672399521, 0.3310093283653259, 0.1742824912071228, 0.12329325824975967, 0.19494354724884033, 0.04770445078611374, 0.056949540972709656, 0.06485360860824585, 0.059720464050769806, 0.05133354663848877]
We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006).	[188, 87, 163, 77, 91, 4, 214, 96, 78, 220]	[1, 1, 1, 0, 0, 0, 0, 0, 1, 0]	[0.7943077683448792, 0.7124714851379395, 0.6662052273750305, 0.3166511058807373, 0.09140968322753906, 0.400534987449646, 0.3221229016780853, 0.21907122433185577, 0.5986505150794983, 0.24339881539344788]
We draw on and extend the work of Marcu and Echihabi (2002).	[37, 50, 142, 21, 19, 38, 9, 1, 17, 23]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5816782116889954, 0.739259660243988, 0.2619343101978302, 0.07324954867362976, 0.10925508290529251, 0.18855302035808563, 0.18701836466789246, 0.09433785080909729, 0.10671093314886093, 0.05654425546526909]
We employ the method of Ritter et al (2011) to tokenise messages, and use token unigrams as features, including any hash tags, but ignoring twitter mentions, URLs and purely numeric tokens.	[50, 52, 84, 83, 144, 56, 155, 57, 51, 59]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49586254358291626, 0.12020159512758255, 0.11776459962129593, 0.05774999409914017, 0.18369251489639282, 0.3181384205818176, 0.07770080864429474, 0.44528791308403015, 0.043983057141304016, 0.32094982266426086]
We employ the technique of Support Vector Machines (SVMs) (Vapnik, 1998) as the machine learning technique, which has been successfully applied to various natural language processing tasks including chunking tasks such as phrase chunking (Kudo and Matsumoto, 2001) and named entity chunking (Mayfield et al, 2003).	[7, 0, 8, 180, 21, 189, 1, 68, 10, 104]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7607755661010742, 0.7047542333602905, 0.29181623458862305, 0.39644333720207214, 0.3556479811668396, 0.3649970591068268, 0.5067126154899597, 0.2430914044380188, 0.36431458592414856, 0.22027181088924408]
We employ this decomposition mainly for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker, following Toutanova et al (2005).	[37, 35, 39, 38, 36, 34, 101, 75, 41, 59]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6897643804550171, 0.6599488854408264, 0.5256869792938232, 0.17767447233200073, 0.06929359585046768, 0.18033243715763092, 0.1609136313199997, 0.28790396451950073, 0.18718087673187256, 0.11119873076677322]
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets.	[27, 0, 29, 7, 16, 14, 4, 32, 2, 26]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7859317660331726, 0.7481066584587097, 0.1303354799747467, 0.06709735840559006, 0.08830602467060089, 0.08252787590026855, 0.09075228124856949, 0.06709735840559006, 0.11358558386564255, 0.0498516820371151]
We evaluate DPLP on the RST Discourse Tree bank (Carlson et al, 2001), comparing against state-of-the-art results.	[101, 154, 24, 26, 25, 79, 157, 57, 28, 34]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4369720220565796, 0.10887262970209122, 0.06460534781217575, 0.0845784917473793, 0.18574538826942444, 0.14291884005069733, 0.0673772469162941, 0.13147425651550293, 0.06756202131509781, 0.07170337438583374]
We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007).	[82, 100, 55, 16, 130, 81, 73, 150, 53, 114]	[1, 1, 1, 1, 0, 1, 1, 0, 0, 0]	[0.7787834405899048, 0.7410078644752502, 0.5520370602607727, 0.6120529770851135, 0.3065797984600067, 0.6742488741874695, 0.7373355031013489, 0.3560744524002075, 0.3114379644393921, 0.48599883913993835]
We evaluate the distribution of these rules in the same way as Chiang (2007).	[4, 9, 192, 99, 240, 74, 101, 146, 100, 54]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5509112477302551, 0.5509112477302551, 0.32066577672958374, 0.37589049339294434, 0.26731377840042114, 0.10997317731380463, 0.2630534768104553, 0.43393492698669434, 0.31731224060058594, 0.23568415641784668]
We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003).	[22, 56, 17, 85, 16, 163, 19, 69, 94, 26]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6226618885993958, 0.5656689405441284, 0.10669475793838501, 0.19491338729858398, 0.2812652885913849, 0.09955745190382004, 0.12891870737075806, 0.14699751138687134, 0.04651935026049614, 0.1405206173658371]
We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level.	[58, 30, 22, 3, 24, 80, 12, 46, 78, 10]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7192522883415222, 0.3980538845062256, 0.06135167181491852, 0.0821947380900383, 0.09627829492092133, 0.05417558178305626, 0.05649302154779434, 0.31550198793411255, 0.2878175377845764, 0.3051583170890808]
We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007).	[11, 36, 12, 156, 143, 1, 33, 28, 30, 148]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6738102436065674, 0.7567143440246582, 0.25056925415992737, 0.4697335958480835, 0.2859917879104614, 0.0740833580493927, 0.04782120883464813, 0.1127244085073471, 0.24289575219154358, 0.18630869686603546]
We explore this suggestion, implementing a lexical substitution (McCarthy and Navigli,2007) approach to dialogue generation with sentiment, using the Valentino approach and associated resources.	[33, 0, 116, 1, 120, 11, 3, 12, 126, 20]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5014480352401733, 0.7124494910240173, 0.06257105618715286, 0.3084537088871002, 0.1218411922454834, 0.13848456740379333, 0.09118441492319107, 0.08595526218414307, 0.04447896033525467, 0.055643852800130844]
We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b).	[165, 0, 6, 353, 371, 2, 335, 5, 338, 10]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.5503638386726379, 0.6528791785240173, 0.529136598110199, 0.3369421064853668, 0.5341783761978149, 0.22188018262386322, 0.34046050906181335, 0.3761204183101654, 0.4866960942745209, 0.45699214935302734]
We first converted the gold standard annotation of the GENIA treebank (Tateisi et al, 2005) into a dependency representation using the Stanford parser tools (de Marneffe et al, 2006) and then determined the shortest paths in the dependency analyses connecting each relevant entity with each NE.	[99, 8, 26, 169, 2, 22, 100, 186, 27, 17]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6232765316963196, 0.13569997251033783, 0.10817544907331467, 0.0669926255941391, 0.058392997831106186, 0.04582352563738823, 0.16748544573783875, 0.14266662299633026, 0.15427803993225098, 0.07120411098003387]
We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1.	[9, 6, 55, 101, 24, 119, 20, 103, 30, 123]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24210546910762787, 0.1301393061876297, 0.10767638683319092, 0.08615823835134506, 0.060917239636182785, 0.051960427314043045, 0.06567685306072235, 0.07616929709911346, 0.060578420758247375, 0.0623459592461586]
We follow Wiebe and Mihalcea (2006) in that we see subjective expressions as private states that are not open to objective observation or verification.	[23, 99, 26, 42, 77, 32, 94, 47, 41, 98]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7833099961280823, 0.5318805575370789, 0.18723970651626587, 0.4648129343986511, 0.1649961769580841, 0.2511167526245117, 0.20984269678592682, 0.1196649968624115, 0.05896798521280289, 0.2859087884426117]
We follow a standard procedure to extract statements, as similarly adopted by Nakashole et al (2012), using Stanford CoreNLP (Klein and Manning, 2003) to lemmatize and parse sentences.	[131, 61, 228, 145, 127, 30, 188, 119, 276, 101]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2331327348947525, 0.31342825293540955, 0.31342825293540955, 0.06936925649642944, 0.11346537619829178, 0.07796360552310944, 0.11660584807395935, 0.04724510759115219, 0.18452922999858856, 0.07919676601886749]
We follow the approach by Clark and Weir (2002) to create the test data.	[231, 241, 85, 253, 88, 81, 260, 82, 259, 236]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7398109436035156, 0.16165974736213684, 0.2861248552799225, 0.2702321410179138, 0.4355415999889374, 0.07855071127414703, 0.2565094232559204, 0.37780022621154785, 0.16057270765304565, 0.1412031650543213]
We follow the format from Peng et al (2004).	[69, 127, 181, 149, 92, 137, 13, 28, 148, 62]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6692519187927246, 0.07562969624996185, 0.047169312834739685, 0.04346325993537903, 0.04556465521454811, 0.04050002992153168, 0.042570725083351135, 0.047316960990428925, 0.04778033867478371, 0.10153191536664963]
We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories.	[89, 23, 33, 37, 125, 47, 20, 34, 26, 28]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.640299379825592, 0.6736201643943787, 0.09127628803253174, 0.6463052034378052, 0.09622113406658173, 0.23286935687065125, 0.2862232029438019, 0.22643131017684937, 0.12201555073261261, 0.0881170704960823]
We followed the benchmark assessment procedure in WMT and NIST MetricsMaTr (Callison-Burch et al, 2008, 2010), assessing the performance of the propose devaluation metric at the sentence level using ranking preference consistency, which also known as Kendall's rank correlation coefficient, to evaluate the correlation of the proposed metric with human judgments on translation adequacy ranking.	[147, 154, 166, 178, 3, 170, 12, 234, 143, 167]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4363475441932678, 0.20241138339042664, 0.3877265751361847, 0.28200799226760864, 0.44062331318855286, 0.2670844793319702, 0.11136960238218307, 0.09624160826206207, 0.2687685787677765, 0.10742858052253723]
We found that Brown et al's (1992) older information-theoretic approach, which does not explicitly address the problems of rare and ambiguous words (Clark, 2000) and was designed to induce large numbers of plausible syntactic and semantic clusters, can perform just as well.	[14, 15, 12, 3, 2, 38, 30, 4, 69, 0]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8123959302902222, 0.11004892736673355, 0.1108446791768074, 0.1186232715845108, 0.18549032509326935, 0.09781496226787567, 0.404653400182724, 0.166353240609169, 0.1686774045228958, 0.4343273937702179]
We found that our approach of using lemmatization improved both the word alignment and the quality of SMT with a small amounts of training data, and, while much work indicates that MA is useless in training large amounts of data (Lee, 2004), our intensive experiments proved that the chance to get a better MT quality using lemmatization is higher than that without it for large amounts of training data.	[40, 31, 1, 0, 5, 60, 24, 8, 53, 4]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1440451294183731, 0.1349935382604599, 0.06389494985342026, 0.11970649659633636, 0.06438887864351273, 0.22362980246543884, 0.06602969020605087, 0.08702518045902252, 0.09184864163398743, 0.08760195970535278]
We found that the deletion of lead parts did not occur very often in our summary, unlike the case of Jing and McKeown (2000).	[64, 45, 85, 3, 149, 162, 139, 43, 49, 146]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.040576476603746414, 0.13394249975681305, 0.2121572643518448, 0.09326013177633286, 0.27399399876594543, 0.14588649570941925, 0.2561433017253876, 0.04722588136792183, 0.07647087424993515, 0.04633484408259392]
We found that the oldest system (Brown et al, 1992) yielded the best prototypes, and that using these prototypes gave state-of-the-art performance on WSJ, as well as improvements on nearly all of the non-English corpora.	[26, 71, 37, 36, 106, 38, 141, 142, 39, 159]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3178907632827759, 0.3625994026660919, 0.07353923469781876, 0.13535906374454498, 0.07105890661478043, 0.045056067407131195, 0.18866899609565735, 0.04835956543684006, 0.049142349511384964, 0.13201119005680084]
We have implemented the Greedy Agreement Algorithm (Abney, 2002) which, based on two independent views of the data, is able to learn two binary classifiers from a set of hand-typed seed rules.	[9, 75, 102, 129, 145, 40, 127, 80, 138, 185]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46180450916290283, 0.7624644637107849, 0.14890320599079132, 0.494004487991333, 0.07757019996643066, 0.09855508804321289, 0.3240768611431122, 0.16169215738773346, 0.13250195980072021, 0.062435947358608246]
We have proposed a modification and extension of Eisner (1996)'s normal form that is more appropriate for commonly used variants of CCG with grammatical type-raising and generalized composition of bounded degree, as well as some non-combinatory extensions to CCG.	[136, 123, 22, 28, 41, 19, 29, 150, 13, 92]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3323422968387604, 0.1269698292016983, 0.12912927567958832, 0.13145671784877777, 0.16615061461925507, 0.060439400374889374, 0.06021690368652344, 0.06094125658273697, 0.06898972392082214, 0.09990686178207397]
We have used the same weights, listed in table 2, proposed by Lappin and Leass (1994).	[99, 190, 36, 9, 21, 288, 86, 60, 128, 30]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.7606446743011475, 0.47063666582107544, 0.3577168583869934, 0.4554601311683655, 0.4554601311683655, 0.3303101360797882, 0.5275545716285706, 0.07602711766958237, 0.16638223826885223, 0.06301778554916382]
We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline.	[96, 7, 20, 72, 35, 170, 32, 21, 60, 131]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4154926836490631, 0.2248646765947342, 0.22632452845573425, 0.20948290824890137, 0.13275867700576782, 0.1268627643585205, 0.08376660943031311, 0.1331866830587387, 0.08957725763320923, 0.11653918772935867]
We implemented the constituent-based preordering rule set in Wang et al (2007) for comparison, which is called WR07 below.	[5, 25, 3, 31, 48, 41, 79, 72, 23, 29]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6550394296646118, 0.1702822893857956, 0.08480432629585266, 0.19696883857250214, 0.15081803500652313, 0.14294008910655975, 0.31421852111816406, 0.08046100288629532, 0.09932371973991394, 0.047924868762493134]
We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline.	[118, 100, 107, 124, 2, 119, 17, 86, 123, 22]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.771245002746582, 0.7787606716156006, 0.17044490575790405, 0.36150139570236206, 0.14967867732048035, 0.2510610520839691, 0.11947987973690033, 0.24428750574588776, 0.1866656392812729, 0.468224436044693]
We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)).	[71, 48, 35, 110, 29, 130, 129, 43, 143, 187]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6869686841964722, 0.5828812122344971, 0.1012636125087738, 0.08883902430534363, 0.2911379039287567, 0.055526748299598694, 0.05134173110127449, 0.056663043797016144, 0.36417117714881897, 0.07920648157596588]
We looked at three different lemmatizers: the lemmatizing backend of the XTAG project (XTAG Re search Group, 2001) 4, Celex (Baayen et al, 1995), and the lemmatizing component of an enhancedTBL tagger (Brill, 1992).	[98, 85, 28, 77, 58, 21, 96, 80, 16, 5]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41523823142051697, 0.5191404819488525, 0.07203111052513123, 0.21643762290477753, 0.05024701729416847, 0.10181958228349686, 0.2308499962091446, 0.06091756001114845, 0.10164712369441986, 0.06068667769432068]
We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common.	[0, 66, 23, 79, 64, 81, 25, 49, 24, 29]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7425676584243774, 0.35073336958885193, 0.14263251423835754, 0.15572522580623627, 0.061917420476675034, 0.07215221226215363, 0.11997587978839874, 0.1720173954963684, 0.1580989807844162, 0.06606678664684296]
We next tested U DOP on two additional domains from Chinese and German which were also used in Klein and Manning (2002, 2004): the Chinese tree bank (Xue et al 2002) and the NEGRA corpus (Skut et al 1997).	[0, 9, 7, 1, 11, 3, 14, 12, 2, 10]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30998650193214417, 0.04958787560462952, 0.13970685005187988, 0.08924981206655502, 0.126895472407341, 0.30801522731781006, 0.08353414386510849, 0.13464570045471191, 0.06145775690674782, 0.10730621963739395]
We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al 1997) and the Chinese CTB10 (Xue et al 2002) both containing 2200+ sentences 10 words after removing punctuation.	[10, 9, 21, 11, 6, 4, 1, 3, 14, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31227099895477295, 0.06370490044355392, 0.11568673700094223, 0.07324839383363724, 0.0678914412856102, 0.08077111095190048, 0.055234868079423904, 0.15459667146205902, 0.05325322225689888, 0.18472087383270264]
We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008).	[49, 15, 118, 116, 147, 113, 22, 5, 57, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6348181962966919, 0.10581067204475403, 0.06698381900787354, 0.43234363198280334, 0.18684309720993042, 0.33259522914886475, 0.294016569852829, 0.10017526894807816, 0.05113702267408371, 0.24160681664943695]
We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in (Poon and Domingos, 2009).	[5, 18, 219, 187, 218, 189, 161, 193, 67, 192]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4072883129119873, 0.6065992116928101, 0.2649255394935608, 0.09603320807218552, 0.34828513860702515, 0.3704189360141754, 0.365932434797287, 0.10083025693893433, 0.10971692949533463, 0.10556333512067795]
We now turn to the concise integer LP formulation of Martins et al (2009).	[50, 0, 144, 126, 145, 82, 154, 1, 80, 83]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.7272094488143921, 0.7484076023101807, 0.670577883720398, 0.6480871438980103, 0.458097904920578, 0.40702196955680847, 0.4781479239463806, 0.12479553371667862, 0.05450151860713959, 0.16563427448272705]
We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for.	[116, 106, 1, 101, 93, 108, 115, 51, 107, 26]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5124148726463318, 0.36541828513145447, 0.22660930454730988, 0.5385513305664062, 0.11548605561256409, 0.1964002102613449, 0.23684905469417572, 0.15969669818878174, 0.24539975821971893, 0.08184631168842316]
We obtained 155,409 positive instances from the English sentences using an off-the-shelf relation extraction system, ReVerb (Fader et al., 2011).	[148, 8, 147, 146, 27, 151, 57, 67, 9, 36]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6191065907478333, 0.537276029586792, 0.31665652990341187, 0.28378310799598694, 0.08286158740520477, 0.3334217965602875, 0.06607362627983093, 0.048053912818431854, 0.21290577948093414, 0.05539088323712349]
We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set.	[101, 90, 100, 83, 99, 30, 123, 98, 73, 122]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4992515444755554, 0.3805444836616516, 0.2029692381620407, 0.0669039636850357, 0.11949246376752853, 0.14550240337848663, 0.2142539620399475, 0.15748967230319977, 0.27649325132369995, 0.08079525828361511]
We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008).	[12, 64, 67, 29, 44, 58, 0, 2, 145, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1883663833141327, 0.1923649162054062, 0.17993351817131042, 0.3753837049007416, 0.053188566118478775, 0.15834912657737732, 0.43896040320396423, 0.1092962771654129, 0.04447739198803902, 0.06885883212089539]
We parse questions and candidate sentences with MiniPar (Lin, 1994), a fast and robust parser for grammatical dependency relations.	[196, 198, 1, 7, 35, 197, 162, 57, 179, 160]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7178124785423279, 0.6007196307182312, 0.3151171803474426, 0.1240958571434021, 0.07196715474128723, 0.25310060381889343, 0.17407430708408356, 0.07729644328355789, 0.13089218735694885, 0.06082276627421379]
We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994).	[0, 57, 35, 1, 12, 14, 162, 194, 64, 177]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6397379040718079, 0.743383526802063, 0.10489248484373093, 0.06548137217760086, 0.05129136145114899, 0.0631469264626503, 0.1945861428976059, 0.07817620038986206, 0.04494987800717354, 0.05418863520026207]
We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004).	[25, 124, 86, 204, 21, 30, 0, 3, 95, 26]	[1, 0, 1, 0, 0, 0, 1, 0, 0, 0]	[0.7533348202705383, 0.29087984561920166, 0.5494208931922913, 0.3022415339946747, 0.3078451156616211, 0.07899284362792969, 0.5200021266937256, 0.06696029752492905, 0.24360963702201843, 0.091087207198143]
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results.	[27, 29, 23, 3, 7, 10, 12, 28, 32, 18]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6844897270202637, 0.288339763879776, 0.15057474374771118, 0.05450127646327019, 0.05285732075572014, 0.4871376156806946, 0.0850534662604332, 0.05450127646327019, 0.05285732075572014, 0.17912206053733826]
We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection.	[22, 0, 2, 148, 16, 1, 64, 23, 29, 80]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34204721450805664, 0.7092764377593994, 0.22581538558006287, 0.4121645987033844, 0.2536705732345581, 0.3357074558734894, 0.1428786665201187, 0.24531984329223633, 0.08315249532461166, 0.12222108244895935]
We propose a method for inferring event templates based on word clustering according to their proximity in the corpus and syntactic function clustering.	[14, 166, 106, 61, 15, 100, 12, 140, 104, 54]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.801547110080719, 0.5492774248123169, 0.4563353955745697, 0.40587592124938965, 0.27184993028640747, 0.10880493372678757, 0.12262041866779327, 0.1839718371629715, 0.6379229426383972, 0.15256811678409576]
We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data.	[73, 8, 128, 4, 101, 58, 88, 36, 81, 120]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20955191552639008, 0.5155400037765503, 0.07778728753328323, 0.4008057713508606, 0.2983914911746979, 0.21787604689598083, 0.32331064343452454, 0.20826707780361176, 0.11752957105636597, 0.21613693237304688]
We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006).	[11, 7, 43, 51, 39, 52, 48, 10, 70, 37]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7809672951698303, 0.6919334530830383, 0.17013396322727203, 0.10079663246870041, 0.07121557742357254, 0.07115815579891205, 0.13133136928081512, 0.21067115664482117, 0.10963835567235947, 0.07338012009859085]
We report results of comparing our lexicon with theWordNet cousins as well as the inter-annotator disagreement observed between two semantically an notated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996).	[148, 3, 132, 121, 31, 150, 117, 177, 188, 173]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09346412122249603, 0.051074497401714325, 0.32926997542381287, 0.04786451905965805, 0.047460298985242844, 0.04578280448913574, 0.0686299204826355, 0.10877463966608047, 0.07419996708631516, 0.06741148978471756]
We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009).	[144, 115, 150, 145, 182, 136, 80, 129, 120, 137]	[1, 1, 0, 1, 1, 1, 0, 0, 0, 0]	[0.552520751953125, 0.7517088651657104, 0.37716057896614075, 0.6300255656242371, 0.5514090061187744, 0.6319247484207153, 0.42296162247657776, 0.4401598572731018, 0.477321594953537, 0.07497786730527878]
We represented features with a parameter format partly inspired by MaltParser (Nivre et al, 2006a).	[24, 5, 17, 19, 14, 12, 15, 1, 44, 20]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2928635776042938, 0.5754597187042236, 0.04782168194651604, 0.1630329042673111, 0.16483168303966522, 0.18070897459983826, 0.11998345702886581, 0.04530932009220123, 0.07551012933254242, 0.15685716271400452]
We run the Stanford Named Entity Recognizer (Finkel et al, 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs.	[46, 15, 61, 128, 116, 62, 142, 63, 16, 91]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4536469578742981, 0.1092902347445488, 0.07323184609413147, 0.08922790735960007, 0.15137511491775513, 0.08645474165678024, 0.09183549880981445, 0.07292069494724274, 0.05024435743689537, 0.06919948011636734]
We say a schema is a textual schema if it has been extracted from free text, such as the Nell (Carlson et al, 2010) and ReVerb (Fader et al, 2011) extracted databases.	[12, 197, 15, 178, 35, 140, 103, 192, 42, 13]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7281429171562195, 0.6012719869613647, 0.14409855008125305, 0.07139606773853302, 0.36438190937042236, 0.07227754592895508, 0.05424519628286362, 0.39520493149757385, 0.1850801706314087, 0.4420080780982971]
We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009).	[61, 89, 2, 11, 65, 84, 101, 10, 0, 88]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25069037079811096, 0.14753636717796326, 0.1167491152882576, 0.05607481300830841, 0.23080724477767944, 0.11763953417539597, 0.18922440707683563, 0.2053670883178711, 0.2417147010564804, 0.05444964021444321]
We should emphasize that the features induced from the addressee's utterance are unique to this task and are hardly available in the related tasks that predicted the emotion of a reader of news articles (Lin and HsinYihn, 2008) or personal stories (Socher et al, 2011).	[153, 5, 48, 128, 4, 256, 155, 144, 38, 136]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1387365162372589, 0.1850222796201706, 0.1388835459947586, 0.11016134917736053, 0.05401378497481346, 0.052991896867752075, 0.0836043506860733, 0.13404208421707153, 0.38387012481689453, 0.14563988149166107]
We show that this approach gives us better practical performance than a mature system that binarizes using the technique of (Zhang et al, 2006).	[13, 146, 145, 3, 135, 119, 33, 142, 96, 89]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3624710738658905, 0.24798838794231415, 0.17988018691539764, 0.05146444961428642, 0.11811445653438568, 0.045893654227256775, 0.051951248198747635, 0.05648099631071091, 0.252324640750885, 0.0647938996553421]
"We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all ""labeled"" data and test on the ""unlabeled"" data)."	[177, 59, 168, 37, 69, 143, 14, 169, 28, 128]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7303855419158936, 0.32712599635124207, 0.43059638142585754, 0.198318213224411, 0.3313351273536682, 0.33351314067840576, 0.22090314328670502, 0.10728242993354797, 0.2460118979215622, 0.13782928884029388]
We start by discussing the work of Grosz and Sidner (1986), which ties speaker's intentions to linguistic structure.	[280, 216, 3, 15, 103, 321, 58, 564, 108, 98]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6226593852043152, 0.5363503694534302, 0.349084734916687, 0.349084734916687, 0.33032143115997314, 0.22949209809303284, 0.2212504744529724, 0.07116830348968506, 0.21279938519001007, 0.09893542528152466]
We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b).	[80, 125, 13, 22, 24, 113, 58, 3, 16, 30]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6550919413566589, 0.3904132544994354, 0.14215579628944397, 0.14888149499893188, 0.12199970334768295, 0.377464234828949, 0.18948261439800262, 0.06902974843978882, 0.08812987804412842, 0.06388897448778152]
We start with the model introduced by Carreras (2007).	[1, 24, 12, 23, 100, 32, 3, 90, 102, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2337075173854828, 0.12795531749725342, 0.16207477450370789, 0.05902810022234917, 0.277072936296463, 0.05468057468533516, 0.0513482391834259, 0.06017737090587616, 0.0781337171792984, 0.12511195242404938]
We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008).	[194, 541, 469, 150, 437, 595, 238, 105, 258, 543]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7566919326782227, 0.6593878865242004, 0.43384411931037903, 0.3665158152580261, 0.277981698513031, 0.19877836108207703, 0.04863884672522545, 0.09744613617658615, 0.07723083347082138, 0.3785976469516754]
We then describe a novel CCG analysis of NP predicate argument structure, which we implement usingNomBank (Meyers et al, 2004).	[156, 163, 35, 57, 178, 177, 1, 45, 6, 148]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.75418621301651, 0.788720965385437, 0.4490804970264435, 0.1964869648218155, 0.19755366444587708, 0.5300776958465576, 0.278703510761261, 0.04766556993126869, 0.31953030824661255, 0.045619722455739975]
We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006).	[142, 41, 82, 36, 45, 152, 135, 92, 4, 81]	[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.5239623785018921, 0.21899127960205078, 0.2798464298248291, 0.20180128514766693, 0.1271505504846573, 0.41670480370521545, 0.5872488021850586, 0.43335026502609253, 0.06647982448339462, 0.48222672939300537]
We therefore analyze the errors made on the development set to determine whether the difficult remaining cases for parsers correspond to the Hindle and Rooth (1993) style PP attachment classification task.	[125, 133, 39, 183, 71, 147, 169, 76, 42, 35]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6090368032455444, 0.32183152437210083, 0.30956462025642395, 0.2831368148326874, 0.44590190052986145, 0.2025407999753952, 0.4047958552837372, 0.048090942203998566, 0.05581652373075485, 0.10201145708560944]
We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005).	[9, 149, 8, 17, 139, 185, 59, 2, 12, 146]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.8070865869522095, 0.5832502245903015, 0.12453588843345642, 0.11933361738920212, 0.25429433584213257, 0.22064103186130524, 0.113629549741745, 0.06918922066688538, 0.07132700830698013, 0.060723982751369476]
We train a baseline phrase-based French-English system using WMT-09 corpora (Callison-Burchetal., 2009) for training and evaluation.	[26, 14, 47, 135, 49, 5, 58, 1, 57, 24]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5282379388809204, 0.6008145809173584, 0.09606697410345078, 0.18264922499656677, 0.06331028789281845, 0.3229334354400635, 0.11892703920602798, 0.05122924596071243, 0.06631441414356232, 0.06827379763126373]
We trained a variant of our system without gold part-of-speech tags, using the unsupervised word clusters (Clark, 2000) computed by Finkel and Manning (2009).	[59, 22, 73, 20, 38, 41, 6, 40, 64, 60]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3164714574813843, 0.49446892738342285, 0.06149289757013321, 0.22921708226203918, 0.1702662855386734, 0.4338468313217163, 0.22710397839546204, 0.13674326241016388, 0.33509427309036255, 0.3919616639614105]
We translate NN compounds by way of a two-phase procedure, incorporating generation and selection (similarly to Cao and Li (2002) and Langkilde and Knight (1998)).	[218, 13, 47, 17, 113, 4, 16, 49, 145, 38]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5387599468231201, 0.08124381303787231, 0.15874703228473663, 0.049837592989206314, 0.07483769953250885, 0.06433968245983124, 0.32217732071876526, 0.29633861780166626, 0.060678355395793915, 0.0639972984790802]
We use Chinese Whispers (Biemann, 2006), a special case of MCL that performs the iteration in a more aggressive way, with an optimized linear complexity with the number of graph edges.	[1, 77, 63, 76, 0, 23, 170, 54, 122, 2]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.720077633857727, 0.7144650816917419, 0.5586186647415161, 0.6476314067840576, 0.1822110116481781, 0.19161905348300934, 0.14719252288341522, 0.2847360670566559, 0.49098384380340576, 0.059822291135787964]
We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf).	[4, 17, 12, 2, 9, 1, 3, 14, 39, 35]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4597536325454712, 0.6037330627441406, 0.09783048182725906, 0.08913075923919678, 0.08928786963224411, 0.053189467638731, 0.07160641252994537, 0.2309199869632721, 0.0618654303252697, 0.06988151371479034]
We use OpinionFinder (Wilson et al, 2005a) to identify polarized words and their polarities.	[39, 4, 24, 11, 32, 13, 68, 37, 50, 17]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6242530941963196, 0.19667354226112366, 0.41631972789764404, 0.16851180791854858, 0.22525490820407867, 0.08398226648569107, 0.09873458743095398, 0.08063109219074249, 0.09985367953777313, 0.07361823320388794]
We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006).	[122, 117, 125, 166, 3, 90, 23, 164, 157, 21]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.8079622983932495, 0.1976393610239029, 0.2767212688922882, 0.645855188369751, 0.15741780400276184, 0.3841824233531952, 0.19735434651374817, 0.15785862505435944, 0.15173405408859253, 0.05258491262793541]
We use a sentiment-annotated data set consisting of movie reviews by (Pang and Lee, 2005) and tweets from http: //help.sentiment140.com/for-students.	[81, 36, 82, 22, 0, 40, 1, 6, 67, 78]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.18850897252559662, 0.2522394359111786, 0.1314215511083603, 0.1270657181739807, 0.2495957612991333, 0.08805505186319351, 0.08540583401918411, 0.0709657147526741, 0.06513839215040207, 0.0976359099149704]
We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output.	[12, 9, 3, 14, 70, 91, 1, 4, 27, 102]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5057616233825684, 0.26101022958755493, 0.08041554689407349, 0.2633635103702545, 0.09636535495519638, 0.08586576581001282, 0.056079503148794174, 0.05506324768066406, 0.0575687512755394, 0.05397257208824158]
We use a support vector machine (SVM) based chunker yamcha (Kudo and Matsumoto, 2001) for the chunking process.	[189, 0, 21, 1, 22, 192, 15, 151, 121, 19]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.583867609500885, 0.6914133429527283, 0.3565457761287689, 0.33060792088508606, 0.14359226822853088, 0.1958511471748352, 0.16521266102790833, 0.45777609944343567, 0.07466062903404236, 0.03971834108233452]
We use all measures used by Schone and Jurafsky (2001) that can be derived from a phrase's contingency table.	[50, 10, 1, 67, 157, 4, 89, 187, 154, 194]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4786670207977295, 0.13325774669647217, 0.13290005922317505, 0.1955530047416687, 0.23976680636405945, 0.11543893814086914, 0.23807530105113983, 0.18056492507457733, 0.20943963527679443, 0.0760878324508667]
We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C&C maximum entropy NER tagger (Curran and Clark, 2003b).	[0, 10, 4, 86, 5, 83, 35, 61, 9, 67]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7562036514282227, 0.3802836239337921, 0.3556942641735077, 0.24432922899723053, 0.10871628671884537, 0.06209470331668854, 0.18711480498313904, 0.053523752838373184, 0.07556324452161789, 0.07560206204652786]
We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009).	[47, 26, 30, 14, 10, 2, 48, 25, 55, 98]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.37978941202163696, 0.6568564772605896, 0.23741593956947327, 0.31797513365745544, 0.12485232204198837, 0.05848456546664238, 0.06169765070080757, 0.10442864149808884, 0.10814708471298218, 0.17252424359321594]
We use the C&C tools (Curran and Clark, 2003) for POS and NE tagging and the and the Berkeley Parser (Petrov and Klein, 2007), trained with default parameters.	[14, 4, 22, 26, 2, 32, 53, 46, 47, 67]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5929098129272461, 0.189653679728508, 0.08224142342805862, 0.09246715903282166, 0.05272801220417023, 0.3193521201610565, 0.1756564825773239, 0.39898380637168884, 0.16746871173381805, 0.0664433017373085]
We use the CMU Twitter Part-of-Speech Tagger (Owoputi et al, 2013) to select only instances in the verb sense.	[18, 7, 0, 167, 76, 16, 35, 156, 129, 41]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.22398023307323456, 0.0881900042295456, 0.370822936296463, 0.06979691982269287, 0.1735067069530487, 0.5132951140403748, 0.3089466392993927, 0.04798862338066101, 0.05590775981545448, 0.05714971944689751]
We use the IOBES notation (Ratinov and Roth, 2009) to represent NE mentions with label sequences, thereby NER is formalized as a multi class classification problem in which a given token is classified into IOBES labels.	[5, 0, 181, 46, 113, 78, 31, 42, 187, 92]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06982375681400299, 0.24119709432125092, 0.052637916058301926, 0.09253538399934769, 0.06507354974746704, 0.08424847573041916, 0.04726980999112129, 0.045411478728055954, 0.05503201112151146, 0.1507873684167862]
We use the Memory-Based Tagger (Daelemans et al, 1996) trained on the Brown corpus to compute the part-of speech tags.	[0, 47, 78, 79, 5, 1, 80, 4, 3, 183]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7753764390945435, 0.7938439846038818, 0.2846994996070862, 0.2688716650009155, 0.17014649510383606, 0.43486180901527405, 0.13026531040668488, 0.07869218289852142, 0.1958199292421341, 0.07508445531129837]
We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action.	[26, 1, 0, 9, 5, 3, 25, 14, 8, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.25084808468818665, 0.10057155042886734, 0.18782338500022888, 0.4658346474170685, 0.11426210403442383, 0.05530204623937607, 0.09842479974031448, 0.05587700754404068, 0.08038130402565002, 0.06019698083400726]
We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose.	[18, 86, 7, 0, 50, 60, 70, 14, 4, 99]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0932084321975708, 0.2378450632095337, 0.29779210686683655, 0.20207011699676514, 0.0855235606431961, 0.26692232489585876, 0.37049052119255066, 0.19186897575855255, 0.2234523594379425, 0.05316722393035889]
We use the emoticons list provided by (Agarwal et al, 2011) in their research.	[38, 44, 172, 29, 196, 95, 235, 28, 152, 197]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2519299387931824, 0.6405982971191406, 0.1933300942182541, 0.10410324484109879, 0.225780189037323, 0.22546659409999847, 0.06238032877445221, 0.22557637095451355, 0.0688956007361412, 0.4238596558570862]
We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model.	[63, 6, 12, 38, 34, 141, 125, 1, 169, 0]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5453099012374878, 0.6798971891403198, 0.15208159387111664, 0.17582978308200836, 0.18681243062019348, 0.2700469195842743, 0.28667038679122925, 0.09888017177581787, 0.1253332495689392, 0.2255198061466217]
We use the same feature representation as (Taskar et al, 2005), with some small exceptions.	[154, 125, 169, 161, 128, 152, 112, 49, 32, 148]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.33673274517059326, 0.05532844364643097, 0.04983681067824364, 0.362983375787735, 0.38036397099494934, 0.052058566361665726, 0.06420927494764328, 0.20267362892627716, 0.40227362513542175, 0.508015513420105]
We use the same token label constraints as Chang et al (2007).	[17, 49, 24, 27, 67, 134, 180, 57, 193, 29]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4013284742832184, 0.3412608504295349, 0.23551268875598907, 0.15046992897987366, 0.22818176448345184, 0.34095287322998047, 0.12307500094175339, 0.05687861889600754, 0.1264188587665558, 0.0418209545314312]
We use the standard hyperparameters values α = 1.0, β = 0.01 and τ = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009).	[88, 104, 121, 119, 98, 101, 108, 24, 120, 93]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6832942366600037, 0.7078220248222351, 0.2370091825723648, 0.4758971035480499, 0.07715041935443878, 0.062362994998693466, 0.17008395493030548, 0.13676492869853973, 0.1173427477478981, 0.0893607959151268]
We use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms.	[19, 191, 208, 83, 48, 2, 3, 188, 198, 85]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6240432858467102, 0.3458051085472107, 0.12552376091480255, 0.25276124477386475, 0.33644914627075195, 0.08277181535959244, 0.08979955315589905, 0.093632772564888, 0.18102942407131195, 0.3791728615760803]
We used 1000 sentence pairs extracted from pre-aligned data (Utiyama and Isahara, 2003) as a gold standard.	[73, 131, 228, 162, 229, 15, 66, 114, 67, 152]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7514267563819885, 0.5788910388946533, 0.2740105092525482, 0.4558514356613159, 0.26015257835388184, 0.06380955874919891, 0.046743057668209076, 0.05840768665075302, 0.05491587892174721, 0.06788189709186554]
We used Charniak's parser as an additional LM (Charniak, 2001) in reranking.	[70, 1, 57, 9, 40, 4, 18, 10, 123, 5]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.26337411999702454, 0.28567543625831604, 0.23672308027744293, 0.06922445446252823, 0.05045792832970619, 0.11371031403541565, 0.5543309450149536, 0.042681753635406494, 0.05172806605696678, 0.06927668303251266]
We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations.	[17, 9, 33, 66, 3, 1, 37, 65, 0, 41]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.721208930015564, 0.2525401711463928, 0.12966398894786835, 0.36549028754234314, 0.054961852729320526, 0.0789150595664978, 0.05645148083567619, 0.1456984132528305, 0.22570672631263733, 0.045065492391586304]
We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al, 2005) and other techniques for deep parsing.	[6, 0, 371, 165, 338, 353, 102, 5, 2, 335]	[0, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.48268458247184753, 0.6451699733734131, 0.47381433844566345, 0.511795699596405, 0.5079821944236755, 0.23490242660045624, 0.36688023805618286, 0.32935893535614014, 0.18324768543243408, 0.2872225046157837]
We used bootstrapping (Abney, 2002) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier.	[5, 154, 6, 22, 34, 84, 82, 44, 25, 167]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.813062310218811, 0.6210818886756897, 0.24692432582378387, 0.10405569523572922, 0.22512800991535187, 0.12881022691726685, 0.3167059123516083, 0.14734934270381927, 0.09207610040903091, 0.09713633358478546]
We used support vector machines (Vapnik, 1995) with (a) polynomial kernels to learn the semantic role classification and (b) Tree Kernels (Moschitti, 2004) for learning both frame and ILC classification.	[3, 22, 182, 30, 1, 19, 46, 181, 16, 149]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5966272354125977, 0.6491363644599915, 0.3472606837749481, 0.17560309171676636, 0.32900670170783997, 0.23649339377880096, 0.27267080545425415, 0.20423245429992676, 0.1741078794002533, 0.1302916407585144]
We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to parse HFE.	[66, 6, 18, 181, 175, 192, 178, 156, 45, 15]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7439902424812317, 0.22277285158634186, 0.1330115646123886, 0.35012388229370117, 0.05539027601480484, 0.12345054745674133, 0.14271610975265503, 0.10183858126401901, 0.18217860162258148, 0.13397373259067535]
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data.	[103, 166, 8, 127, 5, 6, 152, 133, 124, 161]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.36176106333732605, 0.3271157145500183, 0.2183123379945755, 0.19120769202709198, 0.24400849640369415, 0.14377249777317047, 0.2095104604959488, 0.09180408716201782, 0.5329120755195618, 0.2486485093832016]
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009).	[209, 68, 35, 8, 207, 7, 211, 203, 1, 36]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.494020938873291, 0.4301629662513733, 0.446209192276001, 0.07113634794950485, 0.2992476224899292, 0.5976390242576599, 0.2193414866924286, 0.07511617243289948, 0.05071311444044113, 0.05357855185866356]
We used the exactly same training and testing data from the Switchboard corpus as in [Charniak and Johnson 2001].	[146, 144, 23, 99, 110, 9, 87, 83, 148, 33]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7284542322158813, 0.5694789290428162, 0.4968200623989105, 0.21219387650489807, 0.4729556441307068, 0.3693483769893646, 0.11452647298574448, 0.16880710422992706, 0.10493312031030655, 0.14769361913204193]
We used the morpha lemmatizer (Minnen et al, 2000), which is built into the C&C tools, to match tokens across T and H; and we converted all tokens to lowercase.	[97, 92, 28, 83, 43, 36, 71, 42, 18, 151]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.49462124705314636, 0.05734402686357498, 0.0644148513674736, 0.05421888455748558, 0.04780429229140282, 0.07728366553783417, 0.05440904572606087, 0.2868342697620392, 0.22667296230793, 0.054112471640110016]
We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly.	[197, 0, 5, 146, 18, 4, 188, 16, 12, 8]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.686245858669281, 0.7300744652748108, 0.31543096899986267, 0.45742425322532654, 0.25322818756103516, 0.11754132062196732, 0.5854220986366272, 0.2955981194972992, 0.0646108090877533, 0.08626540750265121]
We used the supertagger (Bangalore and Joshi, 1999) to supertag each word in the corpus.	[213, 253, 188, 222, 167, 249, 112, 255, 257, 215]	[1, 0, 1, 0, 1, 0, 0, 0, 0, 1]	[0.6755863428115845, 0.311898410320282, 0.6233612298965454, 0.2638343870639801, 0.5273167490959167, 0.21164320409297943, 0.17522330582141876, 0.18340282142162323, 0.19943183660507202, 0.5287584066390991]
We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al 2007), WePS2 Testing (Javier et al 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census.	[40, 109, 35, 59, 36, 51, 60, 48, 41, 80]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.24157525599002838, 0.4869212508201599, 0.5475345849990845, 0.255156010389328, 0.12259369343519211, 0.2693776786327362, 0.39331942796707153, 0.06853999942541122, 0.12119773030281067, 0.05522345006465912]
We view our use of part-of-speech patterns as a natural extension to the introduction of structural elements to statistical machine translation by Wang [1998] and Och et al [1999].	[48, 0, 27, 227, 43, 84, 24, 33, 87, 140]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6729184985160828, 0.7406319379806519, 0.15044677257537842, 0.17563197016716003, 0.11412017792463303, 0.09604069590568542, 0.07250899076461792, 0.12747246026992798, 0.07987130433320999, 0.11679836362600327]
We will make use of the Mintz et al (2009) sentence-level features in the expeiments, as described in Section 7.	[136, 102, 113, 99, 119, 80, 81, 32, 131, 128]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4721212089061737, 0.7335101962089539, 0.34171292185783386, 0.17889726161956787, 0.0764823630452156, 0.12140443921089172, 0.294411301612854, 0.07226019352674484, 0.067691370844841, 0.4958125650882721]
We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label.	[131, 147, 148, 200, 5, 44, 52, 23, 43, 172]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35051262378692627, 0.49596327543258667, 0.13245414197444916, 0.3848952054977417, 0.3827487528324127, 0.35874858498573303, 0.05190450698137283, 0.05004248395562172, 0.2509457767009735, 0.07608087360858917]
We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator.	[3, 83, 28, 17, 26, 31, 18, 60, 38, 50]	[1, 1, 0, 1, 0, 0, 1, 1, 0, 0]	[0.7286232709884644, 0.5951800346374512, 0.14473561942577362, 0.6116045713424683, 0.48708221316337585, 0.1456955224275589, 0.5169581770896912, 0.6031845211982727, 0.20029190182685852, 0.17722485959529877]
We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004).	[156, 100, 27, 127, 3, 107, 122, 57, 28, 145]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.48760783672332764, 0.38597068190574646, 0.05751728639006615, 0.054514456540346146, 0.05143880844116211, 0.0673353523015976, 0.3011886179447174, 0.22676394879817963, 0.05516311526298523, 0.21219250559806824]
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces.	[112, 145, 147, 23, 2, 16, 115, 148, 109, 110]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.09492633491754532, 0.12006179243326187, 0.24065335094928741, 0.0686151459813118, 0.09177388995885849, 0.21686342358589172, 0.061678167432546616, 0.0943688452243805, 0.07039520144462585, 0.09313338994979858]
When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset.	[30, 29, 25, 1, 15, 26, 19, 18, 31, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12624801695346832, 0.19653727114200592, 0.06921679526567459, 0.05165804177522659, 0.06315196305513382, 0.08956294506788254, 0.07361670583486557, 0.0999576672911644, 0.05901884287595749, 0.08295271545648575]
When compared with the latest results from [Johnson and Charniak 2004], where no punctuations are used for either training or testing data, we also observe the same trend of the improved results.	[149, 151, 144, 150, 71, 35, 72, 67, 153, 104]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7922304272651672, 0.6689898371696472, 0.49810415506362915, 0.2553333640098572, 0.374478816986084, 0.04780746251344681, 0.45585939288139343, 0.2697071433067322, 0.10531333088874817, 0.06849624216556549]
When parsing with Jacy failed, comparisons could still be made with RMRS produced from shallow tools such as ChaSen (Matsumo to et al, 2000), a morphological analyser or CaboCha (Kudo and Matsumoto, 2002), a Japanese dependency parser.	[1, 12, 36, 145, 15, 37, 39, 2, 33, 35]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22495564818382263, 0.1402321606874466, 0.1512686014175415, 0.17474430799484253, 0.17050199210643768, 0.16513638198375702, 0.12637124955654144, 0.1493614763021469, 0.09078174829483032, 0.18860790133476257]
Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005).	[98, 69, 68, 41, 38, 49, 46, 75, 73, 90]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.31055814027786255, 0.5670763850212097, 0.21578159928321838, 0.0985664501786232, 0.07153120636940002, 0.24081633985042572, 0.05973021686077118, 0.06977366656064987, 0.24337171018123627, 0.12533579766750336]
Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006).	[190, 200, 195, 64, 1, 108, 67, 90, 189, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21089859306812286, 0.17331142723560333, 0.19354039430618286, 0.14983314275741577, 0.057820580899715424, 0.18451224267482758, 0.058169230818748474, 0.1349671185016632, 0.06257293373346329, 0.07146428525447845]
While (a) is not true in our setting because Xi is a hyper graph (which is ambiguous), Li et al (2009b) show how to approximate a hyper graph representation of p (x |yi) by an unambiguous WFSA.	[98, 39, 27, 35, 30, 96, 97, 43, 92, 13]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7563645243644714, 0.05648963525891304, 0.05674705654382706, 0.0750664696097374, 0.07197722047567368, 0.06797925382852554, 0.08569730073213577, 0.07783395797014236, 0.06104778125882149, 0.10610082745552063]
While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03).	[111, 126, 118, 8, 7, 39, 29, 72, 5, 71]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.22901998460292816, 0.19605301320552826, 0.14051543176174164, 0.10786624997854233, 0.09353692829608917, 0.05844247713685036, 0.056395500898361206, 0.18121454119682312, 0.07097821682691574, 0.05953259766101837]
While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.	[124, 126, 125, 111, 106, 107, 140, 113, 40, 102]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5546399354934692, 0.7745421528816223, 0.6213756203651428, 0.2695252299308777, 0.20060640573501587, 0.16700471937656403, 0.13956791162490845, 0.12384732067584991, 0.13152004778385162, 0.1918409764766693]
While template-based representations have been proposed for information merging in the past (Radev and McKeown, 1998), they considered only domain-specific scenarios.	[448, 106, 17, 324, 350, 217, 302, 281, 26, 163]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6391478180885315, 0.30192798376083374, 0.057624876499176025, 0.16514912247657776, 0.23921650648117065, 0.5145022869110107, 0.07217220962047577, 0.10303691774606705, 0.21234212815761566, 0.17014162242412567]
While the number of highly non-projective dependency structures is negligible for practical applications (Kuhlmann and Nivre, 2006), the rank can not easily be bounded.	[12, 0, 8, 7, 36, 138, 17, 39, 120, 71]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5837774276733398, 0.8003802299499512, 0.3206336498260498, 0.46039581298828125, 0.4062098264694214, 0.4853980243206024, 0.19924719631671906, 0.4881472885608673, 0.41749823093414307, 0.5073818564414978]
While there are related tags for dialogue act tagging schema? like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al, 2004) which includes a committag these classes are too general to allow identification of action items specifically.	[31, 66, 35, 59, 0, 12, 29, 36, 50, 64]	[1, 1, 1, 0, 0, 0, 0, 1, 0, 0]	[0.5566709041595459, 0.5617934465408325, 0.5808205604553223, 0.05597890540957451, 0.4471004605293274, 0.25617271661758423, 0.24870114028453827, 0.6652839183807373, 0.055513348430395126, 0.478837788105011]
Wiebe et al (2004) focused on the detection of subjective language such as opinions, evaluations, or emotions in text.	[65, 9, 1, 17, 64, 8, 16, 2, 496, 19]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6868554353713989, 0.570227861404419, 0.570227861404419, 0.6479011178016663, 0.3614475131034851, 0.1305227130651474, 0.1305227130651474, 0.09712693840265274, 0.4849082827568054, 0.06688156723976135]
Wilson et al (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features.	[36, 163, 129, 111, 144, 110, 0, 34, 1, 109]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.8152852058410645, 0.5930131673812866, 0.2086474746465683, 0.19618724286556244, 0.36454540491104126, 0.48326775431632996, 0.5662662386894226, 0.06619039922952652, 0.0477316789329052, 0.23178456723690033]
With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement.	[0, 108, 1, 85, 97, 83, 125, 156, 154, 107]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7229436635971069, 0.48197582364082336, 0.717768132686615, 0.1424042135477066, 0.10166116803884506, 0.1998244673013687, 0.10793410986661911, 0.06449717283248901, 0.05741851031780243, 0.11897139251232147]
With respect to the use of Wikipedia as a resource for natural language processing tasks, the work that is most closely related to ours is perhaps the name entity disambiguation algorithm proposed in (Bunescu and Pasca, 2006), where an SVM kernel is trained on the entries found in Wikipediafor ambiguous named entities.	[2, 32, 31, 0, 87, 42, 29, 86, 100, 19]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.3245057761669159, 0.40045827627182007, 0.27425670623779297, 0.5119832754135132, 0.3512071967124939, 0.2778398096561432, 0.35065269470214844, 0.09798195213079453, 0.2567738890647888, 0.14685045182704926]
Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule.	[1, 0, 10, 9, 102, 83, 27, 12, 5, 108]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7299667000770569, 0.7635907530784607, 0.12426118552684784, 0.06477090716362, 0.062158361077308655, 0.0522158108651638, 0.06731373816728592, 0.06358340382575989, 0.17580009996891022, 0.0845634862780571]
WordNet has been used by many researchers for different purposes ranging from the construction or extension of knowledge bases such as SENSUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al, 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000).	[54, 57, 16, 56, 125, 155, 2, 68, 4, 72]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36946800351142883, 0.08690503984689713, 0.09833231568336487, 0.04599037766456604, 0.06885518878698349, 0.09745721518993378, 0.102583147585392, 0.05834829434752464, 0.0517675019800663, 0.05209478735923767]
Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices.	[0, 33, 1, 127, 174, 31, 150, 125, 100, 73]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 1]	[0.7750079035758972, 0.5477249026298523, 0.5118948817253113, 0.6015034317970276, 0.4176957905292511, 0.1299619972705841, 0.2395709604024887, 0.14853909611701965, 0.13423055410385132, 0.5196477770805359]
Wu (Wu, 1996) experimented with Chinese-English translation, while this paper experiments with English-Chinese translation.	[40, 81, 154, 85, 41, 152, 153, 45, 132, 27]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6373423337936401, 0.4504292905330658, 0.2404896467924118, 0.32741662859916687, 0.08414478600025177, 0.4582594633102417, 0.4905482232570648, 0.06048649549484253, 0.07061485946178436, 0.06261487305164337]
Wu and Fung (2009) demonstrated the promise of using features based on semantic predicate argument structure in machine translation, using these feature to re-rank machine translation output.	[3, 38, 1, 50, 6, 27, 72, 20, 34, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.427859365940094, 0.427859365940094, 0.24905376136302948, 0.1892625242471695, 0.17886440455913544, 0.07907049357891083, 0.20832131803035736, 0.08225252479314804, 0.05616389587521553, 0.08808567374944687]
Yarowsky (1992) used Roget's Thesaurus categories as classes for word senses.	[9, 0, 1, 7, 58, 206, 155, 2, 114, 164]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5140447020530701, 0.3794873058795929, 0.6791539192199707, 0.08787762373685837, 0.3540668487548828, 0.14249083399772644, 0.1442091166973114, 0.11573541164398193, 0.2936597466468811, 0.09713254123926163]
Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system.	[47, 0, 49, 40, 187, 11, 126, 6, 39, 110]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6873776316642761, 0.77764493227005, 0.1795089840888977, 0.2355632781982422, 0.2809382677078247, 0.0506228506565094, 0.053528428077697754, 0.11852331459522247, 0.1591114103794098, 0.1804271638393402]
Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles.	[9, 1, 11, 0, 17, 62, 63, 8, 21, 43]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6824653148651123, 0.6582210063934326, 0.3286550045013428, 0.4821339547634125, 0.3897131383419037, 0.1964011788368225, 0.3561899960041046, 0.3974015712738037, 0.11932090669870377, 0.15526069700717926]
Yet, brevity plays a part in all GRE algorithms, sometimes in a strict form (Dale, 1989), or by letting the algorithm approximate the shortest description (for example, in the Dale and Reiter's IA).	[68, 97, 79, 77, 38, 10, 56, 8, 30, 14]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2459697127342224, 0.09816979616880417, 0.23825591802597046, 0.13662737607955933, 0.050942789763212204, 0.07073110342025757, 0.07131604850292206, 0.04729287698864937, 0.04717857390642166, 0.1738518476486206]
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems.	[136, 6, 11, 143, 146, 45, 40, 58, 21, 176]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29689639806747437, 0.08324065059423447, 0.18407367169857025, 0.4288990795612335, 0.07718002796173096, 0.06800664216279984, 0.05912461504340172, 0.054910507053136826, 0.06872349232435226, 0.061872806400060654]
Zhao and Grishman (2005) also evaluated their algorithm on the ACE corpus and got good performance.	[178, 160, 76, 165, 181, 7, 17, 30, 141, 6]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.786048412322998, 0.6767265200614929, 0.34825757145881653, 0.32208263874053955, 0.32599204778671265, 0.31940361857414246, 0.05601304769515991, 0.0540105476975441, 0.10491767525672913, 0.05238230898976326]
Zhu et al (2010) is the least grammatical model. Finally, RevILP preserves the meaning of the target as well as SimpleEW, whereas Zhu et al yields the most distortions.	[19, 319, 1, 21, 29, 100, 98, 35, 33, 174]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07439753413200378, 0.07631294429302216, 0.08620158582925797, 0.05391959473490715, 0.09630904346704483, 0.23127999901771545, 0.11668079346418381, 0.05931367725133896, 0.07746683061122894, 0.19055533409118652]
Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process.	[6, 3, 14, 10, 12, 21, 13, 2, 19, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.26901859045028687, 0.11574136465787888, 0.07896170020103455, 0.04842742905020714, 0.0554087795317173, 0.11413611471652985, 0.1300632655620575, 0.06884889304637909, 0.056968625634908676, 0.13167229294776917]
able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004).	[224, 18, 1, 46, 9, 173, 180, 112, 163, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.586863100528717, 0.07266865670681, 0.35329538583755493, 0.1428608000278473, 0.0976751521229744, 0.10607912391424179, 0.1314443200826645, 0.2031700164079666, 0.08933015912771225, 0.3877526521682739]
based on tree-structures of various complexity in the tree-adjoining grammar model. Using such tags, Brants (2000) has achieved the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a tree bank of German.	[2, 75, 137, 32, 11, 119, 116, 164, 27, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11327105760574341, 0.425712525844574, 0.20218323171138763, 0.35641005635261536, 0.08599966764450073, 0.05107833445072174, 0.09102953225374222, 0.06931637227535248, 0.10214917361736298, 0.3962375819683075]
greatly affects parsing accuracy (Levy and Manning, 2003).	[45, 26, 22, 33, 11, 63, 30, 36, 0, 40]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30272620916366577, 0.3305431604385376, 0.21859729290008545, 0.4098050594329834, 0.13707740604877472, 0.35219770669937134, 0.23329631984233856, 0.21702948212623596, 0.27231937646865845, 0.1356709599494934]
in particular (Knight, 1999) has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task.	[140, 84, 1, 7, 114, 148, 80, 111, 143, 137]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6043370366096497, 0.5369832515716553, 0.05405484139919281, 0.05405484139919281, 0.13455787301063538, 0.05446692928671837, 0.2365616261959076, 0.07008678466081619, 0.06385931372642517, 0.09627055376768112]
is the translation probability score (as the one given for instance by GIZA++ (Gao and Vogel, 2008)).	[38, 85, 47, 56, 117, 58, 5, 50, 39, 8]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.34574365615844727, 0.4551807641983032, 0.0659574493765831, 0.28159478306770325, 0.05115648731589317, 0.17655381560325623, 0.06861497461795807, 0.1650240272283554, 0.07308711111545563, 0.0638904720544815]
it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003).	[28, 35, 3, 126, 110, 61, 100, 26, 20, 12]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7404710054397583, 0.3914234936237335, 0.14143197238445282, 0.09922539442777634, 0.05184660479426384, 0.1655668467283249, 0.06829371303319931, 0.22760871052742004, 0.08657461404800415, 0.20869536697864532]
ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al (2008a).	[129, 139, 128, 52, 88, 60, 143, 82, 56, 89]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.7775532007217407, 0.7472202181816101, 0.6761513352394104, 0.7508201003074646, 0.6800158023834229, 0.4065213203430176, 0.3123113214969635, 0.1373801827430725, 0.12050018459558487, 0.4304136037826538]
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures.	[1, 28, 55, 2, 19, 20, 144, 31, 46, 35]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6963578462600708, 0.6736121773719788, 0.4591991603374481, 0.256652295589447, 0.5594379305839539, 0.073357492685318, 0.3590390980243683, 0.35273611545562744, 0.05450161546468735, 0.16230960190296173]
our own implementation of a feature concerning the presence of certain cue words (Hirschberg and Litman, 1993).	[151, 23, 372, 412, 304, 9, 117, 67, 66, 346]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.44537368416786194, 0.22104018926620483, 0.40063443779945374, 0.05569714680314064, 0.045450352132320404, 0.06823752075433731, 0.05091635510325432, 0.3425612449645996, 0.3040412664413452, 0.11915769428014755]
paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)).	[124, 8, 37, 23, 17, 119, 133, 18, 108, 10]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.43262869119644165, 0.652660608291626, 0.28765684366226196, 0.43698158860206604, 0.44644057750701904, 0.2465607225894928, 0.2679016888141632, 0.20447960495948792, 0.3907056450843811, 0.23277868330478668]
phoneme based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration.	[38, 16, 6, 12, 115, 5, 24, 86, 11, 113]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4579072594642639, 0.7091182470321655, 0.24745669960975647, 0.24745669960975647, 0.4103400409221649, 0.15900646150112152, 0.05430389568209648, 0.046118658035993576, 0.15900646150112152, 0.12715944647789001]
see Klein and Manning (2003c) for details.	[141, 95, 77, 18, 19, 35, 151, 119, 70, 4]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.779457151889801, 0.14838862419128418, 0.09196902066469193, 0.10054084658622742, 0.06422317028045654, 0.06583591550588608, 0.09531987458467484, 0.07176022976636887, 0.508517324924469, 0.06475681066513062]
such as Earley deduction, to coestruct a parser, a.s shown in Pereira and Warren (1983).	[4, 19, 21, 88, 121, 20, 52, 33, 39, 99]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5115767121315002, 0.20404785871505737, 0.12000715732574463, 0.3527727723121643, 0.30192452669143677, 0.1243944764137268, 0.05388285592198372, 0.053466737270355225, 0.4967811703681946, 0.11702362447977066]
task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).	[4, 0, 177, 50, 184, 108, 210, 110, 1, 126]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06088564544916153, 0.09061191976070404, 0.06992923468351364, 0.437148779630661, 0.08375941216945648, 0.12003567814826965, 0.14853674173355103, 0.10466655343770981, 0.23153524100780487, 0.1440018117427826]
the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations.	[30, 28, 21, 192, 171, 177, 202, 25, 117, 2]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.14078597724437714, 0.1715669184923172, 0.18961957097053528, 0.19331365823745728, 0.14141708612442017, 0.05095807462930679, 0.0595141239464283, 0.09865836054086685, 0.06154344603419304, 0.25658419728279114]
the reranker learns directly from a scoring function that is trained to maximize the performance of the reranking task (Collins and Duffy, 2002).	[22, 15, 3, 20, 18, 13, 1, 4, 5, 9]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.13844063878059387, 0.6982191801071167, 0.0574893020093441, 0.05399518460035324, 0.04662565886974335, 0.05031150206923485, 0.0481548011302948, 0.06065551936626434, 0.04850582778453827, 0.06473342329263687]
translating a source language training corpus into target language and creating a corpus based system in target language (Banea et al, 2008).	[35, 42, 28, 29, 101, 57, 61, 10, 40, 43]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.636103630065918, 0.6563824415206909, 0.4096178114414215, 0.1970597803592682, 0.19229306280612946, 0.44564247131347656, 0.6008925437927246, 0.23835958540439606, 0.17163223028182983, 0.2158464938402176]
work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags.	[494, 45, 117, 92, 501, 146, 267, 116, 496, 244]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1704622507095337, 0.23411504924297333, 0.08000024408102036, 0.08748051524162292, 0.10295971482992172, 0.09509529173374176, 0.10119355469942093, 0.39827021956443787, 0.04616764187812805, 0.08368687331676483]
