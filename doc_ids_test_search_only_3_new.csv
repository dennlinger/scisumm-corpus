query	doc_id	label	similiarity
(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here	[85, 162, 57, 230, 209, 15, 81, 8, 5, 183]	[1, 1, 0, 1, 1, 0, 0, 1, 0, 0]	[0.4333542585372925, 0.4218705892562866, 0.05339857190847397, 0.4309704899787903, 0.428667813539505, 0.043599776923656464, 0.047161705791950226, 0.7182043194770813, 0.3226563036441803, 0.06328532099723816]
(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify	[10, 237, 236, 77, 213, 199, 11, 20, 25, 85]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.4154399335384369, 0.5570163130760193, 0.5747160315513611, 0.7169573903083801, 0.387407511472702, 0.4479413628578186, 0.3534424304962158, 0.20227862894535065, 0.05829862132668495, 0.0911894366145134]
(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)	[174, 137, 33, 247, 140, 35, 139, 217, 186, 204]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7741333246231079, 0.3900979161262512, 0.2888347804546356, 0.5502508878707886, 0.36872607469558716, 0.2043369859457016, 0.12353590875864029, 0.2263653725385666, 0.06952737271785736, 0.16437672078609467]
(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages	[3, 158, 16, 71, 124, 70, 72, 1, 21, 0]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5602201819419861, 0.48262858390808105, 0.14259864389896393, 0.2573360204696655, 0.330409973859787, 0.20817752182483673, 0.2317523956298828, 0.059227339923381805, 0.07014041393995285, 0.44839051365852356]
(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents	[27, 16, 21, 145, 139, 118, 35, 64, 54, 24]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.31669244170188904, 0.689876914024353, 0.1896289438009262, 0.45534807443618774, 0.22494551539421082, 0.23598712682724, 0.3604357838630676, 0.10749121755361557, 0.08025213330984116, 0.1134086325764656]
(Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)	[120, 4, 81, 32, 19, 40, 127, 41, 143, 13]	[1, 0, 0, 1, 0, 0, 1, 0, 1, 0]	[0.5269309282302856, 0.3149462938308716, 0.25113406777381897, 0.5172967314720154, 0.08011096715927124, 0.16729283332824707, 0.5961996912956238, 0.34331411123275757, 0.40428483486175537, 0.2511715590953827]
(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined	[70, 98, 78, 35, 15, 1, 77, 76, 100, 141]	[0, 0, 1, 1, 1, 0, 0, 0, 0, 0]	[0.3756406009197235, 0.2996532618999481, 0.5884413123130798, 0.5548194050788879, 0.4982800781726837, 0.14071351289749146, 0.2723590135574341, 0.1043214276432991, 0.1977108269929886, 0.11495925486087799]
(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers	[93, 14, 25, 19, 87, 12, 32, 60, 13, 1]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.2829724848270416, 0.7042714953422546, 0.45191898941993713, 0.28307947516441345, 0.05507037788629532, 0.10801390558481216, 0.0514703206717968, 0.05182546377182007, 0.2872295081615448, 0.19255070388317108]
(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable	[61, 23, 5, 33, 32, 58, 19, 104, 95, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30994805693626404, 0.2594181299209595, 0.17923080921173096, 0.19742293655872345, 0.05933726578950882, 0.12934164702892303, 0.06868921965360641, 0.053825732320547104, 0.059671688824892044, 0.13718947768211365]
(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations	[138, 137, 128, 109, 21, 134, 27, 63, 39, 112]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7857707738876343, 0.6613361835479736, 0.29362043738365173, 0.28258857131004333, 0.4189003109931946, 0.10767248272895813, 0.09465831518173218, 0.17531755566596985, 0.16982729732990265, 0.12212332338094711]
A LCFRS (Vijay-Shanker et al, 1987) is a tuple G= (N, T, V, P, S ) where N is a finite set of non-terminals with a function dim: N? N that determines the fan-out of each A? N; b) T and V are disjoint finite sets of terminals and variables; c) S? N is the start symbol with dim (S)= 1; d) P is a finite set of rewriting rules A (? 1,..	[175, 183, 130, 46, 189, 90, 30, 162, 148, 71]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.08073873072862625, 0.07751578092575073, 0.0936732068657875, 0.08181770890951157, 0.10106702148914337, 0.5083187222480774, 0.06923412531614304, 0.10972876846790314, 0.05726543441414833, 0.08320005238056183]
A comparison of unlexicalised PCFG parsing (Kubler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the Tu? Ba D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991)	[119, 149, 160, 143, 38, 12, 151, 146, 79, 133]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2755569815635681, 0.08799298852682114, 0.34830087423324585, 0.052416350692510605, 0.05718587711453438, 0.04992778226733208, 0.055555425584316254, 0.12654142081737518, 0.08503354340791702, 0.11455142498016357]
A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic	[39, 38, 3, 192, 105, 78, 40, 37, 77, 98]	[0, 0, 1, 1, 0, 0, 0, 0, 1, 0]	[0.3841056525707245, 0.3073938190937042, 0.5186770558357239, 0.5395421981811523, 0.26017865538597107, 0.3173341751098633, 0.1987076848745346, 0.32660624384880066, 0.40830284357070923, 0.3467260003089905]
A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning	[135, 297, 267, 109, 110, 300, 25, 282, 31, 312]	[1, 1, 1, 1, 0, 0, 0, 1, 0, 0]	[0.4815480709075928, 0.5700057148933411, 0.4727526307106018, 0.47537466883659363, 0.2510887682437897, 0.3483549654483795, 0.39827945828437805, 0.4780098497867584, 0.2109704315662384, 0.08888570219278336]
A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003)	[121, 140, 134, 133, 108, 131, 135, 142, 91, 128]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.5532429218292236, 0.5871637463569641, 0.32858437299728394, 0.16883128881454468, 0.06531860679388046, 0.09650999307632446, 0.20843186974525452, 0.4147169888019562, 0.06294430792331696, 0.07025805115699768]
A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)	[38, 64, 170, 0, 90, 26, 12, 33, 8, 62]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08400512486696243, 0.11780331283807755, 0.1273570954799652, 0.3697209358215332, 0.08500857651233673, 0.14949028193950653, 0.13085541129112244, 0.07171793282032013, 0.16746629774570465, 0.09581021964550018]
A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew	[0, 19, 3, 186, 188, 18, 13, 141, 97, 53]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 1]	[0.8419485688209534, 0.5852364301681519, 0.6302567720413208, 0.5434953570365906, 0.15693771839141846, 0.33942654728889465, 0.13550248742103577, 0.11103545874357224, 0.12382978200912476, 0.45094308257102966]
A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)	[120, 23, 27, 38, 40, 41, 46, 21, 39, 19]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3893318474292755, 0.6986731290817261, 0.24955204129219055, 0.2812111973762512, 0.26170769333839417, 0.2283688187599182, 0.06474752724170685, 0.060828935354948044, 0.13147662580013275, 0.14224876463413239]
According to Skut et al (1997) tree banks have to meet the following requirements: 1	[15, 53, 2, 74, 7, 8, 104, 51, 49, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4153471291065216, 0.2746221721172333, 0.12888771295547485, 0.08678679168224335, 0.30065247416496277, 0.21243669092655182, 0.2234131097793579, 0.09214486926794052, 0.0617007352411747, 0.0862300843000412]
After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article	[92, 176, 40, 149, 5, 91, 1, 58, 24, 123]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.29602375626564026, 0.08579456806182861, 0.09622415155172348, 0.12667222321033478, 0.12289263308048248, 0.5379520654678345, 0.12128157168626785, 0.055270418524742126, 0.05582952871918678, 0.17281626164913177]
Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4)	[26, 99, 104, 95, 94, 13, 79, 62, 23, 20]	[1, 0, 1, 0, 0, 1, 1, 1, 0, 0]	[0.6222595572471619, 0.22875288128852844, 0.5801589488983154, 0.24694779515266418, 0.3460019826889038, 0.5112548470497131, 0.7119581699371338, 0.7250078320503235, 0.18491092324256897, 0.3615333139896393]
Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)	[52, 10, 53, 9, 32, 144, 83, 29, 68, 20]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2655178904533386, 0.22006337344646454, 0.06856789439916611, 0.07112708687782288, 0.058006733655929565, 0.15181776881217957, 0.05698932334780693, 0.08135449886322021, 0.128610759973526, 0.04554233327507973]
And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors	[51, 21, 25, 95, 2, 182, 34, 190, 57, 75]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1388721466064453, 0.42254889011383057, 0.23437564074993134, 0.09210239350795746, 0.24801023304462433, 0.1878107190132141, 0.1358872801065445, 0.2824656665325165, 0.16617794334888458, 0.09333597868680954]
Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference	[22, 47, 30, 78, 17, 148, 31, 4, 97, 154]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12427666038274765, 0.16397026181221008, 0.3356742262840271, 0.07882394641637802, 0.06153152137994766, 0.06403584778308868, 0.07485973834991455, 0.04431774094700813, 0.16462941467761993, 0.3533448874950409]
Another popular task in SMT is domain adaptation (Foster et al, 2010)	[0, 7, 32, 1, 10, 14, 36, 34, 4, 24]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.24082955718040466, 0.05741210654377937, 0.1072019562125206, 0.26900357007980347, 0.47079354524612427, 0.1494523137807846, 0.06184801831841469, 0.06629086285829544, 0.18062414228916168, 0.14668603241443634]
Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)	[0, 1, 6, 158, 24, 18, 33, 160, 112, 39]	[1, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.744661271572113, 0.13990478217601776, 0.6246559619903564, 0.2704528570175171, 0.16070741415023804, 0.5339246988296509, 0.19645436108112335, 0.0917719379067421, 0.0731789842247963, 0.09078489243984222]
As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now	[57, 71, 20, 139, 138, 0, 6, 25, 200, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41986557841300964, 0.10681161284446716, 0.13261528313159943, 0.09881260246038437, 0.13846617937088013, 0.33521023392677307, 0.0585196278989315, 0.06010216474533081, 0.07792052626609802, 0.06729147583246231]
As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)	[91, 30, 19, 128, 118, 17, 120, 89, 88, 90]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5056650638580322, 0.4123357832431793, 0.5444206595420837, 0.04800017178058624, 0.05411496385931969, 0.046899132430553436, 0.047435060143470764, 0.291369765996933, 0.19464458525180817, 0.1500619649887085]
As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle	[16, 58, 14, 73, 34, 62, 50, 36, 116, 7]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.14561055600643158, 0.06988568603992462, 0.05709663778543472, 0.06702809780836105, 0.4745570421218872, 0.08530902117490768, 0.14151687920093536, 0.2393639087677002, 0.15450751781463623, 0.06320977956056595]
As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically	[25, 155, 1, 5, 45, 28, 23, 10, 98, 38]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.0638517513871193, 0.10062424093484879, 0.07355447113513947, 0.06596649438142776, 0.05996138975024223, 0.05929658189415932, 0.07754059135913849, 0.06055481359362602, 0.05529112368822098, 0.050390325486660004]
As data we use version 2 of the Negra (Skut et al1997) tree bank, with the common training	[140, 99, 6, 145, 149, 62, 171, 143, 38, 148]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4071466326713562, 0.34182900190353394, 0.05205116420984268, 0.1351509988307953, 0.048423632979393005, 0.30796271562576294, 0.08781759440898895, 0.15572836995124817, 0.04508320987224579, 0.12473934143781662]
As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem	[41, 106, 40, 20, 12, 76, 3, 43, 2, 33]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.7362526655197144, 0.10346020758152008, 0.096631720662117, 0.5330600738525391, 0.0496734119951725, 0.2361772060394287, 0.06474924832582474, 0.06185819208621979, 0.3312966525554657, 0.10866877436637878]
As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively	[28, 25, 35, 37, 71, 126, 72, 80, 87, 64]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06416166573762894, 0.6139301061630249, 0.2775987386703491, 0.31481555104255676, 0.06511550396680832, 0.09900988638401031, 0.06485294550657272, 0.09064765274524689, 0.10445111989974976, 0.057584572583436966]
As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)	[168, 175, 133, 74, 29, 163, 30, 116, 75, 109]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.5082724690437317, 0.6805444955825806, 0.6861085295677185, 0.4781114161014557, 0.7366633415222168, 0.05198865011334419, 0.17751719057559967, 0.04678679630160332, 0.1102033406496048, 0.26936790347099304]
As in (Foster et al, 2010), this approach works at the level of phrase pairs	[23, 26, 1, 144, 152, 151, 25, 143, 28, 140]	[1, 1, 0, 0, 1, 0, 0, 1, 0, 0]	[0.6793642640113831, 0.5137099623680115, 0.31264814734458923, 0.39548593759536743, 0.5984598994255066, 0.12684251368045807, 0.05566324666142464, 0.522629976272583, 0.1028866097331047, 0.10434524714946747]
As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition	[51, 190, 36, 161, 2, 182, 157, 26, 82, 175]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24588720500469208, 0.5395351052284241, 0.29309818148612976, 0.29701319336891174, 0.07168801873922348, 0.09760121256113052, 0.06249046325683594, 0.05784537270665169, 0.07474302500486374, 0.056905895471572876]
At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures	[21, 310, 25, 72, 269, 22, 128, 31, 33, 126]	[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]	[0.4593178331851959, 0.6612553000450134, 0.49584537744522095, 0.7227419018745422, 0.4924687445163727, 0.3064397871494293, 0.1511869579553604, 0.17717355489730835, 0.13319896161556244, 0.28274282813072205]
At the end one has a beam-width's number of best parses (Roark, 2001)	[289, 124, 239, 255, 3, 9, 297, 361, 133, 304]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 1]	[0.291659414768219, 0.2319267839193344, 0.42766547203063965, 0.22858725488185883, 0.37472110986709595, 0.37472110986709595, 0.19462692737579346, 0.04830141365528107, 0.14728862047195435, 0.6341613531112671]
Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque	[20, 0, 2, 110, 104, 3, 95, 91, 96, 24]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.34221965074539185, 0.7567754983901978, 0.39858323335647583, 0.16562522947788239, 0.4696793258190155, 0.08853591978549957, 0.11231734603643417, 0.16047511994838715, 0.0852796882390976, 0.17722365260124207]
Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)	[108, 89, 18, 102, 92, 47, 87, 91, 37, 60]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.33945703506469727, 0.1402287483215332, 0.054211799055337906, 0.6045218110084534, 0.053177982568740845, 0.06505580246448517, 0.053207479417324066, 0.0785997211933136, 0.049647893756628036, 0.049164481461048126]
Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees	[18, 1, 5, 38, 12, 158, 22, 17, 34, 81]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20267152786254883, 0.11094854027032852, 0.09875888377428055, 0.05648885294795036, 0.07784150540828705, 0.11248266696929932, 0.06048575043678284, 0.04739246889948845, 0.12355984002351761, 0.06347392499446869]
But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)	[86, 0, 82, 1, 106, 115, 28, 11, 16, 32]	[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]	[0.7717126607894897, 0.780937135219574, 0.7583137154579163, 0.44173744320869446, 0.5909562706947327, 0.6208252310752869, 0.25325074791908264, 0.1869339495897293, 0.056223563849925995, 0.34952229261398315]
By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987)	[50, 19, 34, 204, 92, 106, 125, 2, 83, 128]	[1, 0, 1, 1, 0, 1, 0, 0, 0, 0]	[0.606630265712738, 0.2688181400299072, 0.43252572417259216, 0.6286175847053528, 0.303972989320755, 0.47615817189216614, 0.30941927433013916, 0.19271165132522583, 0.1432504951953888, 0.16890597343444824]
CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al (1991)	[92, 118, 207, 134, 2, 125, 116, 34, 50, 227]	[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6644840240478516, 0.3760140836238861, 0.48626917600631714, 0.6906941533088684, 0.1684257537126541, 0.27238836884498596, 0.2531796097755432, 0.280353307723999, 0.2620682716369629, 0.2784903943538666]
CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al, 1997)	[151, 137, 168, 141, 51, 44, 72, 143, 12, 145]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.16905276477336884, 0.09971623122692108, 0.23599857091903687, 0.42319270968437195, 0.05846942216157913, 0.055708859115839005, 0.10629202425479889, 0.052215058356523514, 0.05489528179168701, 0.04486849904060364]
Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality	[39, 140, 62, 78, 38, 71, 90, 145, 44, 82]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.799901008605957, 0.765083372592926, 0.2019082009792328, 0.10363772511482239, 0.1787334531545639, 0.12214746326208115, 0.11624585837125778, 0.051575493067502975, 0.13533414900302887, 0.06793572753667831]
Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning	[132, 2, 9, 156, 8, 0, 6, 7, 1, 13]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.372877299785614, 0.07294904440641403, 0.33937689661979675, 0.36027947068214417, 0.07445353269577026, 0.5704273581504822, 0.09263730049133301, 0.14433324337005615, 0.06154150888323784, 0.07523352652788162]
Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available	[1, 9, 7, 132, 172, 106, 14, 142, 117, 146]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.10963479429483414, 0.3616049885749817, 0.15665870904922485, 0.44858598709106445, 0.06968453526496887, 0.09236569702625275, 0.06824813783168793, 0.07315745949745178, 0.04463681951165199, 0.07278749346733093]
Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers	[2, 132, 9, 1, 13, 112, 148, 6, 146, 173]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.5343943238258362, 0.6348833441734314, 0.41771018505096436, 0.08100665360689163, 0.17701473832130432, 0.15074527263641357, 0.09059752523899078, 0.23197069764137268, 0.06783950328826904, 0.05665448680520058]
Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)	[28, 1, 9, 0, 121, 16, 250, 222, 163, 202]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4887066185474396, 0.14984890818595886, 0.14984890818595886, 0.678456723690033, 0.3940031826496124, 0.18410612642765045, 0.13298368453979492, 0.15445545315742493, 0.25949838757514954, 0.07150290161371231]
Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)	[202, 0, 1, 9, 55, 57, 16, 163, 250, 82]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05239071696996689, 0.6517979502677917, 0.09660699218511581, 0.09660699218511581, 0.08063111454248428, 0.0640469342470169, 0.06481260061264038, 0.09793639928102493, 0.0743493139743805, 0.08061354607343674]
Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)	[6, 68, 233, 5, 8, 27, 91, 32, 252, 31]	[1, 1, 0, 1, 1, 1, 0, 1, 0, 1]	[0.7819790244102478, 0.5795238018035889, 0.18013685941696167, 0.6991873383522034, 0.7162070870399475, 0.7390317916870117, 0.1689569354057312, 0.6016047596931458, 0.06970036774873734, 0.6706521511077881]
Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model	[11, 21, 2, 61, 26, 18, 16, 12, 95, 0]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.36984801292419434, 0.11437372118234634, 0.12867115437984467, 0.296700656414032, 0.43777021765708923, 0.11467690020799637, 0.04578608646988869, 0.05226997286081314, 0.05438130721449852, 0.19978706538677216]
DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)	[25, 49, 47, 94, 154, 152, 48, 35, 51, 12]	[0, 0, 0, 1, 1, 0, 0, 0, 0, 1]	[0.3434028625488281, 0.2852439880371094, 0.19729579985141754, 0.6952976584434509, 0.5232201814651489, 0.13519853353500366, 0.07275281101465225, 0.15472233295440674, 0.12696313858032227, 0.45343253016471863]
DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)	[25, 141, 0, 17, 1, 36, 6, 100, 22, 159]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.19005997478961945, 0.11938855051994324, 0.7164512276649475, 0.17546547949314117, 0.11374993622303009, 0.17961253225803375, 0.15373727679252625, 0.06009969860315323, 0.06996751576662064, 0.30815741419792175]
DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus	[254, 3, 57, 163, 0, 1, 9, 202, 55, 222]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.07148009538650513, 0.09184800088405609, 0.07034960389137268, 0.0618523508310318, 0.571236789226532, 0.04928072169423103, 0.04928072169423103, 0.061717260628938675, 0.06035018339753151, 0.046132221817970276]
Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language	[23, 5, 57, 2, 56, 73, 156, 4, 94, 147]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23144346475601196, 0.5244117379188538, 0.25315091013908386, 0.15015612542629242, 0.39421385526657104, 0.1642598956823349, 0.26271697878837585, 0.1016528531908989, 0.1317281723022461, 0.08783453702926636]
Data-Oriented Parsing (DOP)'s methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest ranking parse, not derivation, that is desired	[105, 6, 26, 97, 24, 52, 76, 104, 46, 78]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4680463671684265, 0.17427469789981842, 0.18786410987377167, 0.2921816408634186, 0.2169443517923355, 0.3331010341644287, 0.11065429449081421, 0.23969583213329315, 0.20002630352973938, 0.17559146881103516]
Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)	[25, 21, 0, 36, 35, 173, 48, 171, 70, 1]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4515696167945862, 0.3556923568248749, 0.816637396812439, 0.26618003845214844, 0.10368702560663223, 0.16221317648887634, 0.07209938019514084, 0.17853248119354248, 0.06547951698303223, 0.0843203067779541]
Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010)	[0, 7, 46, 9, 4, 105, 5, 142, 21, 118]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5724415183067322, 0.1068444773554802, 0.26878178119659424, 0.10152459889650345, 0.28165560960769653, 0.05642988160252571, 0.2644144296646118, 0.31402915716171265, 0.04483723267912865, 0.06763900816440582]
Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank	[5, 1, 178, 17, 129, 172, 133, 91, 101, 115]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.30040088295936584, 0.2542126774787903, 0.11394178867340088, 0.09883476793766022, 0.17850738763809204, 0.22828921675682068, 0.14680242538452148, 0.547295331954956, 0.080617755651474, 0.14129546284675598]
Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures	[156, 24, 48, 4, 159, 38, 65, 148, 113, 91]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.054812077432870865, 0.07590174674987793, 0.10240769386291504, 0.06894096732139587, 0.18147504329681396, 0.11585231870412827, 0.5481282472610474, 0.07422018051147461, 0.09395018965005875, 0.14354144036769867]
Entries marked with are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)	[4, 31, 45, 18, 61, 35, 54, 13, 76, 16]	[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]	[0.2858603894710541, 0.35211318731307983, 0.1312115490436554, 0.07425545901060104, 0.4882313907146454, 0.09182324260473251, 0.05755290016531944, 0.7684130668640137, 0.0606149323284626, 0.062105827033519745]
Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator	[39, 140, 35, 43, 156, 4, 36, 108, 155, 95]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7911018133163452, 0.7532532215118408, 0.32184988260269165, 0.07658092677593231, 0.1345428079366684, 0.11175870895385742, 0.07960618287324905, 0.11721104383468628, 0.0702633336186409, 0.06464377790689468]
Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences	[77, 20, 61, 73, 90, 88, 102, 58, 4, 104]	[0, 0, 1, 0, 0, 0, 0, 1, 1, 1]	[0.15995396673679352, 0.22390304505825043, 0.6325448155403137, 0.1491403579711914, 0.07214930653572083, 0.08690018206834793, 0.17009000480175018, 0.6169353723526001, 0.43117445707321167, 0.7041252851486206]
Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew	[33, 153, 161, 21, 83, 72, 146, 34, 81, 8]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.41297557950019836, 0.27490317821502686, 0.41724151372909546, 0.18132194876670837, 0.08329802006483078, 0.061529651284217834, 0.055557068437337875, 0.18276497721672058, 0.062159642577171326, 0.07113204151391983]
Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice	[14, 94, 2, 105, 22, 87, 29, 34, 127, 1]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.242832750082016, 0.23183271288871765, 0.12156768888235092, 0.10758192837238312, 0.1686123013496399, 0.1548692286014557, 0.07396850734949112, 0.10222381353378296, 0.09382383525371552, 0.1360609233379364]
Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics	[52, 51, 47, 28, 29, 46, 56, 37, 49, 16]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.16767588257789612, 0.14133314788341522, 0.6992785334587097, 0.11021551489830017, 0.07910840958356857, 0.050285451114177704, 0.0667869970202446, 0.1557268649339676, 0.09157327562570572, 0.05028530955314636]
Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.	[37, 78, 22, 32, 9, 79, 35, 39, 41, 73]	[1, 0, 1, 1, 1, 0, 0, 0, 0, 0]	[0.6242916584014893, 0.30020779371261597, 0.6097410321235657, 0.44444453716278076, 0.6367147564888, 0.0634634718298912, 0.1922464519739151, 0.18875066936016083, 0.11782194674015045, 0.060557883232831955]
Following this line, (Vijay-Shanker et al, 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community	[207, 92, 118, 2, 3, 116, 4, 134, 74, 29]	[1, 1, 1, 0, 0, 0, 0, 1, 1, 0]	[0.6720825433731079, 0.7106505632400513, 0.4590774178504944, 0.24701529741287231, 0.17577889561653137, 0.31353408098220825, 0.17534705996513367, 0.6430878043174744, 0.5443177223205566, 0.07966822385787964]
For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j	[103, 92, 110, 31, 101, 4, 118, 25, 129, 140]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.17360389232635498, 0.055082760751247406, 0.06644944101572037, 0.05625635385513306, 0.05591661483049393, 0.07363390922546387, 0.07333754003047943, 0.06118513271212578, 0.07296725362539291, 0.06939488649368286]
For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)	[199, 0, 1, 205, 131, 130, 25, 68, 103, 7]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.4197218418121338, 0.5024487972259521, 0.08050649613142014, 0.23725581169128418, 0.10803007334470749, 0.22813639044761658, 0.13930559158325195, 0.42335113883018494, 0.1729239970445633, 0.0777873769402504]
For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)	[170, 21, 182, 152, 196, 111, 154, 148, 147, 52]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.08994907140731812, 0.121729277074337, 0.3170683681964874, 0.2814526855945587, 0.2620445489883423, 0.26365751028060913, 0.21439801156520844, 0.15156102180480957, 0.4127943217754364, 0.1361762136220932]
For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)	[156, 118, 158, 39, 79, 182, 22, 17, 30, 57]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7493409514427185, 0.4327104687690735, 0.24402374029159546, 0.08705727756023407, 0.1766735017299652, 0.13369233906269073, 0.047630637884140015, 0.051833365112543106, 0.08326177299022675, 0.07919234782457352]
For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus	[21, 56, 94, 1, 95, 176, 5, 120, 144, 101]	[0, 0, 0, 1, 0, 0, 1, 0, 0, 0]	[0.09018873423337936, 0.09275874495506287, 0.055588189512491226, 0.6247581839561462, 0.1983894556760788, 0.11988255381584167, 0.5649742484092712, 0.0771954134106636, 0.05532975494861603, 0.34255215525627136]
For efficiency and stability, we use the EM algorithm to find ?, rather than L-BFGS as in (Foster et al., 2010)	[75, 2, 70, 68, 3, 152, 41, 137, 148, 135]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12868386507034302, 0.050940193235874176, 0.157444566488266, 0.10161853581666946, 0.04416907578706741, 0.1519472599029541, 0.09042524546384811, 0.05245431512594223, 0.04650965332984924, 0.09964879602193832]
For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form	[10, 35, 47, 2, 22, 12, 49, 136, 6, 115]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.42346662282943726, 0.38685038685798645, 0.06089787930250168, 0.08953586965799332, 0.06251713633537292, 0.078555166721344, 0.05166676640510559, 0.058664318174123764, 0.06599989533424377, 0.3165612816810608]
For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model	[138, 94, 161, 172, 1, 10, 148, 18, 11, 100]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2970723509788513, 0.252623587846756, 0.23872268199920654, 0.07236181944608688, 0.055991578847169876, 0.09947976469993591, 0.04985136538743973, 0.06690359115600586, 0.053500015288591385, 0.09753098338842392]
For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks	[105, 11, 103, 6, 50, 18, 2, 47, 24, 61]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.15483735501766205, 0.2251405268907547, 0.1641259640455246, 0.08277934789657593, 0.1900244504213333, 0.089146189391613, 0.07444826513528824, 0.08134886622428894, 0.07635529339313507, 0.04807816445827484]
For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset	[99, 143, 163, 128, 176, 180, 26, 199, 173, 162]	[0, 0, 1, 1, 1, 1, 0, 0, 0, 1]	[0.3020628094673157, 0.0491437092423439, 0.5405328869819641, 0.5828251242637634, 0.4642283320426941, 0.6611271500587463, 0.30497196316719055, 0.055394165217876434, 0.08434951305389404, 0.5714637041091919]
For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)	[108, 37, 377, 363, 33, 145, 263, 123, 199, 345]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11790687590837479, 0.29418471455574036, 0.20146846771240234, 0.29840728640556335, 0.15778818726539612, 0.10204710811376572, 0.06874596327543259, 0.24902428686618805, 0.1134180799126625, 0.06692851334810257]
For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)	[13, 21, 23, 70, 1, 10, 24, 114, 161, 40]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.29986050724983215, 0.220758318901062, 0.08397489786148071, 0.20941205322742462, 0.36963507533073425, 0.23292185366153717, 0.06741432100534439, 0.11610864102840424, 0.09180499613285065, 0.06866034865379333]
For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective	[37, 23, 36, 15, 40, 14, 20, 49, 108, 0]	[1, 1, 1, 0, 1, 1, 0, 1, 0, 1]	[0.5931457877159119, 0.4830532371997833, 0.5613977909088135, 0.3590603172779083, 0.5839595794677734, 0.486892968416214, 0.26216188073158264, 0.4051550030708313, 0.2884798049926758, 0.7546072602272034]
For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)	[39, 140, 139, 71, 172, 156, 117, 36, 103, 95]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7938394546508789, 0.7152436971664429, 0.0997905433177948, 0.05529223009943962, 0.059080734848976135, 0.0672416239976883, 0.09936410933732986, 0.06791883707046509, 0.059812843799591064, 0.06810500472784042]
For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)	[0, 1, 199, 7, 68, 103, 11, 135, 184, 6]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.6563559174537659, 0.10500568151473999, 0.21011850237846375, 0.1606687605381012, 0.3766600489616394, 0.7728222608566284, 0.2806268334388733, 0.07028476148843765, 0.20132052898406982, 0.26743990182876587]
For our experiments, we use the NEGRA corpus (Skut et al, 1997)	[168, 144, 148, 141, 72, 127, 137, 140, 12, 112]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.34305211901664734, 0.25518515706062317, 0.13003773987293243, 0.5469622015953064, 0.20022696256637573, 0.05560415983200073, 0.21524852514266968, 0.05686464533209801, 0.045881252735853195, 0.16999822854995728]
For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference	[47, 16, 9, 18, 12, 38, 11, 8, 14, 126]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 1]	[0.1584254801273346, 0.4740713834762573, 0.7328272461891174, 0.1151852086186409, 0.15988175570964813, 0.07606693357229233, 0.15449003875255585, 0.2257099598646164, 0.2963145077228546, 0.4442999064922333]
For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)	[6, 151, 5, 136, 9, 165, 150, 143, 132, 152]	[1, 0, 1, 0, 1, 0, 0, 0, 0, 0]	[0.6076996326446533, 0.33359840512275696, 0.44149184226989746, 0.3580006957054138, 0.6572338342666626, 0.07825715839862823, 0.3436048924922943, 0.25482290983200073, 0.09235596656799316, 0.044889409095048904]
For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing	[199, 0, 1, 131, 52, 11, 103, 68, 7, 223]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.6843495965003967, 0.6505739092826843, 0.1300041526556015, 0.14878028631210327, 0.21346983313560486, 0.30901145935058594, 0.1718817502260208, 0.40981730818748474, 0.07172460108995438, 0.1522698700428009]
For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)	[84, 155, 68, 156, 64, 63, 175, 66, 36, 123]	[0, 0, 1, 0, 1, 1, 1, 0, 0, 1]	[0.3666546940803528, 0.2994122803211212, 0.4840438663959503, 0.34866276383399963, 0.4122786521911621, 0.7134479880332947, 0.753886342048645, 0.18604683876037598, 0.1632404625415802, 0.40078431367874146]
For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)	[71, 2, 20, 109, 0, 16, 1, 104, 95, 24]	[0, 1, 0, 0, 1, 0, 0, 1, 0, 0]	[0.19471193850040436, 0.4628772735595703, 0.18937192857265472, 0.23453517258167267, 0.7633161544799805, 0.2078777700662613, 0.37239542603492737, 0.46503394842147827, 0.09319804608821869, 0.13746774196624756]
Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach	[1, 24, 6, 9, 8, 18, 0, 112, 158, 102]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.5490462779998779, 0.6460283994674683, 0.5051131844520569, 0.3498851954936981, 0.2240494340658188, 0.5187298059463501, 0.2854790985584259, 0.0538594126701355, 0.046426281332969666, 0.3531481623649597]
Foster et al (2010) combine the two, applying linear interpolation to combine the instance weighted out-of-domain model with an in-domain model	[144, 133, 71, 21, 124, 126, 44, 43, 49, 147]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21458998322486877, 0.2878788113594055, 0.1052410677075386, 0.05018052086234093, 0.05868940427899361, 0.11098343133926392, 0.05054648593068123, 0.05843815207481384, 0.1647854894399643, 0.13458020985126495]
Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance	[31, 42, 119, 144, 9, 5, 25, 43, 139, 132]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.2688209116458893, 0.6241990327835083, 0.4476569592952728, 0.29712438583374023, 0.22673852741718292, 0.3992048501968384, 0.05530928075313568, 0.26380860805511475, 0.05670243874192238, 0.23647992312908173]
Foster et al (2010) further perform this on extracted phrase pairs, not just sentences	[28, 68, 95, 24, 152, 113, 23, 65, 144, 38]	[0, 1, 0, 0, 1, 0, 0, 1, 0, 0]	[0.13161012530326843, 0.6176764369010925, 0.30701813101768494, 0.23668134212493896, 0.5622217059135437, 0.3255271315574646, 0.12165659666061401, 0.6126123070716858, 0.07425963878631592, 0.09900475293397903]
Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality	[0, 7, 22, 153, 21, 1, 89, 70, 19, 131]	[1, 0, 1, 1, 0, 0, 0, 1, 0, 0]	[0.42468976974487305, 0.09279367327690125, 0.6814714074134827, 0.5143597722053528, 0.05972839891910553, 0.049352098256349564, 0.24361641705036163, 0.528634250164032, 0.21078616380691528, 0.3304586410522461]
Foster et al (2010), however, uses a different approach to select related sentences from OUT	[31, 42, 149, 152, 6, 64, 62, 59, 143, 101]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6702186465263367, 0.38919568061828613, 0.07738277316093445, 0.4089139401912689, 0.0663481280207634, 0.11431524157524109, 0.04656149819493294, 0.3228560984134674, 0.18789154291152954, 0.2200167328119278]
Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)	[199, 1, 152, 85, 12, 50, 103, 47, 8, 131]	[1, 0, 0, 0, 1, 0, 1, 0, 1, 0]	[0.6287415623664856, 0.10641168802976608, 0.08212833851575851, 0.14284628629684448, 0.5462771654129028, 0.08397740870714188, 0.41434162855148315, 0.39986225962638855, 0.48686420917510986, 0.1732163429260254]
Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English, and achieves accuracies around 85% on the languages that we consider	[1, 18, 6, 158, 0, 112, 21, 9, 24, 70]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.3689093291759491, 0.6366851329803467, 0.20119313895702362, 0.34759363532066345, 0.5939019918441772, 0.3716571033000946, 0.05962695553898811, 0.08825688064098358, 0.2868490517139435, 0.06893149018287659]
GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)	[140, 21, 25, 51, 49, 138, 132, 152, 36, 12]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11817550659179688, 0.06255999952554703, 0.049946609884500504, 0.08589529991149902, 0.05169360712170601, 0.06840220093727112, 0.16899041831493378, 0.07650183886289597, 0.05304192751646042, 0.07854942232370377]
German is considerably more in ectional which means that discarding functional information is more harmful, and which explains why the NEGRA annotation has been conceived to be quite at (Skut et al, 1997)	[121, 41, 48, 40, 166, 163, 109, 110, 74, 69]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3363601267337799, 0.11479503661394119, 0.12155534327030182, 0.07382556051015854, 0.2874208986759186, 0.1705072522163391, 0.09320098906755447, 0.05634346604347229, 0.09004788845777512, 0.20004212856292725]
Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)	[41, 105, 38, 33, 21, 39, 30, 23, 101, 104]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.4545193016529083, 0.33533573150634766, 0.1935761421918869, 0.2030916064977646, 0.05421798303723335, 0.16015975177288055, 0.07861920446157455, 0.6773912906646729, 0.10351180285215378, 0.3749508857727051]
Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach	[4, 47, 188, 0, 49, 48, 158, 46, 189, 51]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4814421832561493, 0.4272575080394745, 0.23595792055130005, 0.8044100999832153, 0.38728418946266174, 0.36710065603256226, 0.22984829545021057, 0.11513511091470718, 0.21178536117076874, 0.22847998142242432]
Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text	[48, 49, 0, 46, 90, 159, 188, 91, 144, 3]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.20702360570430756, 0.30685657262802124, 0.6787695288658142, 0.06912197917699814, 0.10571429878473282, 0.2326839417219162, 0.053545184433460236, 0.16725225746631622, 0.06376346945762634, 0.12484774738550186]
Goldberg and Tsarfaty (2008) propose a generative joint model	[0, 3, 19, 53, 162, 186, 18, 51, 188, 141]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7872059345245361, 0.47583529353141785, 0.10852138698101044, 0.21113626658916473, 0.358832985162735, 0.13655707240104675, 0.06506476551294327, 0.08678756654262543, 0.05186467990279198, 0.12044617533683777]
Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models	[4, 0, 19, 189, 3, 186, 17, 21, 164, 49]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6495993137359619, 0.839792788028717, 0.24613569676876068, 0.1431369185447693, 0.3629724383354187, 0.24593515694141388, 0.2562068998813629, 0.11923709511756897, 0.05765156447887421, 0.28781843185424805]
Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank	[134, 136, 47, 4, 191, 133, 127, 21, 192, 89]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7511469721794128, 0.5469074845314026, 0.1335354447364807, 0.14519409835338593, 0.30780908465385437, 0.528827965259552, 0.2624511420726776, 0.1739925891160965, 0.11329866945743561, 0.21484972536563873]
Goodman's transform, in combination with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model	[3, 145, 135, 98, 39, 134, 38, 44, 107, 130]	[0, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.2191425859928131, 0.492849737405777, 0.7236436009407043, 0.22734323143959045, 0.21457023918628693, 0.4630051553249359, 0.3094155490398407, 0.10166164487600327, 0.25937315821647644, 0.10401755571365356]
Henderson and Brill (1999) also reported that context did not help them to outperform simple voting	[84, 125, 120, 93, 8, 18, 129, 80, 46, 23]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.7000479698181152, 0.3234843313694, 0.3075108826160431, 0.2410033643245697, 0.08493451774120331, 0.10127077251672745, 0.20800727605819702, 0.048631731420755386, 0.0716826468706131, 0.5376736521720886]
Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper	[120, 85, 13, 141, 97, 112, 14, 143, 80, 111]	[1, 1, 1, 0, 0, 1, 1, 0, 0, 1]	[0.6382138729095459, 0.49094775319099426, 0.49261993169784546, 0.33328720927238464, 0.3685890734195709, 0.7094230055809021, 0.5647205710411072, 0.253268301486969, 0.06158912181854248, 0.5756190419197083]
Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined	[70, 98, 78, 35, 15, 1, 77, 76, 100, 141]	[0, 0, 1, 1, 1, 0, 0, 0, 0, 0]	[0.32573944330215454, 0.2677592635154724, 0.5901855826377869, 0.5255557298660278, 0.47463005781173706, 0.14528566598892212, 0.2500370740890503, 0.10900292545557022, 0.1658899039030075, 0.05473317205905914]
Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees	[51, 55, 21, 87, 95, 67, 34, 116, 61, 50]	[0, 1, 0, 0, 0, 1, 0, 1, 0, 0]	[0.14216233789920807, 0.40341874957084656, 0.14446212351322174, 0.11016856133937836, 0.10379687696695328, 0.5612415671348572, 0.2563508450984955, 0.41539904475212097, 0.09952103346586227, 0.09864188730716705]
Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy	[1, 114, 144, 15, 146, 111, 85, 29, 38, 86]	[1, 0, 0, 0, 0, 1, 1, 0, 0, 0]	[0.6111636161804199, 0.09531765431165695, 0.24594558775424957, 0.230486199259758, 0.22810550034046173, 0.5370038151741028, 0.41666680574417114, 0.19904150068759918, 0.09387022256851196, 0.058490313589572906]
Here we use the standard definition of LCFRS (Vijay-Shanker et al, 1987) and only fix our notation; for a more thorough discussion of this formal ism, we refer to the literature. Let G be an LCFRS	[146, 119, 230, 229, 222, 231, 221, 209, 226, 159]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1562805026769638, 0.20825450122356415, 0.19111193716526031, 0.11482631415128708, 0.3110184371471405, 0.1462492197751999, 0.3810300827026367, 0.1146233007311821, 0.10506726056337357, 0.06680047512054443]
However, just as it has been noted that most non-projective structures appearing in practice are only 'slightly' non projective (Nivre and Nilsson, 2005), we characterise a sense in which the structures appearing in tree banks can be viewed as being only 'slightly' ill-nested	[23, 6, 7, 83, 16, 2, 8, 101, 22, 49]	[0, 1, 0, 0, 0, 1, 1, 0, 0, 0]	[0.27565720677375793, 0.5760133266448975, 0.3160052001476288, 0.22098596394062042, 0.34526631236076355, 0.5895017981529236, 0.4693913757801056, 0.26374852657318115, 0.2879824936389923, 0.10511139035224915]
However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)	[8, 13, 34, 96, 58, 35, 116, 45, 46, 90]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.3607621192932129, 0.07445335388183594, 0.5604284405708313, 0.24752739071846008, 0.36415690183639526, 0.058342888951301575, 0.33571314811706543, 0.07373785227537155, 0.17659936845302582, 0.06672506034374237]
In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification	[250, 18, 0, 2, 1, 9, 8, 47, 136, 16]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.6443451046943665, 0.7340043187141418, 0.7618749141693115, 0.3698327839374542, 0.1535564512014389, 0.1535564512014389, 0.2037142664194107, 0.14891715347766876, 0.08897180110216141, 0.1115209087729454]
In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries	[2, 132, 12, 13, 1, 106, 167, 9, 22, 163]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.13651049137115479, 0.4571378827095032, 0.5582122206687927, 0.29477083683013916, 0.052300382405519485, 0.2124517410993576, 0.32773691415786743, 0.08957051485776901, 0.18913227319717407, 0.17058266699314117]
In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)	[1, 5, 48, 117, 175, 133, 109, 178, 9, 91]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.3191640377044678, 0.3141959607601166, 0.19686052203178406, 0.5147882699966431, 0.22100530564785004, 0.20087066292762756, 0.12324130535125732, 0.15531839430332184, 0.14549750089645386, 0.32665902376174927]
In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT	[28, 94, 3, 144, 126, 150, 1, 42, 9, 133]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.579305112361908, 0.4081547260284424, 0.0615825317800045, 0.3737567961215973, 0.3077602982521057, 0.0760863721370697, 0.2633803188800812, 0.6408422589302063, 0.10695366561412811, 0.21020139753818512]
In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)	[95, 144, 68, 26, 54, 152, 2, 96, 94, 70]	[0, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.20735196769237518, 0.41920384764671326, 0.6444735527038574, 0.5823009610176086, 0.18160097301006317, 0.5877426862716675, 0.08403483033180237, 0.3692387342453003, 0.20803964138031006, 0.06501348316669464]
In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))	[188, 163, 1, 8, 15, 82, 112, 175, 173, 111]	[1, 0, 1, 0, 0, 0, 0, 1, 0, 1]	[0.6844447255134583, 0.07211611419916153, 0.40228116512298584, 0.14488036930561066, 0.1245606541633606, 0.14959955215454102, 0.3279460370540619, 0.47332626581192017, 0.06115686893463135, 0.5494565367698669]
In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense	[48, 52, 145, 41, 45, 46, 110, 165, 81, 7]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.6177752614021301, 0.37868228554725647, 0.13545101881027222, 0.29200178384780884, 0.27150315046310425, 0.3979659080505371, 0.18994660675525665, 0.05547300726175308, 0.5006110668182373, 0.07664437592029572]
In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)	[251, 138, 219, 29, 55, 30, 2, 7, 209, 215]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.2959732711315155, 0.2763381898403168, 0.4416857063770294, 0.2476251721382141, 0.1423332393169403, 0.07576445490121841, 0.06484431028366089, 0.11370594799518585, 0.05705223232507706, 0.05357510223984718]
In contrast, some other tree banks, such as the German NeGra and TIGER tree banks allow annotation with crossing branches (Skut et al, 1997). Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node	[47, 111, 25, 55, 24, 39, 168, 51, 48, 89]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6485627889633179, 0.5377257466316223, 0.3499567210674286, 0.29595082998275757, 0.1950976848602295, 0.14014607667922974, 0.13890308141708374, 0.2617452144622803, 0.12180405110120773, 0.06407050788402557]
In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)	[175, 41, 172, 45, 177, 32, 173, 115, 178, 182]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5632771849632263, 0.3417305648326874, 0.1931896209716797, 0.14544983208179474, 0.0536457858979702, 0.2519177794456482, 0.0969318151473999, 0.09524095058441162, 0.3162395656108856, 0.10603128373622894]
In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees	[20, 17, 171, 21, 32, 25, 11, 12, 49, 99]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6109739542007446, 0.09869661182165146, 0.06133004650473595, 0.08476117998361588, 0.06070728227496147, 0.07344204187393188, 0.0482928603887558, 0.21662501990795135, 0.07670904695987701, 0.06366760283708572]
In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)'s parser, (McDonald et al., 2006)'s parser, and so on	[12, 36, 45, 0, 100, 1, 31, 34, 51, 95]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4689235985279083, 0.23768602311611176, 0.616698145866394, 0.3987690508365631, 0.325867623090744, 0.25601905584335327, 0.05223502218723297, 0.19369910657405853, 0.34232398867607117, 0.20117515325546265]
In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005)	[1, 45, 0, 24, 95, 20, 5, 104, 106, 109]	[1, 0, 1, 0, 0, 0, 0, 1, 0, 1]	[0.6138835549354553, 0.3668983280658722, 0.7984599471092224, 0.30887454748153687, 0.2629811465740204, 0.3810974955558777, 0.1019168421626091, 0.5558373332023621, 0.33639246225357056, 0.42032256722450256]
In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance	[2, 132, 13, 12, 106, 9, 22, 20, 6, 32]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.4157474935054779, 0.6789700388908386, 0.2504202127456665, 0.09448068588972092, 0.07568110525608063, 0.19950129091739655, 0.0455985926091671, 0.5608319640159607, 0.04505220055580139, 0.06422329694032669]
In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al (1987))	[2, 214, 92, 207, 118, 106, 34, 134, 116, 83]	[1, 1, 1, 1, 0, 0, 1, 1, 0, 0]	[0.440477192401886, 0.6833209991455078, 0.6444798707962036, 0.6189144253730774, 0.31881797313690186, 0.3520337641239166, 0.4463687539100647, 0.5798704028129578, 0.341848224401474, 0.20995596051216125]
In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours	[55, 25, 42, 152, 19, 129, 111, 148, 57, 170]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.1205105185508728, 0.2546302080154419, 0.1741161197423935, 0.21469783782958984, 0.0828341618180275, 0.703956127166748, 0.11718279123306274, 0.07279255986213684, 0.1045173779129982, 0.18867060542106628]
In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)	[5, 13, 59, 19, 11, 156, 16, 38, 99, 10]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.29888978600502014, 0.2005949169397354, 0.07044427841901779, 0.4203043282032013, 0.2993256151676178, 0.07415366917848587, 0.09350115805864334, 0.15363208949565887, 0.08363685011863708, 0.10231633484363556]
In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application	[1, 103, 6, 7, 40, 67, 154, 0, 244, 11]	[1, 1, 0, 1, 0, 0, 0, 1, 1, 0]	[0.40565383434295654, 0.6702920794487, 0.2751740515232086, 0.48276540637016296, 0.19956529140472412, 0.05959290266036987, 0.1576428860425949, 0.7207558751106262, 0.6197636723518372, 0.3930037319660187]
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)	[133, 302, 136, 223, 268, 137, 390, 99, 115, 31]	[1, 0, 1, 1, 1, 0, 0, 0, 0, 0]	[0.4043242931365967, 0.09514620155096054, 0.6162843704223633, 0.6612966060638428, 0.44048070907592773, 0.3850924074649811, 0.0756772980093956, 0.09810785949230194, 0.2900392413139343, 0.32337677478790283]
In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language	[18, 1, 158, 0, 6, 21, 112, 24, 29, 70]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6529712677001953, 0.16008228063583374, 0.09707324206829071, 0.5154776573181152, 0.20600974559783936, 0.16459767520427704, 0.12140282988548279, 0.12307894229888916, 0.053766317665576935, 0.3129750192165375]
In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here	[31, 41, 402, 98, 75, 36, 209, 137, 401, 32]	[0, 1, 1, 1, 0, 0, 1, 0, 0, 0]	[0.20051981508731842, 0.5867719650268555, 0.6463783383369446, 0.43219342827796936, 0.3794640302658081, 0.19452309608459473, 0.4917008876800537, 0.19591574370861053, 0.128122940659523, 0.16608890891075134]
Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models	[77, 31, 402, 0, 401, 404, 140, 102, 400, 2]	[0, 0, 1, 1, 0, 0, 0, 1, 1, 0]	[0.1196158230304718, 0.24023400247097015, 0.6170224547386169, 0.6714723706245422, 0.31095606088638306, 0.3599536716938019, 0.07365066558122635, 0.5018070936203003, 0.6901835799217224, 0.22399793565273285]
Inference was carried out using the language modeling library described by Heafield (2011)	[274, 1, 129, 68, 25, 26, 239, 5, 130, 19]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 1]	[0.48721835017204285, 0.19991718232631683, 0.2664593458175659, 0.44629308581352234, 0.16318866610527039, 0.047165606170892715, 0.20698805153369904, 0.07276896387338638, 0.10122326016426086, 0.4256673753261566]
Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model	[16, 37, 35, 36, 39, 41, 78, 124, 79, 92]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21325190365314484, 0.25032246112823486, 0.1953705996274948, 0.3874795734882355, 0.287833034992218, 0.17293775081634521, 0.07659231126308441, 0.06150020286440849, 0.05676223337650299, 0.05542024224996567]
Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units	[149, 1, 73, 155, 133, 135, 10, 36, 123, 162]	[0, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.30162784457206726, 0.32322442531585693, 0.5784218907356262, 0.3385642468929291, 0.19223736226558685, 0.40835317969322205, 0.11072729527950287, 0.09272656589746475, 0.11534971743822098, 0.30339857935905457]
Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting	[15, 36, 13, 32, 108, 72, 104, 45, 46, 27]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5288280844688416, 0.09118208289146423, 0.0644112154841423, 0.06282220035791397, 0.1593533754348755, 0.25738799571990967, 0.3969501554965973, 0.1428890973329544, 0.07672102004289627, 0.10954839736223221]
Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)	[11, 7, 57, 5, 32, 10, 86, 87, 37, 19]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6840717792510986, 0.07005723565816879, 0.07066652923822403, 0.49177294969558716, 0.1013735756278038, 0.2910427749156952, 0.15349850058555603, 0.27579593658447266, 0.0679972693324089, 0.1783839613199234]
It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al, 1987)	[78, 55, 93, 2, 16, 83, 47, 112, 40, 10]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.5451233386993408, 0.30686506628990173, 0.2948991358280182, 0.24623200297355652, 0.34120306372642517, 0.16926974058151245, 0.27341604232788086, 0.23569618165493011, 0.4337550699710846, 0.16860507428646088]
It is the same grammar as described in (Goldberg and Tsarfaty, 2008)	[136, 28, 37, 63, 169, 77, 102, 4, 172, 168]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4268770217895508, 0.05148222669959068, 0.09515275061130524, 0.05050830915570259, 0.0549832321703434, 0.06455975025892258, 0.16334284842014313, 0.0548509880900383, 0.07454758137464523, 0.05815295875072479]
It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)	[0, 21, 55, 25, 110, 18, 32, 8, 65, 92]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5741708874702454, 0.09351927042007446, 0.10431443154811859, 0.0575995072722435, 0.16875503957271576, 0.05013137310743332, 0.06680142134428024, 0.06435775756835938, 0.06159191578626633, 0.06608443707227707]
It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)	[1, 8, 25, 40, 153, 79, 80, 63, 111, 6]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.05655677244067192, 0.23291875422000885, 0.04939635470509529, 0.2773423492908478, 0.044975560158491135, 0.05460674688220024, 0.07818973809480667, 0.09517830610275269, 0.23064112663269043, 0.3192905783653259]
It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features	[31, 2, 110, 24, 29, 46, 35, 18, 45, 11]	[1, 0, 0, 0, 1, 1, 0, 0, 0, 1]	[0.44795453548431396, 0.31100329756736755, 0.09789048880338669, 0.2883633077144623, 0.5405820608139038, 0.4017576575279236, 0.12284362316131592, 0.10645418614149094, 0.08449427038431168, 0.42332738637924194]
It should be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in themselves (Nivre and Nilsson, 2005)	[14, 80, 92, 19, 77, 28, 83, 101, 110, 96]	[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.7558090090751648, 0.5806827545166016, 0.6017182469367981, 0.2650033235549927, 0.25251951813697815, 0.050035834312438965, 0.16704753041267395, 0.27854350209236145, 0.15208974480628967, 0.16327819228172302]
It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)	[20, 2, 23, 40, 109, 3, 36, 26, 15, 71]	[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]	[0.41966718435287476, 0.4751215875148773, 0.38885951042175293, 0.4991045594215393, 0.3962264358997345, 0.20441778004169464, 0.43476223945617676, 0.10424553602933884, 0.1108325943350792, 0.12927737832069397]
Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging	[0, 1, 5, 3, 4, 130, 9, 22, 31, 56]	[1, 1, 0, 0, 1, 1, 1, 0, 0, 0]	[0.8111439347267151, 0.7735201120376587, 0.3090917468070984, 0.25580093264579773, 0.46614885330200195, 0.6955852508544922, 0.6035967469215393, 0.3260471522808075, 0.2658073306083679, 0.30262044072151184]
LCFRS (Vijay-Shanker et al, 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals	[60, 74, 209, 54, 41, 101, 192, 21, 217, 138]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10045675188302994, 0.6560661196708679, 0.05824750289320946, 0.29947614669799805, 0.38414084911346436, 0.08226541429758072, 0.06959009170532227, 0.0687057226896286, 0.07696332782506943, 0.07494820654392242]
Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)	[12, 2, 53, 126, 58, 82, 41, 92, 4, 54]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30030518770217896, 0.08305548131465912, 0.2822314202785492, 0.0744815319776535, 0.11087178438901901, 0.07552412897348404, 0.049015145748853683, 0.04783998057246208, 0.05103234574198723, 0.06709509342908859]
Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)	[59, 100, 313, 267, 22, 348, 21, 240, 320, 304]	[1, 0, 1, 1, 0, 0, 0, 1, 0, 1]	[0.5458618402481079, 0.23250892758369446, 0.45420753955841064, 0.7120298147201538, 0.22196991741657257, 0.21446214616298676, 0.261244535446167, 0.4696255922317505, 0.13719019293785095, 0.45119768381118774]
Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al, 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms	[92, 2, 207, 118, 214, 134, 116, 125, 164, 34]	[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]	[0.7779223918914795, 0.5324031114578247, 0.659152626991272, 0.5048945546150208, 0.6223194003105164, 0.6662378907203674, 0.30914029479026794, 0.25131022930145264, 0.22284983098506927, 0.19781140983104706]
MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007))	[6, 0, 5, 18, 154, 158, 104, 35, 1, 112]	[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]	[0.49368903040885925, 0.7438867688179016, 0.11336155235767365, 0.7111005187034607, 0.05612051486968994, 0.3300929069519043, 0.45601215958595276, 0.10289274901151657, 0.05790655314922333, 0.08322709798812866]
Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs	[26, 65, 1, 144, 151, 68, 152, 67, 59, 146]	[1, 1, 0, 1, 0, 0, 1, 1, 1, 0]	[0.5528764724731445, 0.6555314660072327, 0.20622041821479797, 0.43692412972450256, 0.2615233361721039, 0.3915148377418518, 0.709888756275177, 0.7899724245071411, 0.5056560039520264, 0.1670750379562378]
Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world	[141, 0, 7, 49, 166, 135, 152, 11, 165, 17]	[0, 1, 1, 0, 0, 0, 0, 0, 1, 0]	[0.09103168547153473, 0.7296282649040222, 0.48003140091896057, 0.07611485570669174, 0.12235283106565475, 0.0637408122420311, 0.05960084870457649, 0.05813457444310188, 0.6472469568252563, 0.1037261039018631]
McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus	[112, 102, 116, 74, 165, 30, 15, 175, 188, 61]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.46291348338127136, 0.5561773180961609, 0.17571596801280975, 0.13091230392456055, 0.11320921033620834, 0.13968773186206818, 0.18016056716442108, 0.370090514421463, 0.38247615098953247, 0.06258680671453476]
McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997)	[79, 188, 63, 83, 47, 80, 179, 76, 89, 176]	[1, 1, 1, 0, 0, 1, 1, 0, 0, 0]	[0.7131177186965942, 0.4702380299568176, 0.6496500968933105, 0.24436891078948975, 0.16658087074756622, 0.5294381976127625, 0.527351975440979, 0.20517487823963165, 0.21955081820487976, 0.2676481604576111]
McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD	[15, 165, 1, 0, 152, 41, 170, 178, 66, 79]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.10312508791685104, 0.4726200997829437, 0.20533788204193115, 0.6755441427230835, 0.30972713232040405, 0.1884177178144455, 0.0835757628083229, 0.06641481071710587, 0.08688642829656601, 0.12552553415298462]
McDonald et al (2006) use an additional algorithm	[45, 11, 18, 20, 23, 4, 43, 54, 22, 13]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.19727137684822083, 0.6320669651031494, 0.12788695096969604, 0.3748994767665863, 0.12701791524887085, 0.3885897994041443, 0.13475659489631653, 0.04804275557398796, 0.05987473949790001, 0.6944307088851929]
McDonald et al (2006) use post-processing for non-projective dependencies and for labeling	[18, 11, 64, 22, 54, 70, 5, 10, 8, 76]	[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.3057819902896881, 0.6238982677459717, 0.3637688159942627, 0.10712304711341858, 0.12516511976718903, 0.07758870720863342, 0.4145687222480774, 0.25867605209350586, 0.050327032804489136, 0.14311298727989197]
Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels	[6, 2, 104, 58, 38, 59, 61, 34, 21, 78]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.6860017776489258, 0.5456444621086121, 0.5256691575050354, 0.3898089826107025, 0.4368049204349518, 0.3681473731994629, 0.07659052312374115, 0.10957632958889008, 0.05903027206659317, 0.10029392689466476]
Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint	[0, 1, 3, 12, 32, 66, 95, 11, 16, 2]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6121671199798584, 0.20010878145694733, 0.15710832178592682, 0.07140123844146729, 0.0707026869058609, 0.13029970228672028, 0.04781695082783699, 0.12185928225517273, 0.04532254859805107, 0.07028588652610779]
Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees	[34, 52, 33, 41, 51, 95, 35, 83, 58, 40]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.7745980024337769, 0.5186910033226013, 0.5178931355476379, 0.3322126269340515, 0.25746384263038635, 0.6409718990325928, 0.21983563899993896, 0.17751583456993103, 0.07990723848342896, 0.06883446127176285]
Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types	[58, 33, 41, 24, 52, 34, 95, 35, 51, 83]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5697738528251648, 0.455716997385025, 0.3072957694530487, 0.10226038098335266, 0.21164698898792267, 0.15968665480613708, 0.1831914782524109, 0.1017494723200798, 0.0680885910987854, 0.10472404211759567]
Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)	[35, 21, 170, 111, 168, 29, 19, 196, 182, 55]	[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]	[0.664534330368042, 0.47462567687034607, 0.5415958166122437, 0.5517268776893616, 0.47745242714881897, 0.47150999307632446, 0.2533565163612366, 0.19066496193408966, 0.3821578621864319, 0.1296500414609909]
Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora	[126, 194, 168, 170, 29, 91, 15, 109, 25, 114]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.40875351428985596, 0.3406154215335846, 0.19058910012245178, 0.09949849545955658, 0.37147802114486694, 0.484158456325531, 0.061628781259059906, 0.09293020516633987, 0.05758698657155037, 0.2925691604614258]
Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly	[118, 79, 122, 78, 156, 154, 148, 194, 182, 114]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.5601999163627625, 0.46717962622642517, 0.7520660758018494, 0.1945941299200058, 0.4385952055454254, 0.31135645508766174, 0.13674426078796387, 0.08092465251684189, 0.16133032739162445, 0.241127148270607]
Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)	[26, 3, 190, 88, 191, 168, 95, 51, 87, 21]	[0, 1, 1, 1, 0, 1, 0, 1, 0, 0]	[0.31706273555755615, 0.5472773313522339, 0.6791841387748718, 0.7773429155349731, 0.19190150499343872, 0.6605654358863831, 0.16079415380954742, 0.5320380926132202, 0.053868405520915985, 0.08291540294885635]
Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition	[182, 190, 168, 163, 21, 185, 165, 2, 175, 51]	[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.3538062274456024, 0.6064682006835938, 0.18873387575149536, 0.2553454041481018, 0.06996099650859833, 0.14484429359436035, 0.46044468879699707, 0.0622037835419178, 0.05147889629006386, 0.22026124596595764]
Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition	[57, 59, 61, 62, 64, 58, 76, 51, 38, 24]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6169503927230835, 0.19206111133098602, 0.05322642624378204, 0.2129344642162323, 0.09763132780790329, 0.22124895453453064, 0.10039974749088287, 0.24564242362976074, 0.05887005478143692, 0.07040149718523026]
Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge	[57, 64, 59, 65, 61, 62, 63, 177, 51, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30977803468704224, 0.062276896089315414, 0.09145531803369522, 0.18563419580459595, 0.08707781881093979, 0.2781222462654114, 0.08419439196586609, 0.05242263525724411, 0.24380090832710266, 0.06421419233083725]
Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)	[57, 64, 58, 35, 53, 59, 65, 189, 41, 0]	[1, 0, 0, 0, 0, 0, 0, 1, 0, 1]	[0.46656307578086853, 0.06729706376791, 0.0832221508026123, 0.1773473620414734, 0.3536573648452759, 0.19128060340881348, 0.08177365362644196, 0.6072288751602173, 0.1226729229092598, 0.6294981241226196]
Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)	[38, 190, 40, 64, 178, 66, 35, 83, 168, 191]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30655819177627563, 0.4439934492111206, 0.315843403339386, 0.335740864276886, 0.16093093156814575, 0.07314828038215637, 0.3809976577758789, 0.2356244921684265, 0.11602234840393066, 0.23512233793735504]
Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression	[21, 28, 20, 44, 58, 11, 13, 195, 23, 51]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3755301833152771, 0.2665216624736786, 0.3089854419231415, 0.0969771295785904, 0.08271688967943192, 0.23937268555164337, 0.05905462056398392, 0.06064551696181297, 0.056760456413030624, 0.0820322334766388]
Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing	[43, 49, 0, 141, 108, 54, 190, 46, 30, 144]	[0, 1, 1, 1, 0, 0, 1, 0, 0, 0]	[0.16943982243537903, 0.6644443869590759, 0.6837232708930969, 0.602927565574646, 0.06463350355625153, 0.08956922590732574, 0.6567918062210083, 0.09559332579374313, 0.08332064002752304, 0.09056051820516586]
More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)	[43, 16, 24, 41, 35, 32, 36, 37, 2, 21]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.2430761605501175, 0.2103084772825241, 0.08946641534566879, 0.10464727133512497, 0.14886805415153503, 0.07462196052074432, 0.09276283532381058, 0.06273866444826126, 0.09558936208486557, 0.5751824975013733]
More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)	[89, 105, 174, 22, 103, 113, 155, 166, 184, 162]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.10585405677556992, 0.1850234866142273, 0.3702717125415802, 0.25483664870262146, 0.5117060542106628, 0.3504638671875, 0.11061751842498779, 0.07091452181339264, 0.18075096607208252, 0.2697905898094177]
More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins	[25, 21, 0, 45, 171, 47, 49, 152, 51, 12]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.1105465441942215, 0.11723821610212326, 0.7488569021224976, 0.2507207691669464, 0.05620938539505005, 0.0916772335767746, 0.06911097466945648, 0.06630275398492813, 0.06582682579755783, 0.05912550911307335]
Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)	[34, 52, 95, 38, 55, 2, 33, 58, 51, 10]	[1, 1, 1, 1, 0, 0, 1, 0, 0, 0]	[0.6877189874649048, 0.5122918486595154, 0.6045671105384827, 0.6356490254402161, 0.2338070571422577, 0.23121331632137299, 0.4272260069847107, 0.1648513525724411, 0.13770298659801483, 0.0926026850938797]
Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))	[81, 106, 162, 38, 71, 144, 119, 111, 35, 176]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06888096779584885, 0.06390929967164993, 0.13398340344429016, 0.05273378640413284, 0.05825530365109444, 0.13997861742973328, 0.07268790155649185, 0.06119050458073616, 0.052389394491910934, 0.06863244622945786]
Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)	[1, 25, 29, 0, 18, 5, 35, 11, 6, 31]	[0, 0, 1, 1, 0, 0, 1, 0, 1, 1]	[0.07875493913888931, 0.09280838817358017, 0.4040042757987976, 0.7731490731239319, 0.21226440370082855, 0.1129782497882843, 0.5405320525169373, 0.36767640709877014, 0.47797638177871704, 0.4399107098579407]
My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003)	[126, 133, 12, 4, 143, 135, 146, 21, 14, 6]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.4462834596633911, 0.42579954862594604, 0.14963218569755554, 0.059328269213438034, 0.18402095139026642, 0.33820199966430664, 0.1333036571741104, 0.4029427170753479, 0.07577160000801086, 0.13160540163516998]
Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing	[2, 20, 109, 26, 28, 23, 91, 108, 22, 24]	[1, 1, 1, 1, 0, 0, 0, 1, 0, 0]	[0.5969163775444031, 0.6378095149993896, 0.6313064098358154, 0.6274943351745605, 0.33692312240600586, 0.2834320664405823, 0.20202168822288513, 0.4216279983520508, 0.17887338995933533, 0.18519140779972076]
Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN); Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011)	[106, 49, 5, 100, 52, 9, 55, 133, 144, 116]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.05367981269955635, 0.057700712233781815, 0.4060446619987488, 0.047588497400283813, 0.04915596544742584, 0.11177168786525726, 0.09044750779867172, 0.28650933504104614, 0.09541646391153336, 0.06973964720964432]
Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size	[32, 99, 180, 74, 17, 107, 113, 174, 56, 106]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.1049417182803154, 0.07505480945110321, 0.10202043503522873, 0.11054761707782745, 0.1447822004556656, 0.17696982622146606, 0.057932350784540176, 0.12546302378177643, 0.07867870479822159, 0.13500583171844482]
Of course, it is well-known that a supervised parser's f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% f score if tested on the Brown corpus	[139, 21, 11, 32, 108, 36, 39, 125, 126, 86]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5957346558570862, 0.5260578393936157, 0.20082634687423706, 0.11282872408628464, 0.151437446475029, 0.07879750430583954, 0.07084052264690399, 0.06460026651620865, 0.06643939018249512, 0.061716679483652115]
On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi,1985), including, among several others, the tree ad joining grammars (TAGs) of Joshi et al (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases	[214, 27, 74, 2, 24, 92, 207, 116, 118, 139]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.5645356774330139, 0.6694433689117432, 0.6753672361373901, 0.220748633146286, 0.5650634169578552, 0.3627462387084961, 0.35124367475509644, 0.3541874885559082, 0.10707877576351166, 0.1109229177236557]
One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations	[58, 2, 33, 98, 34, 59, 95, 17, 16, 13]	[0, 0, 1, 0, 1, 0, 1, 0, 0, 0]	[0.19494320452213287, 0.23845157027244568, 0.5227165222167969, 0.08243832737207413, 0.5933224558830261, 0.39484521746635437, 0.47714847326278687, 0.0885675773024559, 0.16550831496715546, 0.1846097856760025]
One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Lianget al 2011) or even a binary correct/incorrect signal (Clarke et al2010)	[9, 117, 146, 15, 148, 112, 142, 24, 172, 1]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.5893095135688782, 0.6496108770370483, 0.2455069124698639, 0.16422691941261292, 0.19410617649555206, 0.214743509888649, 0.19568096101284027, 0.5151817798614502, 0.054846540093421936, 0.07619535177946091]
One possibly beneficial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level	[31, 34, 29, 18, 39, 38, 58, 45, 95, 103]	[0, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.140740767121315, 0.4152093827724457, 0.07844521850347519, 0.06731504946947098, 0.07121697068214417, 0.6008169054985046, 0.07337908446788788, 0.04750274494290352, 0.36732175946235657, 0.06750483810901642]
Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition	[15, 16, 399, 14, 108, 88, 17, 40, 13, 247]	[1, 1, 1, 1, 0, 1, 1, 1, 0, 1]	[0.6027376055717468, 0.4100407063961029, 0.4096234440803528, 0.5183083415031433, 0.2065040022134781, 0.5180760622024536, 0.5137648582458496, 0.4630027413368225, 0.2902994453907013, 0.6342892050743103]
Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction	[29, 32, 154, 16, 0, 13, 129, 11, 168, 18]	[1, 1, 1, 0, 1, 1, 1, 0, 0, 0]	[0.6970226168632507, 0.6004592180252075, 0.4408954977989197, 0.2867909073829651, 0.78116375207901, 0.4803219735622406, 0.5581291317939758, 0.3919607698917389, 0.18888375163078308, 0.2195582389831543]
Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights	[130, 199, 152, 6, 0, 244, 12, 1, 135, 218]	[0, 1, 0, 0, 1, 0, 1, 0, 0, 0]	[0.2467295080423355, 0.45783311128616333, 0.07553083449602127, 0.14236845076084137, 0.4230652153491974, 0.21792399883270264, 0.5757631063461304, 0.07959118485450745, 0.08797519654035568, 0.1337546408176422]
Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)	[67, 35, 110, 66, 100, 18, 169, 96, 98, 30]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2794405519962311, 0.6283392310142517, 0.17616814374923706, 0.15258871018886566, 0.05450085178017616, 0.33182236552238464, 0.061032019555568695, 0.06242704018950462, 0.059046242386102676, 0.13195635378360748]
Our best performing model is more accurate than all these previous models except (Bod, 2003)	[3, 11, 6, 46, 13, 119, 134, 136, 69, 107]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2895100712776184, 0.45964112877845764, 0.3258848190307617, 0.3560083508491516, 0.06446130573749542, 0.08302213251590729, 0.16583529114723206, 0.2689937353134155, 0.07624252140522003, 0.34735557436943054]
Our data source is the German NeGra tree bank (Skut et al, 1997)	[168, 20, 13, 166, 11, 25, 79, 142, 65, 47]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07397985458374023, 0.1552771031856537, 0.08964210748672485, 0.22458632290363312, 0.06780040264129639, 0.14600351452827454, 0.11076444387435913, 0.3200588822364807, 0.22962819039821625, 0.06837903708219528]
Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation	[62, 50, 79, 34, 71, 59, 23, 45, 31, 4]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07975565642118454, 0.07102604955434799, 0.07816040515899658, 0.057825662195682526, 0.10972068458795547, 0.17378145456314087, 0.08134707063436508, 0.054720230400562286, 0.08443403989076614, 0.052265673875808716]
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)	[32, 25, 100, 128, 1, 83, 15, 27, 98, 177]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6831811666488647, 0.085433728992939, 0.07664826512336731, 0.12026963382959366, 0.06306756287813187, 0.13283474743366241, 0.09341473877429962, 0.10260133445262909, 0.07728849351406097, 0.05989345163106918]
Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets	[204, 149, 81, 92, 110, 253, 192, 90, 75, 230]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.4237081706523895, 0.059478845447301865, 0.2321348339319229, 0.6207680106163025, 0.12942400574684143, 0.20596420764923096, 0.04973225295543671, 0.06955663114786148, 0.056553054600954056, 0.10593754053115845]
Our technique for setting ? m is similar to that outlined in Foster et al (2010)	[18, 105, 34, 32, 17, 30, 133, 92, 13, 29]	[1, 1, 0, 0, 1, 1, 0, 0, 0, 0]	[0.5207495093345642, 0.6833568215370178, 0.05616000294685364, 0.2732641398906708, 0.5052809715270996, 0.6035639047622681, 0.08705160766839981, 0.06122869998216629, 0.05585194751620293, 0.3488340675830841]
Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference	[1, 0, 4, 21, 130, 199, 52, 244, 6, 68]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.11868260055780411, 0.4705061912536621, 0.10351692140102386, 0.6948369145393372, 0.31767550110816956, 0.19418840110301971, 0.19400179386138916, 0.09828273952007294, 0.1613171249628067, 0.1328444927930832]
Parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008)	[0, 141, 49, 144, 89, 13, 5, 106, 21, 163]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7591835856437683, 0.19081272184848785, 0.3221592605113983, 0.12275262176990509, 0.08301247656345367, 0.06721233576536179, 0.04925663769245148, 0.2863300144672394, 0.13637806475162506, 0.10040397197008133]
Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003)	[134, 107, 133, 25, 45, 28, 39, 3, 140, 116]	[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.4190816283226013, 0.3387048840522766, 0.297323077917099, 0.16281376779079437, 0.18863581120967865, 0.09668195247650146, 0.22510944306850433, 0.05564093962311745, 0.6813488006591797, 0.05614902079105377]
Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)	[0, 58, 1, 3, 31, 5, 25, 4, 28, 113]	[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.4721885919570923, 0.4510034918785095, 0.4648348093032837, 0.18381518125534058, 0.6368262767791748, 0.21978344023227692, 0.12277102470397949, 0.2350340336561203, 0.20006109774112701, 0.3456491529941559]
Previous research in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction	[153, 15, 0, 175, 68, 152, 33, 156, 23, 177]	[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.3365533947944641, 0.27935823798179626, 0.6161789894104004, 0.6375777721405029, 0.06886489689350128, 0.14460337162017822, 0.22422806918621063, 0.04796687886118889, 0.09524279832839966, 0.05473679304122925]
Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005)	[0, 16, 1, 104, 2, 7, 6, 95, 110, 24]	[1, 0, 1, 0, 1, 0, 1, 0, 0, 0]	[0.792293906211853, 0.32494598627090454, 0.51746666431427, 0.38953453302383423, 0.5619039535522461, 0.26527053117752075, 0.5520884990692139, 0.0911187008023262, 0.3385408818721771, 0.09948918223381042]
Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser	[109, 0, 2, 104, 16, 22, 1, 20, 62, 24]	[1, 1, 1, 0, 0, 0, 1, 0, 1, 0]	[0.5265262722969055, 0.8068597912788391, 0.5654818415641785, 0.3731555640697479, 0.27286484837532043, 0.3280296325683594, 0.4827880263328552, 0.28873953223228455, 0.47763803601264954, 0.08909871429204941]
Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on	[127, 91, 220, 214, 134, 138, 192, 132, 202, 139]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11561047285795212, 0.25565436482429504, 0.20013225078582764, 0.05420771241188049, 0.3920068144798279, 0.23758959770202637, 0.07361016422510147, 0.17933793365955353, 0.053518932312726974, 0.16237130761146545]
Recent work by Das and Petrov (2011 ) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text	[10, 160, 1, 2, 24, 57, 29, 106, 21, 159]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6266764402389526, 0.5833120942115784, 0.47533008456230164, 0.41037827730178833, 0.09366440027952194, 0.1508210301399231, 0.09110938757658005, 0.06579317152500153, 0.0930371880531311, 0.12557202577590942]
Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)	[107, 20, 109, 6, 2, 26, 9, 5, 19, 28]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08986108750104904, 0.16417668759822845, 0.21358762681484222, 0.05235046148300171, 0.10475718230009079, 0.07819538563489914, 0.07619179040193558, 0.06098523736000061, 0.05463511496782303, 0.06872043758630753]
Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)	[0, 12, 1, 104, 106, 36, 18, 45, 47, 95]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.38185396790504456, 0.06174509972333908, 0.09461887925863266, 0.7618558406829834, 0.08022957295179367, 0.07453029602766037, 0.12236884981393814, 0.17877642810344696, 0.06116513907909393, 0.0657566487789154]
Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees	[51, 55, 97, 95, 21, 87, 139, 56, 70, 50]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.1210319846868515, 0.39950188994407654, 0.6288049817085266, 0.091382697224617, 0.1473093032836914, 0.095670685172081, 0.2222222238779068, 0.04916917532682419, 0.1652134507894516, 0.07676021009683609]
Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn	[165, 15, 31, 152, 3, 95, 163, 33, 127, 170]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08849187940359116, 0.17394889891147614, 0.16023030877113342, 0.28175753355026245, 0.23122861981391907, 0.0806669294834137, 0.26206350326538086, 0.05855996906757355, 0.12947970628738403, 0.08638180047273636]
Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)	[1, 130, 152, 223, 0, 273, 6, 129, 11, 131]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.08342637121677399, 0.06825301796197891, 0.04850563779473305, 0.10436184704303741, 0.3126491904258728, 0.07154596596956253, 0.09052406996488571, 0.06681723892688751, 0.1186709851026535, 0.05871998146176338]
Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns	[6, 104, 28, 44, 29, 18, 74, 16, 110, 17]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10364321619272232, 0.08139295876026154, 0.08473148196935654, 0.04894394800066948, 0.06299548596143723, 0.10007409006357193, 0.08394156396389008, 0.04776105657219887, 0.05351066216826439, 0.07052154839038849]
Sagae and Tsujii (2007)'s dependency parser, based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)	[24, 0, 109, 20, 9, 104, 1, 8, 65, 2]	[1, 1, 1, 0, 1, 1, 0, 0, 1, 0]	[0.5143632292747498, 0.8055278062820435, 0.46864181756973267, 0.2250882238149643, 0.4657979905605316, 0.40577223896980286, 0.32983365654945374, 0.20583476126194, 0.7425392866134644, 0.16495685279369354]
Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework	[70, 118, 32, 98, 64, 116, 27, 35, 65, 56]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.20740729570388794, 0.2923603653907776, 0.2928502559661865, 0.24986334145069122, 0.15608076751232147, 0.15908101201057434, 0.3578178584575653, 0.3567750155925751, 0.11299217492341995, 0.058887988328933716]
Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))	[372, 355, 14, 106, 15, 107, 93, 92, 399, 377]	[0, 0, 0, 0, 0, 0, 1, 1, 0, 0]	[0.24314439296722412, 0.19399449229240417, 0.32774731516838074, 0.24969328939914703, 0.32095834612846375, 0.08372721821069717, 0.4428020119667053, 0.6505639553070068, 0.09783639758825302, 0.053245723247528076]
Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003)	[143, 126, 136, 23, 110, 88, 1, 45, 113, 127]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.3169671297073364, 0.3316270411014557, 0.279682993888855, 0.2216227948665619, 0.22064507007598877, 0.09461329132318497, 0.05262752249836922, 0.07124185562133789, 0.7192317247390747, 0.3076704144477844]
Section 5 compares our approach too thiers in the literature, in particular that of (Miller et al., 2000)	[11, 40, 57, 66, 80, 52, 25, 33, 28, 18]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.26137295365333557, 0.4663510024547577, 0.06608730554580688, 0.26583132147789, 0.48322710394859314, 0.1818779855966568, 0.0865006297826767, 0.06700572371482849, 0.050959065556526184, 0.0706118568778038]
Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2	[105, 10, 48, 52, 94, 50, 33, 108, 51, 25]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.25963717699050903, 0.15204240381717682, 0.2722932994365692, 0.1742721050977707, 0.0874730572104454, 0.6266312003135681, 0.04764704033732414, 0.22335585951805115, 0.053750790655612946, 0.06196088716387749]
Similar to the approach in (Miller et al, 2000) we initialized the SLM statistics from the UPenn Tree bank parse trees	[60, 41, 52, 42, 33, 95, 51, 46, 1, 0]	[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]	[0.4577358365058899, 0.3831869959831238, 0.3558472692966461, 0.07080533355474472, 0.45350974798202515, 0.34138602018356323, 0.14363794028759003, 0.10977689176797867, 0.07838112860918045, 0.2302335500717163]
Similarly, (Bod, 2003) changes the way frequencies fi are counted, with a similar effect	[66, 44, 37, 22, 4, 84, 53, 115, 19, 130]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.30873578786849976, 0.056623734533786774, 0.3926553726196289, 0.15288612246513367, 0.10296838730573654, 0.11376656591892242, 0.04540899768471718, 0.05592791736125946, 0.06054026260972023, 0.059937287122011185]
Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages	[35, 192, 105, 3, 148, 17, 9, 39, 40, 0]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]	[0.5913510322570801, 0.4884824752807617, 0.3066811263561249, 0.3336747884750366, 0.22180771827697754, 0.2095240354537964, 0.18084904551506042, 0.06482639163732529, 0.055139798671007156, 0.8016296029090881]
Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details	[9, 18, 153, 60, 28, 103, 175, 151, 138, 196]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7398744225502014, 0.260341078042984, 0.06479763239622116, 0.2188861072063446, 0.07288637012243271, 0.17872515320777893, 0.05111320689320564, 0.3834017515182495, 0.06114303320646286, 0.08357677608728409]
Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases	[94, 19, 39, 7, 110, 49, 106, 14, 4, 99]	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	[0.21796749532222748, 0.3695933222770691, 0.051968030631542206, 0.2159508913755417, 0.267564058303833, 0.19439226388931274, 0.12033294886350632, 0.497361958026886, 0.22561144828796387, 0.21367241442203522]
Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)	[54, 13, 97, 134, 154, 83, 74, 35, 121, 106]	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.07072344422340393, 0.22967742383480072, 0.1628894805908203, 0.4029266834259033, 0.04778023436665535, 0.17872026562690735, 0.09891746938228607, 0.09949582815170288, 0.045326340943574905, 0.055907268077135086]
Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features	[18, 113, 1, 0, 6, 158, 96, 20, 112, 118]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6519967317581177, 0.7260134220123291, 0.22544068098068237, 0.6494348049163818, 0.16619250178337097, 0.08450612425804138, 0.12934648990631104, 0.12383083999156952, 0.09298857301473618, 0.14039702713489532]
Subramanya et al's model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers	[1, 0, 6, 18, 8, 158, 33, 40, 9, 23]	[0, 1, 1, 1, 0, 0, 0, 1, 0, 0]	[0.14946414530277252, 0.7717084884643555, 0.6527918577194214, 0.6584405899047852, 0.09979712963104248, 0.1068611815571785, 0.11542003601789474, 0.4803953468799591, 0.18119142949581146, 0.097603440284729]
System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))	[0, 12, 5, 53, 54, 88, 6, 1, 49, 9]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.21517857909202576, 0.2525731027126312, 0.053927697241306305, 0.06686654686927795, 0.05012736842036247, 0.06222144886851311, 0.1113862618803978, 0.08187709003686905, 0.06068303436040878, 0.35657596588134766]
Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])	[91, 100, 42, 13, 22, 124, 35, 110, 41, 15]	[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.4713049530982971, 0.11053489148616791, 0.40338799357414246, 0.08342863619327545, 0.06185665726661682, 0.05394027754664421, 0.27866458892822266, 0.062053218483924866, 0.16204088926315308, 0.060624800622463226]
Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006)	[4, 58, 104, 45, 56, 0, 1, 13, 37, 36]	[1, 1, 1, 0, 1, 0, 0, 1, 0, 0]	[0.5859960913658142, 0.6844165921211243, 0.7256483435630798, 0.20374585688114166, 0.5639132857322693, 0.3332649767398834, 0.07894424349069595, 0.7713251709938049, 0.13679327070713043, 0.08023373037576675]
Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable	[84, 51, 46, 42, 43, 10, 135, 103, 39, 47]	[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]	[0.643866240978241, 0.3773798644542694, 0.13256137073040009, 0.6851722002029419, 0.3426479697227478, 0.17355382442474365, 0.04811033979058266, 0.06170615926384926, 0.08491642028093338, 0.05666482448577881]
The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)	[126, 9, 16, 21, 18, 28, 17, 5, 13, 58]	[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]	[0.6518046259880066, 0.6255596280097961, 0.2885110676288605, 0.46838805079460144, 0.14380890130996704, 0.3401162028312683, 0.06171536445617676, 0.6420349478721619, 0.07334289699792862, 0.2956150770187378]
The annotation scheme (Skut et al, 1997) is modeled to a certain extent on that of the Penn Treebank (Marcuset al, 1993), with crucial differences	[160, 174, 15, 45, 127, 22, 159, 31, 0, 2]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.7817614078521729, 0.1435225009918213, 0.12994377315044403, 0.22959281504154205, 0.14634394645690918, 0.8074087500572205, 0.31047412753105164, 0.15917682647705078, 0.27758362889289856, 0.10376137495040894]
The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression	[65, 161, 2, 64, 48, 183, 155, 60, 50, 51]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.06693775951862335, 0.27587616443634033, 0.14673413336277008, 0.10471247136592865, 0.0572945810854435, 0.18129375576972961, 0.24102477729320526, 0.05184301361441612, 0.06448239833116531, 0.056273460388183594]
The approach that Vijay-Shanker et al (1987) and Weir (1988) take, elaborated on by Becker et al (1992), is to identify a very general class of formalisms, which they call linear context free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared	[2, 92, 207, 4, 118, 14, 222, 1, 29, 117]	[0, 1, 1, 1, 1, 1, 0, 0, 0, 1]	[0.3851950466632843, 0.740727961063385, 0.5966414213180542, 0.6581125855445862, 0.4942287504673004, 0.58384770154953, 0.1073654294013977, 0.34327390789985657, 0.05973782390356064, 0.6146302819252014]
The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua	[1, 244, 223, 45, 52, 131, 0, 266, 129, 10]	[0, 0, 0, 1, 0, 0, 1, 0, 0, 0]	[0.09790327399969101, 0.13266609609127045, 0.16872309148311615, 0.45799100399017334, 0.18713879585266113, 0.27444347739219666, 0.5735377669334412, 0.1286308765411377, 0.09039641171693802, 0.07157815992832184]
The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)	[14, 38, 11, 26, 34, 33, 90, 12, 27, 36]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2046300172805786, 0.056079935282468796, 0.1522955298423767, 0.05889628455042839, 0.10359354317188263, 0.07427370548248291, 0.10297806560993195, 0.07286597788333893, 0.06661710143089294, 0.07055003196001053]
The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major differences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller	[11, 60, 78, 18, 36, 61, 52, 33, 24, 43]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2345355898141861, 0.25034859776496887, 0.14679250121116638, 0.05821343511343002, 0.050742022693157196, 0.09034283459186554, 0.052540089935064316, 0.05245213583111763, 0.05457407981157303, 0.05951019003987312]
The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008)	[8, 85, 140, 101, 44, 5, 14, 135, 53, 136]	[0, 1, 0, 1, 0, 0, 0, 1, 0, 1]	[0.2494150996208191, 0.5063446760177612, 0.10683105885982513, 0.7095223665237427, 0.38930171728134155, 0.18957030773162842, 0.21982745826244354, 0.45318758487701416, 0.07905064523220062, 0.4070570170879364]
The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)	[6, 39, 8, 140, 151, 64, 132, 152, 150, 66]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.5920237898826599, 0.7693634033203125, 0.2845350503921509, 0.6393664479255676, 0.17995516955852509, 0.08119796216487885, 0.16695624589920044, 0.0676475390791893, 0.12107425928115845, 0.07353535294532776]
The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)	[22, 39, 40, 77, 78, 155, 38, 98, 4, 37]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2372240275144577, 0.2075868397951126, 0.11600670218467712, 0.33401554822921753, 0.17583929002285004, 0.16148151457309723, 0.14812612533569336, 0.11006420105695724, 0.10964129120111465, 0.10180296003818512]
The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows	[1, 102, 123, 5, 18, 113, 132, 17, 187, 155]	[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]	[0.2111196666955948, 0.5307945013046265, 0.5015532374382019, 0.174606591463089, 0.08709153532981873, 0.060129038989543915, 0.17111998796463013, 0.1315307319164276, 0.06833773106336594, 0.10649673640727997]
The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and co reference in formation (Negra) (Skut et al, 1997)	[152, 149, 168, 14, 166, 137, 151, 39, 84, 12]	[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]	[0.05922054499387741, 0.054137226194143295, 0.23919931054115295, 0.47366949915885925, 0.5659332275390625, 0.11326580494642258, 0.26076629757881165, 0.04989820718765259, 0.06963981688022614, 0.36785629391670227]
The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions	[5, 1, 44, 38, 152, 49, 17, 91, 172, 129]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.12967464327812195, 0.14878243207931519, 0.11852733045816422, 0.05250931158661842, 0.26471146941185, 0.04641475901007652, 0.04738980531692505, 0.0621311217546463, 0.0466693751513958, 0.04488677904009819]
The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)	[131, 199, 108, 133, 69, 130, 47, 24, 0, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.1157946065068245, 0.36492985486984253, 0.0537552647292614, 0.06561475992202759, 0.07829070836305618, 0.10874683409929276, 0.06994447857141495, 0.07894556224346161, 0.5358261466026306, 0.07625839859247208]
The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account (McCarthy et al, 2004)	[8, 14, 2, 107, 1, 25, 48, 12, 108, 15]	[1, 1, 1, 1, 0, 0, 1, 0, 1, 0]	[0.8170369267463684, 0.7007721066474915, 0.6927208304405212, 0.7364673018455505, 0.38047605752944946, 0.20102408528327942, 0.6184541583061218, 0.23512278497219086, 0.6716681122779846, 0.18286307156085968]
The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems	[8, 1, 25, 15, 13, 153, 40, 24, 164, 172]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7749439477920532, 0.15245628356933594, 0.10243482142686844, 0.09090177714824677, 0.22304077446460724, 0.0726202130317688, 0.2187490612268448, 0.05495547875761986, 0.16344523429870605, 0.12612615525722504]
The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)	[4, 18, 13, 37, 66, 41, 76, 43, 20, 106]	[1, 0, 1, 0, 1, 0, 0, 0, 1, 0]	[0.6915162801742554, 0.10549934953451157, 0.43694040179252625, 0.18669770658016205, 0.5412521362304688, 0.14736458659172058, 0.15224869549274445, 0.07417917251586914, 0.5745298266410828, 0.07316066324710846]
The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run	[223, 0, 1, 203, 244, 266, 45, 32, 256, 129]	[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]	[0.8018863201141357, 0.6096485257148743, 0.13142310082912445, 0.19002565741539001, 0.213042750954628, 0.12210594117641449, 0.24402621388435364, 0.20294195413589478, 0.437835693359375, 0.05582113564014435]
The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)	[176, 103, 162, 10, 9, 174, 8, 178, 5, 121]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.11242477595806122, 0.0988539606332779, 0.1422959864139557, 0.0726255401968956, 0.057675957679748535, 0.04763747379183769, 0.060427289456129074, 0.23940949141979218, 0.053781334310770035, 0.05253354087471962]
The method is described in (McCarthy et al, 2004), which we summarise here	[64, 49, 133, 137, 29, 109, 80, 79, 41, 32]	[1, 1, 1, 0, 1, 1, 0, 0, 0, 0]	[0.7760521173477173, 0.45596516132354736, 0.547673225402832, 0.10642523318529129, 0.4130781590938568, 0.42717647552490234, 0.06694062054157257, 0.05208638310432434, 0.16996680200099945, 0.15107882022857666]
The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token	[133, 89, 75, 87, 1, 51, 112, 141, 18, 83]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6705251336097717, 0.3763546943664551, 0.2703687846660614, 0.2916543781757355, 0.10258549451828003, 0.239139586687088, 0.38324880599975586, 0.14980106055736542, 0.05640851706266403, 0.08080516010522842]
The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Nova? k, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)	[109, 0, 1, 20, 16, 23, 104, 7, 24, 107]	[1, 1, 1, 1, 0, 0, 1, 0, 0, 0]	[0.597217321395874, 0.7740991115570068, 0.5461528897285461, 0.4356888234615326, 0.3425270617008209, 0.36863934993743896, 0.4954398274421692, 0.2637823224067688, 0.3385814428329468, 0.20456135272979736]
The most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task	[3, 52, 158, 51, 21, 94, 125, 47, 65, 19]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.24261949956417084, 0.38164278864860535, 0.26742544770240784, 0.10461899638175964, 0.18121235072612762, 0.07224097102880478, 0.06010092422366142, 0.2246934473514557, 0.10425130277872086, 0.05861155688762665]
The n-best lists were provided by Brian Roark (Roark, 2001)	[355, 372, 76, 91, 92, 39, 61, 96, 377, 40]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.1824396848678589, 0.1829715371131897, 0.48366162180900574, 0.05373820289969444, 0.09078305959701538, 0.23356524109840393, 0.06844210624694824, 0.05348958447575569, 0.05511227622628212, 0.05339083448052406]
The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)	[47, 70, 17, 7, 35, 8, 108, 155, 124, 165]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.2630673944950104, 0.40127119421958923, 0.08917395770549774, 0.1705716997385025, 0.42427390813827515, 0.2647593915462494, 0.35166019201278687, 0.1805156171321869, 0.2961154282093048, 0.04089123010635376]
The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights	[137, 268, 115, 297, 32, 223, 385, 133, 136, 134]	[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.612716019153595, 0.7558044195175171, 0.2817326486110687, 0.23755879700183868, 0.18401864171028137, 0.7323358654975891, 0.2876529097557068, 0.07858996838331223, 0.3645072877407074, 0.12721899151802063]
The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents	[98, 92, 171, 33, 174, 129, 91, 133, 188, 27]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.6797097325325012, 0.48792219161987305, 0.7289711833000183, 0.30220526456832886, 0.22104471921920776, 0.5190066695213318, 0.20679867267608643, 0.10701585561037064, 0.2840629816055298, 0.1863943189382553]
The parsing models we present are trained and tested on the NEGRA corpus (Skut et al, 1997), a hand parsed corpus of German newspaper text containing approximately 20,000 sentences	[168, 151, 144, 141, 167, 72, 137, 39, 143, 12]	[0, 0, 1, 1, 1, 0, 0, 0, 0, 0]	[0.3182663917541504, 0.27133068442344666, 0.4293935000896454, 0.4584326446056366, 0.43376827239990234, 0.1358698010444641, 0.09111485630273819, 0.06191135197877884, 0.050304293632507324, 0.05020257085561752]
The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank	[277, 20, 32, 4, 364, 367, 10, 377, 25, 347]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	[0.11926335096359253, 0.21592378616333008, 0.19475749135017395, 0.34984076023101807, 0.12647832930088043, 0.2593550682067871, 0.34984076023101807, 0.09746911376714706, 0.05019108206033707, 0.41971760988235474]
The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al, 1997), a syntactically annotated corpus for German	[4, 166, 168, 31, 151, 72, 143, 144, 159, 14]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5695381760597229, 0.7459650039672852, 0.26904481649398804, 0.11342747509479523, 0.20187373459339142, 0.3192720413208008, 0.12850485742092133, 0.2977856397628784, 0.07523747533559799, 0.3331232964992523]
The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)	[0, 7, 1, 24, 62, 9, 142, 53, 20, 25]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6001771688461304, 0.08117544651031494, 0.2585286796092987, 0.3116917610168457, 0.21782757341861725, 0.07849200814962387, 0.22211016714572906, 0.21983356773853302, 0.0639759972691536, 0.1239156574010849]
The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005)	[79, 0, 95, 40, 104, 24, 15, 20, 2, 110]	[0, 1, 0, 1, 1, 0, 0, 0, 0, 0]	[0.3773467242717743, 0.7648032307624817, 0.12725946307182312, 0.4249803125858307, 0.4456465244293213, 0.273926317691803, 0.13654227554798126, 0.25047722458839417, 0.31944817304611206, 0.1128460168838501]
The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)	[38, 40, 90, 171, 62, 151, 78, 26, 140, 12]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.1529472917318344, 0.3096838593482971, 0.5282045006752014, 0.3731883466243744, 0.2582971751689911, 0.28617241978645325, 0.3268122971057892, 0.3669194281101227, 0.18267275393009186, 0.14498312771320343]
The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)	[108, 10, 41, 50, 48, 9, 18, 4, 33, 51]	[1, 0, 0, 1, 0, 1, 0, 0, 0, 0]	[0.6314544677734375, 0.1381499469280243, 0.33644869923591614, 0.5473529696464539, 0.22133786976337433, 0.46696707606315613, 0.20725002884864807, 0.23695465922355652, 0.046601247042417526, 0.04476560652256012]
The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase	[85, 88, 25, 55, 100, 79, 117, 28, 124, 97]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3995382785797119, 0.38350075483322144, 0.09680578857660294, 0.07761835306882858, 0.24187611043453217, 0.11629028618335724, 0.11146124452352524, 0.06268712878227234, 0.04827507585287094, 0.05209815874695778]
The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.	[20, 21, 16, 35, 57, 7, 3, 41, 18, 2]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.46969887614250183, 0.5926174521446228, 0.2866189479827881, 0.48436930775642395, 0.18442778289318085, 0.07511371374130249, 0.17451736330986023, 0.1115911677479744, 0.06560121476650238, 0.25833359360694885]
The syntactic model in (Miller et al, 2000) is similar to Collins', but does not use features like subcat frames and distance measures	[28, 60, 106, 3, 42, 82, 26, 33, 1, 0]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.5010505318641663, 0.5208327770233154, 0.38232484459877014, 0.057975128293037415, 0.07693581283092499, 0.07774052768945694, 0.5201370120048523, 0.07777348905801773, 0.06695165485143661, 0.3272462487220764]
The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime	[254, 199, 12, 244, 45, 223, 68, 27, 200, 130]	[0, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.11413288116455078, 0.5439638495445251, 0.6793796420097351, 0.1626216322183609, 0.4751085638999939, 0.1147293969988823, 0.2783624231815338, 0.34265393018722534, 0.06358186900615692, 0.13880546391010284]
There are many (structural) mildly context sensitive grammar formalisms ,e.g .mcfg ,lcfrs, mg, and they have been shown to be equivalent (Vijay-Shanker et al., 1987)	[214, 119, 4, 13, 230, 62, 139, 34, 2, 215]	[1, 0, 1, 0, 0, 1, 0, 0, 0, 0]	[0.6215198040008545, 0.23033304512500763, 0.4917752146720886, 0.25113874673843384, 0.058717451989650726, 0.709345817565918, 0.06765735894441605, 0.29792776703834534, 0.17592355608940125, 0.2299993485212326]
These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011)	[10, 1, 2, 160, 21, 41, 110, 106, 72, 16]	[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]	[0.7614569067955017, 0.6248440146446228, 0.47794806957244873, 0.6036250591278076, 0.1797318309545517, 0.4054807424545288, 0.08476223796606064, 0.06609135121107101, 0.07111068814992905, 0.10501524806022644]
They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al, 1987)	[118, 92, 207, 2, 209, 134, 116, 60, 34, 50]	[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]	[0.5654577612876892, 0.7468159794807434, 0.6364489197731018, 0.1766878068447113, 0.04656235873699188, 0.6923677325248718, 0.3314415514469147, 0.05338773876428604, 0.3244289755821228, 0.25479015707969666]
This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)	[8, 1, 161, 74, 13, 86, 85, 164, 63, 159]	[1, 1, 1, 0, 0, 1, 1, 0, 0, 0]	[0.6420417428016663, 0.5700799822807312, 0.47639667987823486, 0.13336017727851868, 0.3656243085861206, 0.44881531596183777, 0.6025390625, 0.07128103822469711, 0.06810810416936874, 0.22672313451766968]
This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization	[16, 27, 139, 47, 45, 55, 118, 32, 2, 106]	[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.7978329658508301, 0.6299459338188171, 0.2209264189004898, 0.054786138236522675, 0.31538113951683044, 0.3216055929660797, 0.3855912983417511, 0.49947234988212585, 0.22124694287776947, 0.05515275150537491]
This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)	[7, 39, 3, 32, 169, 29, 28, 36, 222, 147]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.23452860116958618, 0.18042007088661194, 0.16219361126422882, 0.06152479723095894, 0.07353019714355469, 0.047946203500032425, 0.04334321245551109, 0.06804439425468445, 0.10116128623485565, 0.0655883252620697]
This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well	[100, 46, 104, 102, 116, 105, 98, 26, 69, 78]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.19161555171012878, 0.184429332613945, 0.24054130911827087, 0.1820054054260254, 0.22867022454738617, 0.28713104128837585, 0.18396523594856262, 0.13478825986385345, 0.09544415026903152, 0.05341525003314018]
This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs	[23, 17, 172, 22, 186, 33, 32, 39, 196, 20]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.1541207879781723, 0.08299518376588821, 0.157156839966774, 0.20152340829372406, 0.26994073390960693, 0.09167209267616272, 0.4396775960922241, 0.06819791346788406, 0.06434060633182526, 0.2257656753063202]
This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)	[61, 58, 101, 22, 21, 95, 38, 2, 63, 0]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33715060353279114, 0.2785404324531555, 0.10853781551122665, 0.2916160821914673, 0.08894119411706924, 0.058081552386283875, 0.05473494157195091, 0.0694766566157341, 0.06099243089556694, 0.19357609748840332]
This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al, 1987)	[118, 2, 92, 207, 3, 231, 116, 230, 203, 15]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.43731802701950073, 0.41861051321029663, 0.6545549631118774, 0.6054884791374207, 0.13800539076328278, 0.11104646325111389, 0.34333983063697815, 0.09923785924911499, 0.05363912880420685, 0.10959754139184952]
This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)	[1, 24, 10, 161, 160, 0, 6, 158, 18, 112]	[1, 1, 1, 0, 1, 1, 0, 0, 0, 0]	[0.6450045704841614, 0.5376384258270264, 0.4570160508155823, 0.24565497040748596, 0.500070333480835, 0.6374714374542236, 0.30464085936546326, 0.16649261116981506, 0.1148669570684433, 0.09483742713928223]
This result is only slightly higher than the highest reported result for this test-set, Bod's (.907) (Bod,2003)	[133, 38, 87, 20, 134, 105, 42, 86, 43, 140]	[1, 0, 1, 0, 0, 0, 0, 0, 1, 1]	[0.6604093313217163, 0.3856479525566101, 0.43671995401382446, 0.055498965084552765, 0.18093864619731903, 0.18148061633110046, 0.21057763695716858, 0.06299188733100891, 0.757357656955719, 0.7106640934944153]
This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)	[36, 68, 90, 72, 91, 35, 116, 28, 80, 53]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.5354068875312805, 0.6148256063461304, 0.41143545508384705, 0.454067200422287, 0.27464255690574646, 0.3462417423725128, 0.35944196581840515, 0.0759977251291275, 0.3555189073085785, 0.32933279871940613]
This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al, 1997)	[168, 12, 137, 38, 72, 20, 166, 148, 14, 31]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6277791261672974, 0.38237765431404114, 0.2962630093097687, 0.09529479593038559, 0.3632909655570984, 0.20967914164066315, 0.38912850618362427, 0.25287991762161255, 0.20034877955913544, 0.04968012869358063]
This was used to create a KenLM (Heafield, 2011)	[0, 1, 281, 68, 25, 130, 11, 45, 6, 7]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.35780975222587585, 0.08319523930549622, 0.14980998635292053, 0.21058546006679535, 0.03929468244314194, 0.04541927948594093, 0.05633315071463585, 0.04661555960774422, 0.03956029936671257, 0.048231273889541626]
Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states	[0, 135, 1, 131, 11, 152, 142, 136, 227, 129]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.7644100785255432, 0.42114749550819397, 0.23124383389949799, 0.3109811246395111, 0.5953933596611023, 0.154596209526062, 0.20720160007476807, 0.10784749686717987, 0.10868124663829803, 0.058921847492456436]
To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments	[94, 40, 89, 65, 0, 153, 1, 146, 149, 145]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.36474427580833435, 0.3152499198913574, 0.16694778203964233, 0.37948352098464966, 0.09612338989973068, 0.10227478295564651, 0.0887141078710556, 0.13167080283164978, 0.06870497018098831, 0.3864932954311371]
To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments	[3, 9, 21, 0, 48, 25, 7, 1, 117, 26]	[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.17619305849075317, 0.5901460647583008, 0.1504809856414795, 0.6105563044548035, 0.06175724416971207, 0.07217433303594589, 0.1936456561088562, 0.12402735650539398, 0.18450385332107544, 0.0741199180483818]
To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses	[291, 282, 367, 267, 270, 227, 402, 76, 260, 376]	[1, 1, 0, 1, 1, 0, 1, 0, 1, 0]	[0.5928165316581726, 0.5302989482879639, 0.09398403018712997, 0.5974306464195251, 0.6051719188690186, 0.14741812646389008, 0.4991711974143982, 0.28398361802101135, 0.49984878301620483, 0.2062091827392578]
To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time	[95, 20, 37, 23, 104, 91, 7, 24, 9, 0]	[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]	[0.3449699878692627, 0.3953779637813568, 0.12672121822834015, 0.3301258683204651, 0.412026584148407, 0.39021167159080505, 0.2599457800388336, 0.1604469269514084, 0.09281696379184723, 0.7642446756362915]
To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics	[3, 192, 18, 9, 169, 153, 29, 24, 25, 27]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3825562298297882, 0.41304337978363037, 0.082499660551548, 0.32894405722618103, 0.08481644093990326, 0.2239052802324295, 0.37019744515419006, 0.09331310540437698, 0.09655384719371796, 0.05914000794291496]
Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word	[231, 215, 80, 224, 183, 145, 108, 54, 263, 100]	[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]	[0.5797309875488281, 0.5435250997543335, 0.409493625164032, 0.5695480108261108, 0.15697677433490753, 0.26155635714530945, 0.05173109099268913, 0.060705073177814484, 0.058613963425159454, 0.05166332423686981]
Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)	[9, 11, 6, 12, 13, 27, 120, 40, 10, 26]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7919750213623047, 0.5291462540626526, 0.14762380719184875, 0.09814848005771637, 0.2145606279373169, 0.3254612684249878, 0.0869302824139595, 0.06384551525115967, 0.08960860967636108, 0.08630037307739258]
WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)	[138, 24, 45, 149, 139, 133, 140, 137, 171, 141]	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	[0.22353149950504303, 0.16433902084827423, 0.06337243318557739, 0.0644899383187294, 0.15705092251300812, 0.051991093903779984, 0.4990490972995758, 0.04933290556073189, 0.050554778426885605, 0.0745394229888916]
We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English	[17, 5, 104, 4, 95, 0, 24, 15, 57, 18]	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.2381119579076767, 0.24354569613933563, 0.3633527159690857, 0.1976388841867447, 0.07051610946655273, 0.7398166060447693, 0.09080888330936432, 0.08257124572992325, 0.07410585135221481, 0.07425544410943985]
We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)	[123, 38, 68, 118, 130, 40, 106, 170, 0, 64]	[1, 0, 1, 1, 1, 1, 0, 0, 0, 0]	[0.7791327238082886, 0.24503089487552643, 0.5178094506263733, 0.5999576449394226, 0.7766045928001404, 0.6392391920089722, 0.3886550962924957, 0.05421879142522812, 0.1368965059518814, 0.09705408662557602]
We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis	[165, 91, 117, 88, 90, 92, 18, 95, 30, 34]	[0, 1, 1, 1, 1, 0, 0, 0, 1, 0]	[0.32281529903411865, 0.6694132089614868, 0.412718266248703, 0.5311222076416016, 0.41632041335105896, 0.19767959415912628, 0.04844406992197037, 0.04740575700998306, 0.4210263788700104, 0.048407506197690964]
We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006)	[39, 140, 10, 156, 150, 7, 5, 44, 95, 167]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7945643067359924, 0.7661356329917908, 0.052114337682724, 0.10416202992200851, 0.06767462939023972, 0.09109888225793839, 0.06421514600515366, 0.05829198285937309, 0.07152757048606873, 0.08986838161945343]
We approximated the most probable parse as follows (following (Bod, 2003))	[26, 135, 116, 97, 28, 71, 70, 90, 68, 63]	[1, 0, 1, 1, 1, 1, 1, 1, 0, 0]	[0.4287312924861908, 0.3560832738876343, 0.4867000877857208, 0.537265419960022, 0.44886940717697144, 0.6014584302902222, 0.7686595916748047, 0.468453586101532, 0.19727440178394318, 0.16205865144729614]
We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English	[6, 8, 12, 29, 31, 38, 18, 64, 151, 132]	[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]	[0.6980234980583191, 0.40599215030670166, 0.20697182416915894, 0.5288938283920288, 0.3779120445251465, 0.06551612913608551, 0.065106600522995, 0.1418052762746811, 0.3905985355377197, 0.08091078698635101]
We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al, 1987)	[221, 122, 119, 210, 226, 146, 2, 229, 207, 11]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4359806478023529, 0.062227681279182434, 0.14998963475227356, 0.0556957945227623, 0.08180590718984604, 0.059896256774663925, 0.0502086877822876, 0.05378817766904831, 0.08109666407108307, 0.063811294734478]
We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al (1987)	[221, 122, 119, 210, 226, 146, 2, 229, 207, 11]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3802572190761566, 0.06521087139844894, 0.15150363743305206, 0.0546322800219059, 0.07508258521556854, 0.05713948979973793, 0.05156923457980156, 0.054366789758205414, 0.07822129130363464, 0.06804066896438599]
We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)	[28, 30, 4, 10, 18, 169, 97, 100, 166, 3]	[0, 1, 0, 1, 0, 0, 0, 0, 1, 0]	[0.24830538034439087, 0.6533938050270081, 0.30159202218055725, 0.4303928017616272, 0.05312595143914223, 0.08391287177801132, 0.10704629868268967, 0.056720633059740067, 0.6703625321388245, 0.3220421373844147]
We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models	[7, 40, 0, 97, 62, 44, 31, 3, 114, 129]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3131493926048279, 0.09850626438856125, 0.3032514154911041, 0.17458470165729523, 0.10481865704059601, 0.2942931652069092, 0.21323458850383759, 0.061398983001708984, 0.05658611282706261, 0.10870432108640671]
We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm	[58, 105, 81, 5, 0, 4, 3, 48, 31, 96]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4089198410511017, 0.17937035858631134, 0.3916531205177307, 0.11543536931276321, 0.18867909908294678, 0.04659903421998024, 0.04713991656899452, 0.09343036264181137, 0.10193862020969391, 0.23111863434314728]
We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results	[5, 4, 22, 105, 0, 31, 1, 3, 10, 48]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.2146981954574585, 0.16507212817668915, 0.22786511480808258, 0.10772892087697983, 0.29668867588043213, 0.2219996452331543, 0.12401527911424637, 0.09162689745426178, 0.054898422211408615, 0.057049065828323364]
We have proposed a method for unsupervised POS tagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)	[4, 23, 93, 147, 0, 5, 85, 94, 1, 158]	[0, 1, 0, 1, 1, 0, 1, 0, 0, 0]	[0.23792463541030884, 0.5658214688301086, 0.34157201647758484, 0.4356871545314789, 0.7459530830383301, 0.3045044541358948, 0.6093606352806091, 0.3998650312423706, 0.056127529591321945, 0.1478133350610733]
We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search	[347, 364, 32, 5, 11, 367, 303, 302, 20, 4]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.41634130477905273, 0.16222615540027618, 0.28220483660697937, 0.3614526093006134, 0.3614526093006134, 0.2714540958404541, 0.11163756996393204, 0.08018315583467484, 0.21422326564788818, 0.077963687479496]
We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time	[107, 41, 245, 25, 4, 10, 251, 401, 329, 297]	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	[0.15472450852394104, 0.1428196132183075, 0.25522181391716003, 0.3245027959346771, 0.221684530377388, 0.221684530377388, 0.20099927484989166, 0.11212090402841568, 0.6073994636535645, 0.2167433202266693]
We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3)	[39, 140, 70, 50, 154, 31, 82, 71, 54, 68]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7976581454277039, 0.6690807342529297, 0.06232895702123642, 0.08335620909929276, 0.08062971383333206, 0.0678262785077095, 0.11020001769065857, 0.053250499069690704, 0.0791735053062439, 0.06724238395690918]
We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r? h, where r is the original label and h is the label of the original head in the non-projective dependency graph	[49, 37, 51, 50, 36, 41, 40, 22, 21, 45]	[1, 1, 1, 1, 1, 1, 1, 0, 1, 0]	[0.7617429494857788, 0.6405256390571594, 0.7610414624214172, 0.7791486382484436, 0.6318402290344238, 0.47194716334342957, 0.5607886910438538, 0.15812605619430542, 0.48000919818878174, 0.09289440512657166]
We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)	[18, 58, 13, 21, 126, 15, 47, 142, 17, 20]	[0, 1, 0, 1, 1, 0, 0, 0, 0, 1]	[0.14509111642837524, 0.5536031126976013, 0.12611444294452667, 0.411266565322876, 0.4495471715927124, 0.18276576697826385, 0.08111635595560074, 0.2172837108373642, 0.0911484882235527, 0.42565882205963135]
We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters	[3, 18, 17, 21, 113, 96, 132, 161, 122, 148]	[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.6001306176185608, 0.6369235515594482, 0.35701805353164673, 0.07592164725065231, 0.4804422855377197, 0.05706708878278732, 0.06450743973255157, 0.05696018785238266, 0.06759901344776154, 0.1221126988530159]
We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus	[5, 1, 91, 17, 133, 32, 174, 176, 47, 9]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.12020983546972275, 0.11256368458271027, 0.5753223299980164, 0.13763955235481262, 0.0626305639743805, 0.08422276377677917, 0.07939820736646652, 0.07002034038305283, 0.14569216966629028, 0.1256428360939026]
We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each senses' Sx, following the approach of McCarthy et al (2004)	[48, 188, 165, 49, 189, 73, 168, 171, 74, 105]	[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]	[0.08226931840181351, 0.46572136878967285, 0.21683765947818756, 0.28682535886764526, 0.09887304157018661, 0.08477215468883514, 0.11441656947135925, 0.4053288996219635, 0.1330932080745697, 0.13621951639652252]
We use Collins and Singer (1999) for our exact specification of Yarowsky	[31, 46, 80, 154, 230, 68, 156, 5, 18, 8]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.09552299976348877, 0.07647647708654404, 0.460226446390152, 0.06645847111940384, 0.06580372899770737, 0.15416891872882843, 0.3318266272544861, 0.07553701102733612, 0.2432989776134491, 0.07436001300811768]
We use other WSM settings following Mitchell and Lapata (2008)	[140, 53, 100, 133, 36, 7, 152, 52, 34, 25]	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	[0.3747396171092987, 0.27714139223098755, 0.4262082278728485, 0.04648531973361969, 0.050366275012493134, 0.3301514685153961, 0.05668306723237038, 0.04897290840744972, 0.27084600925445557, 0.049882397055625916]
We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)	[182, 190, 168, 2, 174, 53, 66, 74, 49, 81]	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.3215250074863434, 0.680618405342102, 0.33388474583625793, 0.18737342953681946, 0.36684173345565796, 0.2278556525707245, 0.15030530095100403, 0.07550164312124252, 0.047996848821640015, 0.07164309918880463]
We use the feature templates the same as Jiang et al, (2008) to extract features form E model	[25, 33, 34, 96, 36, 46, 14, 58, 113, 132]	[0, 1, 1, 0, 1, 0, 0, 0, 0, 0]	[0.06514792144298553, 0.523106038570404, 0.6102756261825562, 0.1333654224872589, 0.6666910648345947, 0.3757072985172272, 0.07779265940189362, 0.15568575263023376, 0.2922343313694, 0.25058624148368835]
We use the same method described in (Koehn and Monz, 2006) to perform the significance test	[52, 49, 102, 113, 107, 50, 48, 57, 47, 106]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6052283048629761, 0.8018410205841064, 0.37638920545578003, 0.31956177949905396, 0.3860003650188446, 0.12916991114616394, 0.08347892016172409, 0.3993722200393677, 0.07625164091587067, 0.05871693789958954]
We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments	[199, 0, 244, 1, 152, 12, 27, 130, 6, 239]	[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]	[0.574142575263977, 0.3580934405326843, 0.16749587655067444, 0.06688854843378067, 0.08330146223306656, 0.7284836173057556, 0.1973283588886261, 0.10483837127685547, 0.23675894737243652, 0.22126321494579315]
We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)	[199, 0, 1, 52, 223, 254, 253, 200, 68, 244]	[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6554350852966309, 0.463441401720047, 0.08107362687587738, 0.2565596401691437, 0.09454254806041718, 0.06676631420850754, 0.15444140136241913, 0.19298319518566132, 0.22070704400539398, 0.08252036571502686]
We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)	[120, 40, 74, 21, 16, 58, 81, 15, 18, 84]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.07471215724945068, 0.25474002957344055, 0.09150108695030212, 0.07029083371162415, 0.08644778281450272, 0.05862890183925629, 0.07422535866498947, 0.07838787883520126, 0.04983382299542427, 0.07135482877492905]
Whereas Miller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance	[2, 0, 34, 52, 55, 26, 41, 61, 51, 104]	[0, 1, 0, 0, 0, 1, 0, 0, 0, 0]	[0.24681410193443298, 0.6449218392372131, 0.3621009290218353, 0.10575736314058304, 0.10153473168611526, 0.5513851642608643, 0.09917398542165756, 0.05126882344484329, 0.07761788368225098, 0.06818532198667526]
While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)	[222, 38, 130, 92, 7, 22, 95, 201, 232, 32]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.10185348242521286, 0.06995183229446411, 0.33234596252441406, 0.10123124718666077, 0.06843569129705429, 0.15902476012706757, 0.07698804140090942, 0.07512959092855453, 0.05791797488927841, 0.06082630902528763]
While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations	[87, 51, 149, 11, 91, 42, 138, 60, 43, 1]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.5440298318862915, 0.07653877139091492, 0.07731037586927414, 0.2068517953157425, 0.1672230064868927, 0.04960665851831436, 0.32530277967453003, 0.050247494131326675, 0.20858807861804962, 0.12377841770648956]
Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available	[15, 3, 154, 17, 25, 180, 10, 82, 22, 12]	[1, 1, 1, 1, 0, 1, 1, 0, 1, 1]	[0.8155338168144226, 0.7192422747612, 0.6972240209579468, 0.527931272983551, 0.3051210641860962, 0.4311024248600006, 0.6981105804443359, 0.24085813760757446, 0.43515169620513916, 0.463714599609375]
With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)	[21, 4, 236, 37, 12, 81, 103, 1, 52, 274]	[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]	[0.1349084973335266, 0.40605244040489197, 0.06712530553340912, 0.05656758323311806, 0.653404712677002, 0.2584492564201355, 0.27442896366119385, 0.1065138727426529, 0.3223576545715332, 0.09905794262886047]
Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models	[62, 24, 1, 143, 89, 25, 26, 9, 31, 100]	[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]	[0.4906579256057739, 0.5179703235626221, 0.2951774299144745, 0.23919007182121277, 0.06552619487047195, 0.22026649117469788, 0.6176221370697021, 0.07610061019659042, 0.39058324694633484, 0.08314411342144012]
Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing	[0, 51, 21, 52, 3, 188, 54, 41, 70, 107]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.6705144643783569, 0.10389628261327744, 0.13219211995601654, 0.2061644345521927, 0.11764373630285263, 0.08284144103527069, 0.06657176464796066, 0.06389251351356506, 0.09535006433725357, 0.06700068712234497]
Zuidema (2006a) shows that also the estimator (Bod, 2003) uses is biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees	[29, 14, 81, 78, 20, 39, 16, 63, 131, 37]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.4558373987674713, 0.05687737837433815, 0.06287591904401779, 0.050241824239492416, 0.07109677046537399, 0.04396894574165344, 0.05061675235629082, 0.06735246628522873, 0.2102297842502594, 0.12381446361541748]
basic processing units are characters which compose words (Jiangetal., 2008a)	[5, 25, 66, 8, 47, 28, 27, 12, 48, 86]	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.33048999309539795, 0.05222692713141441, 0.060346461832523346, 0.05464226379990578, 0.05809709429740906, 0.05032587796449661, 0.05737275257706642, 0.10767760127782822, 0.04856490716338158, 0.22765308618545532]
n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3	[0, 131, 1, 52, 135, 47, 7, 103, 69, 266]	[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	[0.7054060697555542, 0.3159400224685669, 0.19219422340393066, 0.25939178466796875, 0.15468288958072662, 0.15076474845409393, 0.10120213776826859, 0.259891539812088, 0.11420124024152756, 0.14469295740127563]
plates called lexical-target in the column below are introduced by Jiang et al (2008)	[38, 39, 40, 42, 43, 96, 29, 6, 41, 49]	[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]	[0.6315603852272034, 0.39408519864082336, 0.575620710849762, 0.4505106806755066, 0.3329014480113983, 0.10431769490242004, 0.16376993060112, 0.2580474019050598, 0.10725755989551544, 0.04832567647099495]
